id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/pull/3715:136,Testability,test,test,136,"The `validateWomNamespace` method was using `NoIoFunctionSet` instead of the available ioFunctions, causing the workflow in the centaur test to fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3715
https://github.com/broadinstitute/cromwell/issues/3716:89,Availability,reboot,reboot,89,"Tracking production issue that occurred on May 30th at 6pm. Cromwell had 65K files open, reboot resolved the issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3716
https://github.com/broadinstitute/cromwell/issues/3717:599,Deployability,pipeline,pipeline,599,"When using call-caching using a MySQL database, one of my jobs gets incorrectly rerun. It finishes successfully on the first run, and it's output isn't deleted or changed afterwards.; This happens in the `samtoolsIndex` call in this workflow: https://github.com/biowdl/aligning/tree/BIOWDL-25 which is run as a subworkflow of this one: https://github.com/biowdl/RNA-seq. I noticed that the command which is run on the rerun is different from the one during the initial run.; The initial run produces this command:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/257197fe-a034-4c51-addc-3dc47564b967/call-sample/shard-0/sample/0e4e2e8d-d042-4907-9fb5-1d4f894f8366/call-library/shard-0/library/d995de1f-ecc6-4034-b35b-a860506821ff/call-starAlignment/AlignStar/d339740c-c1be-44ae-b4dd-8db2b47237c7/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717
https://github.com/broadinstitute/cromwell/issues/3717:1206,Deployability,pipeline,pipeline,1206,al run.; The initial run produces this command:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/257197fe-a034-4c51-addc-3dc47564b967/call-sample/shard-0/sample/0e4e2e8d-d042-4907-9fb5-1d4f894f8366/call-library/shard-0/library/d995de1f-ecc6-4034-b35b-a860506821ff/call-starAlignment/AlignStar/d339740c-c1be-44ae-b4dd-8db2b47237c7/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The second argument changes from the intended path to a path inside of the execution folder. It looks like the output from the preceding mapping job gets linked to in the execution fold,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717
https://github.com/broadinstitute/cromwell/issues/3717:1537,Deployability,pipeline,pipeline,1537,al run.; The initial run produces this command:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/257197fe-a034-4c51-addc-3dc47564b967/call-sample/shard-0/sample/0e4e2e8d-d042-4907-9fb5-1d4f894f8366/call-library/shard-0/library/d995de1f-ecc6-4034-b35b-a860506821ff/call-starAlignment/AlignStar/d339740c-c1be-44ae-b4dd-8db2b47237c7/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The second argument changes from the intended path to a path inside of the execution folder. It looks like the output from the preceding mapping job gets linked to in the execution fold,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717
https://github.com/broadinstitute/cromwell/issues/3717:1936,Deployability,pipeline,pipeline,1936,"testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The second argument changes from the intended path to a path inside of the execution folder. It looks like the output from the preceding mapping job gets linked to in the execution folder after restarting the workflow. Which is used as output for that job(?). This output `File` is used to determine what the name for the output of the indexing call is. Which is now different because it now points at the link in the execution folder, rather than the actual output the mapping job produced. As such the expected output doesn't exist and the job gets rerun. ; Am I correct in these statements? If so, is there a way this can be avoided? (ie. Is there a way the original output path can be remembered between restarts?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717
https://github.com/broadinstitute/cromwell/issues/3717:2909,Safety,avoid,avoided,2909,"testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The second argument changes from the intended path to a path inside of the execution folder. It looks like the output from the preceding mapping job gets linked to in the execution folder after restarting the workflow. Which is used as output for that job(?). This output `File` is used to determine what the name for the output of the indexing call is. Which is now different because it now points at the link in the execution folder, rather than the actual output the mapping job produced. As such the expected output doesn't exist and the job gets rerun. ; Am I correct in these statements? If so, is there a way this can be avoided? (ie. Is there a way the original output path can be remembered between restarts?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717
https://github.com/broadinstitute/cromwell/issues/3717:556,Testability,test,testing,556,"When using call-caching using a MySQL database, one of my jobs gets incorrectly rerun. It finishes successfully on the first run, and it's output isn't deleted or changed afterwards.; This happens in the `samtoolsIndex` call in this workflow: https://github.com/biowdl/aligning/tree/BIOWDL-25 which is run as a subworkflow of this one: https://github.com/biowdl/RNA-seq. I noticed that the command which is run on the rerun is different from the one during the initial run.; The initial run produces this command:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/257197fe-a034-4c51-addc-3dc47564b967/call-sample/shard-0/sample/0e4e2e8d-d042-4907-9fb5-1d4f894f8366/call-library/shard-0/library/d995de1f-ecc6-4034-b35b-a860506821ff/call-starAlignment/AlignStar/d339740c-c1be-44ae-b4dd-8db2b47237c7/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717
https://github.com/broadinstitute/cromwell/issues/3717:887,Testability,test,testing,887,"When using call-caching using a MySQL database, one of my jobs gets incorrectly rerun. It finishes successfully on the first run, and it's output isn't deleted or changed afterwards.; This happens in the `samtoolsIndex` call in this workflow: https://github.com/biowdl/aligning/tree/BIOWDL-25 which is run as a subworkflow of this one: https://github.com/biowdl/RNA-seq. I noticed that the command which is run on the rerun is different from the one during the initial run.; The initial run produces this command:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/257197fe-a034-4c51-addc-3dc47564b967/call-sample/shard-0/sample/0e4e2e8d-d042-4907-9fb5-1d4f894f8366/call-library/shard-0/library/d995de1f-ecc6-4034-b35b-a860506821ff/call-starAlignment/AlignStar/d339740c-c1be-44ae-b4dd-8db2b47237c7/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717
https://github.com/broadinstitute/cromwell/issues/3717:999,Testability,test,testing,999,"aching using a MySQL database, one of my jobs gets incorrectly rerun. It finishes successfully on the first run, and it's output isn't deleted or changed afterwards.; This happens in the `samtoolsIndex` call in this workflow: https://github.com/biowdl/aligning/tree/BIOWDL-25 which is run as a subworkflow of this one: https://github.com/biowdl/RNA-seq. I noticed that the command which is run on the rerun is different from the one during the initial run.; The initial run produces this command:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/257197fe-a034-4c51-addc-3dc47564b967/call-sample/shard-0/sample/0e4e2e8d-d042-4907-9fb5-1d4f894f8366/call-library/shard-0/library/d995de1f-ecc6-4034-b35b-a860506821ff/call-starAlignment/AlignStar/d339740c-c1be-44ae-b4dd-8db2b47237c7/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717
https://github.com/broadinstitute/cromwell/issues/3717:1163,Testability,test,testing,1163,al run.; The initial run produces this command:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/257197fe-a034-4c51-addc-3dc47564b967/call-sample/shard-0/sample/0e4e2e8d-d042-4907-9fb5-1d4f894f8366/call-library/shard-0/library/d995de1f-ecc6-4034-b35b-a860506821ff/call-starAlignment/AlignStar/d339740c-c1be-44ae-b4dd-8db2b47237c7/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The second argument changes from the intended path to a path inside of the execution folder. It looks like the output from the preceding mapping job gets linked to in the execution fold,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717
https://github.com/broadinstitute/cromwell/issues/3717:1494,Testability,test,testing,1494,al run.; The initial run produces this command:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/257197fe-a034-4c51-addc-3dc47564b967/call-sample/shard-0/sample/0e4e2e8d-d042-4907-9fb5-1d4f894f8366/call-library/shard-0/library/d995de1f-ecc6-4034-b35b-a860506821ff/call-starAlignment/AlignStar/d339740c-c1be-44ae-b4dd-8db2b47237c7/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The second argument changes from the intended path to a path inside of the execution folder. It looks like the output from the preceding mapping job gets linked to in the execution fold,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717
https://github.com/broadinstitute/cromwell/issues/3717:1893,Testability,test,testing,1893,"testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The second argument changes from the intended path to a path inside of the execution folder. It looks like the output from the preceding mapping job gets linked to in the execution folder after restarting the workflow. Which is used as output for that job(?). This output `File` is used to determine what the name for the output of the indexing call is. Which is now different because it now points at the link in the execution folder, rather than the actual output the mapping job produced. As such the expected output doesn't exist and the job gets rerun. ; Am I correct in these statements? If so, is there a way this can be avoided? (ie. Is there a way the original output path can be remembered between restarts?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717
https://github.com/broadinstitute/cromwell/issues/3719:123,Modifiability,flexible,flexible,123,"Currently, when trying to query for labels, one is required to provide the label key & value. Make the query endpoint more flexible so that it's possible to support querying by key.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3719
https://github.com/broadinstitute/cromwell/pull/3721:76,Performance,perform,performance,76,Setting the tmpdir to be word accessible poses a security risk at some high-performance compute clusters. Original idea seems to make it world writabled: https://github.com/broadinstitute/cromwell/pull/2053. I would even prefer to set it to be *only* accessable for the `cromwell` user (mod: `700`)... or just leave it as it was provided...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721
https://github.com/broadinstitute/cromwell/pull/3721:58,Safety,risk,risk,58,Setting the tmpdir to be word accessible poses a security risk at some high-performance compute clusters. Original idea seems to make it world writabled: https://github.com/broadinstitute/cromwell/pull/2053. I would even prefer to set it to be *only* accessable for the `cromwell` user (mod: `700`)... or just leave it as it was provided...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721
https://github.com/broadinstitute/cromwell/pull/3721:30,Security,access,accessible,30,Setting the tmpdir to be word accessible poses a security risk at some high-performance compute clusters. Original idea seems to make it world writabled: https://github.com/broadinstitute/cromwell/pull/2053. I would even prefer to set it to be *only* accessable for the `cromwell` user (mod: `700`)... or just leave it as it was provided...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721
https://github.com/broadinstitute/cromwell/pull/3721:49,Security,secur,security,49,Setting the tmpdir to be word accessible poses a security risk at some high-performance compute clusters. Original idea seems to make it world writabled: https://github.com/broadinstitute/cromwell/pull/2053. I would even prefer to set it to be *only* accessable for the `cromwell` user (mod: `700`)... or just leave it as it was provided...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721
https://github.com/broadinstitute/cromwell/pull/3721:251,Security,access,accessable,251,Setting the tmpdir to be word accessible poses a security risk at some high-performance compute clusters. Original idea seems to make it world writabled: https://github.com/broadinstitute/cromwell/pull/2053. I would even prefer to set it to be *only* accessable for the `cromwell` user (mod: `700`)... or just leave it as it was provided...,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721
https://github.com/broadinstitute/cromwell/pull/3722:23,Availability,error,error,23,"Cleans up a little the error reporting for PAPI2 failure and add some context.; It's not perfect and can be improved but is better than what's there now and will hopefully make it a bit easier to debug failures going forward.; e.g:. ```; WorkflowManagerActor Workflow 0c939095-9e15-4a20-9d35-9b3a7494304c failed (during ExecutingWorkflowState): java.lang.Exception: Task my_workflow.my_task:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 2: unexpected exit status 1 was not ignored; [Localization] Input name: my_input - Unexpected exit status 1 while running ""gsutil cp gs://my_bucket/input.txt /cromwell_root/my_bucket/input.txt"": CommandException: No URLs matched: gs://my_bucket/input.txt. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stdout"": CommandException: No URLs matched: /cromwell_root/stdout. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stderr"": CommandException: No URLs matched: /cromwell_root/stderr. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/rc gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/rc"": CommandException: No URLs matched: /cromwell_root/rc; ```. I thought about omitting the delocalization failures if there was a localization failure (as the task did not run so obviously there won't be any stdout/stderr/rc), but it seemed a bit too magical. Can always be done later.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3722
https://github.com/broadinstitute/cromwell/pull/3722:49,Availability,failure,failure,49,"Cleans up a little the error reporting for PAPI2 failure and add some context.; It's not perfect and can be improved but is better than what's there now and will hopefully make it a bit easier to debug failures going forward.; e.g:. ```; WorkflowManagerActor Workflow 0c939095-9e15-4a20-9d35-9b3a7494304c failed (during ExecutingWorkflowState): java.lang.Exception: Task my_workflow.my_task:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 2: unexpected exit status 1 was not ignored; [Localization] Input name: my_input - Unexpected exit status 1 while running ""gsutil cp gs://my_bucket/input.txt /cromwell_root/my_bucket/input.txt"": CommandException: No URLs matched: gs://my_bucket/input.txt. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stdout"": CommandException: No URLs matched: /cromwell_root/stdout. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stderr"": CommandException: No URLs matched: /cromwell_root/stderr. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/rc gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/rc"": CommandException: No URLs matched: /cromwell_root/rc; ```. I thought about omitting the delocalization failures if there was a localization failure (as the task did not run so obviously there won't be any stdout/stderr/rc), but it seemed a bit too magical. Can always be done later.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3722
https://github.com/broadinstitute/cromwell/pull/3722:202,Availability,failure,failures,202,"Cleans up a little the error reporting for PAPI2 failure and add some context.; It's not perfect and can be improved but is better than what's there now and will hopefully make it a bit easier to debug failures going forward.; e.g:. ```; WorkflowManagerActor Workflow 0c939095-9e15-4a20-9d35-9b3a7494304c failed (during ExecutingWorkflowState): java.lang.Exception: Task my_workflow.my_task:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 2: unexpected exit status 1 was not ignored; [Localization] Input name: my_input - Unexpected exit status 1 while running ""gsutil cp gs://my_bucket/input.txt /cromwell_root/my_bucket/input.txt"": CommandException: No URLs matched: gs://my_bucket/input.txt. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stdout"": CommandException: No URLs matched: /cromwell_root/stdout. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stderr"": CommandException: No URLs matched: /cromwell_root/stderr. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/rc gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/rc"": CommandException: No URLs matched: /cromwell_root/rc; ```. I thought about omitting the delocalization failures if there was a localization failure (as the task did not run so obviously there won't be any stdout/stderr/rc), but it seemed a bit too magical. Can always be done later.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3722
https://github.com/broadinstitute/cromwell/pull/3722:458,Availability,error,error,458,"Cleans up a little the error reporting for PAPI2 failure and add some context.; It's not perfect and can be improved but is better than what's there now and will hopefully make it a bit easier to debug failures going forward.; e.g:. ```; WorkflowManagerActor Workflow 0c939095-9e15-4a20-9d35-9b3a7494304c failed (during ExecutingWorkflowState): java.lang.Exception: Task my_workflow.my_task:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 2: unexpected exit status 1 was not ignored; [Localization] Input name: my_input - Unexpected exit status 1 while running ""gsutil cp gs://my_bucket/input.txt /cromwell_root/my_bucket/input.txt"": CommandException: No URLs matched: gs://my_bucket/input.txt. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stdout"": CommandException: No URLs matched: /cromwell_root/stdout. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stderr"": CommandException: No URLs matched: /cromwell_root/stderr. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/rc gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/rc"": CommandException: No URLs matched: /cromwell_root/rc; ```. I thought about omitting the delocalization failures if there was a localization failure (as the task did not run so obviously there won't be any stdout/stderr/rc), but it seemed a bit too magical. Can always be done later.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3722
https://github.com/broadinstitute/cromwell/pull/3722:1522,Availability,failure,failures,1522,"Cleans up a little the error reporting for PAPI2 failure and add some context.; It's not perfect and can be improved but is better than what's there now and will hopefully make it a bit easier to debug failures going forward.; e.g:. ```; WorkflowManagerActor Workflow 0c939095-9e15-4a20-9d35-9b3a7494304c failed (during ExecutingWorkflowState): java.lang.Exception: Task my_workflow.my_task:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 2: unexpected exit status 1 was not ignored; [Localization] Input name: my_input - Unexpected exit status 1 while running ""gsutil cp gs://my_bucket/input.txt /cromwell_root/my_bucket/input.txt"": CommandException: No URLs matched: gs://my_bucket/input.txt. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stdout"": CommandException: No URLs matched: /cromwell_root/stdout. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stderr"": CommandException: No URLs matched: /cromwell_root/stderr. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/rc gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/rc"": CommandException: No URLs matched: /cromwell_root/rc; ```. I thought about omitting the delocalization failures if there was a localization failure (as the task did not run so obviously there won't be any stdout/stderr/rc), but it seemed a bit too magical. Can always be done later.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3722
https://github.com/broadinstitute/cromwell/pull/3722:1559,Availability,failure,failure,1559,"Cleans up a little the error reporting for PAPI2 failure and add some context.; It's not perfect and can be improved but is better than what's there now and will hopefully make it a bit easier to debug failures going forward.; e.g:. ```; WorkflowManagerActor Workflow 0c939095-9e15-4a20-9d35-9b3a7494304c failed (during ExecutingWorkflowState): java.lang.Exception: Task my_workflow.my_task:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 2: unexpected exit status 1 was not ignored; [Localization] Input name: my_input - Unexpected exit status 1 while running ""gsutil cp gs://my_bucket/input.txt /cromwell_root/my_bucket/input.txt"": CommandException: No URLs matched: gs://my_bucket/input.txt. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stdout"": CommandException: No URLs matched: /cromwell_root/stdout. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/stderr"": CommandException: No URLs matched: /cromwell_root/stderr. [DeLocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/rc gs://my_bucket/my_workflow/0c939095-9e15-4a20-9d35-9b3a7494304c/call-my_task/rc"": CommandException: No URLs matched: /cromwell_root/rc; ```. I thought about omitting the delocalization failures if there was a localization failure (as the task did not run so obviously there won't be any stdout/stderr/rc), but it seemed a bit too magical. Can always be done later.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3722
https://github.com/broadinstitute/cromwell/issues/3723:14,Testability,test,test,14,Run a nightly test of running BCBIO CWL workflows against the PAPI backend.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3723
https://github.com/broadinstitute/cromwell/pull/3725:442,Availability,echo,echo,442,"I was just going to ignore this, but then @abaumann mentioned he was confused by the same thing... so why not. Used to be confusing because it could be referring to a value of type Boolean, rather than literally ""Boolean"":; ```; Expected equal, got ""Boolean"". Boolean conditional = incremented != 0; ^; ```; Used to be confusing because integers could suggest you're using the wrong number of identifiers:; ```; Expected identifier, got ""1"". echo ""~{one.1}"" > ~{one.1}.txt; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3725
https://github.com/broadinstitute/cromwell/pull/3729:18,Deployability,hotfix,hotfix,18,"copying over from hotfix branch,. fix for looking for ""config"" in the papi provider config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3729
https://github.com/broadinstitute/cromwell/pull/3729:55,Modifiability,config,config,55,"copying over from hotfix branch,. fix for looking for ""config"" in the papi provider config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3729
https://github.com/broadinstitute/cromwell/pull/3729:84,Modifiability,config,config,84,"copying over from hotfix branch,. fix for looking for ""config"" in the papi provider config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3729
https://github.com/broadinstitute/cromwell/issues/3731:5,Testability,test,tests,5,Cron tests are currently failing for PapiV1 and PapiV2. Example [build](https://travis-ci.org/broadinstitute/cromwell/builds/388137113) and WIP [PR](https://github.com/broadinstitute/cromwell/pull/3700).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3731
https://github.com/broadinstitute/cromwell/issues/3732:254,Availability,ERROR,ERROR,254,"See [this travis build](https://travis-ci.org/broadinstitute/cromwell/builds/388562204) for an example where Papi V1 was retrying preemption and Papi V2 was failing. ```; 9605 2018-06-06 11:02:59,838 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow c9dfd3ed-8be8-413f-af46-4692142b3248 failed (during ExecutingWorkflowState): Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:444,Availability,error,error,444,"See [this travis build](https://travis-ci.org/broadinstitute/cromwell/builds/388562204) for an example where Papi V1 was retrying preemption and Papi V2 was failing. ```; 9605 2018-06-06 11:02:59,838 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow c9dfd3ed-8be8-413f-af46-4692142b3248 failed (during ExecutingWorkflowState): Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:469,Availability,error,error,469,"See [this travis build](https://travis-ci.org/broadinstitute/cromwell/builds/388562204) for an example where Papi V1 was retrying preemption and Papi V2 was failing. ```; 9605 2018-06-06 11:02:59,838 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow c9dfd3ed-8be8-413f-af46-4692142b3248 failed (during ExecutingWorkflowState): Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:973,Availability,error,error,973,"See [this travis build](https://travis-ci.org/broadinstitute/cromwell/builds/388562204) for an example where Papi V1 was retrying preemption and Papi V2 was failing. ```; 9605 2018-06-06 11:02:59,838 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow c9dfd3ed-8be8-413f-af46-4692142b3248 failed (during ExecutingWorkflowState): Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:998,Availability,error,error,998,"ee [this travis build](https://travis-ci.org/broadinstitute/cromwell/builds/388562204) for an example where Papi V1 was retrying preemption and Papi V2 was failing. ```; 9605 2018-06-06 11:02:59,838 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow c9dfd3ed-8be8-413f-af46-4692142b3248 failed (during ExecutingWorkflowState): Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1433,Deployability,pipeline,pipelines,1433,"ut an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1450,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1450,"ror code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1511,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1511,"expected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 962",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1596,Deployability,pipeline,pipelines,1596,"pected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1613,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1613,"red; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.run",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1679,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1679,"/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1765,Deployability,pipeline,pipelines,1765,"travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1782,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1782,"be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.Batc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1847,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1847,"ard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1933,Deployability,pipeline,pipelines,1933,"16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1950,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1950,"n error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecuto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:2015,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2015,"cution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorCo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:483,Integrability,Message,Message,483,"See [this travis build](https://travis-ci.org/broadinstitute/cromwell/builds/388562204) for an example where Papi V1 was retrying preemption and Papi V2 was failing. ```; 9605 2018-06-06 11:02:59,838 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow c9dfd3ed-8be8-413f-af46-4692142b3248 failed (during ExecutingWorkflowState): Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:1012,Integrability,Message,Message,1012,"m-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow c9dfd3ed-8be8-413f-af46-4692142b3248 failed (during ExecutingWorkflowState): Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at sca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:2218,Performance,concurren,concurrent,2218," gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.di",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:2288,Performance,concurren,concurrent,2288,"ution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:2370,Performance,concurren,concurrent,2370,"bration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:2711,Performance,concurren,concurrent,2711,"ctor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:3930,Performance,concurren,concurrent,3930,"BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; 9634 at centaur.test.Operations$$anon$9.doPerform(Test.scala:185); 9635 at centaur.test.Operations$$anon$9.$anonfun$run$7(Test.scala:200); 9636 at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 9637 at scala.util.Success.$anonfun$map$1(Try.scala:251); 9638 at scala.util.Success.map(Try.scala:209); 9639 at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 9640 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 9641 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 9642 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9643 at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402); 9644 ...; ```. NOTE: Preemption has since been disabled in Travis due to CRON builds being terminated after three hours.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:4104,Performance,concurren,concurrent,4104,"BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; 9634 at centaur.test.Operations$$anon$9.doPerform(Test.scala:185); 9635 at centaur.test.Operations$$anon$9.$anonfun$run$7(Test.scala:200); 9636 at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 9637 at scala.util.Success.$anonfun$map$1(Try.scala:251); 9638 at scala.util.Success.map(Try.scala:209); 9639 at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 9640 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 9641 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 9642 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9643 at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402); 9644 ...; ```. NOTE: Preemption has since been disabled in Travis due to CRON builds being terminated after three hours.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:4170,Performance,concurren,concurrent,4170,"BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; 9634 at centaur.test.Operations$$anon$9.doPerform(Test.scala:185); 9635 at centaur.test.Operations$$anon$9.$anonfun$run$7(Test.scala:200); 9636 at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 9637 at scala.util.Success.$anonfun$map$1(Try.scala:251); 9638 at scala.util.Success.map(Try.scala:209); 9639 at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 9640 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 9641 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 9642 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9643 at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402); 9644 ...; ```. NOTE: Preemption has since been disabled in Travis due to CRON builds being terminated after three hours.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:4241,Performance,concurren,concurrent,4241,"BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; 9634 at centaur.test.Operations$$anon$9.doPerform(Test.scala:185); 9635 at centaur.test.Operations$$anon$9.$anonfun$run$7(Test.scala:200); 9636 at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 9637 at scala.util.Success.$anonfun$map$1(Try.scala:251); 9638 at scala.util.Success.map(Try.scala:209); 9639 at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 9640 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 9641 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 9642 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9643 at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402); 9644 ...; ```. NOTE: Preemption has since been disabled in Travis due to CRON builds being terminated after three hours.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:4319,Performance,concurren,concurrent,4319,"BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; 9634 at centaur.test.Operations$$anon$9.doPerform(Test.scala:185); 9635 at centaur.test.Operations$$anon$9.$anonfun$run$7(Test.scala:200); 9636 at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 9637 at scala.util.Success.$anonfun$map$1(Try.scala:251); 9638 at scala.util.Success.map(Try.scala:209); 9639 at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 9640 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 9641 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 9642 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9643 at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402); 9644 ...; ```. NOTE: Preemption has since been disabled in Travis due to CRON builds being terminated after three hours.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:4393,Performance,concurren,concurrent,4393,"BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; 9634 at centaur.test.Operations$$anon$9.doPerform(Test.scala:185); 9635 at centaur.test.Operations$$anon$9.$anonfun$run$7(Test.scala:200); 9636 at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 9637 at scala.util.Success.$anonfun$map$1(Try.scala:251); 9638 at scala.util.Success.map(Try.scala:209); 9639 at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 9640 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 9641 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 9642 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9643 at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402); 9644 ...; ```. NOTE: Preemption has since been disabled in Travis due to CRON builds being terminated after three hours.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:3793,Testability,test,test,3793,"BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; 9634 at centaur.test.Operations$$anon$9.doPerform(Test.scala:185); 9635 at centaur.test.Operations$$anon$9.$anonfun$run$7(Test.scala:200); 9636 at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 9637 at scala.util.Success.$anonfun$map$1(Try.scala:251); 9638 at scala.util.Success.map(Try.scala:209); 9639 at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 9640 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 9641 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 9642 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9643 at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402); 9644 ...; ```. NOTE: Preemption has since been disabled in Travis due to CRON builds being terminated after three hours.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:3827,Testability,Test,Test,3827,"BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; 9634 at centaur.test.Operations$$anon$9.doPerform(Test.scala:185); 9635 at centaur.test.Operations$$anon$9.$anonfun$run$7(Test.scala:200); 9636 at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 9637 at scala.util.Success.$anonfun$map$1(Try.scala:251); 9638 at scala.util.Success.map(Try.scala:209); 9639 at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 9640 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 9641 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 9642 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9643 at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402); 9644 ...; ```. NOTE: Preemption has since been disabled in Travis due to CRON builds being terminated after three hours.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:3860,Testability,test,test,3860,"BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; 9634 at centaur.test.Operations$$anon$9.doPerform(Test.scala:185); 9635 at centaur.test.Operations$$anon$9.$anonfun$run$7(Test.scala:200); 9636 at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 9637 at scala.util.Success.$anonfun$map$1(Try.scala:251); 9638 at scala.util.Success.map(Try.scala:209); 9639 at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 9640 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 9641 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 9642 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9643 at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402); 9644 ...; ```. NOTE: Preemption has since been disabled in Travis due to CRON builds being terminated after three hours.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3732:3899,Testability,Test,Test,3899,"BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 9626 at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 9627 at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 9628 at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 9629 at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 9630 ; 9631 2018-06-06 11:02:59,840 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor WorkflowActor-c9dfd3ed-8be8-413f-af46-4692142b3248 is in a terminal state: WorkflowFailedState; 9632 - should successfully run jointdiscovery *** FAILED *** (2 hours, 13 minutes, 31 seconds); 9633 java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; 9634 at centaur.test.Operations$$anon$9.doPerform(Test.scala:185); 9635 at centaur.test.Operations$$anon$9.$anonfun$run$7(Test.scala:200); 9636 at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 9637 at scala.util.Success.$anonfun$map$1(Try.scala:251); 9638 at scala.util.Success.map(Try.scala:209); 9639 at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 9640 at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 9641 at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 9642 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9643 at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402); 9644 ...; ```. NOTE: Preemption has since been disabled in Travis due to CRON builds being terminated after three hours.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732
https://github.com/broadinstitute/cromwell/issues/3734:11,Testability,test,testing,11,Regression testing for CWL on Local backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3734
https://github.com/broadinstitute/cromwell/issues/3736:14,Availability,error,error,14,"I assume this error has to do with my config, but not particularly clear what is going on... Here is the workflow, inputs, and config; I am running from swagger:; [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2077985/myWorkflow_awsbatch.wdl.txt). [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2077989/aws.conf.txt). [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2078033/hello.inputs.txt). ```; 2018-06-06 16:18:30,442 cromwell-system-akka.dispatchers.api-dispatcher-215 INFO - WDL (Unspecified version) workflow 948bf608-f91b-46a7-b892-86454be067fd submitted; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - 1 new workflows fetched; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:1475,Availability,ERROR,ERROR,1475,"tcher-215 INFO - WDL (Unspecified version) workflow 948bf608-f91b-46a7-b892-86454be067fd submitted; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - 1 new workflows fetched; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$ada",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:1740,Availability,Error,Error,1740,"her-32 INFO - 1 new workflows fetched; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2181,Availability,failure,failure,2181,"rkflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2234,Availability,failure,failure,2234,":47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2308,Availability,failure,failure,2308,"ializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(Batching",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2531,Energy Efficiency,adapt,adapted,2531,91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:38,Modifiability,config,config,38,"I assume this error has to do with my config, but not particularly clear what is going on... Here is the workflow, inputs, and config; I am running from swagger:; [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2077985/myWorkflow_awsbatch.wdl.txt). [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2077989/aws.conf.txt). [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2078033/hello.inputs.txt). ```; 2018-06-06 16:18:30,442 cromwell-system-akka.dispatchers.api-dispatcher-215 INFO - WDL (Unspecified version) workflow 948bf608-f91b-46a7-b892-86454be067fd submitted; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - 1 new workflows fetched; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:127,Modifiability,config,config,127,"I assume this error has to do with my config, but not particularly clear what is going on... Here is the workflow, inputs, and config; I am running from swagger:; [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2077985/myWorkflow_awsbatch.wdl.txt). [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2077989/aws.conf.txt). [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2078033/hello.inputs.txt). ```; 2018-06-06 16:18:30,442 cromwell-system-akka.dispatchers.api-dispatcher-215 INFO - WDL (Unspecified version) workflow 948bf608-f91b-46a7-b892-86454be067fd submitted; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - 1 new workflows fetched; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2531,Modifiability,adapt,adapted,2531,91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:1753,Performance,concurren,concurrent,1753,"8:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.eff",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:1812,Performance,concurren,concurrent,1812,"-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workfl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:1843,Performance,concurren,concurrent,1843,"flow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWork",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:1904,Performance,concurren,concurrent,1904,"; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:1981,Performance,concurren,concurrent,1981,"32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2034,Performance,concurren,concurrent,2034,"orkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurren",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2088,Performance,concurren,concurrent,2088,"8-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2162,Performance,concurren,concurrent,2162," INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.di",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2215,Performance,concurren,concurrent,2215,"ctor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(Ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2269,Performance,concurren,concurrent,2269,"patchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2969,Performance,concurren,concurrent,2969, scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinW,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:3031,Performance,concurren,concurrent,3031,ncurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.e,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:3105,Performance,concurren,concurrent,3105,e$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:3414,Performance,concurren,concurrent,3414,er.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorAc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2360,Safety,unsafe,unsafeToFuture,2360,"arsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2458,Safety,unsafe,unsafeToFuture,2458,spatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockCon,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2514,Safety,unsafe,unsafeToFuture,2514,91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2721,Safety,unsafe,unsafeRunAsync,2721,d:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:2766,Safety,unsafe,unsafeToFuture,2766,e$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:4877,Testability,Log,LoggingFSM,4877,ool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:173); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:4970,Testability,Log,LoggingFSM,4970,orkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:173); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:5025,Testability,Log,LoggingFSM,5025,mwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:173); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3736:67,Usability,clear,clear,67,"I assume this error has to do with my config, but not particularly clear what is going on... Here is the workflow, inputs, and config; I am running from swagger:; [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2077985/myWorkflow_awsbatch.wdl.txt). [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2077989/aws.conf.txt). [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2078033/hello.inputs.txt). ```; 2018-06-06 16:18:30,442 cromwell-system-akka.dispatchers.api-dispatcher-215 INFO - WDL (Unspecified version) workflow 948bf608-f91b-46a7-b892-86454be067fd submitted; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - 1 new workflows fetched; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736
https://github.com/broadinstitute/cromwell/issues/3737:93,Availability,error,errors,93,"Running Centaur has several ""noop"" tasks. When passed to AWS Batch, we see failing tests and errors from the AWS SDK that the Command field cannot have empty strings. Assuming Cromwell expects no output/return code 0, this can be special-cased in AWSBatchJob.scala.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3737
https://github.com/broadinstitute/cromwell/issues/3737:83,Testability,test,tests,83,"Running Centaur has several ""noop"" tasks. When passed to AWS Batch, we see failing tests and errors from the AWS SDK that the Command field cannot have empty strings. Assuming Cromwell expects no output/return code 0, this can be special-cased in AWSBatchJob.scala.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3737
https://github.com/broadinstitute/cromwell/issues/3740:420,Availability,error,error,420,"Hi, ; I understand cromwell that call caching will copy whatever it finished last time to current run if I keep the script unchanged. However, it seems to be not that case to me. I have run Mutect2, one of the GATK tool on fireclouds. one of the major step is it splits to 50 jobs based on some genomic interval and runs separately on different VM. Most jobs are successful done but a few fail because of some transient error. Once I relanuch the job, it splits to 50 jobs based on the same interval, but it is still running all 50 jobs simultaneously without knowing some of the jobs have run successfully last time (of course, I enabled call caching). So I ends up spending more and more money and time on it. Could you please advise on this ?. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3740
https://github.com/broadinstitute/cromwell/issues/3742:28,Availability,error,errors,28,"An example from a couple of errors that failed two PapiV2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$trans",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:389,Availability,ERROR,ERROR,389,"An example from a couple of errors that failed two PapiV2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$trans",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:606,Availability,error,error,606,"An example from a couple of errors that failed two PapiV2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$trans",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:631,Availability,error,error,631,"An example from a couple of errors that failed two PapiV2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$trans",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1036,Availability,Failure,Failure,1036,"2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:167,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,167,"An example from a couple of errors that failed two PapiV2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$trans",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1138,Deployability,pipeline,pipelines,1138,"d-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1155,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1155,"ApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.sca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1216,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1216,"edEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$Blockable",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1296,Deployability,pipeline,pipelines,1296," Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.J",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1313,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1313," cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1379,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1379," ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.wi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1460,Deployability,pipeline,pipelines,1460,"ailed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$Bloc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1477,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1477,"ate): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1542,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1542,"leWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(Abstract",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1623,Deployability,pipeline,pipelines,1623,"PI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaFork",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1640,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1640,": action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorCon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1705,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1705,"; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1898,Performance,concurren,concurrent,1898,"5-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:197",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1963,Performance,concurren,concurrent,1963,"our ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:2040,Performance,concurren,concurrent,2040,"Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2018-06-07 08:24:07,064 cromwell-system-akka.dispatcher",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:2361,Performance,concurren,concurrent,2361,"dRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-656ddc45-2d1d-4e24-a086-c47fa847c658 is in a terminal state: WorkflowFailedState; ```. [centaur_log.txt](https://github.com/broadinstitute/cromwell/files/2081024/centaur_log.txt). Best viewed with `less -R centaur_log.txt`. Note: Build exceeded three hours",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:1029,Testability,log,login,1029,"2 CRON workflows:. ```; 2018-06-07 08:24:03,050 cromwell-system-akka.dispatchers.backend-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:3390,Testability,log,logs,3390,"xecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-656ddc45-2d1d-4e24-a086-c47fa847c658 is in a terminal state: WorkflowFailedState; ```. [centaur_log.txt](https://github.com/broadinstitute/cromwell/files/2081024/centaur_log.txt). Best viewed with `less -R centaur_log.txt`. Note: Build exceeded three hours and was killed so the logs are truncated. The build was also manually restarted so these logs will not match what is currently in Travis.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3742:3457,Testability,log,logs,3457,"xecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-656ddc45-2d1d-4e24-a086-c47fa847c658 is in a terminal state: WorkflowFailedState; ```. [centaur_log.txt](https://github.com/broadinstitute/cromwell/files/2081024/centaur_log.txt). Best viewed with `less -R centaur_log.txt`. Note: Build exceeded three hours and was killed so the logs are truncated. The build was also manually restarted so these logs will not match what is currently in Travis.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742
https://github.com/broadinstitute/cromwell/issues/3743:17,Availability,error,error,17,"Perhaps a config error... I am using the TES backend, which is configured to utilize my funnel server, that in turn is passing tasks to AWS Batch (nice, right?). This seems to work OK with the simplest workflow possible, but now that I have added some inputs, I am getting an error. Here is my setup and error trace from the server, and I am running on the latest from the develop branch:. [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2081428/hello.inputs.txt); [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081429/my-cromwell.conf.txt); [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081843/myWorkflow_awsbatch.wdl.txt). ```; 2018-06-07 13:09:05,646 cromwell-system-akka.dispatchers.api-dispatcher-119 INFO - Unspecified type (Unspecified version) workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 submitted; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(af282f7a-1e95-4390-8cf7-c3bbd93b10b2); 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2; 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 13:09:15,815 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Parsing workflow as WDL draft-2; 2018-06-07 13:09:15,826 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Call-to-Backend assignments: wf_hello.hello -> TES; 2018-06-07 13:09:16,844 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2 [UUID(af282f7a)]:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:276,Availability,error,error,276,"Perhaps a config error... I am using the TES backend, which is configured to utilize my funnel server, that in turn is passing tasks to AWS Batch (nice, right?). This seems to work OK with the simplest workflow possible, but now that I have added some inputs, I am getting an error. Here is my setup and error trace from the server, and I am running on the latest from the develop branch:. [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2081428/hello.inputs.txt); [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081429/my-cromwell.conf.txt); [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081843/myWorkflow_awsbatch.wdl.txt). ```; 2018-06-07 13:09:05,646 cromwell-system-akka.dispatchers.api-dispatcher-119 INFO - Unspecified type (Unspecified version) workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 submitted; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(af282f7a-1e95-4390-8cf7-c3bbd93b10b2); 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2; 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 13:09:15,815 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Parsing workflow as WDL draft-2; 2018-06-07 13:09:15,826 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Call-to-Backend assignments: wf_hello.hello -> TES; 2018-06-07 13:09:16,844 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2 [UUID(af282f7a)]:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:304,Availability,error,error,304,"Perhaps a config error... I am using the TES backend, which is configured to utilize my funnel server, that in turn is passing tasks to AWS Batch (nice, right?). This seems to work OK with the simplest workflow possible, but now that I have added some inputs, I am getting an error. Here is my setup and error trace from the server, and I am running on the latest from the develop branch:. [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2081428/hello.inputs.txt); [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081429/my-cromwell.conf.txt); [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081843/myWorkflow_awsbatch.wdl.txt). ```; 2018-06-07 13:09:05,646 cromwell-system-akka.dispatchers.api-dispatcher-119 INFO - Unspecified type (Unspecified version) workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 submitted; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(af282f7a-1e95-4390-8cf7-c3bbd93b10b2); 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2; 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 13:09:15,815 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Parsing workflow as WDL draft-2; 2018-06-07 13:09:15,826 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Call-to-Backend assignments: wf_hello.hello -> TES; 2018-06-07 13:09:16,844 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2 [UUID(af282f7a)]:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:2185,Availability,echo,echo,2185,"system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2; 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 13:09:15,815 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Parsing workflow as WDL draft-2; 2018-06-07 13:09:15,826 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Call-to-Backend assignments: wf_hello.hello -> TES; 2018-06-07 13:09:16,844 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2 [UUID(af282f7a)]: Starting wf_hello.hello; 2018-06-07 13:09:17,680 cromwell-system-akka.dispatchers.backend-dispatcher-182 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: `echo ""Hello World! Welcome to Cromwell . . . on AWS!""`; 2018-06-07 13:09:17,684 cromwell-system-akka.dispatchers.backend-dispatcher-182 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Calculated TES outputs (found 4):; Output(Some(rc),Some(wf_hello.hello.rc),Some(/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc),/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc,Some(FILE)); Output(Some(stdout),Some(wf_hello.hello.stdout),Some(/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stdout),/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stdout,Some(FILE)); Output(Some(stderr),Some(wf_hello.hello.stderr),Some(/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stderr),/cromwell-executions/wf_h",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:4200,Availability,echo,echo,4200,"ecution/stderr,Some(FILE)); Output(Some(commandScript),Some(wf_hello.hello.commandScript),Some(/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/script),/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/script,Some(FILE)). 2018-06-07 13:09:17,684 cromwell-system-akka.dispatchers.backend-dispatcher-182 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Calculated TES inputs (found 1):; Input(Some(commandScript),Some(wf_hello.hello.commandScript),None,/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/script,Some(FILE),Some(#!/bin/bash. cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; tmpDir=`mkdir -p ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32"" && echo ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. echo ""Hello World! Welcome to Cromwell . . . on AWS!""; ) > '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stdout' 2> '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stderr'; echo $? > /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; find . -type d -empty -print | xargs -I % touch %/.file; ); (; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:4640,Availability,echo,echo,4640,"182 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Calculated TES inputs (found 1):; Input(Some(commandScript),Some(wf_hello.hello.commandScript),None,/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/script,Some(FILE),Some(#!/bin/bash. cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; tmpDir=`mkdir -p ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32"" && echo ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. echo ""Hello World! Welcome to Cromwell . . . on AWS!""; ) > '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stdout' 2> '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stderr'; echo $? > /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; sync. ); mv /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; )). 2018-06-07 13:09:22,723 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecuti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:4897,Availability,echo,echo,4897,"cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/script,Some(FILE),Some(#!/bin/bash. cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; tmpDir=`mkdir -p ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32"" && echo ""/Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/tmp.69437f32""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution. echo ""Hello World! Welcome to Cromwell . . . on AWS!""; ) > '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stdout' 2> '/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/stderr'; echo $? > /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution; sync. ); mv /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc.tmp /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; )). 2018-06-07 13:09:22,723 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: job id: bccmdfdd6o377kru9q6g; 2018-06-07 13:09:22,744 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:6395,Availability,ERROR,ERROR,6395,"c.tmp /cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; )). 2018-06-07 13:09:22,723 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: job id: bccmdfdd6o377kru9q6g; 2018-06-07 13:09:22,744 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from - to Running; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Job bccmdfdd6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlock",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:6967,Availability,recover,recoverWith,6967,"patchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Job bccmdfdd6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:10,Modifiability,config,config,10,"Perhaps a config error... I am using the TES backend, which is configured to utilize my funnel server, that in turn is passing tasks to AWS Batch (nice, right?). This seems to work OK with the simplest workflow possible, but now that I have added some inputs, I am getting an error. Here is my setup and error trace from the server, and I am running on the latest from the develop branch:. [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2081428/hello.inputs.txt); [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081429/my-cromwell.conf.txt); [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081843/myWorkflow_awsbatch.wdl.txt). ```; 2018-06-07 13:09:05,646 cromwell-system-akka.dispatchers.api-dispatcher-119 INFO - Unspecified type (Unspecified version) workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 submitted; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(af282f7a-1e95-4390-8cf7-c3bbd93b10b2); 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2; 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 13:09:15,815 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Parsing workflow as WDL draft-2; 2018-06-07 13:09:15,826 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Call-to-Backend assignments: wf_hello.hello -> TES; 2018-06-07 13:09:16,844 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2 [UUID(af282f7a)]:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:63,Modifiability,config,configured,63,"Perhaps a config error... I am using the TES backend, which is configured to utilize my funnel server, that in turn is passing tasks to AWS Batch (nice, right?). This seems to work OK with the simplest workflow possible, but now that I have added some inputs, I am getting an error. Here is my setup and error trace from the server, and I am running on the latest from the develop branch:. [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2081428/hello.inputs.txt); [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081429/my-cromwell.conf.txt); [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081843/myWorkflow_awsbatch.wdl.txt). ```; 2018-06-07 13:09:05,646 cromwell-system-akka.dispatchers.api-dispatcher-119 INFO - Unspecified type (Unspecified version) workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 submitted; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(af282f7a-1e95-4390-8cf7-c3bbd93b10b2); 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2; 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 13:09:15,815 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Parsing workflow as WDL draft-2; 2018-06-07 13:09:15,826 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Call-to-Backend assignments: wf_hello.hello -> TES; 2018-06-07 13:09:16,844 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2 [UUID(af282f7a)]:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:6940,Performance,concurren,concurrent,6940,"883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Job bccmdfdd6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:7010,Performance,concurren,concurrent,7010,"AsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Job bccmdfdd6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:7088,Performance,concurren,concurrent,7088,"d6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:7413,Performance,concurren,concurrent,7413,Actor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:8721,Performance,load,loadBytes,8721,ala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.Files.readAllBytes(Files.java:3152); 	at better.files.File.loadBytes(File.scala:171); 	at better.files.File.byteArray(File.scala:174); 	at better.files.File.contentAsString(File.scala:214); 	at cromwell.core.path.BetterFileMethods.contentAsString(BetterFileMethods.scala:112); 	at cromwell.core.path.BetterFileMethods.contentAsString$(BetterFileMethods.scala:112); 	at cromwell.core.path.DefaultPath.contentAsString(DefaultPathBuilder.scala:55); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString$(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.DefaultPath.readContentAsString(DefaultPathBuilder.scala:55); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$4(NioFlow.scala:91); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.lif,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:9478,Performance,concurren,concurrent,9478,rkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.Files.readAllBytes(Files.java:3152); 	at better.files.File.loadBytes(File.scala:171); 	at better.files.File.byteArray(File.scala:174); 	at better.files.File.contentAsString(File.scala:214); 	at cromwell.core.path.BetterFileMethods.contentAsString(BetterFileMethods.scala:112); 	at cromwell.core.path.BetterFileMethods.contentAsString$(BetterFileMethods.scala:112); 	at cromwell.core.path.DefaultPath.contentAsString(DefaultPathBuilder.scala:55); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString$(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.DefaultPath.readContentAsString(DefaultPathBuilder.scala:55); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$4(NioFlow.scala:91); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:9640,Performance,concurren,concurrent,9640,rkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.Files.readAllBytes(Files.java:3152); 	at better.files.File.loadBytes(File.scala:171); 	at better.files.File.byteArray(File.scala:174); 	at better.files.File.contentAsString(File.scala:214); 	at cromwell.core.path.BetterFileMethods.contentAsString(BetterFileMethods.scala:112); 	at cromwell.core.path.BetterFileMethods.contentAsString$(BetterFileMethods.scala:112); 	at cromwell.core.path.DefaultPath.contentAsString(DefaultPathBuilder.scala:55); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString$(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.DefaultPath.readContentAsString(DefaultPathBuilder.scala:55); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$4(NioFlow.scala:91); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:9702,Performance,concurren,concurrent,9702,rkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.Files.readAllBytes(Files.java:3152); 	at better.files.File.loadBytes(File.scala:171); 	at better.files.File.byteArray(File.scala:174); 	at better.files.File.contentAsString(File.scala:214); 	at cromwell.core.path.BetterFileMethods.contentAsString(BetterFileMethods.scala:112); 	at cromwell.core.path.BetterFileMethods.contentAsString$(BetterFileMethods.scala:112); 	at cromwell.core.path.DefaultPath.contentAsString(DefaultPathBuilder.scala:55); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString$(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.DefaultPath.readContentAsString(DefaultPathBuilder.scala:55); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$4(NioFlow.scala:91); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:9769,Performance,concurren,concurrent,9769,rkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.Files.readAllBytes(Files.java:3152); 	at better.files.File.loadBytes(File.scala:171); 	at better.files.File.byteArray(File.scala:174); 	at better.files.File.contentAsString(File.scala:214); 	at cromwell.core.path.BetterFileMethods.contentAsString(BetterFileMethods.scala:112); 	at cromwell.core.path.BetterFileMethods.contentAsString$(BetterFileMethods.scala:112); 	at cromwell.core.path.DefaultPath.contentAsString(DefaultPathBuilder.scala:55); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString$(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.DefaultPath.readContentAsString(DefaultPathBuilder.scala:55); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$4(NioFlow.scala:91); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:6967,Safety,recover,recoverWith,6967,"patchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Job bccmdfdd6o377kru9q6g is complete; 2018-06-07 13:13:04,883 cromwell-system-akka.dispatchers.backend-dispatcher-183 INFO - TesAsyncBackendJobExecutionActor [UUID(af282f7a)wf_hello.hello:NA:1]: Status change from Running to Complete; 2018-06-07 13:13:06,346 cromwell-system-akka.dispatchers.engine-dispatcher-59 ERROR - WorkflowManagerActor Workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: /Users/tdyar/workspace/cromwell/cromwell-executions/wf_hello/af282f7a-1e95-4390-8cf7-c3bbd93b10b2/call-hello/execution/rc; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3743:193,Usability,simpl,simplest,193,"Perhaps a config error... I am using the TES backend, which is configured to utilize my funnel server, that in turn is passing tasks to AWS Batch (nice, right?). This seems to work OK with the simplest workflow possible, but now that I have added some inputs, I am getting an error. Here is my setup and error trace from the server, and I am running on the latest from the develop branch:. [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2081428/hello.inputs.txt); [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081429/my-cromwell.conf.txt); [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081843/myWorkflow_awsbatch.wdl.txt). ```; 2018-06-07 13:09:05,646 cromwell-system-akka.dispatchers.api-dispatcher-119 INFO - Unspecified type (Unspecified version) workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 submitted; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(af282f7a-1e95-4390-8cf7-c3bbd93b10b2); 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2; 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 13:09:15,815 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Parsing workflow as WDL draft-2; 2018-06-07 13:09:15,826 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Call-to-Backend assignments: wf_hello.hello -> TES; 2018-06-07 13:09:16,844 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2 [UUID(af282f7a)]:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743
https://github.com/broadinstitute/cromwell/issues/3746:181,Deployability,configurat,configuration,181,S3fs can currently only be configured via environment variables - see https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/README.md. This should be wired through Cromwell configuration and the following TODO addressed: https://github.com/broadinstitute/cromwell/blob/aws_backend/filesystems/s3/src/main/scala/cromwell/filesystems/s3/S3PathBuilder.scala#L146,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3746
https://github.com/broadinstitute/cromwell/issues/3746:27,Modifiability,config,configured,27,S3fs can currently only be configured via environment variables - see https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/README.md. This should be wired through Cromwell configuration and the following TODO addressed: https://github.com/broadinstitute/cromwell/blob/aws_backend/filesystems/s3/src/main/scala/cromwell/filesystems/s3/S3PathBuilder.scala#L146,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3746
https://github.com/broadinstitute/cromwell/issues/3746:54,Modifiability,variab,variables,54,S3fs can currently only be configured via environment variables - see https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/README.md. This should be wired through Cromwell configuration and the following TODO addressed: https://github.com/broadinstitute/cromwell/blob/aws_backend/filesystems/s3/src/main/scala/cromwell/filesystems/s3/S3PathBuilder.scala#L146,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3746
https://github.com/broadinstitute/cromwell/issues/3746:181,Modifiability,config,configuration,181,S3fs can currently only be configured via environment variables - see https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/README.md. This should be wired through Cromwell configuration and the following TODO addressed: https://github.com/broadinstitute/cromwell/blob/aws_backend/filesystems/s3/src/main/scala/cromwell/filesystems/s3/S3PathBuilder.scala#L146,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3746
https://github.com/broadinstitute/cromwell/issues/3747:15,Deployability,configurat,configuration,15,"Authentication configuration has been coded but not properly tested for the ability to assume roles, etc. Integration tests should exist for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3747
https://github.com/broadinstitute/cromwell/issues/3747:106,Deployability,Integrat,Integration,106,"Authentication configuration has been coded but not properly tested for the ability to assume roles, etc. Integration tests should exist for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3747
https://github.com/broadinstitute/cromwell/issues/3747:106,Integrability,Integrat,Integration,106,"Authentication configuration has been coded but not properly tested for the ability to assume roles, etc. Integration tests should exist for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3747
https://github.com/broadinstitute/cromwell/issues/3747:15,Modifiability,config,configuration,15,"Authentication configuration has been coded but not properly tested for the ability to assume roles, etc. Integration tests should exist for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3747
https://github.com/broadinstitute/cromwell/issues/3747:0,Security,Authenticat,Authentication,0,"Authentication configuration has been coded but not properly tested for the ability to assume roles, etc. Integration tests should exist for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3747
https://github.com/broadinstitute/cromwell/issues/3747:61,Testability,test,tested,61,"Authentication configuration has been coded but not properly tested for the ability to assume roles, etc. Integration tests should exist for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3747
https://github.com/broadinstitute/cromwell/issues/3747:118,Testability,test,tests,118,"Authentication configuration has been coded but not properly tested for the ability to assume roles, etc. Integration tests should exist for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3747
https://github.com/broadinstitute/cromwell/issues/3748:294,Testability,test,test,294,"The parseOutput function https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L55, is brittle, but at least it's slow and not idiomatic scala. On the plus side, it is working in simple cases and has a unit test around it. This should be processing output line by line and handle proper MIME multipart format. Speaking of which, we should be outputting to CloudWatch logs in MIME multipart format as well. We're missing the empty line between headers and body in both cases. As an edge case, if a task outputs ""Content-Disposition: attachment; filename=blahblah"" on it's own line anywhere in the body of stdout or stderr, the brittle code here will happily process it as a header. At least it should throw in these circumstances. Bonus points if negative unit tests for invalid logs output are also added. This last second change (d81b3e9) has also apparently has also broken a bunch of Centaur tests, probably due to the hacking it's doing to the script output (https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L150). The idea, at least, appears solid.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3748
https://github.com/broadinstitute/cromwell/issues/3748:454,Testability,log,logs,454,"The parseOutput function https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L55, is brittle, but at least it's slow and not idiomatic scala. On the plus side, it is working in simple cases and has a unit test around it. This should be processing output line by line and handle proper MIME multipart format. Speaking of which, we should be outputting to CloudWatch logs in MIME multipart format as well. We're missing the empty line between headers and body in both cases. As an edge case, if a task outputs ""Content-Disposition: attachment; filename=blahblah"" on it's own line anywhere in the body of stdout or stderr, the brittle code here will happily process it as a header. At least it should throw in these circumstances. Bonus points if negative unit tests for invalid logs output are also added. This last second change (d81b3e9) has also apparently has also broken a bunch of Centaur tests, probably due to the hacking it's doing to the script output (https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L150). The idea, at least, appears solid.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3748
https://github.com/broadinstitute/cromwell/issues/3748:847,Testability,test,tests,847,"The parseOutput function https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L55, is brittle, but at least it's slow and not idiomatic scala. On the plus side, it is working in simple cases and has a unit test around it. This should be processing output line by line and handle proper MIME multipart format. Speaking of which, we should be outputting to CloudWatch logs in MIME multipart format as well. We're missing the empty line between headers and body in both cases. As an edge case, if a task outputs ""Content-Disposition: attachment; filename=blahblah"" on it's own line anywhere in the body of stdout or stderr, the brittle code here will happily process it as a header. At least it should throw in these circumstances. Bonus points if negative unit tests for invalid logs output are also added. This last second change (d81b3e9) has also apparently has also broken a bunch of Centaur tests, probably due to the hacking it's doing to the script output (https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L150). The idea, at least, appears solid.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3748
https://github.com/broadinstitute/cromwell/issues/3748:865,Testability,log,logs,865,"The parseOutput function https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L55, is brittle, but at least it's slow and not idiomatic scala. On the plus side, it is working in simple cases and has a unit test around it. This should be processing output line by line and handle proper MIME multipart format. Speaking of which, we should be outputting to CloudWatch logs in MIME multipart format as well. We're missing the empty line between headers and body in both cases. As an edge case, if a task outputs ""Content-Disposition: attachment; filename=blahblah"" on it's own line anywhere in the body of stdout or stderr, the brittle code here will happily process it as a header. At least it should throw in these circumstances. Bonus points if negative unit tests for invalid logs output are also added. This last second change (d81b3e9) has also apparently has also broken a bunch of Centaur tests, probably due to the hacking it's doing to the script output (https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L150). The idea, at least, appears solid.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3748
https://github.com/broadinstitute/cromwell/issues/3748:982,Testability,test,tests,982,"The parseOutput function https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L55, is brittle, but at least it's slow and not idiomatic scala. On the plus side, it is working in simple cases and has a unit test around it. This should be processing output line by line and handle proper MIME multipart format. Speaking of which, we should be outputting to CloudWatch logs in MIME multipart format as well. We're missing the empty line between headers and body in both cases. As an edge case, if a task outputs ""Content-Disposition: attachment; filename=blahblah"" on it's own line anywhere in the body of stdout or stderr, the brittle code here will happily process it as a header. At least it should throw in these circumstances. Bonus points if negative unit tests for invalid logs output are also added. This last second change (d81b3e9) has also apparently has also broken a bunch of Centaur tests, probably due to the hacking it's doing to the script output (https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L150). The idea, at least, appears solid.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3748
https://github.com/broadinstitute/cromwell/issues/3748:266,Usability,simpl,simple,266,"The parseOutput function https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L55, is brittle, but at least it's slow and not idiomatic scala. On the plus side, it is working in simple cases and has a unit test around it. This should be processing output line by line and handle proper MIME multipart format. Speaking of which, we should be outputting to CloudWatch logs in MIME multipart format as well. We're missing the empty line between headers and body in both cases. As an edge case, if a task outputs ""Content-Disposition: attachment; filename=blahblah"" on it's own line anywhere in the body of stdout or stderr, the brittle code here will happily process it as a header. At least it should throw in these circumstances. Bonus points if negative unit tests for invalid logs output are also added. This last second change (d81b3e9) has also apparently has also broken a bunch of Centaur tests, probably due to the hacking it's doing to the script output (https://github.com/broadinstitute/cromwell/blob/aws_backend/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L150). The idea, at least, appears solid.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3748
https://github.com/broadinstitute/cromwell/issues/3750:176,Performance,perform,perform,176,"Currently a new job definition is created per job submitted to AWS Batch. There should be built into AwsBatchJob.scala a mechanism to reuse definitions. One thought here is to perform a hash of the parameters used to create a job definition, then setting the job definition name to that hash. It is then relatively easy to perform a describeJobDefinitions call against aws batch to look for that name, and create it if it does not exist.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3750
https://github.com/broadinstitute/cromwell/issues/3750:323,Performance,perform,perform,323,"Currently a new job definition is created per job submitted to AWS Batch. There should be built into AwsBatchJob.scala a mechanism to reuse definitions. One thought here is to perform a hash of the parameters used to create a job definition, then setting the job definition name to that hash. It is then relatively easy to perform a describeJobDefinitions call against aws batch to look for that name, and create it if it does not exist.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3750
https://github.com/broadinstitute/cromwell/issues/3750:186,Security,hash,hash,186,"Currently a new job definition is created per job submitted to AWS Batch. There should be built into AwsBatchJob.scala a mechanism to reuse definitions. One thought here is to perform a hash of the parameters used to create a job definition, then setting the job definition name to that hash. It is then relatively easy to perform a describeJobDefinitions call against aws batch to look for that name, and create it if it does not exist.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3750
https://github.com/broadinstitute/cromwell/issues/3750:287,Security,hash,hash,287,"Currently a new job definition is created per job submitted to AWS Batch. There should be built into AwsBatchJob.scala a mechanism to reuse definitions. One thought here is to perform a hash of the parameters used to create a job definition, then setting the job definition name to that hash. It is then relatively easy to perform a describeJobDefinitions call against aws batch to look for that name, and create it if it does not exist.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3750
https://github.com/broadinstitute/cromwell/issues/3751:46,Availability,error,error,46,"Tested the WDL below with 831512a and got the error below:. ```wdl; version 1.0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElement",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:266,Availability,echo,echo,266,"Tested the WDL below with 831512a and got the error below:. ```wdl; version 1.0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElement",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:353,Availability,error,error,353,"Tested the WDL below with 831512a and got the error below:. ```wdl; version 1.0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElement",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:1635,Availability,Error,ErrorOr,1635,t_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:1685,Availability,Error,ErrorOr,1685,llection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:2430,Availability,Error,ErrorOr,2430,49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:2480,Availability,Error,ErrorOr,2480,lom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:2846,Availability,Error,ErrorOr,2846,la:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToW,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:2896,Availability,Error,ErrorOr,2896,.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:3285,Availability,Error,ErrorOr,3285,ntToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:3335,Availability,Error,ErrorOr,3335,nonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(Wor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:3372,Availability,Error,ErrorOr,3372,efinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:3429,Availability,Error,ErrorOr,3429,dation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.f,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:3986,Availability,Error,ErrorOr,3986,rtGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:4036,Availability,Error,ErrorOr,4036,itionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:4781,Availability,Error,ErrorOr,4781,49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.trans,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:4831,Availability,Error,ErrorOr,4831,lom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementTo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:5197,Availability,Error,ErrorOr,5197,la:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:5247,Availability,Error,ErrorOr,5247,.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cat,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:6708,Availability,Error,ErrorOr,6708,$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(Wom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:6758,Availability,Error,ErrorOr,6758,nCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:7086,Availability,Error,ErrorOr,7086,vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:7136,Availability,Error,ErrorOr,7136,advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProj,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:7173,Availability,Error,ErrorOr,7173,ats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstan,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:7230,Availability,Error,ErrorOr,7230,al.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:7388,Availability,Error,ErrorOr,7388,:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:7438,Availability,Error,ErrorOr,7438,rse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:9342,Performance,concurren,concurrent,9342,7); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinW,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:9404,Performance,concurren,concurrent,9404,(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.e,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:9478,Performance,concurren,concurrent,9478,eFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:9787,Performance,concurren,concurrent,9787,nfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorAc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:9094,Safety,unsafe,unsafeRunAsync,9094, scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:9139,Safety,unsafe,unsafeToFuture,9139,ther.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:1061,Security,Validat,Validated,1061,"; version 1.0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOpti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:1075,Security,Validat,Validated,1075,"0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:1624,Security,validat,validation,1624,_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkfl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:2419,Security,validat,validation,2419,mentToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extens,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:2835,Security,validat,validation,2835,WorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.Workflo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:3274,Security,validat,validation,3274,orkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementTo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:3361,Security,validat,validation,3361,tionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:3975,Security,validat,validation,3975,kflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertOuterScatter$10(ScatterElementToGraphNode.scala:72); scala.Function3.$anonfun$tupled$1(Function3.scala:35); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple3$.flatMapN$extension(ErrorOr.scala:53); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertOuterScatter(ScatterElementToGraphNode.scala:65); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:33); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkfl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:4770,Security,validat,validation,4770,mentToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:5186,Security,validat,validation,5186,WorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); scala.collection.immutable.List.foldLeft(List.scala:86); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$5(WorkflowDefinitionElementToWomWorkflowDefinition.scala:88); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.makeWomGraph(WorkflowDefinitionElementToWomWorkflowDefinition.scala:87); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$convertGraphElements$3(WorkflowDefinitionElementToWomWorkflowDefinition.scala:64); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convertGraphElements(WorkflowDefinitionElementToWomWorkflowDefinition.scala:63); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.convert(WorkflowDefinitionElementToWomWorkflowDefinition.scala:38); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$workflowDefinitionElementToWomWorkflowDefinition$1(package.scala:12); common.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); c,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:6697,Security,validat,validation,6697,mon.transforms.package$CheckedAtoB$.$anonfun$runThenCheck$1(package.scala:15); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$6(FileElementToWomBundle.scala:54); cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); cats.instances.VectorInstances$$anon$1.loop$2(vector.scala:40); cats.instances.VectorInstances$$anon$1.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMake,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:7075,Security,validat,validation,7075,.$anonfun$foldRight$2(vector.scala:41); cats.Eval$.advance(Eval.scala:272); cats.Eval$.loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:7162,Security,validat,validation,7162,loop$1(Eval.scala:354); cats.Eval$.cats$Eval$$evaluate(Eval.scala:372); cats.Eval$Defer.value(Eval.scala:258); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:76); cats.instances.VectorInstances$$anon$1.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cat,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:7377,Security,validat,validation,7377,.traverse(vector.scala:12); cats.Traverse$Ops.traverse(Traverse.scala:19); cats.Traverse$Ops.traverse$(Traverse.scala:19); cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$5(FileElementToWomBundle.scala:51); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWorkflowInner$1(FileElementToWomBundle.scala:48); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$14(FileElementToWomBundle.scala:77); scala.Function2.$anonfun$tupled$1(Function2.scala:48); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); common.validation.ErrorOr$ShortCircuitingFlatMapTuple2$.flatMapN$extension(ErrorOr.scala:49); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$12(FileElementToWomBundle.scala:77); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3La,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:8501,Security,validat,validateNamespace,8501,.toWomBundle(FileElementToWomBundle.scala:75); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Prom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:8648,Security,validat,validateNamespace,8648,:30); wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:82); wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); scala.util.Either$RightProjection.flatMap(Either.scala:702); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); languages.wdl.draft3.WdlDraft3LanguageFactory.$anonfun$validateNamespace$2(WdlDraft3LanguageFactory.scala:39); scala.util.Either.flatMap(Either.scala:338); languages.wdl.draft3.WdlDraft3LanguageFactory.validateNamespace(WdlDraft3LanguageFactory.scala:38); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$buildWorkflowDescriptor$7(MaterializeWorkflowDescriptorActor.scala:242); cats.data.EitherT.$anonfun$flatMap$1(EitherT.scala:80); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:128); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$ano,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:0,Testability,Test,Tested,0,"Tested the WDL below with 831512a and got the error below:. ```wdl; version 1.0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElement",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:11250,Testability,Log,LoggingFSM,11250,ool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:173); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:11343,Testability,Log,LoggingFSM,11343,orkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:173); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3751:11398,Testability,Log,LoggingFSM,11398,mwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:173); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:665); 	at akka.actor.FSM.processEvent$(FSM.scala:662); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:801); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:783); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:659); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:653); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751
https://github.com/broadinstitute/cromwell/issues/3753:20,Safety,abort,abort,20,A batch endpoint to abort workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3753
https://github.com/broadinstitute/cromwell/issues/3755:20,Deployability,update,update,20,A batch endpoint to update labels for workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3755
https://github.com/broadinstitute/cromwell/issues/3758:13,Safety,abort,aborted,13,"After I have aborted a job, VM's are not being apropriately killed. Manually killing the VM does not have the desire effect, as if the task was preemptible PAPI will launch another VM until all of your preemptible tries have been consumed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758
https://github.com/broadinstitute/cromwell/issues/3759:306,Modifiability,config,config,306,"The use case here is that a user wants to pass in a dos url, with a specific docker to handle the localization of that docker. Cromwell's job is to put both those pieces together to allow for custom localization in addition to the default localization performed on the filesystems declared in the Cromwell config. Requirements:; 1) A new option that allows for one to pass in the custom localization docker (globally or per-workflow); 2) Parse and accept dos urls as task inputs; 3) Feed the dos url + the final destination for that file to the docker ; 4) Fail gracefully if any functions (read_lines, size, etc) are performed on this input type, not required for this stage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3759
https://github.com/broadinstitute/cromwell/issues/3759:252,Performance,perform,performed,252,"The use case here is that a user wants to pass in a dos url, with a specific docker to handle the localization of that docker. Cromwell's job is to put both those pieces together to allow for custom localization in addition to the default localization performed on the filesystems declared in the Cromwell config. Requirements:; 1) A new option that allows for one to pass in the custom localization docker (globally or per-workflow); 2) Parse and accept dos urls as task inputs; 3) Feed the dos url + the final destination for that file to the docker ; 4) Fail gracefully if any functions (read_lines, size, etc) are performed on this input type, not required for this stage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3759
https://github.com/broadinstitute/cromwell/issues/3759:618,Performance,perform,performed,618,"The use case here is that a user wants to pass in a dos url, with a specific docker to handle the localization of that docker. Cromwell's job is to put both those pieces together to allow for custom localization in addition to the default localization performed on the filesystems declared in the Cromwell config. Requirements:; 1) A new option that allows for one to pass in the custom localization docker (globally or per-workflow); 2) Parse and accept dos urls as task inputs; 3) Feed the dos url + the final destination for that file to the docker ; 4) Fail gracefully if any functions (read_lines, size, etc) are performed on this input type, not required for this stage.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3759
https://github.com/broadinstitute/cromwell/issues/3760:128,Availability,error,error,128,"Latest ""develop"" branch, simple workflow with single parameter in the inputs (will post here if deemed important), and get this error once I run it the second, third, etc. times:. ```; 2018-06-11 16:10:16,080 cromwell-system-akka.dispatchers.api-dispatcher-630 INFO - Unspecified type (Unspecified version) workflow ab42cf3c-726f-4148-a30f-0f907c843361 submitted; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - 1 new workflows fetched; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowManagerActor Starting workflow UUID(ab42cf3c-726f-4148-a30f-0f907c843361); 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowManagerActor Successfully started WorkflowActor-ab42cf3c-726f-4148-a30f-0f907c843361; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-11 16:10:35,959 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Parsing workflow as WDL draft-2; 2018-06-11 16:10:35,970 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:1589,Availability,ERROR,ERROR,1589,"f-0f907c843361); 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowManagerActor Successfully started WorkflowActor-ab42cf3c-726f-4148-a30f-0f907c843361; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-11 16:10:35,959 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Parsing workflow as WDL draft-2; 2018-06-11 16:10:35,970 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:2185,Availability,recover,recoverWith,2185,"-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:3480,Deployability,pipeline,pipeline,3480,or$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableSta,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:3604,Deployability,pipeline,pipeline,3604,sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:3722,Deployability,pipeline,pipeline,3722,cutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:3833,Deployability,pipeline,pipeline,3833,ala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.e,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:3944,Deployability,pipeline,pipeline,3944,a:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(Strea,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:4085,Deployability,pipeline,pipeline,4085,oinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:4212,Deployability,pipeline,pipeline,4212,oinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.ama,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:4339,Deployability,pipeline,pipeline,4339, specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:4457,Deployability,pipeline,pipeline,4457,n.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.am,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:4573,Deployability,pipeline,pipeline,4573,re.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.j,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:4674,Deployability,pipeline,pipeline,4674,a:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipeli,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:4775,Deployability,pipeline,pipeline,4775,eStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stage,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:5106,Deployability,pipeline,pipeline,5106,ptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImp,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:5238,Deployability,pipeline,pipeline,5238,HandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.ja,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:5361,Deployability,pipeline,pipeline,5361,tage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:5484,Deployability,pipeline,pipeline,5484,RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.cl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:5625,Deployability,pipeline,pipeline,5625,va:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientH,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:5766,Deployability,pipeline,pipeline,5766,http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.copyObject(DefaultS3Client.java:466); 	at org.lerch.s3fs.S3FileSystem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:5917,Deployability,pipeline,pipeline,5917,mManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.copyObject(DefaultS3Client.java:466); 	at org.lerch.s3fs.S3FileSystemProvider.copy(S3FileSystemProvider.java:434); 	at java.nio.file.Files.copy(Files.java:1274); 	at better.files.File.copyTo(File.scala:663); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:1612,Performance,cache,cache,1612,"f-0f907c843361); 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowManagerActor Successfully started WorkflowActor-ab42cf3c-726f-4148-a30f-0f907c843361; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-11 16:10:35,959 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Parsing workflow as WDL draft-2; 2018-06-11 16:10:35,970 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:1708,Performance,cache,cache,1708,"gine-dispatcher-47 INFO - WorkflowManagerActor Successfully started WorkflowActor-ab42cf3c-726f-4148-a30f-0f907c843361; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-11 16:10:35,959 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Parsing workflow as WDL draft-2; 2018-06-11 16:10:35,970 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:2158,Performance,concurren,concurrent,2158,"stem-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:2228,Performance,concurren,concurrent,2228,"escriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:2306,Performance,concurren,concurrent,2306,"> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:2631,Performance,concurren,concurrent,2631,"ckendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseSta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:7291,Performance,concurren,concurrent,7291,"e.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.copyObject(DefaultS3Client.java:466); 	at org.lerch.s3fs.S3FileSystemProvider.copy(S3FileSystemProvider.java:434); 	at java.nio.file.Files.copy(Files.java:1274); 	at better.files.File.copyTo(File.scala:663); 	at cromwell.core.path.BetterFileMethods.copyTo(BetterFileMethods.scala:425); 	at cromwell.core.path.BetterFileMethods.copyTo$(BetterFileMethods.scala:424); 	at cromwell.filesystems.s3.S3Path.copyTo(S3PathBuilder.scala:157); 	at cromwell.engine.io.nio.NioFlow.$anonfun$copy$1(NioFlow.scala:65); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 common frames omitted; 2018-06-11 16:10:38,047 INFO - Submitting job to AWS Batch; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:7453,Performance,concurren,concurrent,7453,"e.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.copyObject(DefaultS3Client.java:466); 	at org.lerch.s3fs.S3FileSystemProvider.copy(S3FileSystemProvider.java:434); 	at java.nio.file.Files.copy(Files.java:1274); 	at better.files.File.copyTo(File.scala:663); 	at cromwell.core.path.BetterFileMethods.copyTo(BetterFileMethods.scala:425); 	at cromwell.core.path.BetterFileMethods.copyTo$(BetterFileMethods.scala:424); 	at cromwell.filesystems.s3.S3Path.copyTo(S3PathBuilder.scala:157); 	at cromwell.engine.io.nio.NioFlow.$anonfun$copy$1(NioFlow.scala:65); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 common frames omitted; 2018-06-11 16:10:38,047 INFO - Submitting job to AWS Batch; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:7515,Performance,concurren,concurrent,7515,"e.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.copyObject(DefaultS3Client.java:466); 	at org.lerch.s3fs.S3FileSystemProvider.copy(S3FileSystemProvider.java:434); 	at java.nio.file.Files.copy(Files.java:1274); 	at better.files.File.copyTo(File.scala:663); 	at cromwell.core.path.BetterFileMethods.copyTo(BetterFileMethods.scala:425); 	at cromwell.core.path.BetterFileMethods.copyTo$(BetterFileMethods.scala:424); 	at cromwell.filesystems.s3.S3Path.copyTo(S3PathBuilder.scala:157); 	at cromwell.engine.io.nio.NioFlow.$anonfun$copy$1(NioFlow.scala:65); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 common frames omitted; 2018-06-11 16:10:38,047 INFO - Submitting job to AWS Batch; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:7582,Performance,concurren,concurrent,7582,"e.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.copyObject(DefaultS3Client.java:466); 	at org.lerch.s3fs.S3FileSystemProvider.copy(S3FileSystemProvider.java:434); 	at java.nio.file.Files.copy(Files.java:1274); 	at better.files.File.copyTo(File.scala:663); 	at cromwell.core.path.BetterFileMethods.copyTo(BetterFileMethods.scala:425); 	at cromwell.core.path.BetterFileMethods.copyTo$(BetterFileMethods.scala:424); 	at cromwell.filesystems.s3.S3Path.copyTo(S3PathBuilder.scala:157); 	at cromwell.engine.io.nio.NioFlow.$anonfun$copy$1(NioFlow.scala:65); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 common frames omitted; 2018-06-11 16:10:38,047 INFO - Submitting job to AWS Batch; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:2185,Safety,recover,recoverWith,2185,"-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/issues/3760:25,Usability,simpl,simple,25,"Latest ""develop"" branch, simple workflow with single parameter in the inputs (will post here if deemed important), and get this error once I run it the second, third, etc. times:. ```; 2018-06-11 16:10:16,080 cromwell-system-akka.dispatchers.api-dispatcher-630 INFO - Unspecified type (Unspecified version) workflow ab42cf3c-726f-4148-a30f-0f907c843361 submitted; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - 1 new workflows fetched; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowManagerActor Starting workflow UUID(ab42cf3c-726f-4148-a30f-0f907c843361); 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowManagerActor Successfully started WorkflowActor-ab42cf3c-726f-4148-a30f-0f907c843361; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-11 16:10:35,959 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Parsing workflow as WDL draft-2; 2018-06-11 16:10:35,970 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760
https://github.com/broadinstitute/cromwell/pull/3761:12,Availability,heartbeat,heartbeat,12,The batched heartbeat writer and workflow picker upper both try to lock multiple rows in the workflow store table inside a transaction and were often observed to deadlock. These two workflow store accesses are now routed through an actor that effectively serializes access to the workflow store table (other accesses are not affected). If this manages to run the gauntlet of gulls [batch abort](https://github.com/broadinstitute/cromwell/issues/3753) would likely need to be added to this system. Known shortcomings:; - ~~Should probably give more thought as to the thread on which the blocking happens.~~ now on the IO dispatcher; - ~~Should consider actor supervision because if this one actor ever dies that will be bad times.~~ default Akka supervision is reasonable here; - ~~May keep one writer Cromwell from tripping over itself but wouldn't keep multiple writer Cromwells from tripping over each other~~ ticketed in #3795,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761
https://github.com/broadinstitute/cromwell/pull/3761:214,Integrability,rout,routed,214,The batched heartbeat writer and workflow picker upper both try to lock multiple rows in the workflow store table inside a transaction and were often observed to deadlock. These two workflow store accesses are now routed through an actor that effectively serializes access to the workflow store table (other accesses are not affected). If this manages to run the gauntlet of gulls [batch abort](https://github.com/broadinstitute/cromwell/issues/3753) would likely need to be added to this system. Known shortcomings:; - ~~Should probably give more thought as to the thread on which the blocking happens.~~ now on the IO dispatcher; - ~~Should consider actor supervision because if this one actor ever dies that will be bad times.~~ default Akka supervision is reasonable here; - ~~May keep one writer Cromwell from tripping over itself but wouldn't keep multiple writer Cromwells from tripping over each other~~ ticketed in #3795,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761
https://github.com/broadinstitute/cromwell/pull/3761:388,Safety,abort,abort,388,The batched heartbeat writer and workflow picker upper both try to lock multiple rows in the workflow store table inside a transaction and were often observed to deadlock. These two workflow store accesses are now routed through an actor that effectively serializes access to the workflow store table (other accesses are not affected). If this manages to run the gauntlet of gulls [batch abort](https://github.com/broadinstitute/cromwell/issues/3753) would likely need to be added to this system. Known shortcomings:; - ~~Should probably give more thought as to the thread on which the blocking happens.~~ now on the IO dispatcher; - ~~Should consider actor supervision because if this one actor ever dies that will be bad times.~~ default Akka supervision is reasonable here; - ~~May keep one writer Cromwell from tripping over itself but wouldn't keep multiple writer Cromwells from tripping over each other~~ ticketed in #3795,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761
https://github.com/broadinstitute/cromwell/pull/3761:197,Security,access,accesses,197,The batched heartbeat writer and workflow picker upper both try to lock multiple rows in the workflow store table inside a transaction and were often observed to deadlock. These two workflow store accesses are now routed through an actor that effectively serializes access to the workflow store table (other accesses are not affected). If this manages to run the gauntlet of gulls [batch abort](https://github.com/broadinstitute/cromwell/issues/3753) would likely need to be added to this system. Known shortcomings:; - ~~Should probably give more thought as to the thread on which the blocking happens.~~ now on the IO dispatcher; - ~~Should consider actor supervision because if this one actor ever dies that will be bad times.~~ default Akka supervision is reasonable here; - ~~May keep one writer Cromwell from tripping over itself but wouldn't keep multiple writer Cromwells from tripping over each other~~ ticketed in #3795,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761
https://github.com/broadinstitute/cromwell/pull/3761:266,Security,access,access,266,The batched heartbeat writer and workflow picker upper both try to lock multiple rows in the workflow store table inside a transaction and were often observed to deadlock. These two workflow store accesses are now routed through an actor that effectively serializes access to the workflow store table (other accesses are not affected). If this manages to run the gauntlet of gulls [batch abort](https://github.com/broadinstitute/cromwell/issues/3753) would likely need to be added to this system. Known shortcomings:; - ~~Should probably give more thought as to the thread on which the blocking happens.~~ now on the IO dispatcher; - ~~Should consider actor supervision because if this one actor ever dies that will be bad times.~~ default Akka supervision is reasonable here; - ~~May keep one writer Cromwell from tripping over itself but wouldn't keep multiple writer Cromwells from tripping over each other~~ ticketed in #3795,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761
https://github.com/broadinstitute/cromwell/pull/3761:308,Security,access,accesses,308,The batched heartbeat writer and workflow picker upper both try to lock multiple rows in the workflow store table inside a transaction and were often observed to deadlock. These two workflow store accesses are now routed through an actor that effectively serializes access to the workflow store table (other accesses are not affected). If this manages to run the gauntlet of gulls [batch abort](https://github.com/broadinstitute/cromwell/issues/3753) would likely need to be added to this system. Known shortcomings:; - ~~Should probably give more thought as to the thread on which the blocking happens.~~ now on the IO dispatcher; - ~~Should consider actor supervision because if this one actor ever dies that will be bad times.~~ default Akka supervision is reasonable here; - ~~May keep one writer Cromwell from tripping over itself but wouldn't keep multiple writer Cromwells from tripping over each other~~ ticketed in #3795,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761
https://github.com/broadinstitute/cromwell/issues/3762:44,Availability,error,error,44,Since version 32 Womtool validate give this error on wdl file without a workflow: ; `Namespace does not have a local workflow to run`. Is this required by spec? Would be better to still accept this for reusable tasks like this: https://github.com/biowdl/tasks,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762
https://github.com/broadinstitute/cromwell/issues/3762:25,Security,validat,validate,25,Since version 32 Womtool validate give this error on wdl file without a workflow: ; `Namespace does not have a local workflow to run`. Is this required by spec? Would be better to still accept this for reusable tasks like this: https://github.com/biowdl/tasks,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762
https://github.com/broadinstitute/cromwell/pull/3763:256,Modifiability,variab,variable,256,"The workflow; ```; version 1.0. workflow scatter_chain {; Array[Pair[Int, Int]] pairs = [(1, 2), (3, 4), (5, 6)]; scatter (p in pairs) {; Int x = p.left; }; }; ```; used to fail WOM validation because expressions consisting of member access on the scatter variable were erroneously counted as being consumed within the scatter's inner graph. This PR stops this misaccounting. ( On Friday @cjllanwarne and I hypothesized that `ifElementUnlinkedValueConsumer` would need a similar adjustment, but I no longer think this is the case; an `IfElement` does not create a new variable in the scope of its graph like a `ScatterElement` does. )",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3763
https://github.com/broadinstitute/cromwell/pull/3763:568,Modifiability,variab,variable,568,"The workflow; ```; version 1.0. workflow scatter_chain {; Array[Pair[Int, Int]] pairs = [(1, 2), (3, 4), (5, 6)]; scatter (p in pairs) {; Int x = p.left; }; }; ```; used to fail WOM validation because expressions consisting of member access on the scatter variable were erroneously counted as being consumed within the scatter's inner graph. This PR stops this misaccounting. ( On Friday @cjllanwarne and I hypothesized that `ifElementUnlinkedValueConsumer` would need a similar adjustment, but I no longer think this is the case; an `IfElement` does not create a new variable in the scope of its graph like a `ScatterElement` does. )",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3763
https://github.com/broadinstitute/cromwell/pull/3763:182,Security,validat,validation,182,"The workflow; ```; version 1.0. workflow scatter_chain {; Array[Pair[Int, Int]] pairs = [(1, 2), (3, 4), (5, 6)]; scatter (p in pairs) {; Int x = p.left; }; }; ```; used to fail WOM validation because expressions consisting of member access on the scatter variable were erroneously counted as being consumed within the scatter's inner graph. This PR stops this misaccounting. ( On Friday @cjllanwarne and I hypothesized that `ifElementUnlinkedValueConsumer` would need a similar adjustment, but I no longer think this is the case; an `IfElement` does not create a new variable in the scope of its graph like a `ScatterElement` does. )",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3763
https://github.com/broadinstitute/cromwell/pull/3763:234,Security,access,access,234,"The workflow; ```; version 1.0. workflow scatter_chain {; Array[Pair[Int, Int]] pairs = [(1, 2), (3, 4), (5, 6)]; scatter (p in pairs) {; Int x = p.left; }; }; ```; used to fail WOM validation because expressions consisting of member access on the scatter variable were erroneously counted as being consumed within the scatter's inner graph. This PR stops this misaccounting. ( On Friday @cjllanwarne and I hypothesized that `ifElementUnlinkedValueConsumer` would need a similar adjustment, but I no longer think this is the case; an `IfElement` does not create a new variable in the scope of its graph like a `ScatterElement` does. )",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3763
https://github.com/broadinstitute/cromwell/issues/3765:307,Availability,error,error,307,"When submitted to a local Cromwell built from latest `develop`, the workflow; ```; version 1.0. workflow short_circuit {; input {; Int c = if true then 1 else 1 / 0; Int d = if false then 1 / 0 else 1; }; if (false && 1 / 0 == 0) {; Int a = 1; }; if (true || 1 / 0 == 0) {; Int b = 1; }. }; ```; fails with error; ```; Failed to evaluate 'if_condition' (reason 1 of 1):; Evaluating (true || ((1 / 0) == 0)) failed: Divide by zero error: 1 / WomInteger(0); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3765
https://github.com/broadinstitute/cromwell/issues/3765:430,Availability,error,error,430,"When submitted to a local Cromwell built from latest `develop`, the workflow; ```; version 1.0. workflow short_circuit {; input {; Int c = if true then 1 else 1 / 0; Int d = if false then 1 / 0 else 1; }; if (false && 1 / 0 == 0) {; Int a = 1; }; if (true || 1 / 0 == 0) {; Int b = 1; }. }; ```; fails with error; ```; Failed to evaluate 'if_condition' (reason 1 of 1):; Evaluating (true || ((1 / 0) == 0)) failed: Divide by zero error: 1 / WomInteger(0); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3765
https://github.com/broadinstitute/cromwell/issues/3766:528,Availability,error,error,528,"In WDL 1.0, variables defined in nested if-statements are ""doubly optional"" (`Int??`) and break `select_first()`. Note that the degree of optionality maxes out at two, regardless of how deep the nesting is. This workflow is fine; ```; workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; but this one fails; ```; version 1.0. workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; with error; ```; Failed to process workflow definition 'Test' (reason 1 of 1):; Failed to process declaration 'Int c = select_first([a, b])' (reason 1 of 1):; Cannot coerce expression of type 'Int??' to 'Int'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3766
https://github.com/broadinstitute/cromwell/issues/3766:12,Modifiability,variab,variables,12,"In WDL 1.0, variables defined in nested if-statements are ""doubly optional"" (`Int??`) and break `select_first()`. Note that the degree of optionality maxes out at two, regardless of how deep the nesting is. This workflow is fine; ```; workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; but this one fails; ```; version 1.0. workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; with error; ```; Failed to process workflow definition 'Test' (reason 1 of 1):; Failed to process declaration 'Int c = select_first([a, b])' (reason 1 of 1):; Cannot coerce expression of type 'Int??' to 'Int'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3766
https://github.com/broadinstitute/cromwell/issues/3766:244,Testability,Test,Test,244,"In WDL 1.0, variables defined in nested if-statements are ""doubly optional"" (`Int??`) and break `select_first()`. Note that the degree of optionality maxes out at two, regardless of how deep the nesting is. This workflow is fine; ```; workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; but this one fails; ```; version 1.0. workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; with error; ```; Failed to process workflow definition 'Test' (reason 1 of 1):; Failed to process declaration 'Int c = select_first([a, b])' (reason 1 of 1):; Cannot coerce expression of type 'Int??' to 'Int'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3766
https://github.com/broadinstitute/cromwell/issues/3766:407,Testability,Test,Test,407,"In WDL 1.0, variables defined in nested if-statements are ""doubly optional"" (`Int??`) and break `select_first()`. Note that the degree of optionality maxes out at two, regardless of how deep the nesting is. This workflow is fine; ```; workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; but this one fails; ```; version 1.0. workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; with error; ```; Failed to process workflow definition 'Test' (reason 1 of 1):; Failed to process declaration 'Int c = select_first([a, b])' (reason 1 of 1):; Cannot coerce expression of type 'Int??' to 'Int'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3766
https://github.com/broadinstitute/cromwell/issues/3766:579,Testability,Test,Test,579,"In WDL 1.0, variables defined in nested if-statements are ""doubly optional"" (`Int??`) and break `select_first()`. Note that the degree of optionality maxes out at two, regardless of how deep the nesting is. This workflow is fine; ```; workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; but this one fails; ```; version 1.0. workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; with error; ```; Failed to process workflow definition 'Test' (reason 1 of 1):; Failed to process declaration 'Int c = select_first([a, b])' (reason 1 of 1):; Cannot coerce expression of type 'Int??' to 'Int'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3766
https://github.com/broadinstitute/cromwell/pull/3770:93,Integrability,depend,depending,93,"Prevent workflow log file handle leakage. There are two distinct workflow log shutdown paths depending on whether the `final_workflow_log_dir` workflow option is set, this addresses both.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3770
https://github.com/broadinstitute/cromwell/pull/3770:17,Testability,log,log,17,"Prevent workflow log file handle leakage. There are two distinct workflow log shutdown paths depending on whether the `final_workflow_log_dir` workflow option is set, this addresses both.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3770
https://github.com/broadinstitute/cromwell/pull/3770:74,Testability,log,log,74,"Prevent workflow log file handle leakage. There are two distinct workflow log shutdown paths depending on whether the `final_workflow_log_dir` workflow option is set, this addresses both.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3770
https://github.com/broadinstitute/cromwell/pull/3772:30,Security,validat,validate,30,Allows for and tests `womtool validate draft2_task.wdl`. Fixes #3762,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3772
https://github.com/broadinstitute/cromwell/pull/3772:15,Testability,test,tests,15,Allows for and tests `womtool validate draft2_task.wdl`. Fixes #3762,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3772
https://github.com/broadinstitute/cromwell/issues/3774:656,Availability,error,error,656,"I am trying to adapt a WDL workflow originally developed for DNAnexus, to work in AWS Batch. I am running from ""develop"" branch on Mac, server mode. The workflow seems to run on AWS, but then fails when checking for output logs in S3... inputs: [demux_plus_inputs.json.txt](https://github.com/broadinstitute/cromwell/files/2099495/demux_plus_inputs.json.txt); workflow: [demux_only.wdl.txt](https://github.com/broadinstitute/cromwell/files/2099496/demux_only.wdl.txt); resource file: [workflows.zip](https://github.com/broadinstitute/cromwell/files/2099470/workflows.zip); config file, which shows some attempts to add the local filesystem, since I get an error about that: ; [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2099528/aws.conf.txt). ```; 2018-06-13 14:29:27,796 cromwell-system-akka.dispatchers.api-dispatcher-72 INFO - Unspecified type (Unspecified version) workflow a67833cb-b894-4790-872f-9f3104cab60c submitted; 2018-06-13 14:29:44,760 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-13 14:29:44,761 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(a67833cb-b894-4790-872f-9f3104cab60c); 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-a67833cb-b894-4790-872f-9f3104cab60c; 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:5014,Availability,ERROR,ERROR,5014,"or.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2018-06-13 14:29:47,368 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowExecutionActor-a67833cb-b894-4790-872f-9f3104cab60c [UUID(a67833cb)]: Starting demux_only.illumina_demux; 2018-06-13 14:29:48,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - Failed to hash /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at scala.util.Try$.apply(Try.scala:209);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:8794,Availability,ERROR,ERROR,8794,"sMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,009 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - a67833cb:demux_only.illumina_demux:-1:1: Hash error, disabling call caching for this job.; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:8848,Availability,error,error,8848,"Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,009 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - a67833cb:demux_only.illumina_demux:-1:1: Hash error, disabling call caching for this job.; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:15182,Availability,echo,echo,15182,"0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; 2018-06-13 14:29:48,873 INFO - reconfiguredScript: #!/bin/bash; mkdir -p /cromwell_root; #!/bin/bash. cd /cromwell_root; tmpDir=`mkdir -p ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1"" && echo ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); (; cd /cromwell_root. set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:16419,Availability,echo,echo,16419,"ell_root. ); (; cd /cromwell_root. set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; ) > '/cromwell_root/illumina_demux-stdout.log' 2> '/cromwell_root/illumina_demux-stderr.log'; echo $? > /cromwell_root/illumina_demux-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell_root; sync; # make the directory which will keep the matching files; mkdir /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 2> /dev/null ) || ( ln *.bam /cromwell_root/glob-3bcbe4e7489c9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:16891,Availability,echo,echo,16891,"d benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; ) > '/cromwell_root/illumina_demux-stdout.log' 2> '/cromwell_root/illumina_demux-stderr.log'; echo $? > /cromwell_root/illumina_demux-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell_root; sync; # make the directory which will keep the matching files; mkdir /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 2> /dev/null ) || ( ln *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 | grep -v cromwell_glob_control_file > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385.list. # make the directory which will keep the matching files; mkdir /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63. # create the glob control file that w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:17947,Availability,echo,echo,17947,"r globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 2> /dev/null ) || ( ln *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 | grep -v cromwell_glob_control_file > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385.list. # make the directory which will keep the matching files; mkdir /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alte",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:18886,Availability,echo,echo,18886," the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:19152,Availability,echo,echo,19152," list of globbed files."" > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:19325,Availability,echo,echo,19325,".html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:19498,Availability,echo,echo,19498,"0748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:20718,Availability,ERROR,ERROR,20718,"13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:21335,Availability,recover,recoverWith,21335,"/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:2662,Deployability,update,updateCache,2662,"ith batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Call-to-Backend assignments: demux_only.illumina_demux -> AWSBATCH; 2018-06-13 14:29:46,085 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:15,Energy Efficiency,adapt,adapt,15,"I am trying to adapt a WDL workflow originally developed for DNAnexus, to work in AWS Batch. I am running from ""develop"" branch on Mac, server mode. The workflow seems to run on AWS, but then fails when checking for output logs in S3... inputs: [demux_plus_inputs.json.txt](https://github.com/broadinstitute/cromwell/files/2099495/demux_plus_inputs.json.txt); workflow: [demux_only.wdl.txt](https://github.com/broadinstitute/cromwell/files/2099496/demux_only.wdl.txt); resource file: [workflows.zip](https://github.com/broadinstitute/cromwell/files/2099470/workflows.zip); config file, which shows some attempts to add the local filesystem, since I get an error about that: ; [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2099528/aws.conf.txt). ```; 2018-06-13 14:29:27,796 cromwell-system-akka.dispatchers.api-dispatcher-72 INFO - Unspecified type (Unspecified version) workflow a67833cb-b894-4790-872f-9f3104cab60c submitted; 2018-06-13 14:29:44,760 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-13 14:29:44,761 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(a67833cb-b894-4790-872f-9f3104cab60c); 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-a67833cb-b894-4790-872f-9f3104cab60c; 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:15,Modifiability,adapt,adapt,15,"I am trying to adapt a WDL workflow originally developed for DNAnexus, to work in AWS Batch. I am running from ""develop"" branch on Mac, server mode. The workflow seems to run on AWS, but then fails when checking for output logs in S3... inputs: [demux_plus_inputs.json.txt](https://github.com/broadinstitute/cromwell/files/2099495/demux_plus_inputs.json.txt); workflow: [demux_only.wdl.txt](https://github.com/broadinstitute/cromwell/files/2099496/demux_only.wdl.txt); resource file: [workflows.zip](https://github.com/broadinstitute/cromwell/files/2099470/workflows.zip); config file, which shows some attempts to add the local filesystem, since I get an error about that: ; [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2099528/aws.conf.txt). ```; 2018-06-13 14:29:27,796 cromwell-system-akka.dispatchers.api-dispatcher-72 INFO - Unspecified type (Unspecified version) workflow a67833cb-b894-4790-872f-9f3104cab60c submitted; 2018-06-13 14:29:44,760 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-13 14:29:44,761 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(a67833cb-b894-4790-872f-9f3104cab60c); 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-a67833cb-b894-4790-872f-9f3104cab60c; 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:573,Modifiability,config,config,573,"I am trying to adapt a WDL workflow originally developed for DNAnexus, to work in AWS Batch. I am running from ""develop"" branch on Mac, server mode. The workflow seems to run on AWS, but then fails when checking for output logs in S3... inputs: [demux_plus_inputs.json.txt](https://github.com/broadinstitute/cromwell/files/2099495/demux_plus_inputs.json.txt); workflow: [demux_only.wdl.txt](https://github.com/broadinstitute/cromwell/files/2099496/demux_only.wdl.txt); resource file: [workflows.zip](https://github.com/broadinstitute/cromwell/files/2099470/workflows.zip); config file, which shows some attempts to add the local filesystem, since I get an error about that: ; [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2099528/aws.conf.txt). ```; 2018-06-13 14:29:27,796 cromwell-system-akka.dispatchers.api-dispatcher-72 INFO - Unspecified type (Unspecified version) workflow a67833cb-b894-4790-872f-9f3104cab60c submitted; 2018-06-13 14:29:44,760 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-13 14:29:44,761 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(a67833cb-b894-4790-872f-9f3104cab60c); 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-a67833cb-b894-4790-872f-9f3104cab60c; 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:1653,Modifiability,config,configured,1653,"rror about that: ; [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2099528/aws.conf.txt). ```; 2018-06-13 14:29:27,796 cromwell-system-akka.dispatchers.api-dispatcher-72 INFO - Unspecified type (Unspecified version) workflow a67833cb-b894-4790-872f-9f3104cab60c submitted; 2018-06-13 14:29:44,760 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-13 14:29:44,761 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(a67833cb-b894-4790-872f-9f3104cab60c); 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-a67833cb-b894-4790-872f-9f3104cab60c; 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Call-to-Backend assignments: demux_only.illumina_demux -> AWSBATCH; 2018-06-13 14:29:46,085 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:5400,Modifiability,config,configure,5400,"patch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2018-06-13 14:29:47,368 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowExecutionActor-a67833cb-b894-4790-872f-9f3104cab60c [UUID(a67833cb)]: Starting demux_only.illumina_demux; 2018-06-13 14:29:48,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - Failed to hash /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.io.WorkflowPaths.getPath(WorkflowPaths.scala:43); 	at cromwell.backend.io.WorkflowPaths.getPath$(WorkflowPaths.scala:43); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.getPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.standard.StandardCachingActorHelper.getPath(StandardCachingActorHelper.scala:41); 	at c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:8511,Modifiability,config,configure,8511,"s.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,009 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - a67833cb:demux_only.illumina_demux:-1:1: Hash error, disabling call caching for this job.; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:9194,Modifiability,config,configure,9194,"in.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,009 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - a67833cb:demux_only.illumina_demux:-1:1: Hash error, disabling call caching for this job.; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.io.WorkflowPaths.getPath(WorkflowPaths.scala:43); 	at cromwell.backend.io.WorkflowPaths.getPath$(WorkflowPaths.scala:43); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.getPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.standard.StandardCachingActorHelper.getPath(StandardCachingActorHelper.scala:41); 	at c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:12305,Modifiability,config,configure,12305,"s.aroundReceive$(Timers.scala:36); 	at cromwell.backend.standard.callcaching.StandardFileHashingActor.aroundReceive(StandardFileHashingActor.scala:59); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,013 cromwell-system-akka.dispatchers.backend-dispatcher-95 WARN - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Unrecognized runtime attribute keys: preemptible, dx_instance_type; 2018-06-13 14:29:48,218 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: `set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:2508,Performance,concurren,concurrent,2508,"e WorkflowStoreActor; 2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Call-to-Backend assignments: demux_only.illumina_demux -> AWSBATCH; 2018-06-13 14:29:46,085 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:2557,Performance,concurren,concurrent,2557,"mwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Call-to-Backend assignments: demux_only.illumina_demux -> AWSBATCH; 2018-06-13 14:29:46,085 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAnd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:14000,Performance,queue,queue,14000,"fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done`; 2018-06-13 14:29:48,871 INFO - Submitting job to AWS Batch; 2018-06-13 14:29:48,871 INFO - dockerImage: quay.io/broadinstitute/viral-ngs; 2018-06-13 14:29:48,871 INFO - jobQueueArn: arn:aws:batch:us-east-1:140059717706:job-queue/funnel-job-queue; 2018-06-13 14:29:48,871 INFO - commandLine: set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; 2018-06-13 14:29:48,873 INFO - reconfiguredScript: #!/bin/bash; mk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:14017,Performance,queue,queue,14017,"fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done`; 2018-06-13 14:29:48,871 INFO - Submitting job to AWS Batch; 2018-06-13 14:29:48,871 INFO - dockerImage: quay.io/broadinstitute/viral-ngs; 2018-06-13 14:29:48,871 INFO - jobQueueArn: arn:aws:batch:us-east-1:140059717706:job-queue/funnel-job-queue; 2018-06-13 14:29:48,871 INFO - commandLine: set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; 2018-06-13 14:29:48,873 INFO - reconfiguredScript: #!/bin/bash; mk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:21308,Performance,concurren,concurrent,21308,"romwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:21378,Performance,concurren,concurrent,21378,"-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.ru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:21456,Performance,concurren,concurrent,21456,"stem-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:21781,Performance,concurren,concurrent,21781,cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at org.lerch.s3fs.S3SeekableByteChannel.<init>(S3SeekableByteChannel.java:50); 	at org.lerch.s3fs.S3FileSystemProvider.newByteChannel(S3FileSystemProvider,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:22988,Performance,load,loadBytes,22988, 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at org.lerch.s3fs.S3SeekableByteChannel.<init>(S3SeekableByteChannel.java:50); 	at org.lerch.s3fs.S3FileSystemProvider.newByteChannel(S3FileSystemProvider.java:359); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.Files.readAllBytes(Files.java:3152); 	at better.files.File.loadBytes(File.scala:171); 	at better.files.File.byteArray(File.scala:174); 	at better.files.File.contentAsString(File.scala:214); 	at cromwell.core.path.BetterFileMethods.contentAsString(BetterFileMethods.scala:112); 	at cromwell.core.path.BetterFileMethods.contentAsString$(BetterFileMethods.scala:112); 	at cromwell.filesystems.s3.S3Path.contentAsString(S3PathBuilder.scala:157); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString$(EvenBetterPathMethods.scala:90); 	at cromwell.filesystems.s3.S3Path.readContentAsString(S3PathBuilder.scala:157); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$4(NioFlow.scala:92); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:23737,Performance,concurren,concurrent,23737,akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at org.lerch.s3fs.S3SeekableByteChannel.<init>(S3SeekableByteChannel.java:50); 	at org.lerch.s3fs.S3FileSystemProvider.newByteChannel(S3FileSystemProvider.java:359); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.Files.readAllBytes(Files.java:3152); 	at better.files.File.loadBytes(File.scala:171); 	at better.files.File.byteArray(File.scala:174); 	at better.files.File.contentAsString(File.scala:214); 	at cromwell.core.path.BetterFileMethods.contentAsString(BetterFileMethods.scala:112); 	at cromwell.core.path.BetterFileMethods.contentAsString$(BetterFileMethods.scala:112); 	at cromwell.filesystems.s3.S3Path.contentAsString(S3PathBuilder.scala:157); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString$(EvenBetterPathMethods.scala:90); 	at cromwell.filesystems.s3.S3Path.readContentAsString(S3PathBuilder.scala:157); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$4(NioFlow.scala:92); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:23899,Performance,concurren,concurrent,23899,akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at org.lerch.s3fs.S3SeekableByteChannel.<init>(S3SeekableByteChannel.java:50); 	at org.lerch.s3fs.S3FileSystemProvider.newByteChannel(S3FileSystemProvider.java:359); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.Files.readAllBytes(Files.java:3152); 	at better.files.File.loadBytes(File.scala:171); 	at better.files.File.byteArray(File.scala:174); 	at better.files.File.contentAsString(File.scala:214); 	at cromwell.core.path.BetterFileMethods.contentAsString(BetterFileMethods.scala:112); 	at cromwell.core.path.BetterFileMethods.contentAsString$(BetterFileMethods.scala:112); 	at cromwell.filesystems.s3.S3Path.contentAsString(S3PathBuilder.scala:157); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString$(EvenBetterPathMethods.scala:90); 	at cromwell.filesystems.s3.S3Path.readContentAsString(S3PathBuilder.scala:157); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$4(NioFlow.scala:92); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:23961,Performance,concurren,concurrent,23961,akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at org.lerch.s3fs.S3SeekableByteChannel.<init>(S3SeekableByteChannel.java:50); 	at org.lerch.s3fs.S3FileSystemProvider.newByteChannel(S3FileSystemProvider.java:359); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.Files.readAllBytes(Files.java:3152); 	at better.files.File.loadBytes(File.scala:171); 	at better.files.File.byteArray(File.scala:174); 	at better.files.File.contentAsString(File.scala:214); 	at cromwell.core.path.BetterFileMethods.contentAsString(BetterFileMethods.scala:112); 	at cromwell.core.path.BetterFileMethods.contentAsString$(BetterFileMethods.scala:112); 	at cromwell.filesystems.s3.S3Path.contentAsString(S3PathBuilder.scala:157); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString$(EvenBetterPathMethods.scala:90); 	at cromwell.filesystems.s3.S3Path.readContentAsString(S3PathBuilder.scala:157); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$4(NioFlow.scala:92); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:24028,Performance,concurren,concurrent,24028,akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at org.lerch.s3fs.S3SeekableByteChannel.<init>(S3SeekableByteChannel.java:50); 	at org.lerch.s3fs.S3FileSystemProvider.newByteChannel(S3FileSystemProvider.java:359); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.Files.readAllBytes(Files.java:3152); 	at better.files.File.loadBytes(File.scala:171); 	at better.files.File.byteArray(File.scala:174); 	at better.files.File.contentAsString(File.scala:214); 	at cromwell.core.path.BetterFileMethods.contentAsString(BetterFileMethods.scala:112); 	at cromwell.core.path.BetterFileMethods.contentAsString$(BetterFileMethods.scala:112); 	at cromwell.filesystems.s3.S3Path.contentAsString(S3PathBuilder.scala:157); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString(EvenBetterPathMethods.scala:90); 	at cromwell.core.path.EvenBetterPathMethods.readContentAsString$(EvenBetterPathMethods.scala:90); 	at cromwell.filesystems.s3.S3Path.readContentAsString(S3PathBuilder.scala:157); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$4(NioFlow.scala:92); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:654); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); 	... 12 more; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:2519,Safety,Timeout,TimeoutException,2519,"2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Call-to-Backend assignments: demux_only.illumina_demux -> AWSBATCH; 2018-06-13 14:29:46,085 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:21335,Safety,recover,recoverWith,21335,"/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:5032,Security,hash,hash,5032,"or.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2018-06-13 14:29:47,368 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowExecutionActor-a67833cb-b894-4790-872f-9f3104cab60c [UUID(a67833cb)]: Starting demux_only.illumina_demux; 2018-06-13 14:29:48,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - Failed to hash /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at scala.util.Try$.apply(Try.scala:209);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:8843,Security,Hash,Hash,8843,"Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,009 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - a67833cb:demux_only.illumina_demux:-1:1: Hash error, disabling call caching for this job.; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:223,Testability,log,logs,223,"I am trying to adapt a WDL workflow originally developed for DNAnexus, to work in AWS Batch. I am running from ""develop"" branch on Mac, server mode. The workflow seems to run on AWS, but then fails when checking for output logs in S3... inputs: [demux_plus_inputs.json.txt](https://github.com/broadinstitute/cromwell/files/2099495/demux_plus_inputs.json.txt); workflow: [demux_only.wdl.txt](https://github.com/broadinstitute/cromwell/files/2099496/demux_only.wdl.txt); resource file: [workflows.zip](https://github.com/broadinstitute/cromwell/files/2099470/workflows.zip); config file, which shows some attempts to add the local filesystem, since I get an error about that: ; [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2099528/aws.conf.txt). ```; 2018-06-13 14:29:27,796 cromwell-system-akka.dispatchers.api-dispatcher-72 INFO - Unspecified type (Unspecified version) workflow a67833cb-b894-4790-872f-9f3104cab60c submitted; 2018-06-13 14:29:44,760 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-13 14:29:44,761 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(a67833cb-b894-4790-872f-9f3104cab60c); 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-a67833cb-b894-4790-872f-9f3104cab60c; 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:2975,Testability,log,logback,2975," MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Call-to-Backend assignments: demux_only.illumina_demux -> AWSBATCH; 2018-06-13 14:29:46,085 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3059,Testability,log,logback,3059,"emux_only.illumina_demux -> AWSBATCH; 2018-06-13 14:29:46,085 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Acto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3139,Testability,log,logback,3139,"ka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3205,Testability,log,logback,3205,"b)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3274,Testability,log,logback,3274,"ckend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3380,Testability,log,logback,3380,"-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3396,Testability,Log,Logger,3396,"rs.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3425,Testability,Log,Logger,3425,"her-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3454,Testability,log,logback,3454,"ailed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3470,Testability,Log,Logger,3470,the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(Fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3491,Testability,Log,Logger,3491,vailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.jav,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3520,Testability,log,logback,3520,utException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.F,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3536,Testability,Log,Logger,3536,ll; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$Work,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3570,Testability,Log,Logger,3570,current.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3599,Testability,log,logback,3599,; 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3615,Testability,Log,Logger,3615,ntry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3645,Testability,Log,Logger,3645,entBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoin,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3674,Testability,log,logback,3674,tBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3690,Testability,Log,Logger,3690,91); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorke,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:3702,Testability,Log,Logger,3702,om.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.logback.classic.Logger.warn(Logger.java:716); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.$anonfun$applyOrElse$2(Slf4jLogger.scala:75); 	at akka.event.slf4j.Slf4jLogger.withMdc(Slf4jLogger.scala:100); 	at akka.event.slf4j.Slf4jLogger$$anonfun$receive$1.applyOrElse(Slf4jLogger.scala:75); 	at akka.actor.Actor.aroundReceive(Actor.scala:514); 	at akka.actor.Actor.aroundReceive$(Actor.scala:512); 	at akka.event.slf4j.Slf4jLogger.aroundReceive(Slf4jLogger.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); 	at akka.actor.ActorCell.invoke(ActorCell.scala:496); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:13131,Testability,log,loglevel,13131,"NJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,013 cromwell-system-akka.dispatchers.backend-dispatcher-95 WARN - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Unrecognized runtime attribute keys: preemptible, dx_instance_type; 2018-06-13 14:29:48,218 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: `set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done`; 2018-06-13 14:29:48,871 INFO - Submitting job to AWS Batch; 2018-06-13 14:29:48,871 INFO - dockerImage: quay.io/broadinstitute/viral-ngs; 2018-06-13 14:29:48,871 INFO - jobQueueArn: arn:aws:batch:us-east-1:140059717706:job-queue/funnel-job-queue; 2018-06-13 14:29:48,871 INFO - commandLine: set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:13629,Testability,log,loglevel,13629,"s.backend-dispatcher-95 WARN - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Unrecognized runtime attribute keys: preemptible, dx_instance_type; 2018-06-13 14:29:48,218 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: `set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done`; 2018-06-13 14:29:48,871 INFO - Submitting job to AWS Batch; 2018-06-13 14:29:48,871 INFO - dockerImage: quay.io/broadinstitute/viral-ngs; 2018-06-13 14:29:48,871 INFO - jobQueueArn: arn:aws:batch:us-east-1:140059717706:job-queue/funnel-job-queue; 2018-06-13 14:29:48,871 INFO - commandLine: set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:14276,Testability,log,loglevel,14276,"ds to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done`; 2018-06-13 14:29:48,871 INFO - Submitting job to AWS Batch; 2018-06-13 14:29:48,871 INFO - dockerImage: quay.io/broadinstitute/viral-ngs; 2018-06-13 14:29:48,871 INFO - jobQueueArn: arn:aws:batch:us-east-1:140059717706:job-queue/funnel-job-queue; 2018-06-13 14:29:48,871 INFO - commandLine: set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; 2018-06-13 14:29:48,873 INFO - reconfiguredScript: #!/bin/bash; mkdir -p /cromwell_root; #!/bin/bash. cd /cromwell_root; tmpDir=`mkdir -p ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1"" && echo ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:14774,Testability,log,loglevel,14774,"$bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done`; 2018-06-13 14:29:48,871 INFO - Submitting job to AWS Batch; 2018-06-13 14:29:48,871 INFO - dockerImage: quay.io/broadinstitute/viral-ngs; 2018-06-13 14:29:48,871 INFO - jobQueueArn: arn:aws:batch:us-east-1:140059717706:job-queue/funnel-job-queue; 2018-06-13 14:29:48,871 INFO - commandLine: set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; 2018-06-13 14:29:48,873 INFO - reconfiguredScript: #!/bin/bash; mkdir -p /cromwell_root; #!/bin/bash. cd /cromwell_root; tmpDir=`mkdir -p ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1"" && echo ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); (; cd /cromwell_root. set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% mem",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:15680,Testability,log,loglevel,15680,"-minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; 2018-06-13 14:29:48,873 INFO - reconfiguredScript: #!/bin/bash; mkdir -p /cromwell_root; #!/bin/bash. cd /cromwell_root; tmpDir=`mkdir -p ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1"" && echo ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); (; cd /cromwell_root. set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; ) > '/cromwell_root/illumina_demux-stdout.log' 2> '/cromwell_root/illumina_demux-stderr.log'; echo $? > /cromwell_root/illumina_demux-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell_root; sync; # m",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:16178,Testability,log,loglevel,16178,"cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1"" && echo ""/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/tmp.e5175ec1""`; chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); (; cd /cromwell_root. set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; ) > '/cromwell_root/illumina_demux-stdout.log' 2> '/cromwell_root/illumina_demux-stderr.log'; echo $? > /cromwell_root/illumina_demux-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell_root; sync; # make the directory which will keep the matching files; mkdir /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:16367,Testability,log,log,16367,"IR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell_root. ); (; cd /cromwell_root. set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; ) > '/cromwell_root/illumina_demux-stdout.log' 2> '/cromwell_root/illumina_demux-stderr.log'; echo $? > /cromwell_root/illumina_demux-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell_root; sync; # make the directory which will keep the matching files; mkdir /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 2> /dev/null )",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:16413,Testability,log,log,16413,"ell_root. ); (; cd /cromwell_root. set -ex -o pipefail. if [ -d /mnt/tmp ]; then; TMPDIR=/mnt/tmp; fi; FLOWCELL_DIR=$(mktemp -d). read_utils.py extract_tarball \; /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 $FLOWCELL_DIR \; --loglevel=DEBUG. # find 95% memory; mem_in_mb=`/opt/viral-ngs/source/docker/mem_in_mb_95.sh`. # note that we are intentionally setting --threads to about 2x the core; # count. seems to still provide speed benefit (over 1x) when doing so.; illumina.py illumina_demux \; $FLOWCELL_DIR \; 1 \; . \; \; \; --outMetrics=metrics.txt \; --commonBarcodes=barcodes.txt \; \; \; --max_mismatches=1 \; \; \; \; --minimum_quality=10 \; \; --JVMmemory=""$mem_in_mb""m \; --threads=64 \; --compression_level=5 \; --loglevel=DEBUG. rm -f Unmatched.bam; for bam in *.bam; do; fastqc_out=$(basename $bam .bam)_fastqc.html; reports.py fastqc $bam $fastqc_out; done; ) > '/cromwell_root/illumina_demux-stdout.log' 2> '/cromwell_root/illumina_demux-stderr.log'; echo $? > /cromwell_root/illumina_demux-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /cromwell_root; find . -type d -empty -print | xargs -I % touch %/.file; ); (; cd /cromwell_root; sync; # make the directory which will keep the matching files; mkdir /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385. # create the glob control file that will allow for the globbing to succeed even if there is 0 match; echo ""This file is used by Cromwell to allow for globs that would not match any file.; By its presence it works around the limitation of some backends that do not allow empty globs.; Regardless of the outcome of the glob, this file will not be part of the final list of globbed files."" > /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385/cromwell_glob_control_file. # symlink all the files into the glob directory; ( ln -L *.bam /cromwell_root/glob-3bcbe4e7489c90f75e0523ac6f3a9385 2> /dev/null ) || ( ln *.bam /cromwell_root/glob-3bcbe4e7489c9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:19320,Testability,log,log,19320,".html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 2> /dev/null ) || ( ln *_fastqc.html /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:19493,Testability,log,log,19493,"0748eb4a7f63 ). # list all the files (except the control file) that match the glob into a file called glob-[md5 of glob].list; ls -1 /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63 | grep -v cromwell_glob_control_file > /cromwell_root/glob-9c776de0acb8005c55560748eb4a7f63.list. ); mv /cromwell_root/illumina_demux-rc.txt.tmp /cromwell_root/illumina_demux-rc.txt. echo ""MIME-Version: 1.0; Content-Type: multipart/alternative; boundary=""bdbdba51eee253d75fcf6d84ee981016"". --bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""rc.txt""; ""; cat /cromwell_root/illumina_demux-rc.txt; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stdout.txt""; ""; cat /cromwell_root/illumina_demux-stdout.log; echo ""--bdbdba51eee253d75fcf6d84ee981016; Content-Type: text/plain; Content-Disposition: attachment; filename=""stderr.txt""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3774:20421,Testability,log,log,20421,"""; ""; cat /cromwell_root/illumina_demux-stderr.log; echo ""--bdbdba51eee253d75fcf6d84ee981016--"". 2018-06-13 14:29:54,112 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: job id: e9e747cf-2da8-4117-aedb-ac68d83b7c70; 2018-06-13 14:29:54,182 cromwell-system-akka.dispatchers.backend-dispatcher-94 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from - to Initializing; 2018-06-13 14:32:37,206 cromwell-system-akka.dispatchers.backend-dispatcher-95 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Initializing to Running; 2018-06-13 14:41:13,832 INFO - Job Complete. Exit code: 0; 2018-06-13 14:41:13,833 INFO - Output path: s3://atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-stdout.log; 2018-06-13 14:41:14,088 cromwell-system-akka.dispatchers.backend-dispatcher-112 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(a67833cb)demux_only.illumina_demux:NA:1]: Status change from Running to Succeeded; 2018-06-13 14:41:15,905 cromwell-system-akka.dispatchers.engine-dispatcher-37 ERROR - WorkflowManagerActor Workflow a67833cb-b894-4790-872f-9f3104cab60c failed (during ExecutingWorkflowState): cromwell.core.CromwellFatalException: java.nio.file.NoSuchFileException: target not exists: s3://s3.amazonaws.com/atbiofx-cromwell/cromwell-execution/demux_only/a67833cb-b894-4790-872f-9f3104cab60c/call-illumina_demux/illumina_demux-rc.txt; 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774
https://github.com/broadinstitute/cromwell/issues/3776:673,Availability,error,error,673,"On Wed, Jun 13, 2018 at 4:08 PM Dan Billings <danb@broadinstitute.org> wrote:; I think we can patch it on our end, doesn't sound too bad. OK, thanks and sorry again.; ; On Wed, Jun 13, 2018 at 3:54 PM Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Ok good to know, doesn't sound too hard to patch but we'd also have to patch the previous release if we don't want it to be broken there too. I'm adding Dan and Ruchi who can decide on what to do. Thibault. On Wed, Jun 13, 2018 at 3:43 PM Aaron Kemp <kemp@google.com> wrote:; On Wed, Jun 13, 2018 at 3:38 PM, Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Thank you,. I just saw a bunch of those today:. PAPI error code 3. Execution failed: creating instance: inserting instance: Invalid value for field 'resource.disks[1].initializeParams.diskType': 'zones/us-central1-b/diskTypes/PERSISTENT_SSD'. The referenced diskType resource cannot be found. I don't think I've touched anything related to disk types. Is this a quota issue ?. No this was a change on our side. There was a bug in the v2alph1 backend where it was ignoring the disk type requested. I fixed it and pushed it out. The diskType in v2alpha1 is the standard GCE names (so in this case, pd-ssd) - in v1 they were a custom enumeration. This will probably require a cromwell change depending on where that value is coming from (eg, maybe it's in the WDL file itself?); I think we're translating it from WDL-speak. Where can I find the ""standard GCE names"" (e.g. ""pd-ssd""). ""gcloud compute disk-types list"" shows the full per-zone list, but in practice it's really just one of ""pd-ssd, pd-standard or local-ssd"".; ; and would it become ""zones/us-central1-b/diskTypes/pd-ssd"" or just ""pd-ssd"" ?. Pipelines adds the zonal resource chunk based on the zone we choose from the set you give us. So you just say 'pd-ssd' and we staple on the appropriate zone to the front. Aaron",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776
https://github.com/broadinstitute/cromwell/issues/3776:94,Deployability,patch,patch,94,"On Wed, Jun 13, 2018 at 4:08 PM Dan Billings <danb@broadinstitute.org> wrote:; I think we can patch it on our end, doesn't sound too bad. OK, thanks and sorry again.; ; On Wed, Jun 13, 2018 at 3:54 PM Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Ok good to know, doesn't sound too hard to patch but we'd also have to patch the previous release if we don't want it to be broken there too. I'm adding Dan and Ruchi who can decide on what to do. Thibault. On Wed, Jun 13, 2018 at 3:43 PM Aaron Kemp <kemp@google.com> wrote:; On Wed, Jun 13, 2018 at 3:38 PM, Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Thank you,. I just saw a bunch of those today:. PAPI error code 3. Execution failed: creating instance: inserting instance: Invalid value for field 'resource.disks[1].initializeParams.diskType': 'zones/us-central1-b/diskTypes/PERSISTENT_SSD'. The referenced diskType resource cannot be found. I don't think I've touched anything related to disk types. Is this a quota issue ?. No this was a change on our side. There was a bug in the v2alph1 backend where it was ignoring the disk type requested. I fixed it and pushed it out. The diskType in v2alpha1 is the standard GCE names (so in this case, pd-ssd) - in v1 they were a custom enumeration. This will probably require a cromwell change depending on where that value is coming from (eg, maybe it's in the WDL file itself?); I think we're translating it from WDL-speak. Where can I find the ""standard GCE names"" (e.g. ""pd-ssd""). ""gcloud compute disk-types list"" shows the full per-zone list, but in practice it's really just one of ""pd-ssd, pd-standard or local-ssd"".; ; and would it become ""zones/us-central1-b/diskTypes/pd-ssd"" or just ""pd-ssd"" ?. Pipelines adds the zonal resource chunk based on the zone we choose from the set you give us. So you just say 'pd-ssd' and we staple on the appropriate zone to the front. Aaron",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776
https://github.com/broadinstitute/cromwell/issues/3776:299,Deployability,patch,patch,299,"On Wed, Jun 13, 2018 at 4:08 PM Dan Billings <danb@broadinstitute.org> wrote:; I think we can patch it on our end, doesn't sound too bad. OK, thanks and sorry again.; ; On Wed, Jun 13, 2018 at 3:54 PM Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Ok good to know, doesn't sound too hard to patch but we'd also have to patch the previous release if we don't want it to be broken there too. I'm adding Dan and Ruchi who can decide on what to do. Thibault. On Wed, Jun 13, 2018 at 3:43 PM Aaron Kemp <kemp@google.com> wrote:; On Wed, Jun 13, 2018 at 3:38 PM, Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Thank you,. I just saw a bunch of those today:. PAPI error code 3. Execution failed: creating instance: inserting instance: Invalid value for field 'resource.disks[1].initializeParams.diskType': 'zones/us-central1-b/diskTypes/PERSISTENT_SSD'. The referenced diskType resource cannot be found. I don't think I've touched anything related to disk types. Is this a quota issue ?. No this was a change on our side. There was a bug in the v2alph1 backend where it was ignoring the disk type requested. I fixed it and pushed it out. The diskType in v2alpha1 is the standard GCE names (so in this case, pd-ssd) - in v1 they were a custom enumeration. This will probably require a cromwell change depending on where that value is coming from (eg, maybe it's in the WDL file itself?); I think we're translating it from WDL-speak. Where can I find the ""standard GCE names"" (e.g. ""pd-ssd""). ""gcloud compute disk-types list"" shows the full per-zone list, but in practice it's really just one of ""pd-ssd, pd-standard or local-ssd"".; ; and would it become ""zones/us-central1-b/diskTypes/pd-ssd"" or just ""pd-ssd"" ?. Pipelines adds the zonal resource chunk based on the zone we choose from the set you give us. So you just say 'pd-ssd' and we staple on the appropriate zone to the front. Aaron",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776
https://github.com/broadinstitute/cromwell/issues/3776:327,Deployability,patch,patch,327,"On Wed, Jun 13, 2018 at 4:08 PM Dan Billings <danb@broadinstitute.org> wrote:; I think we can patch it on our end, doesn't sound too bad. OK, thanks and sorry again.; ; On Wed, Jun 13, 2018 at 3:54 PM Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Ok good to know, doesn't sound too hard to patch but we'd also have to patch the previous release if we don't want it to be broken there too. I'm adding Dan and Ruchi who can decide on what to do. Thibault. On Wed, Jun 13, 2018 at 3:43 PM Aaron Kemp <kemp@google.com> wrote:; On Wed, Jun 13, 2018 at 3:38 PM, Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Thank you,. I just saw a bunch of those today:. PAPI error code 3. Execution failed: creating instance: inserting instance: Invalid value for field 'resource.disks[1].initializeParams.diskType': 'zones/us-central1-b/diskTypes/PERSISTENT_SSD'. The referenced diskType resource cannot be found. I don't think I've touched anything related to disk types. Is this a quota issue ?. No this was a change on our side. There was a bug in the v2alph1 backend where it was ignoring the disk type requested. I fixed it and pushed it out. The diskType in v2alpha1 is the standard GCE names (so in this case, pd-ssd) - in v1 they were a custom enumeration. This will probably require a cromwell change depending on where that value is coming from (eg, maybe it's in the WDL file itself?); I think we're translating it from WDL-speak. Where can I find the ""standard GCE names"" (e.g. ""pd-ssd""). ""gcloud compute disk-types list"" shows the full per-zone list, but in practice it's really just one of ""pd-ssd, pd-standard or local-ssd"".; ; and would it become ""zones/us-central1-b/diskTypes/pd-ssd"" or just ""pd-ssd"" ?. Pipelines adds the zonal resource chunk based on the zone we choose from the set you give us. So you just say 'pd-ssd' and we staple on the appropriate zone to the front. Aaron",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776
https://github.com/broadinstitute/cromwell/issues/3776:346,Deployability,release,release,346,"On Wed, Jun 13, 2018 at 4:08 PM Dan Billings <danb@broadinstitute.org> wrote:; I think we can patch it on our end, doesn't sound too bad. OK, thanks and sorry again.; ; On Wed, Jun 13, 2018 at 3:54 PM Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Ok good to know, doesn't sound too hard to patch but we'd also have to patch the previous release if we don't want it to be broken there too. I'm adding Dan and Ruchi who can decide on what to do. Thibault. On Wed, Jun 13, 2018 at 3:43 PM Aaron Kemp <kemp@google.com> wrote:; On Wed, Jun 13, 2018 at 3:38 PM, Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Thank you,. I just saw a bunch of those today:. PAPI error code 3. Execution failed: creating instance: inserting instance: Invalid value for field 'resource.disks[1].initializeParams.diskType': 'zones/us-central1-b/diskTypes/PERSISTENT_SSD'. The referenced diskType resource cannot be found. I don't think I've touched anything related to disk types. Is this a quota issue ?. No this was a change on our side. There was a bug in the v2alph1 backend where it was ignoring the disk type requested. I fixed it and pushed it out. The diskType in v2alpha1 is the standard GCE names (so in this case, pd-ssd) - in v1 they were a custom enumeration. This will probably require a cromwell change depending on where that value is coming from (eg, maybe it's in the WDL file itself?); I think we're translating it from WDL-speak. Where can I find the ""standard GCE names"" (e.g. ""pd-ssd""). ""gcloud compute disk-types list"" shows the full per-zone list, but in practice it's really just one of ""pd-ssd, pd-standard or local-ssd"".; ; and would it become ""zones/us-central1-b/diskTypes/pd-ssd"" or just ""pd-ssd"" ?. Pipelines adds the zonal resource chunk based on the zone we choose from the set you give us. So you just say 'pd-ssd' and we staple on the appropriate zone to the front. Aaron",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776
https://github.com/broadinstitute/cromwell/issues/3776:1721,Deployability,Pipeline,Pipelines,1721,"On Wed, Jun 13, 2018 at 4:08 PM Dan Billings <danb@broadinstitute.org> wrote:; I think we can patch it on our end, doesn't sound too bad. OK, thanks and sorry again.; ; On Wed, Jun 13, 2018 at 3:54 PM Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Ok good to know, doesn't sound too hard to patch but we'd also have to patch the previous release if we don't want it to be broken there too. I'm adding Dan and Ruchi who can decide on what to do. Thibault. On Wed, Jun 13, 2018 at 3:43 PM Aaron Kemp <kemp@google.com> wrote:; On Wed, Jun 13, 2018 at 3:38 PM, Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Thank you,. I just saw a bunch of those today:. PAPI error code 3. Execution failed: creating instance: inserting instance: Invalid value for field 'resource.disks[1].initializeParams.diskType': 'zones/us-central1-b/diskTypes/PERSISTENT_SSD'. The referenced diskType resource cannot be found. I don't think I've touched anything related to disk types. Is this a quota issue ?. No this was a change on our side. There was a bug in the v2alph1 backend where it was ignoring the disk type requested. I fixed it and pushed it out. The diskType in v2alpha1 is the standard GCE names (so in this case, pd-ssd) - in v1 they were a custom enumeration. This will probably require a cromwell change depending on where that value is coming from (eg, maybe it's in the WDL file itself?); I think we're translating it from WDL-speak. Where can I find the ""standard GCE names"" (e.g. ""pd-ssd""). ""gcloud compute disk-types list"" shows the full per-zone list, but in practice it's really just one of ""pd-ssd, pd-standard or local-ssd"".; ; and would it become ""zones/us-central1-b/diskTypes/pd-ssd"" or just ""pd-ssd"" ?. Pipelines adds the zonal resource chunk based on the zone we choose from the set you give us. So you just say 'pd-ssd' and we staple on the appropriate zone to the front. Aaron",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776
https://github.com/broadinstitute/cromwell/issues/3776:1309,Integrability,depend,depending,1309,"On Wed, Jun 13, 2018 at 4:08 PM Dan Billings <danb@broadinstitute.org> wrote:; I think we can patch it on our end, doesn't sound too bad. OK, thanks and sorry again.; ; On Wed, Jun 13, 2018 at 3:54 PM Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Ok good to know, doesn't sound too hard to patch but we'd also have to patch the previous release if we don't want it to be broken there too. I'm adding Dan and Ruchi who can decide on what to do. Thibault. On Wed, Jun 13, 2018 at 3:43 PM Aaron Kemp <kemp@google.com> wrote:; On Wed, Jun 13, 2018 at 3:38 PM, Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Thank you,. I just saw a bunch of those today:. PAPI error code 3. Execution failed: creating instance: inserting instance: Invalid value for field 'resource.disks[1].initializeParams.diskType': 'zones/us-central1-b/diskTypes/PERSISTENT_SSD'. The referenced diskType resource cannot be found. I don't think I've touched anything related to disk types. Is this a quota issue ?. No this was a change on our side. There was a bug in the v2alph1 backend where it was ignoring the disk type requested. I fixed it and pushed it out. The diskType in v2alpha1 is the standard GCE names (so in this case, pd-ssd) - in v1 they were a custom enumeration. This will probably require a cromwell change depending on where that value is coming from (eg, maybe it's in the WDL file itself?); I think we're translating it from WDL-speak. Where can I find the ""standard GCE names"" (e.g. ""pd-ssd""). ""gcloud compute disk-types list"" shows the full per-zone list, but in practice it's really just one of ""pd-ssd, pd-standard or local-ssd"".; ; and would it become ""zones/us-central1-b/diskTypes/pd-ssd"" or just ""pd-ssd"" ?. Pipelines adds the zonal resource chunk based on the zone we choose from the set you give us. So you just say 'pd-ssd' and we staple on the appropriate zone to the front. Aaron",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776
https://github.com/broadinstitute/cromwell/issues/3781:709,Deployability,configurat,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3781
https://github.com/broadinstitute/cromwell/issues/3781:709,Modifiability,config,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3781
https://github.com/broadinstitute/cromwell/issues/3781:754,Security,PASSWORD,PASSWORDS,754,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3781
https://github.com/broadinstitute/cromwell/issues/3781:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3781
https://github.com/broadinstitute/cromwell/issues/3782:29,Performance,cache,cache-within-workflow,29,Seeing a lot of centaur `cwl-cache-within-workflow` tests failing. It expects a cache hit and is getting a cache miss.; Just putting it out there whenever someone feels like reducing travis flakiness.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3782
https://github.com/broadinstitute/cromwell/issues/3782:80,Performance,cache,cache,80,Seeing a lot of centaur `cwl-cache-within-workflow` tests failing. It expects a cache hit and is getting a cache miss.; Just putting it out there whenever someone feels like reducing travis flakiness.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3782
https://github.com/broadinstitute/cromwell/issues/3782:107,Performance,cache,cache,107,Seeing a lot of centaur `cwl-cache-within-workflow` tests failing. It expects a cache hit and is getting a cache miss.; Just putting it out there whenever someone feels like reducing travis flakiness.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3782
https://github.com/broadinstitute/cromwell/issues/3782:52,Testability,test,tests,52,Seeing a lot of centaur `cwl-cache-within-workflow` tests failing. It expects a cache hit and is getting a cache miss.; Just putting it out there whenever someone feels like reducing travis flakiness.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3782
https://github.com/broadinstitute/cromwell/issues/3784:91,Availability,echo,echo,91,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784
https://github.com/broadinstitute/cromwell/issues/3784:319,Availability,echo,echo,319,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784
https://github.com/broadinstitute/cromwell/issues/3784:335,Availability,echo,echo,335,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784
https://github.com/broadinstitute/cromwell/issues/3784:584,Availability,echo,echo,584,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784
https://github.com/broadinstitute/cromwell/issues/3784:702,Availability,error,error,702,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784
https://github.com/broadinstitute/cromwell/issues/3784:768,Availability,error,errors,768,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784
https://github.com/broadinstitute/cromwell/issues/3784:26,Deployability,configurat,configuration,26,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784
https://github.com/broadinstitute/cromwell/issues/3784:1096,Integrability,contract,contract,1096,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784
https://github.com/broadinstitute/cromwell/issues/3784:26,Modifiability,config,configuration,26,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784
https://github.com/broadinstitute/cromwell/issues/3784:403,Modifiability,config,config,403,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784
https://github.com/broadinstitute/cromwell/issues/3785:445,Availability,error,error,445,"I have a task with an input 'File' (WDL concept) that is actually a folder. I did not know what would happen, as I could not find explanation of the concept in either wdl or cromwell docs. . For the 'File's that are actually files, cromwell reports ```Localisation via hard-link has failed ... invalid cross device link``` as expected (different disks), and then seems to successfully soft-link (as expected). For the folder, the same hard link error appears, but then there is no soft link error, and the folder is (recursively) copied. The folder hard link would also fail due to folders not being hard-linkable. But the soft-link should probably succeed. I am running an SGE backend, with no modification of the 'localisation' settings https://cromwell.readthedocs.io/en/latest/backends/HPC/#shared-filesystem. What is the expected behaviour? Should I avoid using folders for WDL 'File's? Any advice would be appreciated. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3785
https://github.com/broadinstitute/cromwell/issues/3785:491,Availability,error,error,491,"I have a task with an input 'File' (WDL concept) that is actually a folder. I did not know what would happen, as I could not find explanation of the concept in either wdl or cromwell docs. . For the 'File's that are actually files, cromwell reports ```Localisation via hard-link has failed ... invalid cross device link``` as expected (different disks), and then seems to successfully soft-link (as expected). For the folder, the same hard link error appears, but then there is no soft link error, and the folder is (recursively) copied. The folder hard link would also fail due to folders not being hard-linkable. But the soft-link should probably succeed. I am running an SGE backend, with no modification of the 'localisation' settings https://cromwell.readthedocs.io/en/latest/backends/HPC/#shared-filesystem. What is the expected behaviour? Should I avoid using folders for WDL 'File's? Any advice would be appreciated. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3785
https://github.com/broadinstitute/cromwell/issues/3785:855,Safety,avoid,avoid,855,"I have a task with an input 'File' (WDL concept) that is actually a folder. I did not know what would happen, as I could not find explanation of the concept in either wdl or cromwell docs. . For the 'File's that are actually files, cromwell reports ```Localisation via hard-link has failed ... invalid cross device link``` as expected (different disks), and then seems to successfully soft-link (as expected). For the folder, the same hard link error appears, but then there is no soft link error, and the folder is (recursively) copied. The folder hard link would also fail due to folders not being hard-linkable. But the soft-link should probably succeed. I am running an SGE backend, with no modification of the 'localisation' settings https://cromwell.readthedocs.io/en/latest/backends/HPC/#shared-filesystem. What is the expected behaviour? Should I avoid using folders for WDL 'File's? Any advice would be appreciated. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3785
https://github.com/broadinstitute/cromwell/issues/3786:709,Deployability,configurat,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786
https://github.com/broadinstitute/cromwell/issues/3786:709,Modifiability,config,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786
https://github.com/broadinstitute/cromwell/issues/3786:754,Security,PASSWORD,PASSWORDS,754,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786
https://github.com/broadinstitute/cromwell/issues/3786:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786
https://github.com/broadinstitute/cromwell/issues/3787:134,Performance,optimiz,optimizations,134,It would be great to bring over the optional localization feature from the google backend: https://cromwell.readthedocs.io/en/develop/optimizations/FileLocalization/#the-localization_optional-optimization,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3787
https://github.com/broadinstitute/cromwell/issues/3787:192,Performance,optimiz,optimization,192,It would be great to bring over the optional localization feature from the google backend: https://cromwell.readthedocs.io/en/develop/optimizations/FileLocalization/#the-localization_optional-optimization,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3787
https://github.com/broadinstitute/cromwell/pull/3788:39,Deployability,release,releases,39,"Since ReadTheDocs follows develop, not releases, we should be careful not to over-advertise any as-yet-unreleased features",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3788
https://github.com/broadinstitute/cromwell/issues/3790:177,Availability,error,error,177,The workflow; ```; version 1.0. workflow member_access {; Object myObj = object { an_int: 5 }; if (myObj.an_int == 10) {; Boolean asdf = true; }; }; ```; fails to validate with error; ```; Failed to create wom Graph (reason 1 of 1):; Failed to process workflow definition 'member_access' (reason 1 of 1):; Invalid type for condition variable: Any; ```; It appears that the type checker is not correctly evaluating the type of values reached via member access.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3790
https://github.com/broadinstitute/cromwell/issues/3790:333,Modifiability,variab,variable,333,The workflow; ```; version 1.0. workflow member_access {; Object myObj = object { an_int: 5 }; if (myObj.an_int == 10) {; Boolean asdf = true; }; }; ```; fails to validate with error; ```; Failed to create wom Graph (reason 1 of 1):; Failed to process workflow definition 'member_access' (reason 1 of 1):; Invalid type for condition variable: Any; ```; It appears that the type checker is not correctly evaluating the type of values reached via member access.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3790
https://github.com/broadinstitute/cromwell/issues/3790:163,Security,validat,validate,163,The workflow; ```; version 1.0. workflow member_access {; Object myObj = object { an_int: 5 }; if (myObj.an_int == 10) {; Boolean asdf = true; }; }; ```; fails to validate with error; ```; Failed to create wom Graph (reason 1 of 1):; Failed to process workflow definition 'member_access' (reason 1 of 1):; Invalid type for condition variable: Any; ```; It appears that the type checker is not correctly evaluating the type of values reached via member access.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3790
https://github.com/broadinstitute/cromwell/issues/3790:452,Security,access,access,452,The workflow; ```; version 1.0. workflow member_access {; Object myObj = object { an_int: 5 }; if (myObj.an_int == 10) {; Boolean asdf = true; }; }; ```; fails to validate with error; ```; Failed to create wom Graph (reason 1 of 1):; Failed to process workflow definition 'member_access' (reason 1 of 1):; Invalid type for condition variable: Any; ```; It appears that the type checker is not correctly evaluating the type of values reached via member access.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3790
https://github.com/broadinstitute/cromwell/issues/3791:40,Availability,error,errors,40,"Hi,; I'm seeing a number of compilation errors with head dev etc. I've attached the output from debug. Scala version; scala-2.12.0-1.noarch. Java version; java 10.0.1 2018-04-17; Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10); Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode). CentOS Linux release 7.5.1804 (Core). Error output attached. Any thoughts ? Could be something obvious as I'm new to scala.; Thanks. [comp.log](https://github.com/broadinstitute/cromwell/files/2109138/comp.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3791
https://github.com/broadinstitute/cromwell/issues/3791:343,Availability,Error,Error,343,"Hi,; I'm seeing a number of compilation errors with head dev etc. I've attached the output from debug. Scala version; scala-2.12.0-1.noarch. Java version; java 10.0.1 2018-04-17; Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10); Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode). CentOS Linux release 7.5.1804 (Core). Error output attached. Any thoughts ? Could be something obvious as I'm new to scala.; Thanks. [comp.log](https://github.com/broadinstitute/cromwell/files/2109138/comp.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3791
https://github.com/broadinstitute/cromwell/issues/3791:318,Deployability,release,release,318,"Hi,; I'm seeing a number of compilation errors with head dev etc. I've attached the output from debug. Scala version; scala-2.12.0-1.noarch. Java version; java 10.0.1 2018-04-17; Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10); Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode). CentOS Linux release 7.5.1804 (Core). Error output attached. Any thoughts ? Could be something obvious as I'm new to scala.; Thanks. [comp.log](https://github.com/broadinstitute/cromwell/files/2109138/comp.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3791
https://github.com/broadinstitute/cromwell/issues/3791:444,Testability,log,log,444,"Hi,; I'm seeing a number of compilation errors with head dev etc. I've attached the output from debug. Scala version; scala-2.12.0-1.noarch. Java version; java 10.0.1 2018-04-17; Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10); Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode). CentOS Linux release 7.5.1804 (Core). Error output attached. Any thoughts ? Could be something obvious as I'm new to scala.; Thanks. [comp.log](https://github.com/broadinstitute/cromwell/files/2109138/comp.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3791
https://github.com/broadinstitute/cromwell/issues/3791:511,Testability,log,log,511,"Hi,; I'm seeing a number of compilation errors with head dev etc. I've attached the output from debug. Scala version; scala-2.12.0-1.noarch. Java version; java 10.0.1 2018-04-17; Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10); Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode). CentOS Linux release 7.5.1804 (Core). Error output attached. Any thoughts ? Could be something obvious as I'm new to scala.; Thanks. [comp.log](https://github.com/broadinstitute/cromwell/files/2109138/comp.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3791
https://github.com/broadinstitute/cromwell/issues/3793:19,Performance,load,load,19,Playbooks need new load balancer Cromwell info.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3793
https://github.com/broadinstitute/cromwell/pull/3794:22,Availability,down,down,22,"Closes #3790 . I went down the ""allow WomAnyType"" values route for now, to solve the general case of:; ```; Object o = read_json(some_file); scatter (x in o.blah) {; ...; }; ```. Ideally directing people to `struct`s will mean this problem goes away, but in the mean time, we'll need to handle scatters over `WomAnyType`s (at least in the static analysis).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3794
https://github.com/broadinstitute/cromwell/pull/3794:57,Integrability,rout,route,57,"Closes #3790 . I went down the ""allow WomAnyType"" values route for now, to solve the general case of:; ```; Object o = read_json(some_file); scatter (x in o.blah) {; ...; }; ```. Ideally directing people to `struct`s will mean this problem goes away, but in the mean time, we'll need to handle scatters over `WomAnyType`s (at least in the static analysis).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3794
https://github.com/broadinstitute/cromwell/issues/3795:24,Security,access,access,24,With the present design access to the workflow store would be contentious among multiple worker Cromwells. #3761 addresses the issue of a single Cromwell deadlocking itself but would not address the issue of multiple Cromwells deadlocking each other.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3795
https://github.com/broadinstitute/cromwell/issues/3796:161,Deployability,upgrade,upgrade,161,"When writing out workflow elements, `WdlWriter` currently uses a ""comprehensible enough"" ordering that approximates how most people write their WDLs - we should upgrade to using the true graph ordering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3796
https://github.com/broadinstitute/cromwell/issues/3798:108,Performance,optimiz,optimizations,108,"Use case: Once there is support for [optional file localization](https://cromwell.readthedocs.io/en/develop/optimizations/FileLocalization/#the-localization_optional-optimization), presumably every future version of Cromwell (v33 and beyond) will always not localize files as it contains logic to call cache without localization. However, a WDL author would need to to test whether a task w/`optional_localization` enabled for file inputs has the ability to run successfully in both cases of having access to file localization or file streaming. AC: Regardless of whether a file is marked for localization or not, override that logic on the workflow level so the workflow submitter has the option to enforce file localization, even if the WDL author has written the task to be compatible with remote files and the backend supports that functionality.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3798
https://github.com/broadinstitute/cromwell/issues/3798:166,Performance,optimiz,optimization,166,"Use case: Once there is support for [optional file localization](https://cromwell.readthedocs.io/en/develop/optimizations/FileLocalization/#the-localization_optional-optimization), presumably every future version of Cromwell (v33 and beyond) will always not localize files as it contains logic to call cache without localization. However, a WDL author would need to to test whether a task w/`optional_localization` enabled for file inputs has the ability to run successfully in both cases of having access to file localization or file streaming. AC: Regardless of whether a file is marked for localization or not, override that logic on the workflow level so the workflow submitter has the option to enforce file localization, even if the WDL author has written the task to be compatible with remote files and the backend supports that functionality.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3798
https://github.com/broadinstitute/cromwell/issues/3798:302,Performance,cache,cache,302,"Use case: Once there is support for [optional file localization](https://cromwell.readthedocs.io/en/develop/optimizations/FileLocalization/#the-localization_optional-optimization), presumably every future version of Cromwell (v33 and beyond) will always not localize files as it contains logic to call cache without localization. However, a WDL author would need to to test whether a task w/`optional_localization` enabled for file inputs has the ability to run successfully in both cases of having access to file localization or file streaming. AC: Regardless of whether a file is marked for localization or not, override that logic on the workflow level so the workflow submitter has the option to enforce file localization, even if the WDL author has written the task to be compatible with remote files and the backend supports that functionality.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3798
https://github.com/broadinstitute/cromwell/issues/3798:499,Security,access,access,499,"Use case: Once there is support for [optional file localization](https://cromwell.readthedocs.io/en/develop/optimizations/FileLocalization/#the-localization_optional-optimization), presumably every future version of Cromwell (v33 and beyond) will always not localize files as it contains logic to call cache without localization. However, a WDL author would need to to test whether a task w/`optional_localization` enabled for file inputs has the ability to run successfully in both cases of having access to file localization or file streaming. AC: Regardless of whether a file is marked for localization or not, override that logic on the workflow level so the workflow submitter has the option to enforce file localization, even if the WDL author has written the task to be compatible with remote files and the backend supports that functionality.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3798
https://github.com/broadinstitute/cromwell/issues/3798:288,Testability,log,logic,288,"Use case: Once there is support for [optional file localization](https://cromwell.readthedocs.io/en/develop/optimizations/FileLocalization/#the-localization_optional-optimization), presumably every future version of Cromwell (v33 and beyond) will always not localize files as it contains logic to call cache without localization. However, a WDL author would need to to test whether a task w/`optional_localization` enabled for file inputs has the ability to run successfully in both cases of having access to file localization or file streaming. AC: Regardless of whether a file is marked for localization or not, override that logic on the workflow level so the workflow submitter has the option to enforce file localization, even if the WDL author has written the task to be compatible with remote files and the backend supports that functionality.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3798
https://github.com/broadinstitute/cromwell/issues/3798:369,Testability,test,test,369,"Use case: Once there is support for [optional file localization](https://cromwell.readthedocs.io/en/develop/optimizations/FileLocalization/#the-localization_optional-optimization), presumably every future version of Cromwell (v33 and beyond) will always not localize files as it contains logic to call cache without localization. However, a WDL author would need to to test whether a task w/`optional_localization` enabled for file inputs has the ability to run successfully in both cases of having access to file localization or file streaming. AC: Regardless of whether a file is marked for localization or not, override that logic on the workflow level so the workflow submitter has the option to enforce file localization, even if the WDL author has written the task to be compatible with remote files and the backend supports that functionality.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3798
https://github.com/broadinstitute/cromwell/issues/3798:628,Testability,log,logic,628,"Use case: Once there is support for [optional file localization](https://cromwell.readthedocs.io/en/develop/optimizations/FileLocalization/#the-localization_optional-optimization), presumably every future version of Cromwell (v33 and beyond) will always not localize files as it contains logic to call cache without localization. However, a WDL author would need to to test whether a task w/`optional_localization` enabled for file inputs has the ability to run successfully in both cases of having access to file localization or file streaming. AC: Regardless of whether a file is marked for localization or not, override that logic on the workflow level so the workflow submitter has the option to enforce file localization, even if the WDL author has written the task to be compatible with remote files and the backend supports that functionality.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3798
https://github.com/broadinstitute/cromwell/pull/3802:167,Safety,safe,safer,167,"This fixes the issue in PAPI2 and revert it to what it was before for PAPI1. Since this change was targeted at CWL and we don't plan to support CWL on PAPI1, it seems safer to revert it entirely in case it still breaks something else",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3802
https://github.com/broadinstitute/cromwell/issues/3804:620,Modifiability,config,configured,620,"Functionality is needed to stage data files to/from S3 on a per-task level. For the example WDL file:; ```; task copy_file {; String output_file; File input_file. command {; cp ${input_file} ${output_file}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_copy_file {; call copy_file; }; ```; and corresponding `inputs.json`. ```json; {; ""wf_cop_file.copy_file.input_file"": ""s3://myBucket/hello.txt"",; ""wf_cop_file.copy_file.output_file"": ""greetings.txt""; }; ```. The workflow execution should be able to copy the input file from S3 to the working task directory, and copy the output file ""greetings.txt"" to the configured S3 bucket for writing logs and outputs. An example of files written to the output S3 bucket would be:. ```; # $WF_ID is the workflow identifier (e.g. ""E6D5143C-89BC-4823-AED7-2A6AE00A1C2B""); s3://cromwell-output-bucket/$WF_ID/copy_file/outputs/greetings.txt; s3://cromwell-output-bucket/$WF_ID/copy_file/wf_copy_file-rc.txt; s3://cromwell-output-bucket/$WF_ID/copy_file/wf_copy_file-stdout.txt; s3://cromwell-output-bucket/$WF_ID/copy_file/wf_copy_file-stderr.txt; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804
https://github.com/broadinstitute/cromwell/issues/3804:653,Testability,log,logs,653,"Functionality is needed to stage data files to/from S3 on a per-task level. For the example WDL file:; ```; task copy_file {; String output_file; File input_file. command {; cp ${input_file} ${output_file}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_copy_file {; call copy_file; }; ```; and corresponding `inputs.json`. ```json; {; ""wf_cop_file.copy_file.input_file"": ""s3://myBucket/hello.txt"",; ""wf_cop_file.copy_file.output_file"": ""greetings.txt""; }; ```. The workflow execution should be able to copy the input file from S3 to the working task directory, and copy the output file ""greetings.txt"" to the configured S3 bucket for writing logs and outputs. An example of files written to the output S3 bucket would be:. ```; # $WF_ID is the workflow identifier (e.g. ""E6D5143C-89BC-4823-AED7-2A6AE00A1C2B""); s3://cromwell-output-bucket/$WF_ID/copy_file/outputs/greetings.txt; s3://cromwell-output-bucket/$WF_ID/copy_file/wf_copy_file-rc.txt; s3://cromwell-output-bucket/$WF_ID/copy_file/wf_copy_file-stdout.txt; s3://cromwell-output-bucket/$WF_ID/copy_file/wf_copy_file-stderr.txt; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804
https://github.com/broadinstitute/cromwell/issues/3805:685,Availability,error,error,685,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:741,Availability,error,error,741,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:818,Availability,Error,Error,818,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:1876,Availability,echo,echo,1876,"gumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:1923,Availability,echo,echo,1923,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:2833,Availability,alive,alive,2833,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:2252,Deployability,configurat,configuration,2252,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:1173,Modifiability,config,config,1173,"broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String pre",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:1337,Modifiability,config,config,1337,"tute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERI",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:1451,Modifiability,config,config,1451,"-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Flo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:1458,Modifiability,Config,ConfigAsyncJobExecutionActor,1458," https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; Str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:1521,Modifiability,Config,ConfigAsyncJobExecutionActor,1521,"seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:2252,Modifiability,config,configuration,2252,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:2297,Security,PASSWORD,PASSWORDS,2297,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:696,Testability,log,log,696,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:1716,Testability,test,testsge,1716,"r. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:1882,Testability,test,test,1882,"gumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:1911,Testability,log,log,1911,"gumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:2087,Testability,test,test,2087,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:2097,Testability,test,test,2097,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:2124,Testability,test,test,2124,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; I clone the source code and assembly the jar. when try to run a sge job, I got and error. the log is following:. [2018-06-21 20:36:29,51] [error] DispatchedConfigAsyncJobExecutionActor [7ec0863atestsge.filter:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: No coercion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3805:2557,Usability,clear,clear,2557,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805
https://github.com/broadinstitute/cromwell/issues/3807:165,Testability,test,test,165,[This PR](https://github.com/broadinstitute/cromwell/pull/3769) fixes an issue where non-localized paths were used to evaluate adhoc expressions.; Unfortunately the test still fails on TES backend.; AC: `cwl_input_json` centaur test passes on TES backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3807
https://github.com/broadinstitute/cromwell/issues/3807:228,Testability,test,test,228,[This PR](https://github.com/broadinstitute/cromwell/pull/3769) fixes an issue where non-localized paths were used to evaluate adhoc expressions.; Unfortunately the test still fails on TES backend.; AC: `cwl_input_json` centaur test passes on TES backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3807
https://github.com/broadinstitute/cromwell/pull/3810:2,Deployability,Update,Update,2,"- Update to WDL 1.0 syntax; - Automate PR to homebrew; - Can be used to release minor versions. The brew task supposes `brew` is installed on the machine where Cromwell runs. There is a [Brew Linux docker image](https://hub.docker.com/r/linuxbrew/linuxbrew/) that maybe could be used but the `git` commands don't seem to be authenticated properly in the container (probably because the ssh keys aren't there). I'm sure this can be worked around, maybe in the next iteration.. This can be fully tested with a different `organization` as long as it has a fork of Cromwell and homebrew-core.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3810
https://github.com/broadinstitute/cromwell/pull/3810:72,Deployability,release,release,72,"- Update to WDL 1.0 syntax; - Automate PR to homebrew; - Can be used to release minor versions. The brew task supposes `brew` is installed on the machine where Cromwell runs. There is a [Brew Linux docker image](https://hub.docker.com/r/linuxbrew/linuxbrew/) that maybe could be used but the `git` commands don't seem to be authenticated properly in the container (probably because the ssh keys aren't there). I'm sure this can be worked around, maybe in the next iteration.. This can be fully tested with a different `organization` as long as it has a fork of Cromwell and homebrew-core.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3810
https://github.com/broadinstitute/cromwell/pull/3810:129,Deployability,install,installed,129,"- Update to WDL 1.0 syntax; - Automate PR to homebrew; - Can be used to release minor versions. The brew task supposes `brew` is installed on the machine where Cromwell runs. There is a [Brew Linux docker image](https://hub.docker.com/r/linuxbrew/linuxbrew/) that maybe could be used but the `git` commands don't seem to be authenticated properly in the container (probably because the ssh keys aren't there). I'm sure this can be worked around, maybe in the next iteration.. This can be fully tested with a different `organization` as long as it has a fork of Cromwell and homebrew-core.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3810
https://github.com/broadinstitute/cromwell/pull/3810:324,Security,authenticat,authenticated,324,"- Update to WDL 1.0 syntax; - Automate PR to homebrew; - Can be used to release minor versions. The brew task supposes `brew` is installed on the machine where Cromwell runs. There is a [Brew Linux docker image](https://hub.docker.com/r/linuxbrew/linuxbrew/) that maybe could be used but the `git` commands don't seem to be authenticated properly in the container (probably because the ssh keys aren't there). I'm sure this can be worked around, maybe in the next iteration.. This can be fully tested with a different `organization` as long as it has a fork of Cromwell and homebrew-core.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3810
https://github.com/broadinstitute/cromwell/pull/3810:494,Testability,test,tested,494,"- Update to WDL 1.0 syntax; - Automate PR to homebrew; - Can be used to release minor versions. The brew task supposes `brew` is installed on the machine where Cromwell runs. There is a [Brew Linux docker image](https://hub.docker.com/r/linuxbrew/linuxbrew/) that maybe could be used but the `git` commands don't seem to be authenticated properly in the container (probably because the ssh keys aren't there). I'm sure this can be worked around, maybe in the next iteration.. This can be fully tested with a different `organization` as long as it has a fork of Cromwell and homebrew-core.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3810
https://github.com/broadinstitute/cromwell/issues/3811:141,Availability,echo,echo,141,"The valid WDL file below fails to compile. ```wdl; workflow x {; call cram; call y { input:; cram = cram.cram; }; }. task cram {; command {; echo "".""; }; output {; String cram = "".""; }; }. task y {; String cram; command {; echo "".""; }; }; ```. Running it with Cromwell version 32 gives:. ```; Workflow input processing failed:; ERROR: Bad target for member access 'cram.cram': 'cram' was a String (line 4, col 33):. cram = cram.cram; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811
https://github.com/broadinstitute/cromwell/issues/3811:223,Availability,echo,echo,223,"The valid WDL file below fails to compile. ```wdl; workflow x {; call cram; call y { input:; cram = cram.cram; }; }. task cram {; command {; echo "".""; }; output {; String cram = "".""; }; }. task y {; String cram; command {; echo "".""; }; }; ```. Running it with Cromwell version 32 gives:. ```; Workflow input processing failed:; ERROR: Bad target for member access 'cram.cram': 'cram' was a String (line 4, col 33):. cram = cram.cram; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811
https://github.com/broadinstitute/cromwell/issues/3811:328,Availability,ERROR,ERROR,328,"The valid WDL file below fails to compile. ```wdl; workflow x {; call cram; call y { input:; cram = cram.cram; }; }. task cram {; command {; echo "".""; }; output {; String cram = "".""; }; }. task y {; String cram; command {; echo "".""; }; }; ```. Running it with Cromwell version 32 gives:. ```; Workflow input processing failed:; ERROR: Bad target for member access 'cram.cram': 'cram' was a String (line 4, col 33):. cram = cram.cram; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811
https://github.com/broadinstitute/cromwell/issues/3811:357,Security,access,access,357,"The valid WDL file below fails to compile. ```wdl; workflow x {; call cram; call y { input:; cram = cram.cram; }; }. task cram {; command {; echo "".""; }; output {; String cram = "".""; }; }. task y {; String cram; command {; echo "".""; }; }; ```. Running it with Cromwell version 32 gives:. ```; Workflow input processing failed:; ERROR: Bad target for member access 'cram.cram': 'cram' was a String (line 4, col 33):. cram = cram.cram; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811
https://github.com/broadinstitute/cromwell/pull/3813:62,Deployability,update,updates,62,"This branch was WIP to get BCBIO to work on PAPI but contains updates (namely retyring gsutil commands) that could be useful re: travis flakiness, and it's green so making a PR.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3813
https://github.com/broadinstitute/cromwell/pull/3813:156,Energy Efficiency,green,green,156,"This branch was WIP to get BCBIO to work on PAPI but contains updates (namely retyring gsutil commands) that could be useful re: travis flakiness, and it's green so making a PR.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3813
https://github.com/broadinstitute/cromwell/issues/3814:288,Availability,Echo,Echo,288,"When working with submodules cromwell will only use outputs of a submodule when the complete submodule is finished. In the example below the sleep command is blocking for the cat command while the files are not connected. Backend: SGE or local. test1.wdl:; ```wdl; workflow Test1 {; call Echo; call Sleep. output {; File echoOut = Echo.out; File sleepOut = Sleep.out; }; }. task Echo {; command{; echo bla > bla.txt; }; output {; File out = ""bla.txt""; }; }. task Sleep {; command{; sleep 30 > bla.txt; }; output {; File out = ""bla.txt""; }; }; ```. test2.wdl:; ```wdl; import ""test1.wdl"" as Test1. workflow Test2 {; call Test1.Test1 as Test1; call Cat {; input:; inFile = Test1.echoOut; }; }. task Cat {; File inFile. command{; cat ${inFile} > ""bla.txt""; }; output {; File out = ""bla.txt""; }; }; ```. Command:; ```bash; # no config is given; java -jar <cromwell_32.jar> run test2.wdl; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814
https://github.com/broadinstitute/cromwell/issues/3814:321,Availability,echo,echoOut,321,"When working with submodules cromwell will only use outputs of a submodule when the complete submodule is finished. In the example below the sleep command is blocking for the cat command while the files are not connected. Backend: SGE or local. test1.wdl:; ```wdl; workflow Test1 {; call Echo; call Sleep. output {; File echoOut = Echo.out; File sleepOut = Sleep.out; }; }. task Echo {; command{; echo bla > bla.txt; }; output {; File out = ""bla.txt""; }; }. task Sleep {; command{; sleep 30 > bla.txt; }; output {; File out = ""bla.txt""; }; }; ```. test2.wdl:; ```wdl; import ""test1.wdl"" as Test1. workflow Test2 {; call Test1.Test1 as Test1; call Cat {; input:; inFile = Test1.echoOut; }; }. task Cat {; File inFile. command{; cat ${inFile} > ""bla.txt""; }; output {; File out = ""bla.txt""; }; }; ```. Command:; ```bash; # no config is given; java -jar <cromwell_32.jar> run test2.wdl; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814
https://github.com/broadinstitute/cromwell/issues/3814:331,Availability,Echo,Echo,331,"When working with submodules cromwell will only use outputs of a submodule when the complete submodule is finished. In the example below the sleep command is blocking for the cat command while the files are not connected. Backend: SGE or local. test1.wdl:; ```wdl; workflow Test1 {; call Echo; call Sleep. output {; File echoOut = Echo.out; File sleepOut = Sleep.out; }; }. task Echo {; command{; echo bla > bla.txt; }; output {; File out = ""bla.txt""; }; }. task Sleep {; command{; sleep 30 > bla.txt; }; output {; File out = ""bla.txt""; }; }; ```. test2.wdl:; ```wdl; import ""test1.wdl"" as Test1. workflow Test2 {; call Test1.Test1 as Test1; call Cat {; input:; inFile = Test1.echoOut; }; }. task Cat {; File inFile. command{; cat ${inFile} > ""bla.txt""; }; output {; File out = ""bla.txt""; }; }; ```. Command:; ```bash; # no config is given; java -jar <cromwell_32.jar> run test2.wdl; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814
https://github.com/broadinstitute/cromwell/issues/3814:379,Availability,Echo,Echo,379,"When working with submodules cromwell will only use outputs of a submodule when the complete submodule is finished. In the example below the sleep command is blocking for the cat command while the files are not connected. Backend: SGE or local. test1.wdl:; ```wdl; workflow Test1 {; call Echo; call Sleep. output {; File echoOut = Echo.out; File sleepOut = Sleep.out; }; }. task Echo {; command{; echo bla > bla.txt; }; output {; File out = ""bla.txt""; }; }. task Sleep {; command{; sleep 30 > bla.txt; }; output {; File out = ""bla.txt""; }; }; ```. test2.wdl:; ```wdl; import ""test1.wdl"" as Test1. workflow Test2 {; call Test1.Test1 as Test1; call Cat {; input:; inFile = Test1.echoOut; }; }. task Cat {; File inFile. command{; cat ${inFile} > ""bla.txt""; }; output {; File out = ""bla.txt""; }; }; ```. Command:; ```bash; # no config is given; java -jar <cromwell_32.jar> run test2.wdl; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814
https://github.com/broadinstitute/cromwell/issues/3814:397,Availability,echo,echo,397,"When working with submodules cromwell will only use outputs of a submodule when the complete submodule is finished. In the example below the sleep command is blocking for the cat command while the files are not connected. Backend: SGE or local. test1.wdl:; ```wdl; workflow Test1 {; call Echo; call Sleep. output {; File echoOut = Echo.out; File sleepOut = Sleep.out; }; }. task Echo {; command{; echo bla > bla.txt; }; output {; File out = ""bla.txt""; }; }. task Sleep {; command{; sleep 30 > bla.txt; }; output {; File out = ""bla.txt""; }; }; ```. test2.wdl:; ```wdl; import ""test1.wdl"" as Test1. workflow Test2 {; call Test1.Test1 as Test1; call Cat {; input:; inFile = Test1.echoOut; }; }. task Cat {; File inFile. command{; cat ${inFile} > ""bla.txt""; }; output {; File out = ""bla.txt""; }; }; ```. Command:; ```bash; # no config is given; java -jar <cromwell_32.jar> run test2.wdl; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814
https://github.com/broadinstitute/cromwell/issues/3814:677,Availability,echo,echoOut,677,"When working with submodules cromwell will only use outputs of a submodule when the complete submodule is finished. In the example below the sleep command is blocking for the cat command while the files are not connected. Backend: SGE or local. test1.wdl:; ```wdl; workflow Test1 {; call Echo; call Sleep. output {; File echoOut = Echo.out; File sleepOut = Sleep.out; }; }. task Echo {; command{; echo bla > bla.txt; }; output {; File out = ""bla.txt""; }; }. task Sleep {; command{; sleep 30 > bla.txt; }; output {; File out = ""bla.txt""; }; }; ```. test2.wdl:; ```wdl; import ""test1.wdl"" as Test1. workflow Test2 {; call Test1.Test1 as Test1; call Cat {; input:; inFile = Test1.echoOut; }; }. task Cat {; File inFile. command{; cat ${inFile} > ""bla.txt""; }; output {; File out = ""bla.txt""; }; }; ```. Command:; ```bash; # no config is given; java -jar <cromwell_32.jar> run test2.wdl; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814
https://github.com/broadinstitute/cromwell/issues/3814:824,Modifiability,config,config,824,"When working with submodules cromwell will only use outputs of a submodule when the complete submodule is finished. In the example below the sleep command is blocking for the cat command while the files are not connected. Backend: SGE or local. test1.wdl:; ```wdl; workflow Test1 {; call Echo; call Sleep. output {; File echoOut = Echo.out; File sleepOut = Sleep.out; }; }. task Echo {; command{; echo bla > bla.txt; }; output {; File out = ""bla.txt""; }; }. task Sleep {; command{; sleep 30 > bla.txt; }; output {; File out = ""bla.txt""; }; }; ```. test2.wdl:; ```wdl; import ""test1.wdl"" as Test1. workflow Test2 {; call Test1.Test1 as Test1; call Cat {; input:; inFile = Test1.echoOut; }; }. task Cat {; File inFile. command{; cat ${inFile} > ""bla.txt""; }; output {; File out = ""bla.txt""; }; }; ```. Command:; ```bash; # no config is given; java -jar <cromwell_32.jar> run test2.wdl; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814
https://github.com/broadinstitute/cromwell/issues/3815:57,Deployability,configurat,configuration,57,The documentation for the `AWS 101 tutorial` and backend configuration needs to be updated.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3815
https://github.com/broadinstitute/cromwell/issues/3815:83,Deployability,update,updated,83,The documentation for the `AWS 101 tutorial` and backend configuration needs to be updated.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3815
https://github.com/broadinstitute/cromwell/issues/3815:57,Modifiability,config,configuration,57,The documentation for the `AWS 101 tutorial` and backend configuration needs to be updated.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3815
https://github.com/broadinstitute/cromwell/issues/3817:1667,Availability,reliab,reliably,1667,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3817:1584,Deployability,PATCH,PATCH,1584,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3817:79,Security,access,access,79,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3817:540,Security,Hash,Hash,540,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3817:569,Security,access,access,569,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3817:694,Security,Hash,Hash,694,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3817:804,Security,access,access,804,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3817:938,Security,access,access,938,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3817:954,Security,Hash,Hash,954,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3817:1273,Security,access,access,1273,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3817:1514,Security,access,access,1514,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3817:1617,Security,hash,hash,1617,"# Introduction. The essence of a presigned URL is that it gives you privileged access to data (via HTTP verbs, usually `GET`) for a finite amount of time. Some metadata can be obtained via the `HEAD` verb. DOS URI's can be resolved to presigned URLs, and it's not immediately obvious how to provide the info Cromwell needs to do its job. Hence this document. The essence of this question is how do we leverage HTTP. # Information Needed for Cromwell to work. 1. The data itself, i.e. the file to which the URL refers.; 1. Size Metadata; 1. Hash Metadata; 1. Byte-level access (needed for things like WDL's `read_lines`). ## Information Provided by OpenDJ / Martha as of 6/25/18. * Size ; * MD5 Hash; * Presigned URL . ## Information provided by HTTP (in theory). * Metadata/ETag via `HEAD`; * Byte-level access via `RANGE` header on GET; * Full data of resource. ## Information *Not* Provided by OpenDJ/Martha as of 6/25/18. * Byte-level access; * CRC32 Hash. # Outstanding questions (please comment if you have info). 1. What metadata can be obtained via `HEAD`?; 1. Is the `HEAD` metadata a standard, and do all clouds implement that standard? (I think ETag is common name for this info.); 1. How does call-caching work with an expiration date on the URL?; 1. Byte-level access: HTTP request to the data can be limited to a range via [`Range` header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Range_requests). Do clouds support this feature? Are there other ways of achieving this requirement?; 1. Write access: WDL supports `write_lines`, which AFAIK is only possible via `PATCH` ; 1. Can Cromwell use any hash besides CRC32? If not how do we obtain CRC32 reliably?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3817
https://github.com/broadinstitute/cromwell/issues/3818:709,Deployability,configurat,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3818
https://github.com/broadinstitute/cromwell/issues/3818:709,Modifiability,config,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3818
https://github.com/broadinstitute/cromwell/issues/3818:754,Security,PASSWORD,PASSWORDS,754,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3818
https://github.com/broadinstitute/cromwell/issues/3818:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3818
https://github.com/broadinstitute/cromwell/issues/3819:242,Availability,echo,echo,242,"This is related (at least) to https://github.com/broadinstitute/cromwell/issues/2446. Examine the workflow `wf` below.; ```wdl; workflow wf {; String? who. call hello { input:; who = who; }; }. task hello {; String? who = ""world"". command {; echo ""Hello, ${who}""; }; output {; String result = read_string(stdout()); }; }; ```. When running `wf` with Cromwell v32 the result is `hello.result = ""Hello, world""`. However, since we are calling the task with an empty value, it should override `who`, and the the result should be (I think) `Hello,""`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3819
https://github.com/broadinstitute/cromwell/issues/3822:34,Performance,cache,cached,34,"Before cromwell uses a previously cached call it needs to know if the docker image content has changed. Some images such as the [GATK](https://hub.docker.com/r/broadinstitute/gatk/tags/) are so large that they must not be pulled at scale from DockerHub. Instead these images be hosted elsewhere per cloud service provider, including [ECR](https://aws.amazon.com/ecr/). A/C: When call references an image in ECR cromwell should contact ECR to get and record the image hash.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3822
https://github.com/broadinstitute/cromwell/issues/3822:467,Security,hash,hash,467,"Before cromwell uses a previously cached call it needs to know if the docker image content has changed. Some images such as the [GATK](https://hub.docker.com/r/broadinstitute/gatk/tags/) are so large that they must not be pulled at scale from DockerHub. Instead these images be hosted elsewhere per cloud service provider, including [ECR](https://aws.amazon.com/ecr/). A/C: When call references an image in ECR cromwell should contact ECR to get and record the image hash.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3822
https://github.com/broadinstitute/cromwell/issues/3823:986,Performance,concurren,concurrent,986,"Seems to happen once per minute. Appears to be a [known](https://github.com/slick/slick/issues/1856) slick bug around transactions, possibly caused by [Await.Inf](https://github.com/slick/slick/issues/1890). ![image](https://user-images.githubusercontent.com/165320/41995485-fa9637c2-7a1f-11e8-9613-593c7739a541.png). Kibana-mangled stacktrace:. ```; June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: Exception in thread ""db-14127"" java.lang.IllegalArgumentException: requirement failed: count cannot be decreased; -- | --;   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at scala.Predef$.require(Predef.scala:277).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at slick.util.AsyncExecutor$$anon$2$$anon$1.afterExecute(AsyncExecutor.scala:129).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at java.lang.Thread.run(Thread.java:748).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at slick.util.ManagedArrayBlockingQueue.$anonfun$decreaseInUseCount$1(ManagedArrayBlockingQueue.scala:54).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at slick.util.ManagedArrayBlockingQueue.locked(ManagedArrayBlockingQueue.scala:201).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 crom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3823
https://github.com/broadinstitute/cromwell/issues/3823:1854,Performance,concurren,concurrent,1854,"ecutor.scala:129).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at java.lang.Thread.run(Thread.java:748).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at slick.util.ManagedArrayBlockingQueue.$anonfun$decreaseInUseCount$1(ManagedArrayBlockingQueue.scala:54).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at slick.util.ManagedArrayBlockingQueue.locked(ManagedArrayBlockingQueue.scala:201).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157).   | June 27th 2018, 15:36:54.000 | Jun 27 15:36:54 gce-cromwell-prod601 cromwell-app[955]: #011at slick.util.ManagedArrayBlockingQueue.decreaseInUseCount(ManagedArrayBlockingQueue.scala:53).   | June 27th 2018, 15:35:53.000 | Jun 27 15:35:53 gce-cromwell-prod601 cromwell-app[955]: #011at slick.util.ManagedArrayBlockingQueue.$anonfun$decreaseInUseCount$1(ManagedArrayBlockingQueue.scala:54).   | June 27th 2018, 15:35:53.000 | Jun 27 15:35:53 gce-cromwell-prod601 cromwell-app[955]: #011at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12).   | June 27th 2018, 15:35:53.000 | Jun 27 15:35:53 gce-cromwell-prod601 cromwell-app[955]: #011at slick.util.ManagedArrayBlockingQueue.decreaseInUseCount(ManagedArrayBlockingQueue.scala:53).   | June 27th 2018, 15:35:53.000 | Jun 27 15:35:53 gce-cromwell-prod601 cromwell-app[955]: #011at slick.util.AsyncExecutor$$anon$2$$anon$1.afterExecute(AsyncExecutor.scala:129).   |",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3823
https://github.com/broadinstitute/cromwell/issues/3824:310,Security,hash,hash,310,"Cromwell is localizing inputs to unconventionally named directories. Specifically directories starting with a '-'. - is conventionally used in program arguments to mean 'not a path, this is an option'. This is a pain. Previously cromwell was localising with a full path, but now seems to be using some kind of hash prefixed with an '-'. Maybe you meant to use an unsigned int?. ```; ls cromwell-executions/PreProcessingForVariantDiscovery_GATK4/51a9f551-defa-4e20-8573-d55d25622248/call-SamToFastqAndBwaMem/inputs/; -1232659437/ -21323395/ -941963188/; cd cromwell-executions/PreProcessingForVariantDiscovery_GATK4/51a9f551-defa-4e20-8573-d55d25622248/call-SamToFastqAndBwaMem/inputs/; cd -941963188; cd: Unknown option “-941963188”; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3824
https://github.com/broadinstitute/cromwell/issues/3825:2898,Availability,error,errors,2898,"Statements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/inputs/-21323395/cromwell -> /share/ScratchGeneral/evaben/cromwell```; Is it attempting to localize my entire cromwell directory? IE the CWD of the cromwell server process?. This fails because a previously found workflow input (```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/inputs/share/ClusterShare/biodata/contrib/evaben/genome_one_na12878/reheader/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai```) is a symlink to a file that no long exists. (NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai`). In addition that file is totally unrelated to the workflow I am trying to run ( it is from a different workflow, and a different call, and the inputs.json and wdl are totally different). I am confused about what is happening, none of the actions the log is mentioning as errors seem to be things cromwell should be attempting. The cache does not become invalidated (as it seems to in other cases where the cached file has disappeared).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:303,Modifiability,config,config,303,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:310,Modifiability,Config,ConfigBackendLifecycleActorFactory,310,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:347,Modifiability,config,config,347,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:1048,Modifiability,rewrite,rewriteBatchedStatements,1048,"red(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/inputs/-21323395/cromwell -> /share/ScratchGeneral/evaben/cromwell```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:149,Performance,cache,cache-results,149,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:409,Performance,cache,cached,409,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:2958,Performance,cache,cache,2958,"Statements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/inputs/-21323395/cromwell -> /share/ScratchGeneral/evaben/cromwell```; Is it attempting to localize my entire cromwell directory? IE the CWD of the cromwell server process?. This fails because a previously found workflow input (```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/inputs/share/ClusterShare/biodata/contrib/evaben/genome_one_na12878/reheader/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai```) is a symlink to a file that no long exists. (NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai`). In addition that file is totally unrelated to the workflow I am trying to run ( it is from a different workflow, and a different call, and the inputs.json and wdl are totally different). I am confused about what is happening, none of the actions the log is mentioning as errors seem to be things cromwell should be attempting. The cache does not become invalidated (as it seems to in other cases where the cached file has disappeared).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:3033,Performance,cache,cached,3033,"Statements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/inputs/-21323395/cromwell -> /share/ScratchGeneral/evaben/cromwell```; Is it attempting to localize my entire cromwell directory? IE the CWD of the cromwell server process?. This fails because a previously found workflow input (```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/inputs/share/ClusterShare/biodata/contrib/evaben/genome_one_na12878/reheader/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai```) is a symlink to a file that no long exists. (NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai`). In addition that file is totally unrelated to the workflow I am trying to run ( it is from a different workflow, and a different call, and the inputs.json and wdl are totally different). I am confused about what is happening, none of the actions the log is mentioning as errors seem to be things cromwell should be attempting. The cache does not become invalidated (as it seems to in other cases where the cached file has disappeared).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:615,Security,hash,hash,615,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:671,Security,hash,hash,671,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:844,Security,hash,hashed,844,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:870,Security,hash,hashing-strategy,870,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:1095,Security,password,password,1095,"red(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/inputs/-21323395/cromwell -> /share/ScratchGeneral/evaben/cromwell```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:203,Testability,log,log-temporary,203,"<!-- Which backend are you running? -->; SGE. conf:; ```; include required(classpath(""application"")). call-caching {; enabled = true; invalidate-bad-cache-results = true; }; workflow-options {; workflow-log-temporary = false; }; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; filesystems {; local {; caching {; # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:; duplication-strategy: [; ""soft-link"", ""copy""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:1643,Testability,log,log,1643,"ll compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/inputs/-21323395/cromwell -> /share/ScratchGeneral/evaben/cromwell```; Is it attempting to localize my entire cromwell directory? IE the CWD of the cromwell server process?. This fails because a previously found workflow input (```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/inputs/share/ClusterShare/biodata/contrib/evaben/genome_one_na12878/reheader/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai```) is a symlink to a file that no long exists. (NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai`). In addition that file is t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:1751,Testability,log,log,1751,"ategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; # Default: file; hashing-strategy: ""path""; }; }; }; ...; backend.default = SGE; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql://0.0.0.0:40001/testuser_db?useSSL=false&rewriteBatchedStatements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/inputs/-21323395/cromwell -> /share/ScratchGeneral/evaben/cromwell```; Is it attempting to localize my entire cromwell directory? IE the CWD of the cromwell server process?. This fails because a previously found workflow input (```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/inputs/share/ClusterShare/biodata/contrib/evaben/genome_one_na12878/reheader/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai```) is a symlink to a file that no long exists. (NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai`). In addition that file is totally unrelated to the workflow I am trying to run ( it is from a different workflow, and a differen",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/issues/3825:2877,Testability,log,log,2877,"Statements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/inputs/-21323395/cromwell -> /share/ScratchGeneral/evaben/cromwell```; Is it attempting to localize my entire cromwell directory? IE the CWD of the cromwell server process?. This fails because a previously found workflow input (```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/inputs/share/ClusterShare/biodata/contrib/evaben/genome_one_na12878/reheader/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai```) is a symlink to a file that no long exists. (NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai`). In addition that file is totally unrelated to the workflow I am trying to run ( it is from a different workflow, and a different call, and the inputs.json and wdl are totally different). I am confused about what is happening, none of the actions the log is mentioning as errors seem to be things cromwell should be attempting. The cache does not become invalidated (as it seems to in other cases where the cached file has disappeared).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825
https://github.com/broadinstitute/cromwell/pull/3827:35,Availability,robust,robustness,35,"Seems to generally improve centaur robustness, although it still drops the ball sometimes and the test script just exits for no apparent reason.; Fixes the `cwl_cache_between_workflows` and `cwl_cache_within_workflows` tests; Unit tests transient failures remain",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3827
https://github.com/broadinstitute/cromwell/pull/3827:247,Availability,failure,failures,247,"Seems to generally improve centaur robustness, although it still drops the ball sometimes and the test script just exits for no apparent reason.; Fixes the `cwl_cache_between_workflows` and `cwl_cache_within_workflows` tests; Unit tests transient failures remain",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3827
https://github.com/broadinstitute/cromwell/pull/3827:98,Testability,test,test,98,"Seems to generally improve centaur robustness, although it still drops the ball sometimes and the test script just exits for no apparent reason.; Fixes the `cwl_cache_between_workflows` and `cwl_cache_within_workflows` tests; Unit tests transient failures remain",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3827
https://github.com/broadinstitute/cromwell/pull/3827:219,Testability,test,tests,219,"Seems to generally improve centaur robustness, although it still drops the ball sometimes and the test script just exits for no apparent reason.; Fixes the `cwl_cache_between_workflows` and `cwl_cache_within_workflows` tests; Unit tests transient failures remain",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3827
https://github.com/broadinstitute/cromwell/pull/3827:231,Testability,test,tests,231,"Seems to generally improve centaur robustness, although it still drops the ball sometimes and the test script just exits for no apparent reason.; Fixes the `cwl_cache_between_workflows` and `cwl_cache_within_workflows` tests; Unit tests transient failures remain",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3827
https://github.com/broadinstitute/cromwell/issues/3831:13,Security,validat,validates,13,"When WOMtool validates a WDL with local imports at relative directory paths, it fails:; ```; $ java -jar womtool-33.1.jar validate WdlWithLocalRelativeImports.wdl ; Failed to import workflow ../../other_dir/TasksToImport.wdl.:; Bad import ../../other_dir/TasksToImport.wdl: ../../other_dir/TasksToImport.wdl is not a valid import; Bad import ../../other_dir/TasksToImport.wdl: ../../other_dir/TasksToImport.wdl is not a valid import; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3831
https://github.com/broadinstitute/cromwell/issues/3831:122,Security,validat,validate,122,"When WOMtool validates a WDL with local imports at relative directory paths, it fails:; ```; $ java -jar womtool-33.1.jar validate WdlWithLocalRelativeImports.wdl ; Failed to import workflow ../../other_dir/TasksToImport.wdl.:; Bad import ../../other_dir/TasksToImport.wdl: ../../other_dir/TasksToImport.wdl is not a valid import; Bad import ../../other_dir/TasksToImport.wdl: ../../other_dir/TasksToImport.wdl is not a valid import; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3831
https://github.com/broadinstitute/cromwell/pull/3835:489,Modifiability,variab,variable,489,"This implements the remaining pieces necessary to mount volumes from the container host to the container. I am using a runtime attibutes disk format that looks like the following:. ```disks: ""local-disk, /mount/point, /other/mount/point""```. I add a random guid ""task id"" to the host directory to allow for multiple containers on the host instance. The task id is generated at run time to disambiguate this job from any other job, and is passed through to the container in the environment variable AWS_CROMWELL_TASK_ID) to allow for tracking at a later stage of the project. The host mount scheme is designed in the following manner:. /mount/point/<taskid>. Contrary to the suggestion in issue #3744, the task id is added to the end. This is a conscious decision such that mounts that refer to external volumes can be setup such that the filesystems can properly match the type of container workloads that might be using them. Note that when these mounts have files copied to S3, this should scheme should be reversed (e.g. <taskid>/mount/point to encourage proper spread and optimal S3 performance. This will be addressed when #3804 is implemented.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3835
https://github.com/broadinstitute/cromwell/pull/3835:1087,Performance,perform,performance,1087,"This implements the remaining pieces necessary to mount volumes from the container host to the container. I am using a runtime attibutes disk format that looks like the following:. ```disks: ""local-disk, /mount/point, /other/mount/point""```. I add a random guid ""task id"" to the host directory to allow for multiple containers on the host instance. The task id is generated at run time to disambiguate this job from any other job, and is passed through to the container in the environment variable AWS_CROMWELL_TASK_ID) to allow for tracking at a later stage of the project. The host mount scheme is designed in the following manner:. /mount/point/<taskid>. Contrary to the suggestion in issue #3744, the task id is added to the end. This is a conscious decision such that mounts that refer to external volumes can be setup such that the filesystems can properly match the type of container workloads that might be using them. Note that when these mounts have files copied to S3, this should scheme should be reversed (e.g. <taskid>/mount/point to encourage proper spread and optimal S3 performance. This will be addressed when #3804 is implemented.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3835
https://github.com/broadinstitute/cromwell/pull/3835:257,Usability,guid,guid,257,"This implements the remaining pieces necessary to mount volumes from the container host to the container. I am using a runtime attibutes disk format that looks like the following:. ```disks: ""local-disk, /mount/point, /other/mount/point""```. I add a random guid ""task id"" to the host directory to allow for multiple containers on the host instance. The task id is generated at run time to disambiguate this job from any other job, and is passed through to the container in the environment variable AWS_CROMWELL_TASK_ID) to allow for tracking at a later stage of the project. The host mount scheme is designed in the following manner:. /mount/point/<taskid>. Contrary to the suggestion in issue #3744, the task id is added to the end. This is a conscious decision such that mounts that refer to external volumes can be setup such that the filesystems can properly match the type of container workloads that might be using them. Note that when these mounts have files copied to S3, this should scheme should be reversed (e.g. <taskid>/mount/point to encourage proper spread and optimal S3 performance. This will be addressed when #3804 is implemented.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3835
https://github.com/broadinstitute/cromwell/issues/3838:200,Modifiability,config,config,200,The [Google backend documentation](https://github.com/broadinstitute/cromwell/blob/f1d4209a1917f95d77443e49572f29ec45f30137/docs/backends/Google.md) mentions in multiple places that a Service Account config should use the PEM file format. This is out of date. Service accounts should be specified as JSON not PEM. A/C: Mention in docs that JSON is the preferred way to represent service accounts and that PEM files should not be used. Explain as necessary what the differences are.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3838
https://github.com/broadinstitute/cromwell/issues/3843:294,Availability,down,down,294,"Implement the [paging protocol](https://github.com/ga4gh/workflow-execution-service-schemas/pull/30) specified by Cromwell. If it makes sense to do so this could be a candidate to update Cromwell itself. To the extent that **that** could require a Cromwell API change, contact @ruchim if going down that path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3843
https://github.com/broadinstitute/cromwell/issues/3843:180,Deployability,update,update,180,"Implement the [paging protocol](https://github.com/ga4gh/workflow-execution-service-schemas/pull/30) specified by Cromwell. If it makes sense to do so this could be a candidate to update Cromwell itself. To the extent that **that** could require a Cromwell API change, contact @ruchim if going down that path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3843
https://github.com/broadinstitute/cromwell/issues/3843:22,Integrability,protocol,protocol,22,"Implement the [paging protocol](https://github.com/ga4gh/workflow-execution-service-schemas/pull/30) specified by Cromwell. If it makes sense to do so this could be a candidate to update Cromwell itself. To the extent that **that** could require a Cromwell API change, contact @ruchim if going down that path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3843
https://github.com/broadinstitute/cromwell/issues/3844:241,Security,secur,securely,241,PAPI V1 had a different way of handling private dockerhub credentials and there needs to be a new methodology to pass credentials for operations in PAPI v2. . Implement a system that allows a user to be able to pass in dockerhub credentials securely to their jobs container so a user can pull private docker images.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3844
https://github.com/broadinstitute/cromwell/issues/3849:287,Integrability,depend,dependencies,287,"The cromwell API currently only allows users to submit the actual workflow file when they'd like to run a workflow. Also allow for a user to submit the URL pointing to the workflow file, which will then be consumed by Cromwell. . Edit: While the end goal is to not need a zip bundle for dependencies (as they can now be worked out via relative paths - cc @cjllanwarne ) -- that work is part of [3868](https://github.com/broadinstitute/cromwell/issues/3868). NB: TBD (@ruchim ?) what should we do with `file://` URLs? Perhaps a config option defaulting to off?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3849
https://github.com/broadinstitute/cromwell/issues/3849:527,Modifiability,config,config,527,"The cromwell API currently only allows users to submit the actual workflow file when they'd like to run a workflow. Also allow for a user to submit the URL pointing to the workflow file, which will then be consumed by Cromwell. . Edit: While the end goal is to not need a zip bundle for dependencies (as they can now be worked out via relative paths - cc @cjllanwarne ) -- that work is part of [3868](https://github.com/broadinstitute/cromwell/issues/3868). NB: TBD (@ruchim ?) what should we do with `file://` URLs? Perhaps a config option defaulting to off?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3849
https://github.com/broadinstitute/cromwell/pull/3852:114,Integrability,inject,inject,114,"Allows reading of WDL 1.0 and 1.1 `Ast`s through a shared set of `CheckedAtoB` functions, with the flexibility to inject different transform behavior into each usage of the instantiations of the transforms. Note: of the 9013 added lines, 7484 are the new 1.1 WDL parser and presumably about 810 are files which got moved out of draft-3/transforms and into base/transforms",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3852
https://github.com/broadinstitute/cromwell/pull/3852:114,Security,inject,inject,114,"Allows reading of WDL 1.0 and 1.1 `Ast`s through a shared set of `CheckedAtoB` functions, with the flexibility to inject different transform behavior into each usage of the instantiations of the transforms. Note: of the 9013 added lines, 7484 are the new 1.1 WDL parser and presumably about 810 are files which got moved out of draft-3/transforms and into base/transforms",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3852
https://github.com/broadinstitute/cromwell/issues/3855:16,Availability,Error,Error,16,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3855:58,Availability,error,error,58,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3855:103,Availability,Error,Error,103,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3855:249,Availability,error,error,249,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3855:423,Availability,failure,failure,423,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3855:479,Availability,error,error,479,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3855:546,Availability,error,error,546,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3855:599,Availability,failure,failure,599,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3855:31,Integrability,Message,Message,31,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3855:118,Integrability,Message,Message,118,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3855:390,Integrability,message,message,390,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3855:485,Integrability,message,message,485,"Cromwell treats Error Code 10, Message 14 as a preemption error. When a preemptible machine fails with Error Code 10: Message 14, a user doesn't usually see it as Cromwell retries the preemption. However, we've observed it *is* possible to get this error on a non-preemptible machine, which isn't retried and causes a workflow to fail. The problem here is that it's quite unclear from this message that this is a transient failure and it's best to retry the workflow. Adjust the error message to include more information about the nature of this error and action items one can take to mitigate this failure mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855
https://github.com/broadinstitute/cromwell/issues/3861:364,Availability,ERROR,ERROR,364,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:588,Availability,error,error,588,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:756,Availability,error,error,756,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:764,Availability,Error,Error,764,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1855,Availability,recover,recoverWith,1855,"t or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:128,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,128,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:913,Deployability,pipeline,pipelines,913,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:930,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,930,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:991,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,991,"example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunna",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1071,Deployability,pipeline,pipelines,1071,"ystem-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1088,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1088,"spatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1154,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1154,"nActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1235,Deployability,pipeline,pipelines,1235,"well-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1252,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1252,"750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1317,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1317,"akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1398,Deployability,pipeline,pipelines,1398,"2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$Blockabl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1415,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1415,"dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1480,Deployability,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1480,"Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDisp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1828,Performance,concurren,concurrent,1828,"all not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1897,Performance,concurren,concurrent,1897,end.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1974,Performance,concurren,concurrent,1974,"rdException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2018-07-04 07:20:37,090 cromwell-system-akka.dispatcher",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:2295,Performance,concurren,concurrent,2295,"ExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). 2018-07-04 07:20:37,090 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowManagerActor WorkflowActor-b2e34f33-e643-437f-aa38-b62f6d44f2dc is in a terminal state: WorkflowFailedState; ```. [centaur_log.txt](https://github.com/broadinstitute/cromwell/files/2163881/centaur_log.txt). Best viewed with `less -R centaur_log.txt`. The image has been there since t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:1855,Safety,recover,recoverWith,1855,"t or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:874,Security,access,access,874,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3861:30,Testability,test,test,30,"An example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861
https://github.com/broadinstitute/cromwell/issues/3863:26,Integrability,message,message,26,"```; {; ""causedBy"": [],; ""message"": ""Bad output 'GatherVcfs.output_vcf_index': No such field 'tbi' on type String. Report this bug! Static validation failed.""; }; ```. from wdl snippet: ; ```; output {; File output_vcf = ""~{output_vcf_filename}""; File output_vcf_index = ""~{output_vcf_filename}"".tbi; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863
https://github.com/broadinstitute/cromwell/issues/3863:139,Security,validat,validation,139,"```; {; ""causedBy"": [],; ""message"": ""Bad output 'GatherVcfs.output_vcf_index': No such field 'tbi' on type String. Report this bug! Static validation failed.""; }; ```. from wdl snippet: ; ```; output {; File output_vcf = ""~{output_vcf_filename}""; File output_vcf_index = ""~{output_vcf_filename}"".tbi; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863
https://github.com/broadinstitute/cromwell/pull/3864:262,Deployability,configurat,configurations,262,I cannot test this change as I do not have the build environment set up. Is is intentional that this 'cloud' feature is inherited by the SGE backend? I am fixing an issue caused by a feature I do not need. I wonder if it could be placed in the epilogue for user configurations. Isn't that the purpose of epilogue and configuration?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864
https://github.com/broadinstitute/cromwell/pull/3864:317,Deployability,configurat,configuration,317,I cannot test this change as I do not have the build environment set up. Is is intentional that this 'cloud' feature is inherited by the SGE backend? I am fixing an issue caused by a feature I do not need. I wonder if it could be placed in the epilogue for user configurations. Isn't that the purpose of epilogue and configuration?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864
https://github.com/broadinstitute/cromwell/pull/3864:120,Modifiability,inherit,inherited,120,I cannot test this change as I do not have the build environment set up. Is is intentional that this 'cloud' feature is inherited by the SGE backend? I am fixing an issue caused by a feature I do not need. I wonder if it could be placed in the epilogue for user configurations. Isn't that the purpose of epilogue and configuration?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864
https://github.com/broadinstitute/cromwell/pull/3864:262,Modifiability,config,configurations,262,I cannot test this change as I do not have the build environment set up. Is is intentional that this 'cloud' feature is inherited by the SGE backend? I am fixing an issue caused by a feature I do not need. I wonder if it could be placed in the epilogue for user configurations. Isn't that the purpose of epilogue and configuration?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864
https://github.com/broadinstitute/cromwell/pull/3864:317,Modifiability,config,configuration,317,I cannot test this change as I do not have the build environment set up. Is is intentional that this 'cloud' feature is inherited by the SGE backend? I am fixing an issue caused by a feature I do not need. I wonder if it could be placed in the epilogue for user configurations. Isn't that the purpose of epilogue and configuration?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864
https://github.com/broadinstitute/cromwell/pull/3864:9,Testability,test,test,9,I cannot test this change as I do not have the build environment set up. Is is intentional that this 'cloud' feature is inherited by the SGE backend? I am fixing an issue caused by a feature I do not need. I wonder if it could be placed in the epilogue for user configurations. Isn't that the purpose of epilogue and configuration?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3864
https://github.com/broadinstitute/cromwell/issues/3869:250,Availability,error,error,250,"I am brand new to cromwell and sorry if this question looks so basic.; Following the tutorial of ""Server Mode"", I open a webpage `localhost:8000` but I didn't find a button of `choose file` at `workflowSource` or at `workflowInputs`. I did see a red error sign at the bottom showing `ERROR {...}` and there is a message in it `{""schemaValidationMessages"":[{""level"":""error"",""message"":""Can't read from file /swagger/cromwell.yaml""}]}`; Any suggestions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869
https://github.com/broadinstitute/cromwell/issues/3869:284,Availability,ERROR,ERROR,284,"I am brand new to cromwell and sorry if this question looks so basic.; Following the tutorial of ""Server Mode"", I open a webpage `localhost:8000` but I didn't find a button of `choose file` at `workflowSource` or at `workflowInputs`. I did see a red error sign at the bottom showing `ERROR {...}` and there is a message in it `{""schemaValidationMessages"":[{""level"":""error"",""message"":""Can't read from file /swagger/cromwell.yaml""}]}`; Any suggestions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869
https://github.com/broadinstitute/cromwell/issues/3869:366,Availability,error,error,366,"I am brand new to cromwell and sorry if this question looks so basic.; Following the tutorial of ""Server Mode"", I open a webpage `localhost:8000` but I didn't find a button of `choose file` at `workflowSource` or at `workflowInputs`. I did see a red error sign at the bottom showing `ERROR {...}` and there is a message in it `{""schemaValidationMessages"":[{""level"":""error"",""message"":""Can't read from file /swagger/cromwell.yaml""}]}`; Any suggestions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869
https://github.com/broadinstitute/cromwell/issues/3869:312,Integrability,message,message,312,"I am brand new to cromwell and sorry if this question looks so basic.; Following the tutorial of ""Server Mode"", I open a webpage `localhost:8000` but I didn't find a button of `choose file` at `workflowSource` or at `workflowInputs`. I did see a red error sign at the bottom showing `ERROR {...}` and there is a message in it `{""schemaValidationMessages"":[{""level"":""error"",""message"":""Can't read from file /swagger/cromwell.yaml""}]}`; Any suggestions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869
https://github.com/broadinstitute/cromwell/issues/3869:374,Integrability,message,message,374,"I am brand new to cromwell and sorry if this question looks so basic.; Following the tutorial of ""Server Mode"", I open a webpage `localhost:8000` but I didn't find a button of `choose file` at `workflowSource` or at `workflowInputs`. I did see a red error sign at the bottom showing `ERROR {...}` and there is a message in it `{""schemaValidationMessages"":[{""level"":""error"",""message"":""Can't read from file /swagger/cromwell.yaml""}]}`; Any suggestions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869
https://github.com/broadinstitute/cromwell/issues/3871:186,Security,validat,validate,186,"- [x] Implement https://github.com/openwdl/wdl/pull/219 in the WDL Biscayne classes; - Implement all of the changes in the SPEC text, not just the issue description!; - [x] Add `womtool validate` tests to ensure they are still not allowed in `version 1.0`; - [x] Cycle back to the openWDL issue so that it can be *merged* once an implementation exists",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3871
https://github.com/broadinstitute/cromwell/issues/3871:196,Testability,test,tests,196,"- [x] Implement https://github.com/openwdl/wdl/pull/219 in the WDL Biscayne classes; - Implement all of the changes in the SPEC text, not just the issue description!; - [x] Add `womtool validate` tests to ensure they are still not allowed in `version 1.0`; - [x] Cycle back to the openWDL issue so that it can be *merged* once an implementation exists",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3871
https://github.com/broadinstitute/cromwell/issues/3872:36,Usability,user experience,user experience,36,"We may not be able to make a ""good"" user experience here by ourselves (eg versions may need to happen to align) but we should be able to move the ball forwards by allowing ""if stars align"" relative imports.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3872
https://github.com/broadinstitute/cromwell/issues/3873:173,Availability,error,error,173,"On Cromwell version, ""34-5156b78-SNAP"", when making a request to the query endpoint that includes the `""includeSubworkflows"": ""false""` parameter, the database will throw an error if the query matches ~1000 results. . Example POST request:; ```; curl -X POST ""https://cromwell.caas-dev.broadinstitute.org/api/workflows/v1/query"" -H ""accept: application/json"" -H ""authorization: Bearer XXXXX"" -H ""Content-Type: application/json"" -d ""[ { \""status\"": \""Failed\"", \""includeSubworkflows\"": \""false\"" }]""; ```; Error message:; ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@27386688 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@2e52a800[Running, pool size = 200, active threads = 200, queued tasks = 999, completed tasks = 16375]""; }; ```; This error is very similar to the other issue: https://github.com/broadinstitute/cromwell/issues/3115; but `includeSubworkflows` shouldn't limit the number of results the `/query` endpoint can handle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873
https://github.com/broadinstitute/cromwell/issues/3873:504,Availability,Error,Error,504,"On Cromwell version, ""34-5156b78-SNAP"", when making a request to the query endpoint that includes the `""includeSubworkflows"": ""false""` parameter, the database will throw an error if the query matches ~1000 results. . Example POST request:; ```; curl -X POST ""https://cromwell.caas-dev.broadinstitute.org/api/workflows/v1/query"" -H ""accept: application/json"" -H ""authorization: Bearer XXXXX"" -H ""Content-Type: application/json"" -d ""[ { \""status\"": \""Failed\"", \""includeSubworkflows\"": \""false\"" }]""; ```; Error message:; ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@27386688 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@2e52a800[Running, pool size = 200, active threads = 200, queued tasks = 999, completed tasks = 16375]""; }; ```; This error is very similar to the other issue: https://github.com/broadinstitute/cromwell/issues/3115; but `includeSubworkflows` shouldn't limit the number of results the `/query` endpoint can handle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873
https://github.com/broadinstitute/cromwell/issues/3873:790,Availability,error,error,790,"On Cromwell version, ""34-5156b78-SNAP"", when making a request to the query endpoint that includes the `""includeSubworkflows"": ""false""` parameter, the database will throw an error if the query matches ~1000 results. . Example POST request:; ```; curl -X POST ""https://cromwell.caas-dev.broadinstitute.org/api/workflows/v1/query"" -H ""accept: application/json"" -H ""authorization: Bearer XXXXX"" -H ""Content-Type: application/json"" -d ""[ { \""status\"": \""Failed\"", \""includeSubworkflows\"": \""false\"" }]""; ```; Error message:; ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@27386688 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@2e52a800[Running, pool size = 200, active threads = 200, queued tasks = 999, completed tasks = 16375]""; }; ```; This error is very similar to the other issue: https://github.com/broadinstitute/cromwell/issues/3115; but `includeSubworkflows` shouldn't limit the number of results the `/query` endpoint can handle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873
https://github.com/broadinstitute/cromwell/issues/3873:510,Integrability,message,message,510,"On Cromwell version, ""34-5156b78-SNAP"", when making a request to the query endpoint that includes the `""includeSubworkflows"": ""false""` parameter, the database will throw an error if the query matches ~1000 results. . Example POST request:; ```; curl -X POST ""https://cromwell.caas-dev.broadinstitute.org/api/workflows/v1/query"" -H ""accept: application/json"" -H ""authorization: Bearer XXXXX"" -H ""Content-Type: application/json"" -d ""[ { \""status\"": \""Failed\"", \""includeSubworkflows\"": \""false\"" }]""; ```; Error message:; ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@27386688 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@2e52a800[Running, pool size = 200, active threads = 200, queued tasks = 999, completed tasks = 16375]""; }; ```; This error is very similar to the other issue: https://github.com/broadinstitute/cromwell/issues/3115; but `includeSubworkflows` shouldn't limit the number of results the `/query` endpoint can handle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873
https://github.com/broadinstitute/cromwell/issues/3873:548,Integrability,message,message,548,"On Cromwell version, ""34-5156b78-SNAP"", when making a request to the query endpoint that includes the `""includeSubworkflows"": ""false""` parameter, the database will throw an error if the query matches ~1000 results. . Example POST request:; ```; curl -X POST ""https://cromwell.caas-dev.broadinstitute.org/api/workflows/v1/query"" -H ""accept: application/json"" -H ""authorization: Bearer XXXXX"" -H ""Content-Type: application/json"" -d ""[ { \""status\"": \""Failed\"", \""includeSubworkflows\"": \""false\"" }]""; ```; Error message:; ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@27386688 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@2e52a800[Running, pool size = 200, active threads = 200, queued tasks = 999, completed tasks = 16375]""; }; ```; This error is very similar to the other issue: https://github.com/broadinstitute/cromwell/issues/3115; but `includeSubworkflows` shouldn't limit the number of results the `/query` endpoint can handle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873
https://github.com/broadinstitute/cromwell/issues/3873:730,Performance,queue,queued,730,"On Cromwell version, ""34-5156b78-SNAP"", when making a request to the query endpoint that includes the `""includeSubworkflows"": ""false""` parameter, the database will throw an error if the query matches ~1000 results. . Example POST request:; ```; curl -X POST ""https://cromwell.caas-dev.broadinstitute.org/api/workflows/v1/query"" -H ""accept: application/json"" -H ""authorization: Bearer XXXXX"" -H ""Content-Type: application/json"" -d ""[ { \""status\"": \""Failed\"", \""includeSubworkflows\"": \""false\"" }]""; ```; Error message:; ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@27386688 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@2e52a800[Running, pool size = 200, active threads = 200, queued tasks = 999, completed tasks = 16375]""; }; ```; This error is very similar to the other issue: https://github.com/broadinstitute/cromwell/issues/3115; but `includeSubworkflows` shouldn't limit the number of results the `/query` endpoint can handle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873
https://github.com/broadinstitute/cromwell/issues/3873:362,Security,authoriz,authorization,362,"On Cromwell version, ""34-5156b78-SNAP"", when making a request to the query endpoint that includes the `""includeSubworkflows"": ""false""` parameter, the database will throw an error if the query matches ~1000 results. . Example POST request:; ```; curl -X POST ""https://cromwell.caas-dev.broadinstitute.org/api/workflows/v1/query"" -H ""accept: application/json"" -H ""authorization: Bearer XXXXX"" -H ""Content-Type: application/json"" -d ""[ { \""status\"": \""Failed\"", \""includeSubworkflows\"": \""false\"" }]""; ```; Error message:; ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@27386688 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@2e52a800[Running, pool size = 200, active threads = 200, queued tasks = 999, completed tasks = 16375]""; }; ```; This error is very similar to the other issue: https://github.com/broadinstitute/cromwell/issues/3115; but `includeSubworkflows` shouldn't limit the number of results the `/query` endpoint can handle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873
https://github.com/broadinstitute/cromwell/issues/3874:1099,Availability,heartbeat,heartbeat,1099,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874
https://github.com/broadinstitute/cromwell/issues/3874:772,Modifiability,variab,variable,772,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874
https://github.com/broadinstitute/cromwell/issues/3874:20,Safety,timeout,timeouts,20,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874
https://github.com/broadinstitute/cromwell/issues/3874:1022,Safety,timeout,timeout,1022,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874
https://github.com/broadinstitute/cromwell/issues/3874:58,Testability,test,test,58,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874
https://github.com/broadinstitute/cromwell/issues/3874:94,Testability,test,test,94,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874
https://github.com/broadinstitute/cromwell/issues/3874:386,Testability,test,test,386,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874
https://github.com/broadinstitute/cromwell/issues/3874:1016,Testability,Test,Tests,1016,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874
https://github.com/broadinstitute/cromwell/issues/3874:1123,Testability,test,test,1123,"Centaur has its own timeouts before it gives up. So does `test.inc.sh`. Setting the value in `test.inc.sh` should also set the value for centaur. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/src/main/resources/reference.conf#L37-L38. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/bin/test.inc.sh#L130-L136. Currently values for centaur are set through multiple `-Dkey=value` settings inside `test_cromwel.sh`. https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/centaur/test_cromwell.sh#L127-L134. A couple options among others:; - This can be another `getopts` argument wired into `test_cromwell.sh`; - This could be an environment variable that overrides a default, as is currently used for setting database connection info; https://github.com/broadinstitute/cromwell/blob/5156b786ac5fcf9db3c6c146ab9f78658a29274a/src/ci/resources/build_application.inc.conf#L15-L28. A/C:; - Tests timeout at approximately the same duration in the centaur executable and the heartbeat generated by `test.inc.sh`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874
https://github.com/broadinstitute/cromwell/issues/3876:288,Availability,avail,avail,288,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:342,Availability,error,errors,342,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:1892,Availability,error,error,1892,"cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # reso",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2883,Availability,error,error,2883,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:4540,Availability,alive,alive,4540,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:5074,Availability,fault,fault,5074,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:32,Deployability,pipeline,pipeline,32,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:81,Deployability,pipeline,pipeline,81,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:259,Deployability,configurat,configuration,259,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2050,Energy Efficiency,adapt,adapters,2050,"ion/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if pair",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2136,Energy Efficiency,adapt,adapters,2136,"ion/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if pair",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2499,Energy Efficiency,adapt,adapters,2499,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2636,Energy Efficiency,adapt,adapters,2636,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2762,Energy Efficiency,adapt,adapters,2762,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2875,Energy Efficiency,adapt,adapter,2875,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:3114,Energy Efficiency,adapt,adapters,3114,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:3135,Energy Efficiency,adapt,adapters,3135,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:3264,Energy Efficiency,adapt,adapter,3264,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:349,Integrability,message,message,349,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2050,Integrability,adapter,adapters,2050,"ion/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if pair",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2136,Integrability,adapter,adapters,2136,"ion/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if pair",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2499,Integrability,adapter,adapters,2499,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2636,Integrability,adapter,adapters,2636,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2762,Integrability,adapter,adapters,2762,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2875,Integrability,adapter,adapter,2875,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:3114,Integrability,adapter,adapters,3114,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:3135,Integrability,adapter,adapters,3135,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:3264,Integrability,adapter,adapter,3264,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:259,Modifiability,config,configuration,259,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:564,Modifiability,sandbox,sandbox,564,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:765,Modifiability,sandbox,sandbox,765,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:1025,Modifiability,sandbox,sandbox,1025,"low hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:1250,Modifiability,sandbox,sandbox,1250,"n get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = tri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:1519,Modifiability,sandbox,sandbox,1519,"0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:1696,Modifiability,sandbox,sandbox,1696,"0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2050,Modifiability,adapt,adapters,2050,"ion/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if pair",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2136,Modifiability,adapt,adapters,2136,"ion/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if pair",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2499,Modifiability,adapt,adapters,2499,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2636,Modifiability,adapt,adapters,2636,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2762,Modifiability,adapt,adapters,2762,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2875,Modifiability,adapt,adapter,2875,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:3114,Modifiability,adapt,adapters,3114,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:3135,Modifiability,adapt,adapters,3135,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:3264,Modifiability,adapt,adapter,3264,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:4141,Modifiability,config,config,4141,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:4148,Modifiability,Config,ConfigBackendLifecycleActorFactory,4148,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:4185,Modifiability,config,config,4185,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:5025,Modifiability,config,config,5025,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:4195,Performance,concurren,concurrent-job-limit,4195,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2750,Safety,detect,detect,2750,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:3257,Safety,detect,detect-adapter,3257,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:4739,Security,hash,hashing-strategy,4739,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:4937,Security,hash,hashing-strategy,4937,".py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; String? disks; String? time; String? preemptible; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe smp ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; }; }; }; engine{; filesystems{; local{; localization: [; ""soft-link"",""copy"",""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"",""copy"",""hard-link""]; hashing-strategy: ""file""; }; }; }; }; ```. I wonder if there is something wrong with my config files or if Cromwell's localization is at fault.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:564,Testability,sandbox,sandbox,564,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:765,Testability,sandbox,sandbox,765,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:1025,Testability,sandbox,sandbox,1025,"low hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:1250,Testability,sandbox,sandbox,1250,"n get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = tri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:1519,Testability,sandbox,sandbox,1519,"0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:1696,Testability,sandbox,sandbox,1696,"0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3876:2990,Testability,test,test,2990,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876
https://github.com/broadinstitute/cromwell/issues/3880:143,Deployability,release,release,143,"On Cromwell version, ""34-5156b78-SNAP"", when releasing a workflow with ""On Hold"" status, if the workflow has been assigned with a label, after release, the label will disappear or get reset to the original value.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3880
https://github.com/broadinstitute/cromwell/issues/3881:168,Availability,avail,available,168,Currently all output files will be kept. In most cases not all output files need to be kept and only wasting a lot of disk space. When having to not a lot of diskspace available might block a full scale pipeline run. To do this each jobs and/or (sub)workflow should have a runtime tag `intermediate: Boolean` or `intermediate_files: Array[File]`. When no downstream dependency need those files anymore they can be removed during the pipeline run. The same functionality was already implemented in GATK/Queue.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881
https://github.com/broadinstitute/cromwell/issues/3881:355,Availability,down,downstream,355,Currently all output files will be kept. In most cases not all output files need to be kept and only wasting a lot of disk space. When having to not a lot of diskspace available might block a full scale pipeline run. To do this each jobs and/or (sub)workflow should have a runtime tag `intermediate: Boolean` or `intermediate_files: Array[File]`. When no downstream dependency need those files anymore they can be removed during the pipeline run. The same functionality was already implemented in GATK/Queue.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881
https://github.com/broadinstitute/cromwell/issues/3881:203,Deployability,pipeline,pipeline,203,Currently all output files will be kept. In most cases not all output files need to be kept and only wasting a lot of disk space. When having to not a lot of diskspace available might block a full scale pipeline run. To do this each jobs and/or (sub)workflow should have a runtime tag `intermediate: Boolean` or `intermediate_files: Array[File]`. When no downstream dependency need those files anymore they can be removed during the pipeline run. The same functionality was already implemented in GATK/Queue.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881
https://github.com/broadinstitute/cromwell/issues/3881:433,Deployability,pipeline,pipeline,433,Currently all output files will be kept. In most cases not all output files need to be kept and only wasting a lot of disk space. When having to not a lot of diskspace available might block a full scale pipeline run. To do this each jobs and/or (sub)workflow should have a runtime tag `intermediate: Boolean` or `intermediate_files: Array[File]`. When no downstream dependency need those files anymore they can be removed during the pipeline run. The same functionality was already implemented in GATK/Queue.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881
https://github.com/broadinstitute/cromwell/issues/3881:366,Integrability,depend,dependency,366,Currently all output files will be kept. In most cases not all output files need to be kept and only wasting a lot of disk space. When having to not a lot of diskspace available might block a full scale pipeline run. To do this each jobs and/or (sub)workflow should have a runtime tag `intermediate: Boolean` or `intermediate_files: Array[File]`. When no downstream dependency need those files anymore they can be removed during the pipeline run. The same functionality was already implemented in GATK/Queue.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881
https://github.com/broadinstitute/cromwell/issues/3881:502,Performance,Queue,Queue,502,Currently all output files will be kept. In most cases not all output files need to be kept and only wasting a lot of disk space. When having to not a lot of diskspace available might block a full scale pipeline run. To do this each jobs and/or (sub)workflow should have a runtime tag `intermediate: Boolean` or `intermediate_files: Array[File]`. When no downstream dependency need those files anymore they can be removed during the pipeline run. The same functionality was already implemented in GATK/Queue.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3881
https://github.com/broadinstitute/cromwell/issues/3885:146,Deployability,pipeline,pipelines,146,This workflow reports a cyclic dependency but (a) it probably shouldn't and (b) it's super hard to debug:. https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/single_sample/somatic/SomaticPairedSingleSampleWf.wdl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3885
https://github.com/broadinstitute/cromwell/issues/3885:31,Integrability,depend,dependency,31,This workflow reports a cyclic dependency but (a) it probably shouldn't and (b) it's super hard to debug:. https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/single_sample/somatic/SomaticPairedSingleSampleWf.wdl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3885
https://github.com/broadinstitute/cromwell/issues/3887:109,Deployability,update,update,109,"<script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; . I am proposing to update workflow timing web page as the google timeline chart codes have been revised. Here the current code uses 'http://www.google.com/jsapi' (https://github.com/broadinstitute/cromwell/blob/c8bf8fdc51e2df19d8e8a3d066acc64f588f1b1c/engine/src/main/resources/workflowTimings/workflowTimings.html#L4). It is not suggested by Google Chart documentation (https://developers.google.com/chart/interactive/docs/gallery/timeline), and this URL is blocked by our firewall. In order to use cromwell timing API for myself, I suggest to change relevant lines to:. <script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js""></script>; <script type=""text/javascript"">. var parentWorkflowNames = [];; var expandedParentWorkflows = [];; var chartView;; google.charts.load('current', {packages: ['timeline']});; google.charts.setOnLoadCallback(drawChart);. Can you update cromwell source codes to do that? Or please let me know if you need a pull request. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3887
https://github.com/broadinstitute/cromwell/issues/3887:1068,Deployability,update,update,1068,"<script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; . I am proposing to update workflow timing web page as the google timeline chart codes have been revised. Here the current code uses 'http://www.google.com/jsapi' (https://github.com/broadinstitute/cromwell/blob/c8bf8fdc51e2df19d8e8a3d066acc64f588f1b1c/engine/src/main/resources/workflowTimings/workflowTimings.html#L4). It is not suggested by Google Chart documentation (https://developers.google.com/chart/interactive/docs/gallery/timeline), and this URL is blocked by our firewall. In order to use cromwell timing API for myself, I suggest to change relevant lines to:. <script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js""></script>; <script type=""text/javascript"">. var parentWorkflowNames = [];; var expandedParentWorkflows = [];; var chartView;; google.charts.load('current', {packages: ['timeline']});; google.charts.setOnLoadCallback(drawChart);. Can you update cromwell source codes to do that? Or please let me know if you need a pull request. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3887
https://github.com/broadinstitute/cromwell/issues/3887:67,Performance,load,loader,67,"<script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; . I am proposing to update workflow timing web page as the google timeline chart codes have been revised. Here the current code uses 'http://www.google.com/jsapi' (https://github.com/broadinstitute/cromwell/blob/c8bf8fdc51e2df19d8e8a3d066acc64f588f1b1c/engine/src/main/resources/workflowTimings/workflowTimings.html#L4). It is not suggested by Google Chart documentation (https://developers.google.com/chart/interactive/docs/gallery/timeline), and this URL is blocked by our firewall. In order to use cromwell timing API for myself, I suggest to change relevant lines to:. <script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js""></script>; <script type=""text/javascript"">. var parentWorkflowNames = [];; var expandedParentWorkflows = [];; var chartView;; google.charts.load('current', {packages: ['timeline']});; google.charts.setOnLoadCallback(drawChart);. Can you update cromwell source codes to do that? Or please let me know if you need a pull request. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3887
https://github.com/broadinstitute/cromwell/issues/3887:729,Performance,load,loader,729,"<script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; . I am proposing to update workflow timing web page as the google timeline chart codes have been revised. Here the current code uses 'http://www.google.com/jsapi' (https://github.com/broadinstitute/cromwell/blob/c8bf8fdc51e2df19d8e8a3d066acc64f588f1b1c/engine/src/main/resources/workflowTimings/workflowTimings.html#L4). It is not suggested by Google Chart documentation (https://developers.google.com/chart/interactive/docs/gallery/timeline), and this URL is blocked by our firewall. In order to use cromwell timing API for myself, I suggest to change relevant lines to:. <script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js""></script>; <script type=""text/javascript"">. var parentWorkflowNames = [];; var expandedParentWorkflows = [];; var chartView;; google.charts.load('current', {packages: ['timeline']});; google.charts.setOnLoadCallback(drawChart);. Can you update cromwell source codes to do that? Or please let me know if you need a pull request. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3887
https://github.com/broadinstitute/cromwell/issues/3887:971,Performance,load,load,971,"<script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; . I am proposing to update workflow timing web page as the google timeline chart codes have been revised. Here the current code uses 'http://www.google.com/jsapi' (https://github.com/broadinstitute/cromwell/blob/c8bf8fdc51e2df19d8e8a3d066acc64f588f1b1c/engine/src/main/resources/workflowTimings/workflowTimings.html#L4). It is not suggested by Google Chart documentation (https://developers.google.com/chart/interactive/docs/gallery/timeline), and this URL is blocked by our firewall. In order to use cromwell timing API for myself, I suggest to change relevant lines to:. <script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js""></script>; <script type=""text/javascript"">. var parentWorkflowNames = [];; var expandedParentWorkflows = [];; var chartView;; google.charts.load('current', {packages: ['timeline']});; google.charts.setOnLoadCallback(drawChart);. Can you update cromwell source codes to do that? Or please let me know if you need a pull request. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3887
https://github.com/broadinstitute/cromwell/issues/3887:564,Security,firewall,firewall,564,"<script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; . I am proposing to update workflow timing web page as the google timeline chart codes have been revised. Here the current code uses 'http://www.google.com/jsapi' (https://github.com/broadinstitute/cromwell/blob/c8bf8fdc51e2df19d8e8a3d066acc64f588f1b1c/engine/src/main/resources/workflowTimings/workflowTimings.html#L4). It is not suggested by Google Chart documentation (https://developers.google.com/chart/interactive/docs/gallery/timeline), and this URL is blocked by our firewall. In order to use cromwell timing API for myself, I suggest to change relevant lines to:. <script type=""text/javascript"" src=""https://www.gstatic.com/charts/loader.js""></script>; <script src=""https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js""></script>; <script type=""text/javascript"">. var parentWorkflowNames = [];; var expandedParentWorkflows = [];; var chartView;; google.charts.load('current', {packages: ['timeline']});; google.charts.setOnLoadCallback(drawChart);. Can you update cromwell source codes to do that? Or please let me know if you need a pull request. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3887
https://github.com/broadinstitute/cromwell/issues/3889:140,Modifiability,config,config,140,"For our CI testing we want to be able to set the location of the `cromwell-executions` folder. This can already be done by messing with the config file, but this is not ideal. The `root` setting is nested within `backend -> providers -> the-used-backend -> config -> root`. This is very hard to set using the command-line with the `-D` option. . Is there a possibility to set an option that is agnostic of background? `-Dbackends.default_root` for example? Or just on option to set it on the command-line `run --root my_preferred_root my_workflow.wdl`?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3889
https://github.com/broadinstitute/cromwell/issues/3889:257,Modifiability,config,config,257,"For our CI testing we want to be able to set the location of the `cromwell-executions` folder. This can already be done by messing with the config file, but this is not ideal. The `root` setting is nested within `backend -> providers -> the-used-backend -> config -> root`. This is very hard to set using the command-line with the `-D` option. . Is there a possibility to set an option that is agnostic of background? `-Dbackends.default_root` for example? Or just on option to set it on the command-line `run --root my_preferred_root my_workflow.wdl`?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3889
https://github.com/broadinstitute/cromwell/issues/3889:11,Testability,test,testing,11,"For our CI testing we want to be able to set the location of the `cromwell-executions` folder. This can already be done by messing with the config file, but this is not ideal. The `root` setting is nested within `backend -> providers -> the-used-backend -> config -> root`. This is very hard to set using the command-line with the `-D` option. . Is there a possibility to set an option that is agnostic of background? `-Dbackends.default_root` for example? Or just on option to set it on the command-line `run --root my_preferred_root my_workflow.wdl`?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3889
https://github.com/broadinstitute/cromwell/pull/3892:58,Availability,error,errors,58,- The engine now retries both of the known Requester pays errors; - The backend (localization/delocalization logic) retries copy failures (no matter what the error) with a project flag.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3892
https://github.com/broadinstitute/cromwell/pull/3892:129,Availability,failure,failures,129,- The engine now retries both of the known Requester pays errors; - The backend (localization/delocalization logic) retries copy failures (no matter what the error) with a project flag.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3892
https://github.com/broadinstitute/cromwell/pull/3892:158,Availability,error,error,158,- The engine now retries both of the known Requester pays errors; - The backend (localization/delocalization logic) retries copy failures (no matter what the error) with a project flag.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3892
https://github.com/broadinstitute/cromwell/pull/3892:109,Testability,log,logic,109,- The engine now retries both of the known Requester pays errors; - The backend (localization/delocalization logic) retries copy failures (no matter what the error) with a project flag.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3892
https://github.com/broadinstitute/cromwell/issues/3893:84,Usability,simpl,simple,84,"Add a travis job which is using the GA4GH orchestrator to submit & process one/some simple workflow(s):. - Submit workflow by URL, not by workflow source; - Workflow(s) should have at least one `File` input; - `File` input(s) should be provided as HTTP URLs in the inputs JSON",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3893
https://github.com/broadinstitute/cromwell/issues/3894:131,Availability,avail,availability,131,"Currently -- the Cromwell (and other?) service logs on the alpha env are around for upto 5 days. . It would be great to have their availability extended to a longer life line, if feasible. . Being investigated by David Bernick.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3894
https://github.com/broadinstitute/cromwell/issues/3894:144,Modifiability,extend,extended,144,"Currently -- the Cromwell (and other?) service logs on the alpha env are around for upto 5 days. . It would be great to have their availability extended to a longer life line, if feasible. . Being investigated by David Bernick.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3894
https://github.com/broadinstitute/cromwell/issues/3894:47,Testability,log,logs,47,"Currently -- the Cromwell (and other?) service logs on the alpha env are around for upto 5 days. . It would be great to have their availability extended to a longer life line, if feasible. . Being investigated by David Bernick.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3894
https://github.com/broadinstitute/cromwell/issues/3896:170,Energy Efficiency,monitor,monitor,170,"From forum post https://gatkforums.broadinstitute.org/wdl/discussion/12361/continue-on-sigterm-code#latest. The situation:; * Local backend; * The python script spawns a monitor which will SIGTERM it when the task completes; * The `128+SIGTERM` exit code was specified as valid in the runtime attributes; * However, Cromwell has already assumed that the job was aborted before it checks against the `continueOnReturnCode` values, the workflow fails instead of continuing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3896
https://github.com/broadinstitute/cromwell/issues/3896:362,Safety,abort,aborted,362,"From forum post https://gatkforums.broadinstitute.org/wdl/discussion/12361/continue-on-sigterm-code#latest. The situation:; * Local backend; * The python script spawns a monitor which will SIGTERM it when the task completes; * The `128+SIGTERM` exit code was specified as valid in the runtime attributes; * However, Cromwell has already assumed that the job was aborted before it checks against the `continueOnReturnCode` values, the workflow fails instead of continuing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3896
https://github.com/broadinstitute/cromwell/issues/3900:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. [WDL Forums discussion](https://gatkforums.broadinstitute.org/wdl/discussion/12295/how-to-submit-labels-to-gcs-backend). In #3233 the ability to customize GCS VM labels was removed. My understanding was that this was to lift Google's restrictions on VM labels from the workflow labels. However, I would benefit greatly from the ability to optionally supply labels to each VM. . Specifically, I'm operating my own fork of the [cromwell_on_google](https://github.com/openwdl/wdl/tree/master/runners/cromwell_on_google/wdl_runner) tool which I use to spin up large batches of VMs to execute workflows. However, it's hard to pick out individual VMs from the compute api console, especially when running multiple unrelated submissions. My tool creates one Cromwell VM for each submission, which then (via the Google JES backend) starts new VMs for each workflow. To distinguish and filter the VMs in the console, I want to attach two labels to every vm:. * `execution-role` : A label to distinguish between Cromwell VMs and worker VMs; * `submission-id` : A label with submission IDs generated by my tool to distinguish between VMs working on different submissions. The `cromwell-workflow-id` label is helpful, but not convenient if I wish to operate on all VMs working on a single submission _en masse_. The Google genomics API allows me to set `execution-role=Cromwell` and `submission-id={ID}` labels on the Cromwell VMs. However, without Cromwell support, I can't set `execution-role=Worker` and `",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3900
https://github.com/broadinstitute/cromwell/pull/3901:68,Availability,down,down,68,"There were a **lot** of general cleanup rabbit holes I wanted to go down but figured I'd stick w/ a safe harbor of ""do what the ticket asks"" and then follow up with a series of more targeted clean up PRs. If people would prefer (or start asking for changes along those lines) I'll pull this back and do this. Along similar lines there's a clear overlap erupting between wes2cromwell code and cromiam, however only in places where I could literally use cromiam code as-is did I do so (and even then I left it in cromiam instead of a shared package). While I was working on this it became clear that the ideal shared abstraction is still too early to tell so I'd rather see it play out a bit before going down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3901
https://github.com/broadinstitute/cromwell/pull/3901:703,Availability,down,down,703,"There were a **lot** of general cleanup rabbit holes I wanted to go down but figured I'd stick w/ a safe harbor of ""do what the ticket asks"" and then follow up with a series of more targeted clean up PRs. If people would prefer (or start asking for changes along those lines) I'll pull this back and do this. Along similar lines there's a clear overlap erupting between wes2cromwell code and cromiam, however only in places where I could literally use cromiam code as-is did I do so (and even then I left it in cromiam instead of a shared package). While I was working on this it became clear that the ideal shared abstraction is still too early to tell so I'd rather see it play out a bit before going down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3901
https://github.com/broadinstitute/cromwell/pull/3901:100,Safety,safe,safe,100,"There were a **lot** of general cleanup rabbit holes I wanted to go down but figured I'd stick w/ a safe harbor of ""do what the ticket asks"" and then follow up with a series of more targeted clean up PRs. If people would prefer (or start asking for changes along those lines) I'll pull this back and do this. Along similar lines there's a clear overlap erupting between wes2cromwell code and cromiam, however only in places where I could literally use cromiam code as-is did I do so (and even then I left it in cromiam instead of a shared package). While I was working on this it became clear that the ideal shared abstraction is still too early to tell so I'd rather see it play out a bit before going down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3901
https://github.com/broadinstitute/cromwell/pull/3901:339,Usability,clear,clear,339,"There were a **lot** of general cleanup rabbit holes I wanted to go down but figured I'd stick w/ a safe harbor of ""do what the ticket asks"" and then follow up with a series of more targeted clean up PRs. If people would prefer (or start asking for changes along those lines) I'll pull this back and do this. Along similar lines there's a clear overlap erupting between wes2cromwell code and cromiam, however only in places where I could literally use cromiam code as-is did I do so (and even then I left it in cromiam instead of a shared package). While I was working on this it became clear that the ideal shared abstraction is still too early to tell so I'd rather see it play out a bit before going down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3901
https://github.com/broadinstitute/cromwell/pull/3901:587,Usability,clear,clear,587,"There were a **lot** of general cleanup rabbit holes I wanted to go down but figured I'd stick w/ a safe harbor of ""do what the ticket asks"" and then follow up with a series of more targeted clean up PRs. If people would prefer (or start asking for changes along those lines) I'll pull this back and do this. Along similar lines there's a clear overlap erupting between wes2cromwell code and cromiam, however only in places where I could literally use cromiam code as-is did I do so (and even then I left it in cromiam instead of a shared package). While I was working on this it became clear that the ideal shared abstraction is still too early to tell so I'd rather see it play out a bit before going down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3901
https://github.com/broadinstitute/cromwell/pull/3903:603,Modifiability,config,config,603,"Important note: I believe this PR improves correctness when `includeSubworkflows=false` is set but at the cost of query-expense. . Because none of the indexed tables include some `""isASubworkflow""` field, if `includeSubworkflows=false` is set we have to cross reference against the metadata table for each potential query result *before* pagination. Ouch!. Regarding the change in result-order, none of the options feel great. Choose your poison:. 1. ""FTFY"" the query return order for people, making it ""newest first"" rather than ""oldest first""; 2. Add another parameter to the query endpoint; 3. Add a config option . I chose option (1) for this PR, but I'm quite happy to change that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3903
https://github.com/broadinstitute/cromwell/issues/3905:2429,Availability,echo,echo,2429,"B""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; }. task gatk_haplotypecaller_task {; String gatk_path = ""/humgen/gsa-hpprojects/GATK/bin/GenomeAnalysisTK-3.7-93-ge9d8068/GenomeAnalysisTK.jar""; String interval; File in_bam; File in_bam_index; String sample_name = ""fumoz_12""; File ? bqsr_table; String ? ploidy; String ? erc; String ? extra_hc_params. File reference_tgz. String out_gvcf_fn = ""${sample_name}.gvcf"". String output_disk_gb; String boot_disk_gb = ""10""; String ram_gb = ""60""; String cpu_cores = ""1""; String preemptible = ""0""; String debug_dump_flag. command {; set -euo pipefail; ln -sT `pwd` /opt/execution; ln -sT `pwd`/../inputs /opt/inputs. /opt/src/algutil/monitor_start.py; python_cmd=""; import subprocess; def run(cmd):; print (cmd); subprocess.check_call(cmd,shell=True). run('ln -s ${in_bam} in.bam'); run('ln -s ${in_bam_index} in.bam.bai'). run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:3154,Availability,echo,echo,3154,"/opt/execution; ln -sT `pwd`/../inputs /opt/inputs. /opt/src/algutil/monitor_start.py; python_cmd=""; import subprocess; def run(cmd):; print (cmd); subprocess.check_call(cmd,shell=True). run('ln -s ${in_bam} in.bam'); run('ln -s ${in_bam_index} in.bam.bai'). run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${bo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:3183,Availability,echo,echo,3183,"r_start.py; python_cmd=""; import subprocess; def run(cmd):; print (cmd); subprocess.check_call(cmd,shell=True). run('ln -s ${in_bam} in.bam'); run('ln -s ${in_bam_index} in.bam.bai'). run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. app",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:3265,Availability,echo,echo,3265,"r_start.py; python_cmd=""; import subprocess; def run(cmd):; print (cmd); subprocess.check_call(cmd,shell=True). run('ln -s ${in_bam} in.bam'); run('ln -s ${in_bam_index} in.bam.bai'). run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. app",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:3330,Availability,failure,failure,3330,"run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. ap",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:3470,Availability,echo,echo,3470,"run('echo STARTING tar xvf to unpack reference'); run('date'); run('tar xvf ${reference_tgz}'). # Add intervals back in when actually scattering; #; #run('''\; #python /opt/src/intervals_creator.py \; # -r ref.fasta \; # -i $ padding interval_size \; # > intervals.list; #'''). #			--intervals intervals.list \; #			--interval_padding 100 \. run('''\. java -Xmx50G -jar ${gatk_path} \; -T HaplotypeCaller \; -R ref.fasta \; --input_file ${in_bam} \; ${""-BQSR "" + bqsr_table} \; -ERC ${default=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. ap",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:4276,Deployability,configurat,configuration,4276,"e conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:5151,Deployability,Pipeline,Pipelines,5151,"""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; caching {; duplication-strategy = ""reference""; }. }; }; }; }; }; }; ```. I executed a haplotype caller wdl with interval scattering. Two of the shards took over 5 before I aborted the workflow, while the others finished in under 1 hour. The timestamps on the RC file, which indicated a 0 return code, were similar to the other shards that finished in under an hour. . This looks like a bug. I've since restarted the worklow with call-caching so it seems unlikely I can reproduce this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:4276,Modifiability,config,configuration,4276,"e conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:4783,Modifiability,config,config,4783,"f = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; caching {; duplication-strategy = ""reference""; }. }; }; }; }; }; }; ```. I executed a haplotype caller wdl with interval scattering. Two of the shards took over 5 before I aborted the workflow, while the others finished in under 1 hour. The timestamps on the RC file, which indicated a 0 return code, were similar to the other shards that finished in under an h",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:4633,Performance,cache,cache-results,4633,"se; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; caching {; duplication-strategy = ""reference""; }. }; }; }; }; }; }; ```. I executed a haplotype caller wdl with interval scattering. Two of the shards took over 5 before I aborted the work",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:5605,Safety,abort,aborted,5605,"""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; caching {; duplication-strategy = ""reference""; }. }; }; }; }; }; }; ```. I executed a haplotype caller wdl with interval scattering. Two of the shards took over 5 before I aborted the workflow, while the others finished in under 1 hour. The timestamps on the RC file, which indicated a 0 return code, were similar to the other shards that finished in under an hour. . This looks like a bug. I've since restarted the worklow with call-caching so it seems unlikely I can reproduce this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:4321,Security,PASSWORD,PASSWORDS,4321,"e conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:3893,Testability,log,log,3893,"ault=""GVCF"" erc} \; -ploidy ${default=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:3931,Testability,log,log,3931,"efault=""2"" ploidy} \; -o ${out_gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:3955,Testability,log,log,3955,"gvcf_fn} \; 			--intervals ${interval} \; 			--interval_padding 100 \; -variant_index_type LINEAR \; -variant_index_parameter 128000 \; ${default=""\n"" extra_hc_params}; '''). run('echo DONE'); run('date'); "". echo ""$python_cmd""; set +e; python -c ""$python_cmd""; export exit_code=$?; set -e; echo exit code is $exit_code; ls. # create bundle conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3905:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. Workflow: ; ```. workflow gatk_haplotypecaller {; String? onprem_download_path; Map[String, String]? handoff_files; File intervals_file; Array[File] intervals_list = read_lines(intervals_file); String output_filename = ""merged.gvcf"". scatter (interval in intervals_list) {; call gatk_haplotypecaller_task {; input:; interval = interval; }; }. call GatherGVCFs {; input:; input_vcfs = gatk_haplotypecaller_task.out_gvcf,; output_filename = output_filename; }; }. task GatherGVCFs {; String picard_path = ""/cil/shed/apps/external/picard/current/bin/picard.jar""; Array[File] input_vcfs; String output_filename; command {; java -jar -Xmx50G ${picard_path} GatherVcfs -i ${input_vcfs} -i ${output_filename}; }; runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; }. task gatk_haplotypecaller_task {; String gatk_path = ""/humgen/gsa-hpprojects/GATK/bin/GenomeAnalysisTK-3.7-93-ge9d8068/GenomeAnalysisTK.jar""; String interval; File in_bam; File in_bam_index; String sample_name = ""fumoz_12""; File ? bqsr_table; String ? ploidy; String ? erc; String ? extra_hc_params. File reference_tgz. String out_gvcf_fn = ""${sample_name}.gvcf"". String output_disk_gb; Str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905
https://github.com/broadinstitute/cromwell/issues/3908:243,Testability,Test,Tested,243,"[space_glob.zip](https://github.com/broadinstitute/cromwell/files/2209665/space_glob.zip); It looks like glob fails to find `${pattern}*` if `pattern` contains a space. All three glob patterns work when using an underscore instead of a space. Tested using `cromwell 31.1`, `cromwell 32`, `cromwell 33.1`. ```; [2018-07-19 13:17:05,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf.glob1"": [],; ""wf.glob2"": [""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_1"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_10"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_2"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_3"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_4"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_5"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_6"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_7"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_8"", ""/home/redmar/devel/wdl/test/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3908
https://github.com/broadinstitute/cromwell/issues/3908:483,Testability,test,test,483,"derscore instead of a space. Tested using `cromwell 31.1`, `cromwell 32`, `cromwell 33.1`. ```; [2018-07-19 13:17:05,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf.glob1"": [],; ""wf.glob2"": [""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_1"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_10"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_2"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_3"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_4"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_5"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_6"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_7"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_8"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_9""],; ""wf.glob3"": []; },; ""id"": ""ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3908
https://github.com/broadinstitute/cromwell/issues/3908:651,Testability,test,test,651,"derscore instead of a space. Tested using `cromwell 31.1`, `cromwell 32`, `cromwell 33.1`. ```; [2018-07-19 13:17:05,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf.glob1"": [],; ""wf.glob2"": [""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_1"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_10"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_2"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_3"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_4"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_5"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_6"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_7"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_8"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_9""],; ""wf.glob3"": []; },; ""id"": ""ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3908
https://github.com/broadinstitute/cromwell/issues/3908:820,Testability,test,test,820,"derscore instead of a space. Tested using `cromwell 31.1`, `cromwell 32`, `cromwell 33.1`. ```; [2018-07-19 13:17:05,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf.glob1"": [],; ""wf.glob2"": [""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_1"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_10"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_2"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_3"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_4"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_5"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_6"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_7"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_8"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_9""],; ""wf.glob3"": []; },; ""id"": ""ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3908
https://github.com/broadinstitute/cromwell/issues/3908:988,Testability,test,test,988,"derscore instead of a space. Tested using `cromwell 31.1`, `cromwell 32`, `cromwell 33.1`. ```; [2018-07-19 13:17:05,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf.glob1"": [],; ""wf.glob2"": [""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_1"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_10"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_2"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_3"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_4"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_5"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_6"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_7"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_8"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_9""],; ""wf.glob3"": []; },; ""id"": ""ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3908
https://github.com/broadinstitute/cromwell/issues/3908:1156,Testability,test,test,1156,"derscore instead of a space. Tested using `cromwell 31.1`, `cromwell 32`, `cromwell 33.1`. ```; [2018-07-19 13:17:05,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf.glob1"": [],; ""wf.glob2"": [""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_1"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_10"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_2"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_3"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_4"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_5"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_6"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_7"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_8"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_9""],; ""wf.glob3"": []; },; ""id"": ""ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3908
https://github.com/broadinstitute/cromwell/issues/3908:1324,Testability,test,test,1324,"derscore instead of a space. Tested using `cromwell 31.1`, `cromwell 32`, `cromwell 33.1`. ```; [2018-07-19 13:17:05,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf.glob1"": [],; ""wf.glob2"": [""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_1"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_10"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_2"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_3"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_4"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_5"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_6"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_7"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_8"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_9""],; ""wf.glob3"": []; },; ""id"": ""ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3908
https://github.com/broadinstitute/cromwell/issues/3908:1492,Testability,test,test,1492,"derscore instead of a space. Tested using `cromwell 31.1`, `cromwell 32`, `cromwell 33.1`. ```; [2018-07-19 13:17:05,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf.glob1"": [],; ""wf.glob2"": [""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_1"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_10"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_2"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_3"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_4"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_5"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_6"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_7"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_8"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_9""],; ""wf.glob3"": []; },; ""id"": ""ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3908
https://github.com/broadinstitute/cromwell/issues/3908:1660,Testability,test,test,1660,"derscore instead of a space. Tested using `cromwell 31.1`, `cromwell 32`, `cromwell 33.1`. ```; [2018-07-19 13:17:05,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf.glob1"": [],; ""wf.glob2"": [""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_1"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_10"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_2"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_3"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_4"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_5"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_6"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_7"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_8"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_9""],; ""wf.glob3"": []; },; ""id"": ""ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3908
https://github.com/broadinstitute/cromwell/issues/3908:1828,Testability,test,test,1828,"derscore instead of a space. Tested using `cromwell 31.1`, `cromwell 32`, `cromwell 33.1`. ```; [2018-07-19 13:17:05,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf.glob1"": [],; ""wf.glob2"": [""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_1"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_10"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_2"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_3"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_4"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_5"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_6"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_7"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_8"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_9""],; ""wf.glob3"": []; },; ""id"": ""ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3908
https://github.com/broadinstitute/cromwell/issues/3908:1996,Testability,test,test,1996,"derscore instead of a space. Tested using `cromwell 31.1`, `cromwell 32`, `cromwell 33.1`. ```; [2018-07-19 13:17:05,99] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""wf.glob1"": [],; ""wf.glob2"": [""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_1"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_10"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_2"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_3"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_4"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_5"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_6"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_7"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_8"", ""/home/redmar/devel/wdl/test/cromwell-executions/wf/ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc/call-spacefile/execution/glob-084dc5941ab6a3e379db0b891a767d3b/File number_9""],; ""wf.glob3"": []; },; ""id"": ""ce2e67fb-79c2-4b90-a1ad-c4b4f6b29bfc""; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3908
https://github.com/broadinstitute/cromwell/issues/3910:1231,Availability,error,errors,1231,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3910:1320,Availability,alive,alive,1320,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3910:22,Deployability,deploy,deploy,22,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3910:117,Deployability,configurat,configuration,117,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3910:229,Deployability,pipeline,pipelines,229,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3910:312,Deployability,pipeline,pipelines,312,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3910:341,Deployability,pipeline,pipelines,341,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3910:987,Deployability,pipeline,pipelines,987,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3910:1332,Deployability,pipeline,pipelines,1332,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3910:117,Modifiability,config,configuration,117,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3910:579,Modifiability,inherit,inherits,579,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3910:920,Modifiability,config,configure,920,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910
https://github.com/broadinstitute/cromwell/issues/3911:432,Availability,error,error,432,"I'm using [CaaS-dev](https://cromwell.caas-dev.broadinstitute.org/) when ran into the issue, with version: `34-66c0fa6-SNAP`. When using the `releaseHold` endpoint to release a workflow, if the workflow_id is in a good format but refers to a non-exist workflow (fake uuid), according to the swagger schema, Cromwell should return 404 to the user. However, now it returns a 500 code which seems to be a wrapper around the actual 404 error:; ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.3, Date: Fri, 20 Jul 2018 13:58:13 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: f4272a19-37dd-4d2c-ba48-ff3844107bf8""; }),HttpProtocol(HTTP/1.1)); ```; This is not a big problem but just brings some incovenience to the error handling process to the users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911
https://github.com/broadinstitute/cromwell/issues/3911:464,Availability,error,error,464,"I'm using [CaaS-dev](https://cromwell.caas-dev.broadinstitute.org/) when ran into the issue, with version: `34-66c0fa6-SNAP`. When using the `releaseHold` endpoint to release a workflow, if the workflow_id is in a good format but refers to a non-exist workflow (fake uuid), according to the swagger schema, Cromwell should return 404 to the user. However, now it returns a 500 code which seems to be a wrapper around the actual 404 error:; ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.3, Date: Fri, 20 Jul 2018 13:58:13 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: f4272a19-37dd-4d2c-ba48-ff3844107bf8""; }),HttpProtocol(HTTP/1.1)); ```; This is not a big problem but just brings some incovenience to the error handling process to the users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911
https://github.com/broadinstitute/cromwell/issues/3911:543,Availability,error,error,543,"I'm using [CaaS-dev](https://cromwell.caas-dev.broadinstitute.org/) when ran into the issue, with version: `34-66c0fa6-SNAP`. When using the `releaseHold` endpoint to release a workflow, if the workflow_id is in a good format but refers to a non-exist workflow (fake uuid), according to the swagger schema, Cromwell should return 404 to the user. However, now it returns a 500 code which seems to be a wrapper around the actual 404 error:; ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.3, Date: Fri, 20 Jul 2018 13:58:13 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: f4272a19-37dd-4d2c-ba48-ff3844107bf8""; }),HttpProtocol(HTTP/1.1)); ```; This is not a big problem but just brings some incovenience to the error handling process to the users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911
https://github.com/broadinstitute/cromwell/issues/3911:888,Availability,error,error,888,"I'm using [CaaS-dev](https://cromwell.caas-dev.broadinstitute.org/) when ran into the issue, with version: `34-66c0fa6-SNAP`. When using the `releaseHold` endpoint to release a workflow, if the workflow_id is in a good format but refers to a non-exist workflow (fake uuid), according to the swagger schema, Cromwell should return 404 to the user. However, now it returns a 500 code which seems to be a wrapper around the actual 404 error:; ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.3, Date: Fri, 20 Jul 2018 13:58:13 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: f4272a19-37dd-4d2c-ba48-ff3844107bf8""; }),HttpProtocol(HTTP/1.1)); ```; This is not a big problem but just brings some incovenience to the error handling process to the users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911
https://github.com/broadinstitute/cromwell/issues/3911:142,Deployability,release,releaseHold,142,"I'm using [CaaS-dev](https://cromwell.caas-dev.broadinstitute.org/) when ran into the issue, with version: `34-66c0fa6-SNAP`. When using the `releaseHold` endpoint to release a workflow, if the workflow_id is in a good format but refers to a non-exist workflow (fake uuid), according to the swagger schema, Cromwell should return 404 to the user. However, now it returns a 500 code which seems to be a wrapper around the actual 404 error:; ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.3, Date: Fri, 20 Jul 2018 13:58:13 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: f4272a19-37dd-4d2c-ba48-ff3844107bf8""; }),HttpProtocol(HTTP/1.1)); ```; This is not a big problem but just brings some incovenience to the error handling process to the users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911
https://github.com/broadinstitute/cromwell/issues/3911:167,Deployability,release,release,167,"I'm using [CaaS-dev](https://cromwell.caas-dev.broadinstitute.org/) when ran into the issue, with version: `34-66c0fa6-SNAP`. When using the `releaseHold` endpoint to release a workflow, if the workflow_id is in a good format but refers to a non-exist workflow (fake uuid), according to the swagger schema, Cromwell should return 404 to the user. However, now it returns a 500 code which seems to be a wrapper around the actual 404 error:; ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.3, Date: Fri, 20 Jul 2018 13:58:13 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: f4272a19-37dd-4d2c-ba48-ff3844107bf8""; }),HttpProtocol(HTTP/1.1)); ```; This is not a big problem but just brings some incovenience to the error handling process to the users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911
https://github.com/broadinstitute/cromwell/issues/3911:402,Integrability,wrap,wrapper,402,"I'm using [CaaS-dev](https://cromwell.caas-dev.broadinstitute.org/) when ran into the issue, with version: `34-66c0fa6-SNAP`. When using the `releaseHold` endpoint to release a workflow, if the workflow_id is in a good format but refers to a non-exist workflow (fake uuid), according to the swagger schema, Cromwell should return 404 to the user. However, now it returns a 500 code which seems to be a wrapper around the actual 404 error:; ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.3, Date: Fri, 20 Jul 2018 13:58:13 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: f4272a19-37dd-4d2c-ba48-ff3844107bf8""; }),HttpProtocol(HTTP/1.1)); ```; This is not a big problem but just brings some incovenience to the error handling process to the users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911
https://github.com/broadinstitute/cromwell/issues/3911:712,Integrability,message,message,712,"I'm using [CaaS-dev](https://cromwell.caas-dev.broadinstitute.org/) when ran into the issue, with version: `34-66c0fa6-SNAP`. When using the `releaseHold` endpoint to release a workflow, if the workflow_id is in a good format but refers to a non-exist workflow (fake uuid), according to the swagger schema, Cromwell should return 404 to the user. However, now it returns a 500 code which seems to be a wrapper around the actual 404 error:; ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.3, Date: Fri, 20 Jul 2018 13:58:13 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: f4272a19-37dd-4d2c-ba48-ff3844107bf8""; }),HttpProtocol(HTTP/1.1)); ```; This is not a big problem but just brings some incovenience to the error handling process to the users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911
https://github.com/broadinstitute/cromwell/pull/3916:549,Testability,test,test,549,"- Keeps us up to date with OpenWDL PR https://github.com/openwdl/wdl/pull/220 (relative imports); - [x] Notify OpenWDL of an existing implementation when this PR merges.; - Closes #3868 ; - Since the WDL spec was previously not opinionated on this (and it turned out to be easier to share this with 1.0 than to sabotage the 1.0 version...) this functionality also works in WDL 1.0!; - This PR doesn't implement relative imports in draft-2. ; - This PR doesn't implement relative imports in CWL. ; - I'm particularly interested in thoughts on how to test HTTP relative imports in centaur. Right now ""raw github URLs to my feature branch"" doesn't feel super great 😨.; - Thought: fake the internet and have a fake `HttpImportResolver` just for the centaur Cromwell?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3916
https://github.com/broadinstitute/cromwell/issues/3917:167,Availability,echo,echo,167,"This fails at runtime but we shouldn't even allow it to validate:. ```wdl; version 1.0; struct Foo {; Int foo_int; }. [...]. task bar {; input {; Foo f; }; command {; echo ~{f} # Bad interpolation, should be ~{f.foo_int}; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3917
https://github.com/broadinstitute/cromwell/issues/3917:56,Security,validat,validate,56,"This fails at runtime but we shouldn't even allow it to validate:. ```wdl; version 1.0; struct Foo {; Int foo_int; }. [...]. task bar {; input {; Foo f; }; command {; echo ~{f} # Bad interpolation, should be ~{f.foo_int}; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3917
https://github.com/broadinstitute/cromwell/pull/3918:24,Availability,error,error,24,This fixes the response error codes. It will now return `404 Not Found` for an unrecognized workflow ID and `400 Bad Request` for a malformed or invalid workflow ID.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3918
https://github.com/broadinstitute/cromwell/issues/3920:13,Deployability,upgrade,upgrade,13,"The `womtool upgrade` command currently [1] drops imports on the floor and [2] breaks the names of imported tasks by dropping their namespace. Example:; ```; import ""sub_workflow_var_refs_import.wdl"" as subworkflow. workflow top_level_workflow {. Array[String] is = [""fee"", ""fi"", ""fo"", ""fum""]; Array[String] pieces = [""hello"", ""lowly"", ""subject!""]. scatter (i in is) {; call subworkflow.subhello as subhello {; input: greeting_pieces = pieces; }; }. output {; Array[Int] sal_len_inner = subhello.sal_len; Int sal_len_outer = length(subhello.hello_out[0]); }; }; ```; upgrades to; ```; version 1.0; # [1] Missing import. workflow top_level_workflow {; input {; Array[String] pieces = [""hello"", ""lowly"", ""subject!""]; Array[String] is = [""fee"", ""fi"", ""fo"", ""fum""]; }; scatter (i in is) {; # [2] should be subworkflow.subhello; call subhello {; input:; greeting_pieces = pieces; }; }. output {; Array[Int] sal_len_inner = subhello.sal_len; Int sal_len_outer = length(subhello.hello_out[0]); }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3920
https://github.com/broadinstitute/cromwell/issues/3920:567,Deployability,upgrade,upgrades,567,"The `womtool upgrade` command currently [1] drops imports on the floor and [2] breaks the names of imported tasks by dropping their namespace. Example:; ```; import ""sub_workflow_var_refs_import.wdl"" as subworkflow. workflow top_level_workflow {. Array[String] is = [""fee"", ""fi"", ""fo"", ""fum""]; Array[String] pieces = [""hello"", ""lowly"", ""subject!""]. scatter (i in is) {; call subworkflow.subhello as subhello {; input: greeting_pieces = pieces; }; }. output {; Array[Int] sal_len_inner = subhello.sal_len; Int sal_len_outer = length(subhello.hello_out[0]); }; }; ```; upgrades to; ```; version 1.0; # [1] Missing import. workflow top_level_workflow {; input {; Array[String] pieces = [""hello"", ""lowly"", ""subject!""]; Array[String] is = [""fee"", ""fi"", ""fo"", ""fum""]; }; scatter (i in is) {; # [2] should be subworkflow.subhello; call subhello {; input:; greeting_pieces = pieces; }; }. output {; Array[Int] sal_len_inner = subhello.sal_len; Int sal_len_outer = length(subhello.hello_out[0]); }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3920
https://github.com/broadinstitute/cromwell/issues/3921:90,Availability,error,error,90,Testing out disabling CWL in FC so I set the CWL language factory to `enabled=false`. The error message was very wrong:. ```; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; WDL draft 3 is not enabled; ```. Due to this [wonky bit](https://github.com/broadinstitute/cromwell/blob/develop/languageFactories/language-factory-core/src/main/scala/cromwell/languages/StandardLanguageFactoryConfig.scala#L10) of code.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3921
https://github.com/broadinstitute/cromwell/issues/3921:96,Integrability,message,message,96,Testing out disabling CWL in FC so I set the CWL language factory to `enabled=false`. The error message was very wrong:. ```; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; WDL draft 3 is not enabled; ```. Due to this [wonky bit](https://github.com/broadinstitute/cromwell/blob/develop/languageFactories/language-factory-core/src/main/scala/cromwell/languages/StandardLanguageFactoryConfig.scala#L10) of code.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3921
https://github.com/broadinstitute/cromwell/issues/3921:0,Testability,Test,Testing,0,Testing out disabling CWL in FC so I set the CWL language factory to `enabled=false`. The error message was very wrong:. ```; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; WDL draft 3 is not enabled; ```. Due to this [wonky bit](https://github.com/broadinstitute/cromwell/blob/develop/languageFactories/language-factory-core/src/main/scala/cromwell/languages/StandardLanguageFactoryConfig.scala#L10) of code.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3921
https://github.com/broadinstitute/cromwell/pull/3922:41,Testability,test,test,41,Closes #3478 . Best explained by the new test case:. ```wdl; workflow bad_autobox {; Int i = 5. # Bad autobox!; Array[Int] bad_is = i. # Good manual-box!; Array[Int] good_is = [ i ]; }. ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3922
https://github.com/broadinstitute/cromwell/pull/3924:63,Modifiability,variab,variable,63,This PR will allow Martha's url to be passed as an environment variable during `docker run`. The variable can be set as `--env MARTHA_URL=<url>`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3924
https://github.com/broadinstitute/cromwell/pull/3924:97,Modifiability,variab,variable,97,This PR will allow Martha's url to be passed as an environment variable during `docker run`. The variable can be set as `--env MARTHA_URL=<url>`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3924
https://github.com/broadinstitute/cromwell/issues/3927:103,Availability,error,error,103,"It looks like cromwell doesn't properly handle optional variables in tasks. If they are undefined this error occurs: ""Optional value was not set and no 'default' attribute was provided"". Setting a default (either in the input block or like so: `~{default="""" optionalValue}`) will cause the workflow to run, but this seems to me to go entirely against the reason for having an optional datatype to begin with. It also looks like setting an optional with default to `null` in the input JSON is ignored, which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(Sha",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:657,Availability,error,error,657,"It looks like cromwell doesn't properly handle optional variables in tasks. If they are undefined this error occurs: ""Optional value was not set and no 'default' attribute was provided"". Setting a default (either in the input block or like so: `~{default="""" optionalValue}`) will cause the workflow to run, but this seems to me to go entirely against the reason for having an optional datatype to begin with. It also looks like setting an optional with default to `null` in the input JSON is ignored, which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(Sha",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:705,Availability,error,error,705,"It looks like cromwell doesn't properly handle optional variables in tasks. If they are undefined this error occurs: ""Optional value was not set and no 'default' attribute was provided"". Setting a default (either in the input block or like so: `~{default="""" optionalValue}`) will cause the workflow to run, but this seems to me to go entirely against the reason for having an optional datatype to begin with. It also looks like setting an optional with default to `null` in the input JSON is ignored, which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(Sha",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:856,Availability,ERROR,ERROR,856,"It looks like cromwell doesn't properly handle optional variables in tasks. If they are undefined this error occurs: ""Optional value was not set and no 'default' attribute was provided"". Setting a default (either in the input block or like so: `~{default="""" optionalValue}`) will cause the workflow to run, but this seems to me to go entirely against the reason for having an optional datatype to begin with. It also looks like setting an optional with default to `null` in the input JSON is ignored, which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(Sha",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:940,Availability,Error,Error,940,"It looks like cromwell doesn't properly handle optional variables in tasks. If they are undefined this error occurs: ""Optional value was not set and no 'default' attribute was provided"". Setting a default (either in the input block or like so: `~{default="""" optionalValue}`) will cause the workflow to run, but this seems to me to go entirely against the reason for having an optional datatype to begin with. It also looks like setting an optional with default to `null` in the input JSON is ignored, which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(Sha",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:3628,Availability,robust,robustExecuteOrRecover,3628,figAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:3968,Availability,robust,robustExecuteOrRecover,3968,mwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:5177,Availability,Error,Error,5177,"PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right plac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:6357,Deployability,configurat,configuration,6357,"edConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:56,Modifiability,variab,variables,56,"It looks like cromwell doesn't properly handle optional variables in tasks. If they are undefined this error occurs: ""Optional value was not set and no 'default' attribute was provided"". Setting a default (either in the input block or like so: `~{default="""" optionalValue}`) will cause the workflow to run, but this seems to me to go entirely against the reason for having an optional datatype to begin with. It also looks like setting an optional with default to `null` in the input JSON is ignored, which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(Sha",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:1284,Modifiability,config,config,1284,"he workflow to run, but this seems to me to go entirely against the reason for having an optional datatype to begin with. It also looks like setting an optional with default to `null` in the input JSON is ignored, which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:1361,Modifiability,Config,ConfigAsyncJobExecutionActor,1361,"n for having an optional datatype to begin with. It also looks like setting an optional with default to `null` in the input JSON is ignored, which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystem",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:1431,Modifiability,config,config,1431,"g an optional with default to `null` in the input JSON is ignored, which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:1497,Modifiability,Config,ConfigAsyncJobExecutionActor,1497,"which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionAct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:1806,Modifiability,config,config,1806,-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:1874,Modifiability,Config,ConfigAsyncJobExecutionActor,1874,figAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standar,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:2213,Modifiability,config,config,2213,tandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.back,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:2279,Modifiability,Config,ConfigAsyncJobExecutionActor,2279,.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardA,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:2594,Modifiability,config,config,2594,mmandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBack,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:2648,Modifiability,Config,ConfigAsyncJobExecutionActor,2648,t cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:141); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncB,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:3101,Modifiability,config,config,3101,onActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:3160,Modifiability,Config,ConfigAsyncJobExecutionActor,3160,tor.scala:140); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.writeScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute(SharedFileSystemAsyncJobExecutionActor.scala:124); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:3459,Modifiability,config,config,3459,sfs.SharedFileSystemAsyncJobExecutionActor.execute$(SharedFileSystemAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:3522,Modifiability,Config,ConfigAsyncJobExecutionActor,3522,emAsyncJobExecutionActor.scala:121); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:599); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:599); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:599); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:912); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:904); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:4438,Modifiability,config,config,4438,.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:4498,Modifiability,Config,ConfigAsyncJobExecutionActor,4498,executeOrRecover(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwel,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:6357,Modifiability,config,configuration,6357,"edConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:5334,Security,validat,validation,5334,"ctor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:5345,Security,Validat,Validation,5345," akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:5356,Security,Validat,ValidationTry,5356," akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:5388,Security,Validat,Validation,5388,"eive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, M",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:5420,Security,validat,validation,5420,"end.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OT",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:5431,Security,Validat,Validation,5431,".DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:5442,Security,Validat,ValidationTry,5442,".DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:5474,Security,Validat,Validation,5474,"edConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:6402,Security,PASSWORD,PASSWORDS,6402,"edConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3927:5695,Usability,feedback,feedback,5695,"edConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927
https://github.com/broadinstitute/cromwell/issues/3929:2534,Availability,down,down,2534,7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentCollection - Processing 292450248 bp from intervals; 21:45:16.221 INFO PrintReads - Done initializing engine; 21:45:16.367 INFO ProgressMeter - Starting traversal; 21:45:16.368 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 21:45:17.033 INFO PrintReads - No reads filtered by: WellformedReadFilter; 21:45:17.035 INFO ProgressMeter - chr6:96496576 0.0 5660 509909.9; 21:45:17.036 INFO ProgressMeter - Traversal complete. Processed 5660 total reads in 0.0 minutes.; 21:45:17.414 INFO PrintReads - Shutting down engine; ```. This is running on Google Cloud on Cromwell 32-c7bcab8.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929
https://github.com/broadinstitute/cromwell/issues/3929:1790,Deployability,patch,patch,1790,7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentCollection - Processing 292450248 bp from intervals; 21:45:16.221 INFO PrintReads - Done initializing engine; 21:45:16.367 INFO ProgressMeter - Starting traversal; 21:45:16.368 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 21:45:17.033 INFO PrintReads - No reads filtered by: WellformedReadFilter; 21:45:17.035 INFO ProgressMeter - chr6:96496576 0.0 5660 509909.9; 21:45:17.036 INFO ProgressMeter - Traversal complete. Processed 5660 total reads in 0.0 minutes.; 21:45:17.414 INFO PrintReads - Shutting down engine; ```. This is running on Google Cloud on Cromwell 32-c7bcab8.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929
https://github.com/broadinstitute/cromwell/issues/3929:1095,Performance,Load,Loading,1095,"-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log"",; ""shardIndex"": 4,; ""stderr"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stderr.log"",; ""attempt"": 1,; ""backendLogs"": {; ""log"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4.log""; }; }```. ```tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentCollection - Processing 292450248 bp from intervals; 21:45:16.221 INFO PrintReads - Done initiali",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929
https://github.com/broadinstitute/cromwell/issues/3929:10,Testability,log,logs,10,"From the `logs` endpoint:; ```{; ""stdout"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log"",; ""shardIndex"": 4,; ""stderr"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stderr.log"",; ""attempt"": 1,; ""backendLogs"": {; ""log"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4.log""; }; }```. ```tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentColle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929
https://github.com/broadinstitute/cromwell/issues/3929:166,Testability,log,log,166,"From the `logs` endpoint:; ```{; ""stdout"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log"",; ""shardIndex"": 4,; ""stderr"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stderr.log"",; ""attempt"": 1,; ""backendLogs"": {; ""log"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4.log""; }; }```. ```tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentColle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929
https://github.com/broadinstitute/cromwell/issues/3929:324,Testability,log,log,324,"From the `logs` endpoint:; ```{; ""stdout"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log"",; ""shardIndex"": 4,; ""stderr"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stderr.log"",; ""attempt"": 1,; ""backendLogs"": {; ""log"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4.log""; }; }```. ```tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentColle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929
https://github.com/broadinstitute/cromwell/issues/3929:365,Testability,log,log,365,"From the `logs` endpoint:; ```{; ""stdout"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log"",; ""shardIndex"": 4,; ""stderr"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stderr.log"",; ""attempt"": 1,; ""backendLogs"": {; ""log"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4.log""; }; }```. ```tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentColle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929
https://github.com/broadinstitute/cromwell/issues/3929:487,Testability,log,log,487,"From the `logs` endpoint:; ```{; ""stdout"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log"",; ""shardIndex"": 4,; ""stderr"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stderr.log"",; ""attempt"": 1,; ""backendLogs"": {; ""log"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4.log""; }; }```. ```tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentColle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929
https://github.com/broadinstitute/cromwell/issues/3929:689,Testability,log,log,689,"From the `logs` endpoint:; ```{; ""stdout"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log"",; ""shardIndex"": 4,; ""stderr"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stderr.log"",; ""attempt"": 1,; ""backendLogs"": {; ""log"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4.log""; }; }```. ```tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentColle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929
https://github.com/broadinstitute/cromwell/issues/3929:851,Testability,log,log,851,"From the `logs` endpoint:; ```{; ""stdout"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log"",; ""shardIndex"": 4,; ""stderr"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stderr.log"",; ""attempt"": 1,; ""backendLogs"": {; ""log"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4.log""; }; }```. ```tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentColle",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929
https://github.com/broadinstitute/cromwell/issues/3929:1050,Testability,log,log,1050,"_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log"",; ""shardIndex"": 4,; ""stderr"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stderr.log"",; ""attempt"": 1,; ""backendLogs"": {; ""log"": ""gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4.log""; }; }```. ```tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log; CommandException: No URLs matched: gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/HaplotypeCaller-4-stdout.log. tlangs at some_computer in /a/working/directory; $ gsutil cat gs://cromwell_execution_bucket/280d07ef-7130-47b7-a5da-e3d0c4705a3e/call-HaplotypeCaller/shard-4/attempt-2/HaplotypeCaller-4-stdout.log; 21:45:13.012 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/gitc/gatk4/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/libgkl_compression.so; 21:45:13.215 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:45:13.216 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:45:13.217 INFO PrintReads - Deflater: IntelDeflater; 21:45:13.217 INFO PrintReads - Inflater: IntelInflater; 21:45:13.218 INFO PrintReads - GCS max retries/reopens: 20; 21:45:13.218 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:45:13.218 INFO PrintReads - Initializing engine; 21:45:16.218 INFO IntervalArgumentCollection - Processing 292450248 bp from intervals; 21:45:16",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3929
https://github.com/broadinstitute/cromwell/pull/3932:502,Deployability,Update,Updated,502,"As per standup today I'm opening up a PR for the work in progress and will transition to more targeted PRs after this. There's a lot in this PR which is just renaming and/or moving stuff around, but feel free to comment fully on everything since I disallowed it the last round. Be aware that my response might wind up being ""that's already one of the things I want to do"" and thus get pushed off to a followup PR. Specific changes I made:. - Converted the actor based model for a future based model; - Updated the URL structure to reflect the [current nomenclature](https://github.com/ga4gh/workflow-execution-service-schemas/pull/33), closes #3877 ; - Where possible I named things after the corresponding concept in the WES spec, which is most of the renaming here; - A few other small changes which were either new-to-WES or not-yet-implemented",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3932
https://github.com/broadinstitute/cromwell/pull/3934:49,Energy Efficiency,monitor,monitors,49,"This implements the cromwell agent (process that monitors docker for Cromwell tasks) as part of #3804. I will include the sidecar process later as a separate PR. After building the agent and running the agent with the build-agent/run-agent scripts, a ""cromwell-agent"" container will be shown in docker ps. This agent will be reading the docker event stream from /events, filter for starts with grep, and pass any start events to the small go program ""spawnoninput"". This program simply spawns a new process when it sees input. In this case, the process is a shell script that pauses the container, checks to see if it matches the target condition, then either resumes/exits (when not meeting the target condition) or leaves the container paused and launches a sidecar, which is still in development. If the target condition does not match, this has an effect of slowing startup for the launched container of approximately 100ms for the inspection process. This image should ideally be under the Broad's account on Dockerhub or ECR - it's based on alpine Linux and is relatively small. It should also be run automatically in the AWS Batch/Cromwell AMI. More documentation will be forthcoming in a later PR, though some inline comments are in the code here.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3934
https://github.com/broadinstitute/cromwell/pull/3934:479,Usability,simpl,simply,479,"This implements the cromwell agent (process that monitors docker for Cromwell tasks) as part of #3804. I will include the sidecar process later as a separate PR. After building the agent and running the agent with the build-agent/run-agent scripts, a ""cromwell-agent"" container will be shown in docker ps. This agent will be reading the docker event stream from /events, filter for starts with grep, and pass any start events to the small go program ""spawnoninput"". This program simply spawns a new process when it sees input. In this case, the process is a shell script that pauses the container, checks to see if it matches the target condition, then either resumes/exits (when not meeting the target condition) or leaves the container paused and launches a sidecar, which is still in development. If the target condition does not match, this has an effect of slowing startup for the launched container of approximately 100ms for the inspection process. This image should ideally be under the Broad's account on Dockerhub or ECR - it's based on alpine Linux and is relatively small. It should also be run automatically in the AWS Batch/Cromwell AMI. More documentation will be forthcoming in a later PR, though some inline comments are in the code here.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3934
https://github.com/broadinstitute/cromwell/pull/3934:576,Usability,pause,pauses,576,"This implements the cromwell agent (process that monitors docker for Cromwell tasks) as part of #3804. I will include the sidecar process later as a separate PR. After building the agent and running the agent with the build-agent/run-agent scripts, a ""cromwell-agent"" container will be shown in docker ps. This agent will be reading the docker event stream from /events, filter for starts with grep, and pass any start events to the small go program ""spawnoninput"". This program simply spawns a new process when it sees input. In this case, the process is a shell script that pauses the container, checks to see if it matches the target condition, then either resumes/exits (when not meeting the target condition) or leaves the container paused and launches a sidecar, which is still in development. If the target condition does not match, this has an effect of slowing startup for the launched container of approximately 100ms for the inspection process. This image should ideally be under the Broad's account on Dockerhub or ECR - it's based on alpine Linux and is relatively small. It should also be run automatically in the AWS Batch/Cromwell AMI. More documentation will be forthcoming in a later PR, though some inline comments are in the code here.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3934
https://github.com/broadinstitute/cromwell/pull/3934:660,Usability,resume,resumes,660,"This implements the cromwell agent (process that monitors docker for Cromwell tasks) as part of #3804. I will include the sidecar process later as a separate PR. After building the agent and running the agent with the build-agent/run-agent scripts, a ""cromwell-agent"" container will be shown in docker ps. This agent will be reading the docker event stream from /events, filter for starts with grep, and pass any start events to the small go program ""spawnoninput"". This program simply spawns a new process when it sees input. In this case, the process is a shell script that pauses the container, checks to see if it matches the target condition, then either resumes/exits (when not meeting the target condition) or leaves the container paused and launches a sidecar, which is still in development. If the target condition does not match, this has an effect of slowing startup for the launched container of approximately 100ms for the inspection process. This image should ideally be under the Broad's account on Dockerhub or ECR - it's based on alpine Linux and is relatively small. It should also be run automatically in the AWS Batch/Cromwell AMI. More documentation will be forthcoming in a later PR, though some inline comments are in the code here.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3934
https://github.com/broadinstitute/cromwell/pull/3934:738,Usability,pause,paused,738,"This implements the cromwell agent (process that monitors docker for Cromwell tasks) as part of #3804. I will include the sidecar process later as a separate PR. After building the agent and running the agent with the build-agent/run-agent scripts, a ""cromwell-agent"" container will be shown in docker ps. This agent will be reading the docker event stream from /events, filter for starts with grep, and pass any start events to the small go program ""spawnoninput"". This program simply spawns a new process when it sees input. In this case, the process is a shell script that pauses the container, checks to see if it matches the target condition, then either resumes/exits (when not meeting the target condition) or leaves the container paused and launches a sidecar, which is still in development. If the target condition does not match, this has an effect of slowing startup for the launched container of approximately 100ms for the inspection process. This image should ideally be under the Broad's account on Dockerhub or ECR - it's based on alpine Linux and is relatively small. It should also be run automatically in the AWS Batch/Cromwell AMI. More documentation will be forthcoming in a later PR, though some inline comments are in the code here.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3934
https://github.com/broadinstitute/cromwell/pull/3938:25,Deployability,hotfix,hotfixes,25,"~Same code change as the hotfixes.~. The code change from the hotfixes, plus a shutdown hook to cleanly close the HTTP backend as suggested by their docs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3938
https://github.com/broadinstitute/cromwell/pull/3938:62,Deployability,hotfix,hotfixes,62,"~Same code change as the hotfixes.~. The code change from the hotfixes, plus a shutdown hook to cleanly close the HTTP backend as suggested by their docs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3938
https://github.com/broadinstitute/cromwell/issues/3942:249,Availability,ERROR,ERROR,249,"a.wdl; ```; workflow a {}. task t {; command {}; output { String s = """" }; }; ```. b.wdl; ```; import ""a.wdl"" as a; workflow b {. call a.t {}. String x = a.t.s; }; ```. Womtool Validate results:; ```; $ java -jar ~/womtool-33.1.jar validate b.wdl . ERROR: Missing value or call: Couldn't find value or call with name 'a' in workflow (line 6):. String x = a.t.s; ^; ```. The declaration for `String x` seems to be spec compliant and yet womtool can't validate this expression properly. . The current workaround is to provide a call alias for the imported task, which passes validation.; ```; import ""a.wdl"" as a; workflow b {. call a.t as test {}. String x = test.s; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3942
https://github.com/broadinstitute/cromwell/issues/3942:177,Security,Validat,Validate,177,"a.wdl; ```; workflow a {}. task t {; command {}; output { String s = """" }; }; ```. b.wdl; ```; import ""a.wdl"" as a; workflow b {. call a.t {}. String x = a.t.s; }; ```. Womtool Validate results:; ```; $ java -jar ~/womtool-33.1.jar validate b.wdl . ERROR: Missing value or call: Couldn't find value or call with name 'a' in workflow (line 6):. String x = a.t.s; ^; ```. The declaration for `String x` seems to be spec compliant and yet womtool can't validate this expression properly. . The current workaround is to provide a call alias for the imported task, which passes validation.; ```; import ""a.wdl"" as a; workflow b {. call a.t as test {}. String x = test.s; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3942
https://github.com/broadinstitute/cromwell/issues/3942:232,Security,validat,validate,232,"a.wdl; ```; workflow a {}. task t {; command {}; output { String s = """" }; }; ```. b.wdl; ```; import ""a.wdl"" as a; workflow b {. call a.t {}. String x = a.t.s; }; ```. Womtool Validate results:; ```; $ java -jar ~/womtool-33.1.jar validate b.wdl . ERROR: Missing value or call: Couldn't find value or call with name 'a' in workflow (line 6):. String x = a.t.s; ^; ```. The declaration for `String x` seems to be spec compliant and yet womtool can't validate this expression properly. . The current workaround is to provide a call alias for the imported task, which passes validation.; ```; import ""a.wdl"" as a; workflow b {. call a.t as test {}. String x = test.s; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3942
https://github.com/broadinstitute/cromwell/issues/3942:450,Security,validat,validate,450,"a.wdl; ```; workflow a {}. task t {; command {}; output { String s = """" }; }; ```. b.wdl; ```; import ""a.wdl"" as a; workflow b {. call a.t {}. String x = a.t.s; }; ```. Womtool Validate results:; ```; $ java -jar ~/womtool-33.1.jar validate b.wdl . ERROR: Missing value or call: Couldn't find value or call with name 'a' in workflow (line 6):. String x = a.t.s; ^; ```. The declaration for `String x` seems to be spec compliant and yet womtool can't validate this expression properly. . The current workaround is to provide a call alias for the imported task, which passes validation.; ```; import ""a.wdl"" as a; workflow b {. call a.t as test {}. String x = test.s; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3942
https://github.com/broadinstitute/cromwell/issues/3942:573,Security,validat,validation,573,"a.wdl; ```; workflow a {}. task t {; command {}; output { String s = """" }; }; ```. b.wdl; ```; import ""a.wdl"" as a; workflow b {. call a.t {}. String x = a.t.s; }; ```. Womtool Validate results:; ```; $ java -jar ~/womtool-33.1.jar validate b.wdl . ERROR: Missing value or call: Couldn't find value or call with name 'a' in workflow (line 6):. String x = a.t.s; ^; ```. The declaration for `String x` seems to be spec compliant and yet womtool can't validate this expression properly. . The current workaround is to provide a call alias for the imported task, which passes validation.; ```; import ""a.wdl"" as a; workflow b {. call a.t as test {}. String x = test.s; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3942
https://github.com/broadinstitute/cromwell/issues/3942:638,Testability,test,test,638,"a.wdl; ```; workflow a {}. task t {; command {}; output { String s = """" }; }; ```. b.wdl; ```; import ""a.wdl"" as a; workflow b {. call a.t {}. String x = a.t.s; }; ```. Womtool Validate results:; ```; $ java -jar ~/womtool-33.1.jar validate b.wdl . ERROR: Missing value or call: Couldn't find value or call with name 'a' in workflow (line 6):. String x = a.t.s; ^; ```. The declaration for `String x` seems to be spec compliant and yet womtool can't validate this expression properly. . The current workaround is to provide a call alias for the imported task, which passes validation.; ```; import ""a.wdl"" as a; workflow b {. call a.t as test {}. String x = test.s; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3942
https://github.com/broadinstitute/cromwell/issues/3942:658,Testability,test,test,658,"a.wdl; ```; workflow a {}. task t {; command {}; output { String s = """" }; }; ```. b.wdl; ```; import ""a.wdl"" as a; workflow b {. call a.t {}. String x = a.t.s; }; ```. Womtool Validate results:; ```; $ java -jar ~/womtool-33.1.jar validate b.wdl . ERROR: Missing value or call: Couldn't find value or call with name 'a' in workflow (line 6):. String x = a.t.s; ^; ```. The declaration for `String x` seems to be spec compliant and yet womtool can't validate this expression properly. . The current workaround is to provide a call alias for the imported task, which passes validation.; ```; import ""a.wdl"" as a; workflow b {. call a.t as test {}. String x = test.s; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3942
https://github.com/broadinstitute/cromwell/pull/3945:362,Testability,test,tests,362,"This issue/PR is still work in progress. This PR adds initial support for submitting a WDL workflow using a URL in Server mode. (Initial support for the same in Run mode has been added, but is still not completely functional). Remaining:. - [x] Documentation; - [ ] Support in Run mode (will be part of another PR); - [x] Submitting CWL workflow; - [x] Few more tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3945
https://github.com/broadinstitute/cromwell/pull/3946:136,Availability,error,error,136,"Based on my understanding of the bash, it seems like we might have been retrying with the requester pays flag project regardless of the error. . The change is to enable localization with the project flag only if the requester pays error exists.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3946
https://github.com/broadinstitute/cromwell/pull/3946:231,Availability,error,error,231,"Based on my understanding of the bash, it seems like we might have been retrying with the requester pays flag project regardless of the error. . The change is to enable localization with the project flag only if the requester pays error exists.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3946
https://github.com/broadinstitute/cromwell/pull/3950:226,Deployability,configurat,configuration,226,"This change set adds support for inputs in the [NCBI SRA](https://www.ncbi.nlm.nih.gov/sra) via [fusera](https://github.com/mitre/fusera). A user has to provide their NGC authentication file and a fusera Docker image in their configuration file like so:. ```; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""<some hosted docker path>/fusera:latest""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. The `ngc` parameter must contain the base64-encoded NGC credential file. The one provided here is an example from the NCBI site.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3950
https://github.com/broadinstitute/cromwell/pull/3950:226,Modifiability,config,configuration,226,"This change set adds support for inputs in the [NCBI SRA](https://www.ncbi.nlm.nih.gov/sra) via [fusera](https://github.com/mitre/fusera). A user has to provide their NGC authentication file and a fusera Docker image in their configuration file like so:. ```; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""<some hosted docker path>/fusera:latest""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. The `ngc` parameter must contain the base64-encoded NGC credential file. The one provided here is an example from the NCBI site.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3950
https://github.com/broadinstitute/cromwell/pull/3950:171,Security,authenticat,authentication,171,"This change set adds support for inputs in the [NCBI SRA](https://www.ncbi.nlm.nih.gov/sra) via [fusera](https://github.com/mitre/fusera). A user has to provide their NGC authentication file and a fusera Docker image in their configuration file like so:. ```; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""<some hosted docker path>/fusera:latest""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. The `ngc` parameter must contain the base64-encoded NGC credential file. The one provided here is an example from the NCBI site.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3950
https://github.com/broadinstitute/cromwell/issues/3955:137,Deployability,upgrade,upgrade,137,- People should be able to change the backend name in the config without losing their call cache; - People should (probably?) be able to upgrade from PAPI1 to PAPI2 without losing their call cache,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3955
https://github.com/broadinstitute/cromwell/issues/3955:58,Modifiability,config,config,58,- People should be able to change the backend name in the config without losing their call cache; - People should (probably?) be able to upgrade from PAPI1 to PAPI2 without losing their call cache,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3955
https://github.com/broadinstitute/cromwell/issues/3955:91,Performance,cache,cache,91,- People should be able to change the backend name in the config without losing their call cache; - People should (probably?) be able to upgrade from PAPI1 to PAPI2 without losing their call cache,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3955
https://github.com/broadinstitute/cromwell/issues/3955:191,Performance,cache,cache,191,- People should be able to change the backend name in the config without losing their call cache; - People should (probably?) be able to upgrade from PAPI1 to PAPI2 without losing their call cache,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3955
https://github.com/broadinstitute/cromwell/pull/3956:106,Integrability,synchroniz,synchronized,106,"Now that we're stopping the appenders for workflow logs, access to the underlying Logback context must be synchronized since an underlying non-threadsafe java.util.HashMap is being modified.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3956
https://github.com/broadinstitute/cromwell/pull/3956:57,Security,access,access,57,"Now that we're stopping the appenders for workflow logs, access to the underlying Logback context must be synchronized since an underlying non-threadsafe java.util.HashMap is being modified.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3956
https://github.com/broadinstitute/cromwell/pull/3956:164,Security,Hash,HashMap,164,"Now that we're stopping the appenders for workflow logs, access to the underlying Logback context must be synchronized since an underlying non-threadsafe java.util.HashMap is being modified.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3956
https://github.com/broadinstitute/cromwell/pull/3956:51,Testability,log,logs,51,"Now that we're stopping the appenders for workflow logs, access to the underlying Logback context must be synchronized since an underlying non-threadsafe java.util.HashMap is being modified.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3956
https://github.com/broadinstitute/cromwell/pull/3956:82,Testability,Log,Logback,82,"Now that we're stopping the appenders for workflow logs, access to the underlying Logback context must be synchronized since an underlying non-threadsafe java.util.HashMap is being modified.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3956
https://github.com/broadinstitute/cromwell/pull/3957:106,Integrability,synchroniz,synchronized,106,"Now that we're stopping the appenders for workflow logs, access to the underlying Logback context must be synchronized since an underlying non-threadsafe java.util.HashMap is being modified.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3957
https://github.com/broadinstitute/cromwell/pull/3957:57,Security,access,access,57,"Now that we're stopping the appenders for workflow logs, access to the underlying Logback context must be synchronized since an underlying non-threadsafe java.util.HashMap is being modified.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3957
https://github.com/broadinstitute/cromwell/pull/3957:164,Security,Hash,HashMap,164,"Now that we're stopping the appenders for workflow logs, access to the underlying Logback context must be synchronized since an underlying non-threadsafe java.util.HashMap is being modified.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3957
https://github.com/broadinstitute/cromwell/pull/3957:51,Testability,log,logs,51,"Now that we're stopping the appenders for workflow logs, access to the underlying Logback context must be synchronized since an underlying non-threadsafe java.util.HashMap is being modified.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3957
https://github.com/broadinstitute/cromwell/pull/3957:82,Testability,Log,Logback,82,"Now that we're stopping the appenders for workflow logs, access to the underlying Logback context must be synchronized since an underlying non-threadsafe java.util.HashMap is being modified.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3957
https://github.com/broadinstitute/cromwell/issues/3958:2202,Availability,ERROR,ERROR,2202,"-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor Successfully started WorkflowActor-cfc7b055-b8a3-40c4-a0eb-b4636d5c7286; 2018-08-02 02:23:03,861 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-08-02 02:23:03,863 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - MaterializeWorkflowDescriptorActor [UUID(cfc7b055)]: Parsing workflow as CWL v1.0; 2018-08-02 02:23:03,869 INFO - Pre-Processing /tmp/cfc7b055-b8a3-40c4-a0eb-b4636d5c7286.temp.1620266732058368635/cfc7b055-b8a3-40c4-a0eb-b4636d5c7286.cwl; 2018-08-02 02:23:05,504 INFO - Pre-Processing /modules/file-parsers/parsetxtxy.cwl; 2018-08-02 02:23:07,089 INFO - Pre-Processing /modules/processing-modules/13C-NMR.cwl; 2018-08-02 02:23:08,703 INFO - Pre-Processing /modules/core-modules/uploadsamples.cwl; 2018-08-02 02:23:10,570 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow cfc7b055-b8a3-40c4-a0eb-b4636d5c7286 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Input frequency is required and is not bound to any value; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:170); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:670); 	at akka.actor.FSM.processEvent$(FSM.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3958
https://github.com/broadinstitute/cromwell/issues/3958:3374,Testability,Log,LoggingFSM,3374,DescriptorActor$$anon$1: Workflow input processing failed:; Input frequency is required and is not bound to any value; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:170); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:670); 	at akka.actor.FSM.processEvent$(FSM.scala:667); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:806); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:788); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:664); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:658); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3958
https://github.com/broadinstitute/cromwell/issues/3958:3467,Testability,Log,LoggingFSM,3467,ency is required and is not bound to any value; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:170); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:670); 	at akka.actor.FSM.processEvent$(FSM.scala:667); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:806); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:788); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:664); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:658); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3958
https://github.com/broadinstitute/cromwell/issues/3958:3522,Testability,Log,LoggingFSM,3522,mwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:200); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:170); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:165); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:670); 	at akka.actor.FSM.processEvent$(FSM.scala:667); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:806); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:788); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:664); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:658); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3958
https://github.com/broadinstitute/cromwell/issues/3964:538,Deployability,Update,Update,538,"- [x] Add AWS creds to Vault. Done: `secret/dsde/cromwell/common/cromwell-aws`; - [x] Use AWS creds to setup an environment in AWS; - [x] Create a queue in AWS Batch; - [x] Create an S3 bucket for storing Cromwell outputs; - [ ] Gitignore the AWS credentials file; - [x] Create [src/ci/resources](https://github.com/broadinstitute/cromwell/tree/develop/src/ci/resources)/aws_application.conf.ctmpl; - [x] Reference the AWS creds from Vault; - [x] Add in the AWS Batch Queue name; - [x] Add the S3 bucket for storing Travis results; - [ ] Update [src/ci/bin/testCentaurAws.sh](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testCentaurAws.sh) and run locally until it passes; - [ ] Insert `cromwell::build::setup_secure_resources` before `cromwell::build::assemble_jars`; - [ ] Exclude failed tests as necessary using `-e should_work_but_does_not_on_aws`; - [ ] If the AWS Batch queue was not hardcoded, export an environment variable with the queue name in `testCentaurAws.sh`; - [ ] Prepend `BUILD_TYPE=centaurAws` into the [.travis.yaml](https://github.com/broadinstitute/cromwell/blob/develop/.travis.yml) env matrix. See other centaur scripts and resources under [src/ci](https://github.com/broadinstitute/cromwell/blob/develop/src/ci) for examples.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3964
https://github.com/broadinstitute/cromwell/issues/3964:945,Modifiability,variab,variable,945,"- [x] Add AWS creds to Vault. Done: `secret/dsde/cromwell/common/cromwell-aws`; - [x] Use AWS creds to setup an environment in AWS; - [x] Create a queue in AWS Batch; - [x] Create an S3 bucket for storing Cromwell outputs; - [ ] Gitignore the AWS credentials file; - [x] Create [src/ci/resources](https://github.com/broadinstitute/cromwell/tree/develop/src/ci/resources)/aws_application.conf.ctmpl; - [x] Reference the AWS creds from Vault; - [x] Add in the AWS Batch Queue name; - [x] Add the S3 bucket for storing Travis results; - [ ] Update [src/ci/bin/testCentaurAws.sh](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testCentaurAws.sh) and run locally until it passes; - [ ] Insert `cromwell::build::setup_secure_resources` before `cromwell::build::assemble_jars`; - [ ] Exclude failed tests as necessary using `-e should_work_but_does_not_on_aws`; - [ ] If the AWS Batch queue was not hardcoded, export an environment variable with the queue name in `testCentaurAws.sh`; - [ ] Prepend `BUILD_TYPE=centaurAws` into the [.travis.yaml](https://github.com/broadinstitute/cromwell/blob/develop/.travis.yml) env matrix. See other centaur scripts and resources under [src/ci](https://github.com/broadinstitute/cromwell/blob/develop/src/ci) for examples.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3964
https://github.com/broadinstitute/cromwell/issues/3964:147,Performance,queue,queue,147,"- [x] Add AWS creds to Vault. Done: `secret/dsde/cromwell/common/cromwell-aws`; - [x] Use AWS creds to setup an environment in AWS; - [x] Create a queue in AWS Batch; - [x] Create an S3 bucket for storing Cromwell outputs; - [ ] Gitignore the AWS credentials file; - [x] Create [src/ci/resources](https://github.com/broadinstitute/cromwell/tree/develop/src/ci/resources)/aws_application.conf.ctmpl; - [x] Reference the AWS creds from Vault; - [x] Add in the AWS Batch Queue name; - [x] Add the S3 bucket for storing Travis results; - [ ] Update [src/ci/bin/testCentaurAws.sh](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testCentaurAws.sh) and run locally until it passes; - [ ] Insert `cromwell::build::setup_secure_resources` before `cromwell::build::assemble_jars`; - [ ] Exclude failed tests as necessary using `-e should_work_but_does_not_on_aws`; - [ ] If the AWS Batch queue was not hardcoded, export an environment variable with the queue name in `testCentaurAws.sh`; - [ ] Prepend `BUILD_TYPE=centaurAws` into the [.travis.yaml](https://github.com/broadinstitute/cromwell/blob/develop/.travis.yml) env matrix. See other centaur scripts and resources under [src/ci](https://github.com/broadinstitute/cromwell/blob/develop/src/ci) for examples.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3964
https://github.com/broadinstitute/cromwell/issues/3964:468,Performance,Queue,Queue,468,"- [x] Add AWS creds to Vault. Done: `secret/dsde/cromwell/common/cromwell-aws`; - [x] Use AWS creds to setup an environment in AWS; - [x] Create a queue in AWS Batch; - [x] Create an S3 bucket for storing Cromwell outputs; - [ ] Gitignore the AWS credentials file; - [x] Create [src/ci/resources](https://github.com/broadinstitute/cromwell/tree/develop/src/ci/resources)/aws_application.conf.ctmpl; - [x] Reference the AWS creds from Vault; - [x] Add in the AWS Batch Queue name; - [x] Add the S3 bucket for storing Travis results; - [ ] Update [src/ci/bin/testCentaurAws.sh](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testCentaurAws.sh) and run locally until it passes; - [ ] Insert `cromwell::build::setup_secure_resources` before `cromwell::build::assemble_jars`; - [ ] Exclude failed tests as necessary using `-e should_work_but_does_not_on_aws`; - [ ] If the AWS Batch queue was not hardcoded, export an environment variable with the queue name in `testCentaurAws.sh`; - [ ] Prepend `BUILD_TYPE=centaurAws` into the [.travis.yaml](https://github.com/broadinstitute/cromwell/blob/develop/.travis.yml) env matrix. See other centaur scripts and resources under [src/ci](https://github.com/broadinstitute/cromwell/blob/develop/src/ci) for examples.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3964
https://github.com/broadinstitute/cromwell/issues/3964:898,Performance,queue,queue,898,"- [x] Add AWS creds to Vault. Done: `secret/dsde/cromwell/common/cromwell-aws`; - [x] Use AWS creds to setup an environment in AWS; - [x] Create a queue in AWS Batch; - [x] Create an S3 bucket for storing Cromwell outputs; - [ ] Gitignore the AWS credentials file; - [x] Create [src/ci/resources](https://github.com/broadinstitute/cromwell/tree/develop/src/ci/resources)/aws_application.conf.ctmpl; - [x] Reference the AWS creds from Vault; - [x] Add in the AWS Batch Queue name; - [x] Add the S3 bucket for storing Travis results; - [ ] Update [src/ci/bin/testCentaurAws.sh](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testCentaurAws.sh) and run locally until it passes; - [ ] Insert `cromwell::build::setup_secure_resources` before `cromwell::build::assemble_jars`; - [ ] Exclude failed tests as necessary using `-e should_work_but_does_not_on_aws`; - [ ] If the AWS Batch queue was not hardcoded, export an environment variable with the queue name in `testCentaurAws.sh`; - [ ] Prepend `BUILD_TYPE=centaurAws` into the [.travis.yaml](https://github.com/broadinstitute/cromwell/blob/develop/.travis.yml) env matrix. See other centaur scripts and resources under [src/ci](https://github.com/broadinstitute/cromwell/blob/develop/src/ci) for examples.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3964
https://github.com/broadinstitute/cromwell/issues/3964:963,Performance,queue,queue,963,"- [x] Add AWS creds to Vault. Done: `secret/dsde/cromwell/common/cromwell-aws`; - [x] Use AWS creds to setup an environment in AWS; - [x] Create a queue in AWS Batch; - [x] Create an S3 bucket for storing Cromwell outputs; - [ ] Gitignore the AWS credentials file; - [x] Create [src/ci/resources](https://github.com/broadinstitute/cromwell/tree/develop/src/ci/resources)/aws_application.conf.ctmpl; - [x] Reference the AWS creds from Vault; - [x] Add in the AWS Batch Queue name; - [x] Add the S3 bucket for storing Travis results; - [ ] Update [src/ci/bin/testCentaurAws.sh](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testCentaurAws.sh) and run locally until it passes; - [ ] Insert `cromwell::build::setup_secure_resources` before `cromwell::build::assemble_jars`; - [ ] Exclude failed tests as necessary using `-e should_work_but_does_not_on_aws`; - [ ] If the AWS Batch queue was not hardcoded, export an environment variable with the queue name in `testCentaurAws.sh`; - [ ] Prepend `BUILD_TYPE=centaurAws` into the [.travis.yaml](https://github.com/broadinstitute/cromwell/blob/develop/.travis.yml) env matrix. See other centaur scripts and resources under [src/ci](https://github.com/broadinstitute/cromwell/blob/develop/src/ci) for examples.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3964
https://github.com/broadinstitute/cromwell/issues/3964:557,Testability,test,testCentaurAws,557,"- [x] Add AWS creds to Vault. Done: `secret/dsde/cromwell/common/cromwell-aws`; - [x] Use AWS creds to setup an environment in AWS; - [x] Create a queue in AWS Batch; - [x] Create an S3 bucket for storing Cromwell outputs; - [ ] Gitignore the AWS credentials file; - [x] Create [src/ci/resources](https://github.com/broadinstitute/cromwell/tree/develop/src/ci/resources)/aws_application.conf.ctmpl; - [x] Reference the AWS creds from Vault; - [x] Add in the AWS Batch Queue name; - [x] Add the S3 bucket for storing Travis results; - [ ] Update [src/ci/bin/testCentaurAws.sh](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testCentaurAws.sh) and run locally until it passes; - [ ] Insert `cromwell::build::setup_secure_resources` before `cromwell::build::assemble_jars`; - [ ] Exclude failed tests as necessary using `-e should_work_but_does_not_on_aws`; - [ ] If the AWS Batch queue was not hardcoded, export an environment variable with the queue name in `testCentaurAws.sh`; - [ ] Prepend `BUILD_TYPE=centaurAws` into the [.travis.yaml](https://github.com/broadinstitute/cromwell/blob/develop/.travis.yml) env matrix. See other centaur scripts and resources under [src/ci](https://github.com/broadinstitute/cromwell/blob/develop/src/ci) for examples.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3964
https://github.com/broadinstitute/cromwell/issues/3964:643,Testability,test,testCentaurAws,643,"- [x] Add AWS creds to Vault. Done: `secret/dsde/cromwell/common/cromwell-aws`; - [x] Use AWS creds to setup an environment in AWS; - [x] Create a queue in AWS Batch; - [x] Create an S3 bucket for storing Cromwell outputs; - [ ] Gitignore the AWS credentials file; - [x] Create [src/ci/resources](https://github.com/broadinstitute/cromwell/tree/develop/src/ci/resources)/aws_application.conf.ctmpl; - [x] Reference the AWS creds from Vault; - [x] Add in the AWS Batch Queue name; - [x] Add the S3 bucket for storing Travis results; - [ ] Update [src/ci/bin/testCentaurAws.sh](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testCentaurAws.sh) and run locally until it passes; - [ ] Insert `cromwell::build::setup_secure_resources` before `cromwell::build::assemble_jars`; - [ ] Exclude failed tests as necessary using `-e should_work_but_does_not_on_aws`; - [ ] If the AWS Batch queue was not hardcoded, export an environment variable with the queue name in `testCentaurAws.sh`; - [ ] Prepend `BUILD_TYPE=centaurAws` into the [.travis.yaml](https://github.com/broadinstitute/cromwell/blob/develop/.travis.yml) env matrix. See other centaur scripts and resources under [src/ci](https://github.com/broadinstitute/cromwell/blob/develop/src/ci) for examples.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3964
https://github.com/broadinstitute/cromwell/issues/3964:812,Testability,test,tests,812,"- [x] Add AWS creds to Vault. Done: `secret/dsde/cromwell/common/cromwell-aws`; - [x] Use AWS creds to setup an environment in AWS; - [x] Create a queue in AWS Batch; - [x] Create an S3 bucket for storing Cromwell outputs; - [ ] Gitignore the AWS credentials file; - [x] Create [src/ci/resources](https://github.com/broadinstitute/cromwell/tree/develop/src/ci/resources)/aws_application.conf.ctmpl; - [x] Reference the AWS creds from Vault; - [x] Add in the AWS Batch Queue name; - [x] Add the S3 bucket for storing Travis results; - [ ] Update [src/ci/bin/testCentaurAws.sh](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testCentaurAws.sh) and run locally until it passes; - [ ] Insert `cromwell::build::setup_secure_resources` before `cromwell::build::assemble_jars`; - [ ] Exclude failed tests as necessary using `-e should_work_but_does_not_on_aws`; - [ ] If the AWS Batch queue was not hardcoded, export an environment variable with the queue name in `testCentaurAws.sh`; - [ ] Prepend `BUILD_TYPE=centaurAws` into the [.travis.yaml](https://github.com/broadinstitute/cromwell/blob/develop/.travis.yml) env matrix. See other centaur scripts and resources under [src/ci](https://github.com/broadinstitute/cromwell/blob/develop/src/ci) for examples.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3964
https://github.com/broadinstitute/cromwell/issues/3964:978,Testability,test,testCentaurAws,978,"- [x] Add AWS creds to Vault. Done: `secret/dsde/cromwell/common/cromwell-aws`; - [x] Use AWS creds to setup an environment in AWS; - [x] Create a queue in AWS Batch; - [x] Create an S3 bucket for storing Cromwell outputs; - [ ] Gitignore the AWS credentials file; - [x] Create [src/ci/resources](https://github.com/broadinstitute/cromwell/tree/develop/src/ci/resources)/aws_application.conf.ctmpl; - [x] Reference the AWS creds from Vault; - [x] Add in the AWS Batch Queue name; - [x] Add the S3 bucket for storing Travis results; - [ ] Update [src/ci/bin/testCentaurAws.sh](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/testCentaurAws.sh) and run locally until it passes; - [ ] Insert `cromwell::build::setup_secure_resources` before `cromwell::build::assemble_jars`; - [ ] Exclude failed tests as necessary using `-e should_work_but_does_not_on_aws`; - [ ] If the AWS Batch queue was not hardcoded, export an environment variable with the queue name in `testCentaurAws.sh`; - [ ] Prepend `BUILD_TYPE=centaurAws` into the [.travis.yaml](https://github.com/broadinstitute/cromwell/blob/develop/.travis.yml) env matrix. See other centaur scripts and resources under [src/ci](https://github.com/broadinstitute/cromwell/blob/develop/src/ci) for examples.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3964
https://github.com/broadinstitute/cromwell/issues/3967:414,Availability,down,downloading,414,"From [a user's forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/12577/stdout-stderr-output-in-bucket-does-not-seem-to-be-updated-while-task-is-running#latest):. 1. `stderr` and `stdout` do not appear to show up until a workflow finishes. ; 2. In some cases `stderr` and `stdout` are of type `application/octet-stream` rather than `text/plain`, not allowing the content to be viewable without downloading. . I was able to see these by running a [five-dollar-genome-analysis-pipeline](https://portal.firecloud.org/#workspaces/fccredits-silver-pumpkin-7172/five-dollar-genome-analysis-pipeline_copy) as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3967
https://github.com/broadinstitute/cromwell/issues/3967:143,Deployability,update,updated-while-task-is-running,143,"From [a user's forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/12577/stdout-stderr-output-in-bucket-does-not-seem-to-be-updated-while-task-is-running#latest):. 1. `stderr` and `stdout` do not appear to show up until a workflow finishes. ; 2. In some cases `stderr` and `stdout` are of type `application/octet-stream` rather than `text/plain`, not allowing the content to be viewable without downloading. . I was able to see these by running a [five-dollar-genome-analysis-pipeline](https://portal.firecloud.org/#workspaces/fccredits-silver-pumpkin-7172/five-dollar-genome-analysis-pipeline_copy) as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3967
https://github.com/broadinstitute/cromwell/issues/3967:495,Deployability,pipeline,pipeline,495,"From [a user's forum post](https://gatkforums.broadinstitute.org/firecloud/discussion/12577/stdout-stderr-output-in-bucket-does-not-seem-to-be-updated-while-task-is-running#latest):. 1. `stderr` and `stdout` do not appear to show up until a workflow finishes. ; 2. In some cases `stderr` and `stdout` are of type `application/octet-stream` rather than `text/plain`, not allowing the content to be viewable without downloading. . I was able to see these by running a [five-dollar-genome-analysis-pipeline](https://portal.firecloud.org/#workspaces/fccredits-silver-pumpkin-7172/five-dollar-genome-analysis-pipeline_copy) as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3967
https://github.com/broadinstitute/cromwell/issues/3969:641,Modifiability,variab,variables,641,"Thanks for the AWS backend, we've been looking forward to that for a long time!. It works fine when there's no proxy between cromwell server and the outside, but from behind our institute proxy it fails (detailed log here: https://gist.github.com/delocalizer/e31e2779b906fba85d99ab9f8eb485b1). The credentials are valid, it's just that the `STSClient` connection is timing out [here](https://github.com/broadinstitute/cromwell/blob/e9f47c923ab7ec0cf6b4c6b2ae45e66d0d88e907/cloudSupport/src/main/scala/cromwell/cloudsupport/aws/auth/AwsAuthMode.scala#L86) because it seems it's not respecting proxy settings specified either with environment variables:; ```; http_proxy; https_proxy; HTTP_PROXY; HTTPS_PROXY; ```; or to java directly with system properties; ```; http.proxyHost; http.proxyPort; https.proxyHost; https.proxyPort; ```. Using the version 1 of `aws-sdk-java` you coud configure clients with [ClientConfiguration](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html?com/amazonaws/ClientConfiguration.html) that by default picks up those settings. I don't know how (or even if you can) do that with the sdk version 2 that's being used here.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3969
https://github.com/broadinstitute/cromwell/issues/3969:880,Modifiability,config,configure,880,"Thanks for the AWS backend, we've been looking forward to that for a long time!. It works fine when there's no proxy between cromwell server and the outside, but from behind our institute proxy it fails (detailed log here: https://gist.github.com/delocalizer/e31e2779b906fba85d99ab9f8eb485b1). The credentials are valid, it's just that the `STSClient` connection is timing out [here](https://github.com/broadinstitute/cromwell/blob/e9f47c923ab7ec0cf6b4c6b2ae45e66d0d88e907/cloudSupport/src/main/scala/cromwell/cloudsupport/aws/auth/AwsAuthMode.scala#L86) because it seems it's not respecting proxy settings specified either with environment variables:; ```; http_proxy; https_proxy; HTTP_PROXY; HTTPS_PROXY; ```; or to java directly with system properties; ```; http.proxyHost; http.proxyPort; https.proxyHost; https.proxyPort; ```. Using the version 1 of `aws-sdk-java` you coud configure clients with [ClientConfiguration](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html?com/amazonaws/ClientConfiguration.html) that by default picks up those settings. I don't know how (or even if you can) do that with the sdk version 2 that's being used here.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3969
https://github.com/broadinstitute/cromwell/issues/3969:213,Testability,log,log,213,"Thanks for the AWS backend, we've been looking forward to that for a long time!. It works fine when there's no proxy between cromwell server and the outside, but from behind our institute proxy it fails (detailed log here: https://gist.github.com/delocalizer/e31e2779b906fba85d99ab9f8eb485b1). The credentials are valid, it's just that the `STSClient` connection is timing out [here](https://github.com/broadinstitute/cromwell/blob/e9f47c923ab7ec0cf6b4c6b2ae45e66d0d88e907/cloudSupport/src/main/scala/cromwell/cloudsupport/aws/auth/AwsAuthMode.scala#L86) because it seems it's not respecting proxy settings specified either with environment variables:; ```; http_proxy; https_proxy; HTTP_PROXY; HTTPS_PROXY; ```; or to java directly with system properties; ```; http.proxyHost; http.proxyPort; https.proxyHost; https.proxyPort; ```. Using the version 1 of `aws-sdk-java` you coud configure clients with [ClientConfiguration](https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/index.html?com/amazonaws/ClientConfiguration.html) that by default picks up those settings. I don't know how (or even if you can) do that with the sdk version 2 that's being used here.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3969
https://github.com/broadinstitute/cromwell/pull/3970:80,Testability,test,test,80,"Closes #3829. Actually it must have been fixed by a previous PR but this adds a test for it, amongst other changes:. - Fixes `write_objects`; - Enable the `read_write_functions` test (which can't have been running because it was failing); - Adds an empty string test for read_json",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3970
https://github.com/broadinstitute/cromwell/pull/3970:178,Testability,test,test,178,"Closes #3829. Actually it must have been fixed by a previous PR but this adds a test for it, amongst other changes:. - Fixes `write_objects`; - Enable the `read_write_functions` test (which can't have been running because it was failing); - Adds an empty string test for read_json",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3970
https://github.com/broadinstitute/cromwell/pull/3970:262,Testability,test,test,262,"Closes #3829. Actually it must have been fixed by a previous PR but this adds a test for it, amongst other changes:. - Fixes `write_objects`; - Enable the `read_write_functions` test (which can't have been running because it was failing); - Adds an empty string test for read_json",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3970
https://github.com/broadinstitute/cromwell/issues/3976:454,Testability,test,test,454,"In the PAPI v1 backend, there was a that any job using a docker with an entrypoint failed to run. In addition, Cromwell has no way to override entrypoint on the docker run command for the PAPI v1 backend. . However, in PAPI v2 backend -- Cromwell controls the `docker run` command and should make sure it overrides in the entrypoint. AC:; - Document entrypoint restrictions for the PAPI backend, and the differences in how v1 and v2 handle this; - Add a test for the v2 backend to ensure that we're regression testing for the entrypoint override",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3976
https://github.com/broadinstitute/cromwell/issues/3976:510,Testability,test,testing,510,"In the PAPI v1 backend, there was a that any job using a docker with an entrypoint failed to run. In addition, Cromwell has no way to override entrypoint on the docker run command for the PAPI v1 backend. . However, in PAPI v2 backend -- Cromwell controls the `docker run` command and should make sure it overrides in the entrypoint. AC:; - Document entrypoint restrictions for the PAPI backend, and the differences in how v1 and v2 handle this; - Add a test for the v2 backend to ensure that we're regression testing for the entrypoint override",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3976
https://github.com/broadinstitute/cromwell/issues/3977:2244,Energy Efficiency,adapt,adapted,2244,); at org.asynchttpclient.resolver.RequestHostnameResolver.resolve(RequestHostnameResolver.java:50); at org.asynchttpclient.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonf,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:2374,Energy Efficiency,adapt,adapted,2374,ent.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.wdl$draft3$transforms$wdlom2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:6133,Energy Efficiency,adapt,adapted,6133,74); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:83); at wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); at scala.util.Either$RightProjection.flatMap(Either.scala:702); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); at scala.util.Either.flatMap(Either.scala:338); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); at womtool.validate.Validate$.validate(Validate.scala:14); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:44); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); at scala.Function0.apply$mcV$sp(Function0.scala:34); at scala.Function0.apply$mcV$sp$(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App.$anonfun$main$1$adapted(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:389); at scala.App.main(App.scala:76); at scala.App.main$(App.scala:74); at womtool.WomtoolMain$.main(WomtoolMain.scala:18); at womtool.WomtoolMain.main(WomtoolMain.scala); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:21,Modifiability,refactor,refactoring,21,"While doing some WDL refactoring, we changed a local (relative) import but forgot to rename the imported file to match. When we ran `womtool validate` on the importing workflow, it dumped the following stack trace and then hung:; ```; Exception in thread ""main"" java.net.UnknownHostException: tasks%2FAlignment.wdl: Name or service not known; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); at java.net.InetAddress.getAllByName0(InetAddress.java:1276); at java.net.InetAddress.getAllByName(InetAddress.java:1192); at java.net.InetAddress.getAllByName(InetAddress.java:1126); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:159); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:156); at java.security.AccessController.doPrivileged(Native Method); at io.netty.util.internal.SocketUtils.allAddressesByName(SocketUtils.java:156); at io.netty.resolver.DefaultNameResolver.doResolveAll(DefaultNameResolver.java:52); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:81); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:73); at org.asynchttpclient.resolver.RequestHostnameResolver.resolve(RequestHostnameResolver.java:50); at org.asynchttpclient.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:2244,Modifiability,adapt,adapted,2244,); at org.asynchttpclient.resolver.RequestHostnameResolver.resolve(RequestHostnameResolver.java:50); at org.asynchttpclient.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonf,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:2374,Modifiability,adapt,adapted,2374,ent.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.wdl$draft3$transforms$wdlom2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:6133,Modifiability,adapt,adapted,6133,74); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:83); at wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); at scala.util.Either$RightProjection.flatMap(Either.scala:702); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); at scala.util.Either.flatMap(Either.scala:338); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); at womtool.validate.Validate$.validate(Validate.scala:14); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:44); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); at scala.Function0.apply$mcV$sp(Function0.scala:34); at scala.Function0.apply$mcV$sp$(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App.$anonfun$main$1$adapted(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:389); at scala.App.main(App.scala:76); at scala.App.main$(App.scala:74); at womtool.WomtoolMain$.main(WomtoolMain.scala:18); at womtool.WomtoolMain.main(WomtoolMain.scala); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:2654,Safety,unsafe,unsafeRunAsync,2654,RequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.wdl$draft3$transforms$wdlom2wom$FileElementToWomBundle$$importWomBundle(FileElementToWomBundle.scala:101); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$11(FileElementToWomBundle.scala:74); at cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); at cats.i,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:2702,Safety,unsafe,unsafeToFuture,2702,netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.wdl$draft3$transforms$wdlom2wom$FileElementToWomBundle$$importWomBundle(FileElementToWomBundle.scala:101); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.$anonfun$toWomBundle$11(FileElementToWomBundle.scala:74); at cats.instances.VectorInstances$$anon$1.$anonfun$traverse$2(vector.scala:77); at cats.instances.VectorInstances$$anon$1.loop$2(vector.s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:141,Security,validat,validate,141,"While doing some WDL refactoring, we changed a local (relative) import but forgot to rename the imported file to match. When we ran `womtool validate` on the importing workflow, it dumped the following stack trace and then hung:; ```; Exception in thread ""main"" java.net.UnknownHostException: tasks%2FAlignment.wdl: Name or service not known; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); at java.net.InetAddress.getAllByName0(InetAddress.java:1276); at java.net.InetAddress.getAllByName(InetAddress.java:1192); at java.net.InetAddress.getAllByName(InetAddress.java:1126); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:159); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:156); at java.security.AccessController.doPrivileged(Native Method); at io.netty.util.internal.SocketUtils.allAddressesByName(SocketUtils.java:156); at io.netty.resolver.DefaultNameResolver.doResolveAll(DefaultNameResolver.java:52); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:81); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:73); at org.asynchttpclient.resolver.RequestHostnameResolver.resolve(RequestHostnameResolver.java:50); at org.asynchttpclient.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:875,Security,secur,security,875,"While doing some WDL refactoring, we changed a local (relative) import but forgot to rename the imported file to match. When we ran `womtool validate` on the importing workflow, it dumped the following stack trace and then hung:; ```; Exception in thread ""main"" java.net.UnknownHostException: tasks%2FAlignment.wdl: Name or service not known; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); at java.net.InetAddress.getAllByName0(InetAddress.java:1276); at java.net.InetAddress.getAllByName(InetAddress.java:1192); at java.net.InetAddress.getAllByName(InetAddress.java:1126); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:159); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:156); at java.security.AccessController.doPrivileged(Native Method); at io.netty.util.internal.SocketUtils.allAddressesByName(SocketUtils.java:156); at io.netty.resolver.DefaultNameResolver.doResolveAll(DefaultNameResolver.java:52); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:81); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:73); at org.asynchttpclient.resolver.RequestHostnameResolver.resolve(RequestHostnameResolver.java:50); at org.asynchttpclient.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:884,Security,Access,AccessController,884,"While doing some WDL refactoring, we changed a local (relative) import but forgot to rename the imported file to match. When we ran `womtool validate` on the importing workflow, it dumped the following stack trace and then hung:; ```; Exception in thread ""main"" java.net.UnknownHostException: tasks%2FAlignment.wdl: Name or service not known; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); at java.net.InetAddress.getAllByName0(InetAddress.java:1276); at java.net.InetAddress.getAllByName(InetAddress.java:1192); at java.net.InetAddress.getAllByName(InetAddress.java:1126); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:159); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:156); at java.security.AccessController.doPrivileged(Native Method); at io.netty.util.internal.SocketUtils.allAddressesByName(SocketUtils.java:156); at io.netty.resolver.DefaultNameResolver.doResolveAll(DefaultNameResolver.java:52); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:81); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:73); at org.asynchttpclient.resolver.RequestHostnameResolver.resolve(RequestHostnameResolver.java:50); at org.asynchttpclient.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:5595,Security,validat,validate,5595,74); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:83); at wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); at scala.util.Either$RightProjection.flatMap(Either.scala:702); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); at scala.util.Either.flatMap(Either.scala:338); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); at womtool.validate.Validate$.validate(Validate.scala:14); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:44); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); at scala.Function0.apply$mcV$sp(Function0.scala:34); at scala.Function0.apply$mcV$sp$(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App.$anonfun$main$1$adapted(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:389); at scala.App.main(App.scala:76); at scala.App.main$(App.scala:74); at womtool.WomtoolMain$.main(WomtoolMain.scala:18); at womtool.WomtoolMain.main(WomtoolMain.scala); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:5604,Security,Validat,Validate,5604,74); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:83); at wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); at scala.util.Either$RightProjection.flatMap(Either.scala:702); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); at scala.util.Either.flatMap(Either.scala:338); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); at womtool.validate.Validate$.validate(Validate.scala:14); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:44); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); at scala.Function0.apply$mcV$sp(Function0.scala:34); at scala.Function0.apply$mcV$sp$(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App.$anonfun$main$1$adapted(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:389); at scala.App.main(App.scala:76); at scala.App.main$(App.scala:74); at womtool.WomtoolMain$.main(WomtoolMain.scala:18); at womtool.WomtoolMain.main(WomtoolMain.scala); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:5614,Security,validat,validate,5614,74); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:83); at wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); at scala.util.Either$RightProjection.flatMap(Either.scala:702); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); at scala.util.Either.flatMap(Either.scala:338); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); at womtool.validate.Validate$.validate(Validate.scala:14); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:44); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); at scala.Function0.apply$mcV$sp(Function0.scala:34); at scala.Function0.apply$mcV$sp$(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App.$anonfun$main$1$adapted(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:389); at scala.App.main(App.scala:76); at scala.App.main$(App.scala:74); at womtool.WomtoolMain$.main(WomtoolMain.scala:18); at womtool.WomtoolMain.main(WomtoolMain.scala); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:5623,Security,Validat,Validate,5623,74); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:83); at wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); at scala.util.Either$RightProjection.flatMap(Either.scala:702); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); at scala.util.Either.flatMap(Either.scala:338); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); at womtool.validate.Validate$.validate(Validate.scala:14); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:44); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); at scala.Function0.apply$mcV$sp(Function0.scala:34); at scala.Function0.apply$mcV$sp$(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App.$anonfun$main$1$adapted(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:389); at scala.App.main(App.scala:76); at scala.App.main$(App.scala:74); at womtool.WomtoolMain$.main(WomtoolMain.scala:18); at womtool.WomtoolMain.main(WomtoolMain.scala); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:1115,Usability,Simpl,SimpleNameResolver,1115," we ran `womtool validate` on the importing workflow, it dumped the following stack trace and then hung:; ```; Exception in thread ""main"" java.net.UnknownHostException: tasks%2FAlignment.wdl: Name or service not known; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); at java.net.InetAddress.getAllByName0(InetAddress.java:1276); at java.net.InetAddress.getAllByName(InetAddress.java:1192); at java.net.InetAddress.getAllByName(InetAddress.java:1126); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:159); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:156); at java.security.AccessController.doPrivileged(Native Method); at io.netty.util.internal.SocketUtils.allAddressesByName(SocketUtils.java:156); at io.netty.resolver.DefaultNameResolver.doResolveAll(DefaultNameResolver.java:52); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:81); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:73); at org.asynchttpclient.resolver.RequestHostnameResolver.resolve(RequestHostnameResolver.java:50); at org.asynchttpclient.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:1145,Usability,Simpl,SimpleNameResolver,1145,"` on the importing workflow, it dumped the following stack trace and then hung:; ```; Exception in thread ""main"" java.net.UnknownHostException: tasks%2FAlignment.wdl: Name or service not known; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); at java.net.InetAddress.getAllByName0(InetAddress.java:1276); at java.net.InetAddress.getAllByName(InetAddress.java:1192); at java.net.InetAddress.getAllByName(InetAddress.java:1126); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:159); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:156); at java.security.AccessController.doPrivileged(Native Method); at io.netty.util.internal.SocketUtils.allAddressesByName(SocketUtils.java:156); at io.netty.resolver.DefaultNameResolver.doResolveAll(DefaultNameResolver.java:52); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:81); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:73); at org.asynchttpclient.resolver.RequestHostnameResolver.resolve(RequestHostnameResolver.java:50); at org.asynchttpclient.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:1195,Usability,Simpl,SimpleNameResolver,1195,"ack trace and then hung:; ```; Exception in thread ""main"" java.net.UnknownHostException: tasks%2FAlignment.wdl: Name or service not known; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); at java.net.InetAddress.getAllByName0(InetAddress.java:1276); at java.net.InetAddress.getAllByName(InetAddress.java:1192); at java.net.InetAddress.getAllByName(InetAddress.java:1126); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:159); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:156); at java.security.AccessController.doPrivileged(Native Method); at io.netty.util.internal.SocketUtils.allAddressesByName(SocketUtils.java:156); at io.netty.resolver.DefaultNameResolver.doResolveAll(DefaultNameResolver.java:52); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:81); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:73); at org.asynchttpclient.resolver.RequestHostnameResolver.resolve(RequestHostnameResolver.java:50); at org.asynchttpclient.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/issues/3977:1225,Usability,Simpl,SimpleNameResolver,1225," ```; Exception in thread ""main"" java.net.UnknownHostException: tasks%2FAlignment.wdl: Name or service not known; at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method); at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); at java.net.InetAddress.getAllByName0(InetAddress.java:1276); at java.net.InetAddress.getAllByName(InetAddress.java:1192); at java.net.InetAddress.getAllByName(InetAddress.java:1126); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:159); at io.netty.util.internal.SocketUtils$9.run(SocketUtils.java:156); at java.security.AccessController.doPrivileged(Native Method); at io.netty.util.internal.SocketUtils.allAddressesByName(SocketUtils.java:156); at io.netty.resolver.DefaultNameResolver.doResolveAll(DefaultNameResolver.java:52); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:81); at io.netty.resolver.SimpleNameResolver.resolveAll(SimpleNameResolver.java:73); at org.asynchttpclient.resolver.RequestHostnameResolver.resolve(RequestHostnameResolver.java:50); at org.asynchttpclient.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977
https://github.com/broadinstitute/cromwell/pull/3978:140,Availability,error,errors,140,Note: this is a tactical fix to allow the cache-fetch post-processing to fail and the workflow to continue.; It does nothing to prevent the errors from happening in the first place - for that issue see #3979,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3978
https://github.com/broadinstitute/cromwell/pull/3978:42,Performance,cache,cache-fetch,42,Note: this is a tactical fix to allow the cache-fetch post-processing to fail and the workflow to continue.; It does nothing to prevent the errors from happening in the first place - for that issue see #3979,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3978
https://github.com/broadinstitute/cromwell/issues/3979:126,Performance,Cache,Cache,126,"Before #3978 the entire workflow would be stuck ""running"" forever.; With that change the workflow will complete, but the Call Cache fetch is still failing when it shouldn't be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3979
https://github.com/broadinstitute/cromwell/pull/3980:252,Deployability,Pipeline,Pipelines,252,Prototype to inform conversation on WDL directories in OpenWDL (https://github.com/openwdl/wdl/pull/241). TODO:; - [x] Add a test to ensure I didn't accidentally add this to WDL 1.0 (where `Directory` would be treated like a `struct` name). Limits:; - Pipelines API v1 cannot handle sub-directories.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3980
https://github.com/broadinstitute/cromwell/pull/3980:125,Testability,test,test,125,Prototype to inform conversation on WDL directories in OpenWDL (https://github.com/openwdl/wdl/pull/241). TODO:; - [x] Add a test to ensure I didn't accidentally add this to WDL 1.0 (where `Directory` would be treated like a `struct` name). Limits:; - Pipelines API v1 cannot handle sub-directories.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3980
https://github.com/broadinstitute/cromwell/issues/3986:776,Availability,echo,echo,776,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:2330,Deployability,configurat,configuration,2330,"), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:2330,Modifiability,config,configuration,2330,"), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:2375,Security,PASSWORD,PASSWORDS,2375,"), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:280,Testability,test,test,280,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:305,Testability,test,test,305,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:412,Testability,test,test,412,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:417,Testability,test,test,417,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:441,Testability,test,test,441,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:503,Testability,test,test,503,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:542,Testability,test,test,542,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:555,Testability,test,test,555,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:584,Testability,test,test,584,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:678,Testability,test,test,678,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:932,Testability,test,test,932,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:978,Testability,test,test,978,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:1094,Testability,test,test,1094,"in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.bro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:1122,Testability,test,test,1122,"les seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:1168,Testability,test,test,1168,"e paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing some",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:1242,Testability,test,test,1242,"t directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3986:1668,Usability,feedback,feedback,1668,"), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986
https://github.com/broadinstitute/cromwell/issues/3990:49,Availability,echo,echo,49,"```WDL; version 1.0. workflow test_regex {; call echo {; input:; }. }. task echo {; input {; String myFile = ""dir/dir/bladiebla.fq.gz""; }; # Chop of .gz; String name = sub(basename(myFile), ""\\.gz$"",""""); ; command {; echo ${myFile}; echo ${basename(myFile)}; echo ${name} ; }; output {; Array[String] result = read_lines(stdout()) ; } ; }; ```. Run with cromwell version 34. ; I expect `name` to be `bladiebla.fq` but unfortunately it is `bladiebla.fq.gz`; ~~The regex used work in earlier versions of cromwell (31).~~; We noticed this when changing to WDL 1.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990
https://github.com/broadinstitute/cromwell/issues/3990:76,Availability,echo,echo,76,"```WDL; version 1.0. workflow test_regex {; call echo {; input:; }. }. task echo {; input {; String myFile = ""dir/dir/bladiebla.fq.gz""; }; # Chop of .gz; String name = sub(basename(myFile), ""\\.gz$"",""""); ; command {; echo ${myFile}; echo ${basename(myFile)}; echo ${name} ; }; output {; Array[String] result = read_lines(stdout()) ; } ; }; ```. Run with cromwell version 34. ; I expect `name` to be `bladiebla.fq` but unfortunately it is `bladiebla.fq.gz`; ~~The regex used work in earlier versions of cromwell (31).~~; We noticed this when changing to WDL 1.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990
https://github.com/broadinstitute/cromwell/issues/3990:217,Availability,echo,echo,217,"```WDL; version 1.0. workflow test_regex {; call echo {; input:; }. }. task echo {; input {; String myFile = ""dir/dir/bladiebla.fq.gz""; }; # Chop of .gz; String name = sub(basename(myFile), ""\\.gz$"",""""); ; command {; echo ${myFile}; echo ${basename(myFile)}; echo ${name} ; }; output {; Array[String] result = read_lines(stdout()) ; } ; }; ```. Run with cromwell version 34. ; I expect `name` to be `bladiebla.fq` but unfortunately it is `bladiebla.fq.gz`; ~~The regex used work in earlier versions of cromwell (31).~~; We noticed this when changing to WDL 1.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990
https://github.com/broadinstitute/cromwell/issues/3990:233,Availability,echo,echo,233,"```WDL; version 1.0. workflow test_regex {; call echo {; input:; }. }. task echo {; input {; String myFile = ""dir/dir/bladiebla.fq.gz""; }; # Chop of .gz; String name = sub(basename(myFile), ""\\.gz$"",""""); ; command {; echo ${myFile}; echo ${basename(myFile)}; echo ${name} ; }; output {; Array[String] result = read_lines(stdout()) ; } ; }; ```. Run with cromwell version 34. ; I expect `name` to be `bladiebla.fq` but unfortunately it is `bladiebla.fq.gz`; ~~The regex used work in earlier versions of cromwell (31).~~; We noticed this when changing to WDL 1.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990
https://github.com/broadinstitute/cromwell/issues/3990:259,Availability,echo,echo,259,"```WDL; version 1.0. workflow test_regex {; call echo {; input:; }. }. task echo {; input {; String myFile = ""dir/dir/bladiebla.fq.gz""; }; # Chop of .gz; String name = sub(basename(myFile), ""\\.gz$"",""""); ; command {; echo ${myFile}; echo ${basename(myFile)}; echo ${name} ; }; output {; Array[String] result = read_lines(stdout()) ; } ; }; ```. Run with cromwell version 34. ; I expect `name` to be `bladiebla.fq` but unfortunately it is `bladiebla.fq.gz`; ~~The regex used work in earlier versions of cromwell (31).~~; We noticed this when changing to WDL 1.0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990
https://github.com/broadinstitute/cromwell/pull/3992:152,Deployability,update,update,152,"Should unblock our CI but may not be necessary if the short-term fix in https://github.com/common-workflow-language/cwltool/pull/865 or the longer-term update in https://github.com/common-workflow-language/cwltool/pull/864 get released soon. On the other hand, as Peter says re ruamel, having open ended dependencies is a bad idea generally, so maybe pinning to a known good version is a good idea regardless?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3992
https://github.com/broadinstitute/cromwell/pull/3992:227,Deployability,release,released,227,"Should unblock our CI but may not be necessary if the short-term fix in https://github.com/common-workflow-language/cwltool/pull/865 or the longer-term update in https://github.com/common-workflow-language/cwltool/pull/864 get released soon. On the other hand, as Peter says re ruamel, having open ended dependencies is a bad idea generally, so maybe pinning to a known good version is a good idea regardless?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3992
https://github.com/broadinstitute/cromwell/pull/3992:304,Integrability,depend,dependencies,304,"Should unblock our CI but may not be necessary if the short-term fix in https://github.com/common-workflow-language/cwltool/pull/865 or the longer-term update in https://github.com/common-workflow-language/cwltool/pull/864 get released soon. On the other hand, as Peter says re ruamel, having open ended dependencies is a bad idea generally, so maybe pinning to a known good version is a good idea regardless?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3992
https://github.com/broadinstitute/cromwell/issues/3993:587,Usability,guid,guided,587,"Dear Cromwell developers,. Our group has gone all-in on WDL with Cromwell, because it fits our use case perfectly. But since no workflow system is perfect, we have had our issues with Cromwell as well. . When I find issues, I prefer to solve it myself and make PR's rather than file an issue and pile more work on you. I have scala experience, so in theory I could contribute. Unfortunately, for the outsider the cromwell code is quite a jungle. A lot of things are interconnected, nothing seems to be defined in just one place and the code base is quite huge. So if there would be some guided tour (text, video, audio or otherwise) that would help a lot understanding what's what in this repository. And that would help me and hopefully others to make contributions later.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3993
https://github.com/broadinstitute/cromwell/issues/3998:312,Performance,cache,cache,312,"The call metadata key `backendStatus` is currently only generated while a polling for a running call, and right after the call succeeds/fails. 1. It appears that in some cases, (perhaps after restart?), the polling doesn't ever store a `backendStatus` entry with value `success`.; 2. It also appears that a call cache copy of a successful run does not have the `backendStatus` metadata key. TODO- Define acceptance criteria including:; - Should a call cache copy have a `backendStatus` field? ... do we also copy other backend info? What fields do we copy on a cache hit?; - Should call `backendStatus` use a [CRDT](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type) like the summarized [`cromwell.core.WorkflowState`](https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/core/src/main/scala/cromwell/core/WorkflowState.scala#L9)?. This temporary log was generated as part of work-in-progress on #3658. The `backendStatus` fails to generate the first time, and then the jobs call cache on retry attempts. [missing_backend_status.txt](https://github.com/broadinstitute/cromwell/files/2279800/missing_backend_status.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3998
https://github.com/broadinstitute/cromwell/issues/3998:452,Performance,cache,cache,452,"The call metadata key `backendStatus` is currently only generated while a polling for a running call, and right after the call succeeds/fails. 1. It appears that in some cases, (perhaps after restart?), the polling doesn't ever store a `backendStatus` entry with value `success`.; 2. It also appears that a call cache copy of a successful run does not have the `backendStatus` metadata key. TODO- Define acceptance criteria including:; - Should a call cache copy have a `backendStatus` field? ... do we also copy other backend info? What fields do we copy on a cache hit?; - Should call `backendStatus` use a [CRDT](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type) like the summarized [`cromwell.core.WorkflowState`](https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/core/src/main/scala/cromwell/core/WorkflowState.scala#L9)?. This temporary log was generated as part of work-in-progress on #3658. The `backendStatus` fails to generate the first time, and then the jobs call cache on retry attempts. [missing_backend_status.txt](https://github.com/broadinstitute/cromwell/files/2279800/missing_backend_status.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3998
https://github.com/broadinstitute/cromwell/issues/3998:561,Performance,cache,cache,561,"The call metadata key `backendStatus` is currently only generated while a polling for a running call, and right after the call succeeds/fails. 1. It appears that in some cases, (perhaps after restart?), the polling doesn't ever store a `backendStatus` entry with value `success`.; 2. It also appears that a call cache copy of a successful run does not have the `backendStatus` metadata key. TODO- Define acceptance criteria including:; - Should a call cache copy have a `backendStatus` field? ... do we also copy other backend info? What fields do we copy on a cache hit?; - Should call `backendStatus` use a [CRDT](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type) like the summarized [`cromwell.core.WorkflowState`](https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/core/src/main/scala/cromwell/core/WorkflowState.scala#L9)?. This temporary log was generated as part of work-in-progress on #3658. The `backendStatus` fails to generate the first time, and then the jobs call cache on retry attempts. [missing_backend_status.txt](https://github.com/broadinstitute/cromwell/files/2279800/missing_backend_status.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3998
https://github.com/broadinstitute/cromwell/issues/3998:1031,Performance,cache,cache,1031,"The call metadata key `backendStatus` is currently only generated while a polling for a running call, and right after the call succeeds/fails. 1. It appears that in some cases, (perhaps after restart?), the polling doesn't ever store a `backendStatus` entry with value `success`.; 2. It also appears that a call cache copy of a successful run does not have the `backendStatus` metadata key. TODO- Define acceptance criteria including:; - Should a call cache copy have a `backendStatus` field? ... do we also copy other backend info? What fields do we copy on a cache hit?; - Should call `backendStatus` use a [CRDT](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type) like the summarized [`cromwell.core.WorkflowState`](https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/core/src/main/scala/cromwell/core/WorkflowState.scala#L9)?. This temporary log was generated as part of work-in-progress on #3658. The `backendStatus` fails to generate the first time, and then the jobs call cache on retry attempts. [missing_backend_status.txt](https://github.com/broadinstitute/cromwell/files/2279800/missing_backend_status.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3998
https://github.com/broadinstitute/cromwell/issues/3998:898,Testability,log,log,898,"The call metadata key `backendStatus` is currently only generated while a polling for a running call, and right after the call succeeds/fails. 1. It appears that in some cases, (perhaps after restart?), the polling doesn't ever store a `backendStatus` entry with value `success`.; 2. It also appears that a call cache copy of a successful run does not have the `backendStatus` metadata key. TODO- Define acceptance criteria including:; - Should a call cache copy have a `backendStatus` field? ... do we also copy other backend info? What fields do we copy on a cache hit?; - Should call `backendStatus` use a [CRDT](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type) like the summarized [`cromwell.core.WorkflowState`](https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/core/src/main/scala/cromwell/core/WorkflowState.scala#L9)?. This temporary log was generated as part of work-in-progress on #3658. The `backendStatus` fails to generate the first time, and then the jobs call cache on retry attempts. [missing_backend_status.txt](https://github.com/broadinstitute/cromwell/files/2279800/missing_backend_status.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3998
https://github.com/broadinstitute/cromwell/issues/3999:335,Availability,echo,echo,335,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:436,Availability,echo,echo,436,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:460,Availability,echo,echo,460,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:564,Availability,echo,echo,564,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:632,Availability,echo,echo,632,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:668,Availability,echo,echo,668,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:999,Availability,echo,echo,999,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:1087,Availability,echo,echo,1087,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:1841,Deployability,configurat,configuration,1841,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:866,Modifiability,variab,variables,866,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:1841,Modifiability,config,configuration,1841,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:1886,Security,PASSWORD,PASSWORDS,1886,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:279,Testability,test,test,279,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:948,Testability,test,test,948,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/issues/3999:1179,Usability,feedback,feedback,1179,"Defining inputs in a call overwrites/affects inputs to other calls when these inputs have the same name. This happens in cromwell 34, as well as the develop version (9bee537). It happens for WDL version 1.0 and Biscayne, but not for draft-2. example:; ```; version 1.0; workflow test {; String out = ""hello""; call echo1 { #Should run `echo hello1`, but runs `echo21` if run second; input:; out = out + ""1""; }; call echo2 { #should run `echo hello2`, but runs `echo 12` if run second; input:; out = out + ""2""; }; }; task echo1 {; input {; String out; }; command {; echo ~{out}; }; }; task echo2 {; input {; String out; }; command {; echo ~{out}; }; }; ```; I added the echo task twice to check if it might be caused by running the same task multiple times, but this also happens when it's two different tasks with equally named inputs. Defining one or both inputs as variables before passing them to the call seems works as expected:; ```; workflow test {; String out = ""hello""; call echo1 { # runs `echo hello1`; input:; out = out + ""1""; }; String out2 = out + ""2""; call echo2 { # runs `echo hello2`; input:; out = out2; }; }; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999
https://github.com/broadinstitute/cromwell/pull/4000:8,Availability,error,error,8,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:346,Integrability,message,messages,346,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:91,Modifiability,variab,variables,91,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:232,Modifiability,variab,variables,232,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:284,Modifiability,variab,variables,284,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:470,Modifiability,config,config,470,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:366,Performance,cache,caches,366,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:550,Performance,load,load,550,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:620,Performance,load,load,620,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:923,Safety,timeout,timeout,923,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:70,Security,Encrypt,Encrypting,70,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:182,Security,secur,secure,182,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:225,Security,secur,secure,225,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:277,Security,secur,secure,277,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:44,Testability,test,testing,44,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:135,Testability,test,tests,135,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:212,Testability,test,tests,212,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:380,Testability,test,tests,380,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:628,Testability,test,test,628,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/pull/4000:773,Testability,log,logback,773,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000
https://github.com/broadinstitute/cromwell/issues/4001:690,Availability,failure,failure,690,"Similar to #3998 (`backendStatus`), but for the metadata key `dockerImageUsed`. This call metadata key is written during job success by the engine. This key may be missing due to restarts of cromwell during centaur tests. Automated restarts of the centaur test end up call caching, where this key isn't written. As a call cache hit technically doesn't _have_ a dockerImage, it should be decided like in #3998 if the key `dockerImageUsed` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L279-L281. Example log of a failure during WIP of #3658 ; [dockerImageUsed_missing.txt](https://github.com/broadinstitute/cromwell/files/2284646/dockerImageUsed_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4001
https://github.com/broadinstitute/cromwell/issues/4001:322,Performance,cache,cache,322,"Similar to #3998 (`backendStatus`), but for the metadata key `dockerImageUsed`. This call metadata key is written during job success by the engine. This key may be missing due to restarts of cromwell during centaur tests. Automated restarts of the centaur test end up call caching, where this key isn't written. As a call cache hit technically doesn't _have_ a dockerImage, it should be decided like in #3998 if the key `dockerImageUsed` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L279-L281. Example log of a failure during WIP of #3658 ; [dockerImageUsed_missing.txt](https://github.com/broadinstitute/cromwell/files/2284646/dockerImageUsed_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4001
https://github.com/broadinstitute/cromwell/issues/4001:460,Performance,cache,cache,460,"Similar to #3998 (`backendStatus`), but for the metadata key `dockerImageUsed`. This call metadata key is written during job success by the engine. This key may be missing due to restarts of cromwell during centaur tests. Automated restarts of the centaur test end up call caching, where this key isn't written. As a call cache hit technically doesn't _have_ a dockerImage, it should be decided like in #3998 if the key `dockerImageUsed` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L279-L281. Example log of a failure during WIP of #3658 ; [dockerImageUsed_missing.txt](https://github.com/broadinstitute/cromwell/files/2284646/dockerImageUsed_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4001
https://github.com/broadinstitute/cromwell/issues/4001:215,Testability,test,tests,215,"Similar to #3998 (`backendStatus`), but for the metadata key `dockerImageUsed`. This call metadata key is written during job success by the engine. This key may be missing due to restarts of cromwell during centaur tests. Automated restarts of the centaur test end up call caching, where this key isn't written. As a call cache hit technically doesn't _have_ a dockerImage, it should be decided like in #3998 if the key `dockerImageUsed` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L279-L281. Example log of a failure during WIP of #3658 ; [dockerImageUsed_missing.txt](https://github.com/broadinstitute/cromwell/files/2284646/dockerImageUsed_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4001
https://github.com/broadinstitute/cromwell/issues/4001:256,Testability,test,test,256,"Similar to #3998 (`backendStatus`), but for the metadata key `dockerImageUsed`. This call metadata key is written during job success by the engine. This key may be missing due to restarts of cromwell during centaur tests. Automated restarts of the centaur test end up call caching, where this key isn't written. As a call cache hit technically doesn't _have_ a dockerImage, it should be decided like in #3998 if the key `dockerImageUsed` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L279-L281. Example log of a failure during WIP of #3658 ; [dockerImageUsed_missing.txt](https://github.com/broadinstitute/cromwell/files/2284646/dockerImageUsed_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4001
https://github.com/broadinstitute/cromwell/issues/4001:681,Testability,log,log,681,"Similar to #3998 (`backendStatus`), but for the metadata key `dockerImageUsed`. This call metadata key is written during job success by the engine. This key may be missing due to restarts of cromwell during centaur tests. Automated restarts of the centaur test end up call caching, where this key isn't written. As a call cache hit technically doesn't _have_ a dockerImage, it should be decided like in #3998 if the key `dockerImageUsed` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L279-L281. Example log of a failure during WIP of #3658 ; [dockerImageUsed_missing.txt](https://github.com/broadinstitute/cromwell/files/2284646/dockerImageUsed_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4001
https://github.com/broadinstitute/cromwell/pull/4002:93,Deployability,install,installed,93,"This is a quick PR to add a Dockerfile (and instructions) so that scala/sbt don't need to be installed on the user system. This is a nice step to have, and I decided to do before working on the issues addressed in https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720. @geoffjentry could you point me to the udocker example you mentioned? If there isn't a clear documentation for it, what I'm generally interested in to develop the singularity bit is:. - where is the entrypoint for the scala application (when the user interacts to do something with docker, for example); - what is the general flow; - is what we want to do for singularity comparable to the udocker example, so mirroring that code would do the trick?. Here are some quick links, if they are helpful!. - https://hub.docker.com/r/vanessa/cromwell-dev/; - https://github.com/vsoch/cromwell/tree/add/singularity/scripts/docker-develop. For the first, the container is still pushing... on my insanely great crappy wireless :)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4002
https://github.com/broadinstitute/cromwell/pull/4002:380,Usability,clear,clear,380,"This is a quick PR to add a Dockerfile (and instructions) so that scala/sbt don't need to be installed on the user system. This is a nice step to have, and I decided to do before working on the issues addressed in https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720. @geoffjentry could you point me to the udocker example you mentioned? If there isn't a clear documentation for it, what I'm generally interested in to develop the singularity bit is:. - where is the entrypoint for the scala application (when the user interacts to do something with docker, for example); - what is the general flow; - is what we want to do for singularity comparable to the udocker example, so mirroring that code would do the trick?. Here are some quick links, if they are helpful!. - https://hub.docker.com/r/vanessa/cromwell-dev/; - https://github.com/vsoch/cromwell/tree/add/singularity/scripts/docker-develop. For the first, the container is still pushing... on my insanely great crappy wireless :)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4002
https://github.com/broadinstitute/cromwell/issues/4004:92,Availability,error,error,92,"When trying one of our new CWL workflow steps in Cromwell, we're encountering an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:674,Availability,echo,echo,674,"When trying one of our new CWL workflow steps in Cromwell, we're encountering an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:1257,Availability,echo,echo,1257,"ing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:1754,Availability,error,error,1754,"; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:2228,Availability,echo,echo,2228,"sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:2953,Availability,error,error,2953,"d assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:3550,Performance,concurren,concurrent,3550,"ackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.disp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:3611,Performance,concurren,concurrent,3611,": job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:3677,Performance,concurren,concurrent,3677,"yncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:3750,Performance,concurren,concurrent,3750,"a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:4071,Performance,concurren,concurrent,4071,"a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:1353,Security,checksum,checksum,1353,"ng contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:172,Testability,test,test,172,"When trying one of our new CWL workflow steps in Cromwell, we're encountering an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:403,Testability,test,test,403,"When trying one of our new CWL workflow steps in Cromwell, we're encountering an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:473,Testability,test,test,473,"When trying one of our new CWL workflow steps in Cromwell, we're encountering an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:482,Testability,test,test,482,"When trying one of our new CWL workflow steps in Cromwell, we're encountering an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:517,Testability,test,test,517,"When trying one of our new CWL workflow steps in Cromwell, we're encountering an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:969,Testability,test,test,969,"When trying one of our new CWL workflow steps in Cromwell, we're encountering an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:997,Testability,test,test,997,"When trying one of our new CWL workflow steps in Cromwell, we're encountering an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:1066,Testability,test,test,1066," steps in Cromwell, we're encountering an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] Workf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:1075,Testability,test,test,1075,"ng an unexpected error with an optional output when that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:1138,Testability,test,test,1138,"that output isn't present:; ```; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:1192,Testability,test,test,1192," 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundCo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:1208,Testability,test,test,1208,"defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:1276,Testability,test,test,1276,"ing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:1308,Testability,test,test,1308,"e$?'.; ```; This only seems to occur when the `glob` in the outputBinding contains a `*`. A small test case (run with `/usr/bin/java -jar cromwell-34.jar run -t cwl -i test.yml test.cwl` demonstrates this:; #### test.cwl; ```yaml; #!/usr/bin/env cwl-runner; cwlVersion: v1.0; class: CommandLineTool; requirements:; - class: ShellCommandRequirement; baseCommand: ['/bin/echo', ""this is a""]; arguments: [; { valueFrom: '>', shellQuote: false },; 'some_output.txt'; ]; inputs:; bonus:; type: string; inputBinding:; position: -1; outputs:; found:; type: File; outputBinding:; glob: '*output.txt'; not_found:; type: File?; outputBinding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:1984,Testability,test,test,1984,"nding:; glob: '*extra.txt'; ```; #### test.yaml; ```yaml; bonus: ""test""; ```. `cwltool` handles this case as expected:; ```; $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [201",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:2116,Testability,test,test,2116," $ cwltool test.cwl test.yml; /usr/local/bin/cwltool 1.0.20170822192924; Resolved 'test.cwl' to 'file:///home/tmooney/cromwell_test/glob/test.cwl'; [job test.cwl] /tmp/tmpqeLl9_$ /bin/sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWork",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:2247,Testability,test,test,2247,"sh \; -c \; '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [job test.cwl] completed success; {; ""found"": {; ""checksum"": ""sha1$6476df3aac780622368173fe6e768a2edc3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:2444,Testability,test,test,2444,"3932c8"", ; ""basename"": ""some_output.txt"", ; ""nameext"": "".txt"", ; ""nameroot"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:2495,Testability,test,test,2495,"root"": ""some_output"", ; ""location"": ""file:///home/tmooney/cromwell_test/glob/some_output.txt"", ; ""path"": ""/home/tmooney/cromwell_test/glob/some_output.txt"", ; ""class"": ""File"", ; ""size"": 15; }, ; ""not_found"": null; }; Final process status is success; ```. Cromwell fails with this error:; ```; [2018-08-14 16:14:05,89] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Parsing workflow as CWL v1.0; [2018-08-14 16:14:07,03] [info] MaterializeWorkflowDescriptorActor [a3d3e011]: Call-to-Backend assignments: test.cwl -> Local; [2018-08-14 16:14:10,44] [info] WorkflowExecutionActor-a3d3e011-3a0c-4203-9edb-3d65564a1d1d [a3d3e011]: Starting test.cwl; [2018-08-14 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4004:3175,Testability,test,test,3175,"4 16:14:11,85] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: '/bin/echo' 'this is a' 'test' > 'some_output.txt'; [2018-08-14 16:14:11,97] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: executing: /bin/bash /home/tmooney/cromwell_test/glob/cromwell-executions/test.cwl/a3d3e011-3a0c-4203-9edb-3d65564a1d1d/call-test.cwl/execution/script; [2018-08-14 16:14:13,16] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: job id: 1832; [2018-08-14 16:14:13,17] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from - to WaitingForReturnCodeFile; [2018-08-14 16:14:14,45] [info] BackgroundConfigAsyncJobExecutionActor [a3d3e011test.cwl:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2018-08-14 16:14:15,67] [error] WorkflowManagerActor Workflow a3d3e011-3a0c-4203-9edb-3d65564a1d1d failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'test.cwl.not_found': No coercion defined from wom value(s) '[]' of type 'Array[Nothing]' to 'class wom.types.WomMaybePopulatedFileType$?'.; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:839); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4004
https://github.com/broadinstitute/cromwell/issues/4005:428,Availability,error,error,428,"Hello,. I am trying to use our on premise cromwell instance to submit jobs to GCP via the genomics api. I seem to have everything working except the VPC network name. We have a naming convention for the VPC network names configured in our GCP project and don't have the flexibility to change the name. However, cromwell seems to expect the VPC network name to be ""default"" and this does not seem to be configurable. Here is the error I am receiving when trying to submit the workflow:. [error] WorkflowManagerActor Workflow 8f55bf4d-9389-40e6-a469-dbd434394dd8 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 3. Invalid value for field 'resource.networkInterfaces[0].network': 'https://www.googleapis.com/compute/v1/projects/projectxyz/global/networks/default'. The referenced network resource cannot be found. Is there a way for me to pass a different network name? If not, where can I request this feature be added to a future version?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4005
https://github.com/broadinstitute/cromwell/issues/4005:487,Availability,error,error,487,"Hello,. I am trying to use our on premise cromwell instance to submit jobs to GCP via the genomics api. I seem to have everything working except the VPC network name. We have a naming convention for the VPC network names configured in our GCP project and don't have the flexibility to change the name. However, cromwell seems to expect the VPC network name to be ""default"" and this does not seem to be configurable. Here is the error I am receiving when trying to submit the workflow:. [error] WorkflowManagerActor Workflow 8f55bf4d-9389-40e6-a469-dbd434394dd8 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 3. Invalid value for field 'resource.networkInterfaces[0].network': 'https://www.googleapis.com/compute/v1/projects/projectxyz/global/networks/default'. The referenced network resource cannot be found. Is there a way for me to pass a different network name? If not, where can I request this feature be added to a future version?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4005
https://github.com/broadinstitute/cromwell/issues/4005:709,Availability,error,error,709,"Hello,. I am trying to use our on premise cromwell instance to submit jobs to GCP via the genomics api. I seem to have everything working except the VPC network name. We have a naming convention for the VPC network names configured in our GCP project and don't have the flexibility to change the name. However, cromwell seems to expect the VPC network name to be ""default"" and this does not seem to be configurable. Here is the error I am receiving when trying to submit the workflow:. [error] WorkflowManagerActor Workflow 8f55bf4d-9389-40e6-a469-dbd434394dd8 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 3. Invalid value for field 'resource.networkInterfaces[0].network': 'https://www.googleapis.com/compute/v1/projects/projectxyz/global/networks/default'. The referenced network resource cannot be found. Is there a way for me to pass a different network name? If not, where can I request this feature be added to a future version?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4005
https://github.com/broadinstitute/cromwell/issues/4005:221,Modifiability,config,configured,221,"Hello,. I am trying to use our on premise cromwell instance to submit jobs to GCP via the genomics api. I seem to have everything working except the VPC network name. We have a naming convention for the VPC network names configured in our GCP project and don't have the flexibility to change the name. However, cromwell seems to expect the VPC network name to be ""default"" and this does not seem to be configurable. Here is the error I am receiving when trying to submit the workflow:. [error] WorkflowManagerActor Workflow 8f55bf4d-9389-40e6-a469-dbd434394dd8 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 3. Invalid value for field 'resource.networkInterfaces[0].network': 'https://www.googleapis.com/compute/v1/projects/projectxyz/global/networks/default'. The referenced network resource cannot be found. Is there a way for me to pass a different network name? If not, where can I request this feature be added to a future version?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4005
https://github.com/broadinstitute/cromwell/issues/4005:402,Modifiability,config,configurable,402,"Hello,. I am trying to use our on premise cromwell instance to submit jobs to GCP via the genomics api. I seem to have everything working except the VPC network name. We have a naming convention for the VPC network names configured in our GCP project and don't have the flexibility to change the name. However, cromwell seems to expect the VPC network name to be ""default"" and this does not seem to be configurable. Here is the error I am receiving when trying to submit the workflow:. [error] WorkflowManagerActor Workflow 8f55bf4d-9389-40e6-a469-dbd434394dd8 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 3. Invalid value for field 'resource.networkInterfaces[0].network': 'https://www.googleapis.com/compute/v1/projects/projectxyz/global/networks/default'. The referenced network resource cannot be found. Is there a way for me to pass a different network name? If not, where can I request this feature be added to a future version?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4005
https://github.com/broadinstitute/cromwell/issues/4006:130,Availability,error,errors,130,"Cromwell Version: `""cromwell"": ""33-e90c4de""`. Problem: The workflow has scattered tasks, a few of the shards finished without any errors, but when looking into the actual results of the task, we can only see files with `0B` size. Example workflow: workflow `42e173c6-7fc3-4a3e-93c7-c9d95836f6a5 ` in `https://cromwell.mint-dev.broadinstitute.org/`, specifically, the task: `call-sc/shard-98/SmartSeq2SingleCell/b4ac422c-e5b1-42ed-8dcf-cca51394e08c/call-RSEMExpression`, shard-98. @jishuxu has run into this issue for several times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006
https://github.com/broadinstitute/cromwell/issues/4007:46,Performance,concurren,concurrent,46,"TESK requires an FTP server that could accept concurrent connections for multiple conformance test suites running in parallel (as our CI runs for each commit / PR).; The one currently used in testing does not provide such scalability, at least not for a single user.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4007
https://github.com/broadinstitute/cromwell/issues/4007:222,Performance,scalab,scalability,222,"TESK requires an FTP server that could accept concurrent connections for multiple conformance test suites running in parallel (as our CI runs for each commit / PR).; The one currently used in testing does not provide such scalability, at least not for a single user.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4007
https://github.com/broadinstitute/cromwell/issues/4007:94,Testability,test,test,94,"TESK requires an FTP server that could accept concurrent connections for multiple conformance test suites running in parallel (as our CI runs for each commit / PR).; The one currently used in testing does not provide such scalability, at least not for a single user.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4007
https://github.com/broadinstitute/cromwell/issues/4007:192,Testability,test,testing,192,"TESK requires an FTP server that could accept concurrent connections for multiple conformance test suites running in parallel (as our CI runs for each commit / PR).; The one currently used in testing does not provide such scalability, at least not for a single user.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4007
https://github.com/broadinstitute/cromwell/pull/4008:48,Deployability,integrat,integration,48,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/pull/4008:48,Integrability,integrat,integration,48,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/pull/4008:826,Modifiability,Config,Configurable,826,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/pull/4008:1080,Performance,concurren,concurrent,1080,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/pull/4008:1179,Performance,concurren,concurrent,1179,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/pull/4008:1249,Performance,concurren,concurrent,1249,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/pull/4008:1328,Performance,concurren,concurrent,1328,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/pull/4008:62,Testability,test,testing,62,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/pull/4008:284,Testability,test,tests,284,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/pull/4008:423,Testability,test,tests,423,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/pull/4008:923,Testability,test,testing,923,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/pull/4008:1484,Testability,log,logic,1484,"[Disclaimer]; This ~~heavily~~ lacks unit ~~and integration~~ testing.; See ~~https://github.com/broadinstitute/cromwell/issues/4010~~, https://github.com/broadinstitute/cromwell/issues/4009, ~~https://github.com/broadinstitute/cromwell/issues/4007~~. Edit: ; - Closed 4010 after new tests were added for the ftp impl of cloud nio. The core cloud nio code is still largely untested so leaving 4009 open.; - CWL conformance tests can be run on Travis as a CRON job once this PR is merged, which should be enough for 4007. ---. I tried to split commits by general topics if that helps...; Contains:; - The ~woodward~ [Woodard](https://en.wikipedia.org/wiki/Alfre_Woodard) abstraction; - An FTP implementation of the abstraction (more on that below); - Changes to the TES backend to accommodate for other filesystems than SFS; - Configurable glob command (the one that links files to be globbed into a directory); - Setup for testing the cwl conformance suite on Travis. ~~Disabled because it would very likely fail most of the time since I doubt the FTP server we have could handle concurrent travis runs.~~ (now as a CRON job). Since FTP inherently does not allow parallelism and concurrent requests, it is necessary to create as many connections as concurrent requests we want. Because most ftp servers will limit the number of concurrent connections, the FTP implementation pools FTP clients and reuses them across async operations in Cromwell. That's the reason for all the pooling logic. Threads in need of a client when the pool is empty will block until they can get one.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4008
https://github.com/broadinstitute/cromwell/issues/4011:174,Deployability,pipeline,pipelines,174,"I am using latest cromwell develop docker container. When I run ""abort"" command it internall executes docker.kill script that looks like:; ""; #!/bin/bash ; docker kill `cat /pipelines/cromwell-executions/vsearch/81c51e4e-756c-47f7-8dd6\; -57b9c2981162/call-global_search/execution/docker_cid`; ""; However, docker_cid is never created. So, all the abort commands that I do stop the cromwell tasks but never stop docker containers that were started by it. My docker-stack configuration is https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml. It uses slightly modified cromwell:develop container https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/Dockerfile. I also share docker sockets there and everything functions well with the exception of abort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011
https://github.com/broadinstitute/cromwell/issues/4011:470,Deployability,configurat,configuration,470,"I am using latest cromwell develop docker container. When I run ""abort"" command it internall executes docker.kill script that looks like:; ""; #!/bin/bash ; docker kill `cat /pipelines/cromwell-executions/vsearch/81c51e4e-756c-47f7-8dd6\; -57b9c2981162/call-global_search/execution/docker_cid`; ""; However, docker_cid is never created. So, all the abort commands that I do stop the cromwell tasks but never stop docker containers that were started by it. My docker-stack configuration is https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml. It uses slightly modified cromwell:develop container https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/Dockerfile. I also share docker sockets there and everything functions well with the exception of abort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011
https://github.com/broadinstitute/cromwell/issues/4011:555,Deployability,pipeline,pipelines,555,"I am using latest cromwell develop docker container. When I run ""abort"" command it internall executes docker.kill script that looks like:; ""; #!/bin/bash ; docker kill `cat /pipelines/cromwell-executions/vsearch/81c51e4e-756c-47f7-8dd6\; -57b9c2981162/call-global_search/execution/docker_cid`; ""; However, docker_cid is never created. So, all the abort commands that I do stop the cromwell tasks but never stop docker containers that were started by it. My docker-stack configuration is https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml. It uses slightly modified cromwell:develop container https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/Dockerfile. I also share docker sockets there and everything functions well with the exception of abort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011
https://github.com/broadinstitute/cromwell/issues/4011:470,Modifiability,config,configuration,470,"I am using latest cromwell develop docker container. When I run ""abort"" command it internall executes docker.kill script that looks like:; ""; #!/bin/bash ; docker kill `cat /pipelines/cromwell-executions/vsearch/81c51e4e-756c-47f7-8dd6\; -57b9c2981162/call-global_search/execution/docker_cid`; ""; However, docker_cid is never created. So, all the abort commands that I do stop the cromwell tasks but never stop docker containers that were started by it. My docker-stack configuration is https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml. It uses slightly modified cromwell:develop container https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/Dockerfile. I also share docker sockets there and everything functions well with the exception of abort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011
https://github.com/broadinstitute/cromwell/issues/4011:65,Safety,abort,abort,65,"I am using latest cromwell develop docker container. When I run ""abort"" command it internall executes docker.kill script that looks like:; ""; #!/bin/bash ; docker kill `cat /pipelines/cromwell-executions/vsearch/81c51e4e-756c-47f7-8dd6\; -57b9c2981162/call-global_search/execution/docker_cid`; ""; However, docker_cid is never created. So, all the abort commands that I do stop the cromwell tasks but never stop docker containers that were started by it. My docker-stack configuration is https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml. It uses slightly modified cromwell:develop container https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/Dockerfile. I also share docker sockets there and everything functions well with the exception of abort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011
https://github.com/broadinstitute/cromwell/issues/4011:347,Safety,abort,abort,347,"I am using latest cromwell develop docker container. When I run ""abort"" command it internall executes docker.kill script that looks like:; ""; #!/bin/bash ; docker kill `cat /pipelines/cromwell-executions/vsearch/81c51e4e-756c-47f7-8dd6\; -57b9c2981162/call-global_search/execution/docker_cid`; ""; However, docker_cid is never created. So, all the abort commands that I do stop the cromwell tasks but never stop docker containers that were started by it. My docker-stack configuration is https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml. It uses slightly modified cromwell:develop container https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/Dockerfile. I also share docker sockets there and everything functions well with the exception of abort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011
https://github.com/broadinstitute/cromwell/issues/4011:798,Safety,abort,abort,798,"I am using latest cromwell develop docker container. When I run ""abort"" command it internall executes docker.kill script that looks like:; ""; #!/bin/bash ; docker kill `cat /pipelines/cromwell-executions/vsearch/81c51e4e-756c-47f7-8dd6\; -57b9c2981162/call-global_search/execution/docker_cid`; ""; However, docker_cid is never created. So, all the abort commands that I do stop the cromwell tasks but never stop docker containers that were started by it. My docker-stack configuration is https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml. It uses slightly modified cromwell:develop container https://github.com/antonkulaga/cromwell-client/blob/master/services/cromwell/Dockerfile. I also share docker sockets there and everything functions well with the exception of abort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4011
https://github.com/broadinstitute/cromwell/pull/4013:220,Availability,avail,available,220,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013
https://github.com/broadinstitute/cromwell/pull/4013:457,Availability,down,down,457,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013
https://github.com/broadinstitute/cromwell/pull/4013:63,Deployability,configurat,configuration,63,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013
https://github.com/broadinstitute/cromwell/pull/4013:835,Deployability,update,update,835,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013
https://github.com/broadinstitute/cromwell/pull/4013:272,Energy Efficiency,power,power,272,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013
https://github.com/broadinstitute/cromwell/pull/4013:63,Modifiability,config,configuration,63,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013
https://github.com/broadinstitute/cromwell/pull/4013:903,Modifiability,Enhance,Enhance,903,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013
https://github.com/broadinstitute/cromwell/pull/4013:969,Modifiability,Enhance,Enhance,969,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013
https://github.com/broadinstitute/cromwell/pull/4013:956,Testability,test,tests,956,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013
https://github.com/broadinstitute/cromwell/pull/4013:1009,Testability,test,tests,1009,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013
https://github.com/broadinstitute/cromwell/pull/4013:1038,Testability,test,tests,1038,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013
https://github.com/broadinstitute/cromwell/issues/4014:65,Performance,cache,cached-and-retried-centaur-test,65,"Not quite similar to #4001 and #3998, but also related to a call-cached-and-retried-centaur-test. In this case the workflow succeeds, producing a metadata key that should _not_ be written. The test is retried, the workflow call-caches, and then copies the same metadata key over, failing again. Example log: [retried_found_unwanted_keys_in_metadata.txt](https://github.com/broadinstitute/cromwell/files/2295492/retried_found_unwanted_keys_in_metadata.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4014
https://github.com/broadinstitute/cromwell/issues/4014:228,Performance,cache,caches,228,"Not quite similar to #4001 and #3998, but also related to a call-cached-and-retried-centaur-test. In this case the workflow succeeds, producing a metadata key that should _not_ be written. The test is retried, the workflow call-caches, and then copies the same metadata key over, failing again. Example log: [retried_found_unwanted_keys_in_metadata.txt](https://github.com/broadinstitute/cromwell/files/2295492/retried_found_unwanted_keys_in_metadata.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4014
https://github.com/broadinstitute/cromwell/issues/4014:92,Testability,test,test,92,"Not quite similar to #4001 and #3998, but also related to a call-cached-and-retried-centaur-test. In this case the workflow succeeds, producing a metadata key that should _not_ be written. The test is retried, the workflow call-caches, and then copies the same metadata key over, failing again. Example log: [retried_found_unwanted_keys_in_metadata.txt](https://github.com/broadinstitute/cromwell/files/2295492/retried_found_unwanted_keys_in_metadata.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4014
https://github.com/broadinstitute/cromwell/issues/4014:193,Testability,test,test,193,"Not quite similar to #4001 and #3998, but also related to a call-cached-and-retried-centaur-test. In this case the workflow succeeds, producing a metadata key that should _not_ be written. The test is retried, the workflow call-caches, and then copies the same metadata key over, failing again. Example log: [retried_found_unwanted_keys_in_metadata.txt](https://github.com/broadinstitute/cromwell/files/2295492/retried_found_unwanted_keys_in_metadata.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4014
https://github.com/broadinstitute/cromwell/issues/4014:303,Testability,log,log,303,"Not quite similar to #4001 and #3998, but also related to a call-cached-and-retried-centaur-test. In this case the workflow succeeds, producing a metadata key that should _not_ be written. The test is retried, the workflow call-caches, and then copies the same metadata key over, failing again. Example log: [retried_found_unwanted_keys_in_metadata.txt](https://github.com/broadinstitute/cromwell/files/2295492/retried_found_unwanted_keys_in_metadata.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4014
https://github.com/broadinstitute/cromwell/pull/4015:61,Deployability,configurat,configuration,61,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:134,Deployability,deploy,deploys,134,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:839,Deployability,deploy,deploy,839,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:879,Deployability,deploy,deploy,879,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:907,Deployability,deploy,deploy,907,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:1263,Deployability,deploy,deploy,1263,"ng the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:1401,Deployability,deploy,deploy,1401,"ng the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:2768,Deployability,update,update,2768," latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties, base directory: /; [info] Set current project to root (in build file:/); [info] 1.2.1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:61,Modifiability,config,configuration,61,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:999,Modifiability,config,config,999,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:1020,Modifiability,config,config,1020,"ill trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:1189,Modifiability,config,config,1189,"a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on buil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:2017,Modifiability,config,config,2017,"eci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:2102,Modifiability,config,config,2102,"ers, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties, base directory: /; [info] Set current project to root (in build file:/); [info",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:2205,Modifiability,variab,variables,2205," latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties, base directory: /; [info] Set current project to root (in build file:/); [info] 1.2.1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:2909,Modifiability,config,config,2909," latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties, base directory: /; [info] Set current project to root (in build file:/); [info] 1.2.1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:2277,Security,authenticat,authenticate,2277," latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties, base directory: /; [info] Set current project to root (in build file:/); [info] 1.2.1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:2315,Security,password,password,2315," latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties, base directory: /; [info] Set current project to root (in build file:/); [info] 1.2.1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:118,Testability,test,test,118,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:226,Testability,test,tested,226,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:454,Testability,test,test,454,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:623,Testability,test,test,623,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:732,Testability,test,tests,732,"This pull request will trigger CI to run for the newly added configuration. . ## What does it do?; Specifically, this test builds and deploys a docker development container with sbt and scala to build cromwell. . ## How is it tested?; The user is instructed to bind the source code with ""sbt assembly"" in the `/code` directory in the container after binding the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:1569,Testability,test,test,1569," sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:1644,Testability,test,test,1644,"s ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:2373,Testability,test,testing,2373," latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties, base directory: /; [info] Set current project to root (in build file:/); [info] 1.2.1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/pull/4015:2712,Testability,log,logic,2712," latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do not turn on also testing of forked pull requests on their branch (different setting from above) as this could compromise these credentials.; - `CONTAINER_NAME` should be something like `broadinstiutute/cromwell-dev`. - The tag will always build the commit id, and then latest. If you want to change this behavior, define `DOCKER_TAG`.; - ensure the branch logic (when things are triggered) is to your liking.; - update the repo badge to be cromwell here and not on vsoch (after you connect the two!). I noticed that there is no sbt version set (in some config file) - would this make sense to do?. ```bash; [warn] No sbt.version set in project/build.properties, base directory: /; [info] Set current project to root (in build file:/); [info] 1.2.1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015
https://github.com/broadinstitute/cromwell/issues/4017:848,Availability,error,error,848,"In Google Compute Engine, one can create custom networks and even delete the default network. ; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. This is commonly done for projects that have high security requirements to enforce firewalls etc. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; - Confirm that Cromwell honors using a non-default network when specified via the config.; - If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4017
https://github.com/broadinstitute/cromwell/issues/4017:540,Modifiability,config,config,540,"In Google Compute Engine, one can create custom networks and even delete the default network. ; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. This is commonly done for projects that have high security requirements to enforce firewalls etc. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; - Confirm that Cromwell honors using a non-default network when specified via the config.; - If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4017
https://github.com/broadinstitute/cromwell/issues/4017:788,Modifiability,config,config,788,"In Google Compute Engine, one can create custom networks and even delete the default network. ; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. This is commonly done for projects that have high security requirements to enforce firewalls etc. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; - Confirm that Cromwell honors using a non-default network when specified via the config.; - If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4017
https://github.com/broadinstitute/cromwell/issues/4017:274,Security,secur,security,274,"In Google Compute Engine, one can create custom networks and even delete the default network. ; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. This is commonly done for projects that have high security requirements to enforce firewalls etc. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; - Confirm that Cromwell honors using a non-default network when specified via the config.; - If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4017
https://github.com/broadinstitute/cromwell/issues/4017:307,Security,firewall,firewalls,307,"In Google Compute Engine, one can create custom networks and even delete the default network. ; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. This is commonly done for projects that have high security requirements to enforce firewalls etc. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; - Confirm that Cromwell honors using a non-default network when specified via the config.; - If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4017
https://github.com/broadinstitute/cromwell/issues/4017:687,Testability,Test,Testing,687,"In Google Compute Engine, one can create custom networks and even delete the default network. ; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. This is commonly done for projects that have high security requirements to enforce firewalls etc. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; - Confirm that Cromwell honors using a non-default network when specified via the config.; - If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4017
https://github.com/broadinstitute/cromwell/pull/4019:141,Modifiability,config,config,141,"OK slight change of plan: execution bucket call cache hints will be opt-in for now via workflow options with no changes required in Cromwell config. This actually does not have to be PAPI specific so the proposed nomenclature would be more general:. ```json; {; call_cache_hit_path_prefixes: [ ""gs://bucket-full-o-cache-hits"", ""gs://another-bucket-o-cache-hits"" ]; }; ```. ""opt-in"" here means that for now the caller would be responsible for specifying the prefixes by which to filter cache hits. If no prefixes are specified Cromwell will do no cache hit filtering. Backends have the ability to specify a root execution path to prepend to this list. For now this root execution path would only be prepended by PAPI since that's the only backend for which a sensible root could be determined by inspection (i.e. the GCS bucket). The number of prefixes in this list would be limited, for now I'm arbitrarily choosing 3 total. @ruchim",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4019
https://github.com/broadinstitute/cromwell/pull/4019:48,Performance,cache,cache,48,"OK slight change of plan: execution bucket call cache hints will be opt-in for now via workflow options with no changes required in Cromwell config. This actually does not have to be PAPI specific so the proposed nomenclature would be more general:. ```json; {; call_cache_hit_path_prefixes: [ ""gs://bucket-full-o-cache-hits"", ""gs://another-bucket-o-cache-hits"" ]; }; ```. ""opt-in"" here means that for now the caller would be responsible for specifying the prefixes by which to filter cache hits. If no prefixes are specified Cromwell will do no cache hit filtering. Backends have the ability to specify a root execution path to prepend to this list. For now this root execution path would only be prepended by PAPI since that's the only backend for which a sensible root could be determined by inspection (i.e. the GCS bucket). The number of prefixes in this list would be limited, for now I'm arbitrarily choosing 3 total. @ruchim",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4019
https://github.com/broadinstitute/cromwell/pull/4019:314,Performance,cache,cache-hits,314,"OK slight change of plan: execution bucket call cache hints will be opt-in for now via workflow options with no changes required in Cromwell config. This actually does not have to be PAPI specific so the proposed nomenclature would be more general:. ```json; {; call_cache_hit_path_prefixes: [ ""gs://bucket-full-o-cache-hits"", ""gs://another-bucket-o-cache-hits"" ]; }; ```. ""opt-in"" here means that for now the caller would be responsible for specifying the prefixes by which to filter cache hits. If no prefixes are specified Cromwell will do no cache hit filtering. Backends have the ability to specify a root execution path to prepend to this list. For now this root execution path would only be prepended by PAPI since that's the only backend for which a sensible root could be determined by inspection (i.e. the GCS bucket). The number of prefixes in this list would be limited, for now I'm arbitrarily choosing 3 total. @ruchim",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4019
https://github.com/broadinstitute/cromwell/pull/4019:350,Performance,cache,cache-hits,350,"OK slight change of plan: execution bucket call cache hints will be opt-in for now via workflow options with no changes required in Cromwell config. This actually does not have to be PAPI specific so the proposed nomenclature would be more general:. ```json; {; call_cache_hit_path_prefixes: [ ""gs://bucket-full-o-cache-hits"", ""gs://another-bucket-o-cache-hits"" ]; }; ```. ""opt-in"" here means that for now the caller would be responsible for specifying the prefixes by which to filter cache hits. If no prefixes are specified Cromwell will do no cache hit filtering. Backends have the ability to specify a root execution path to prepend to this list. For now this root execution path would only be prepended by PAPI since that's the only backend for which a sensible root could be determined by inspection (i.e. the GCS bucket). The number of prefixes in this list would be limited, for now I'm arbitrarily choosing 3 total. @ruchim",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4019
https://github.com/broadinstitute/cromwell/pull/4019:485,Performance,cache,cache,485,"OK slight change of plan: execution bucket call cache hints will be opt-in for now via workflow options with no changes required in Cromwell config. This actually does not have to be PAPI specific so the proposed nomenclature would be more general:. ```json; {; call_cache_hit_path_prefixes: [ ""gs://bucket-full-o-cache-hits"", ""gs://another-bucket-o-cache-hits"" ]; }; ```. ""opt-in"" here means that for now the caller would be responsible for specifying the prefixes by which to filter cache hits. If no prefixes are specified Cromwell will do no cache hit filtering. Backends have the ability to specify a root execution path to prepend to this list. For now this root execution path would only be prepended by PAPI since that's the only backend for which a sensible root could be determined by inspection (i.e. the GCS bucket). The number of prefixes in this list would be limited, for now I'm arbitrarily choosing 3 total. @ruchim",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4019
https://github.com/broadinstitute/cromwell/pull/4019:546,Performance,cache,cache,546,"OK slight change of plan: execution bucket call cache hints will be opt-in for now via workflow options with no changes required in Cromwell config. This actually does not have to be PAPI specific so the proposed nomenclature would be more general:. ```json; {; call_cache_hit_path_prefixes: [ ""gs://bucket-full-o-cache-hits"", ""gs://another-bucket-o-cache-hits"" ]; }; ```. ""opt-in"" here means that for now the caller would be responsible for specifying the prefixes by which to filter cache hits. If no prefixes are specified Cromwell will do no cache hit filtering. Backends have the ability to specify a root execution path to prepend to this list. For now this root execution path would only be prepended by PAPI since that's the only backend for which a sensible root could be determined by inspection (i.e. the GCS bucket). The number of prefixes in this list would be limited, for now I'm arbitrarily choosing 3 total. @ruchim",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4019
https://github.com/broadinstitute/cromwell/pull/4020:96,Deployability,patch,patched,96,"This PR removes the agent/sidecar approach for a lighter proxy to be used in conjunction with a patched ecs agent running on the host. It removes the potential race caused by the agent watching the docker stream, as the ecs agent will launch the proxy directly. It resolves #3804 in all cases and allows the Haplotypecaller test to succeed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4020
https://github.com/broadinstitute/cromwell/pull/4020:324,Testability,test,test,324,"This PR removes the agent/sidecar approach for a lighter proxy to be used in conjunction with a patched ecs agent running on the host. It removes the potential race caused by the agent watching the docker stream, as the ecs agent will launch the proxy directly. It resolves #3804 in all cases and allows the Haplotypecaller test to succeed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4020
https://github.com/broadinstitute/cromwell/pull/4021:126,Availability,alive,alive,126,"- As sentry may drop some metadata, print the metadata to slf4j.; - For centaur-restarting-cromwell, remember if cromwell was alive, and log more of the connection status.; - Pass more jenkins variables through docker.; - Increase papi v2 cwl conformance test timeout due to problematic test 55.; - Use pr branch name during pr builds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4021
https://github.com/broadinstitute/cromwell/pull/4021:193,Modifiability,variab,variables,193,"- As sentry may drop some metadata, print the metadata to slf4j.; - For centaur-restarting-cromwell, remember if cromwell was alive, and log more of the connection status.; - Pass more jenkins variables through docker.; - Increase papi v2 cwl conformance test timeout due to problematic test 55.; - Use pr branch name during pr builds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4021
https://github.com/broadinstitute/cromwell/pull/4021:260,Safety,timeout,timeout,260,"- As sentry may drop some metadata, print the metadata to slf4j.; - For centaur-restarting-cromwell, remember if cromwell was alive, and log more of the connection status.; - Pass more jenkins variables through docker.; - Increase papi v2 cwl conformance test timeout due to problematic test 55.; - Use pr branch name during pr builds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4021
https://github.com/broadinstitute/cromwell/pull/4021:137,Testability,log,log,137,"- As sentry may drop some metadata, print the metadata to slf4j.; - For centaur-restarting-cromwell, remember if cromwell was alive, and log more of the connection status.; - Pass more jenkins variables through docker.; - Increase papi v2 cwl conformance test timeout due to problematic test 55.; - Use pr branch name during pr builds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4021
https://github.com/broadinstitute/cromwell/pull/4021:255,Testability,test,test,255,"- As sentry may drop some metadata, print the metadata to slf4j.; - For centaur-restarting-cromwell, remember if cromwell was alive, and log more of the connection status.; - Pass more jenkins variables through docker.; - Increase papi v2 cwl conformance test timeout due to problematic test 55.; - Use pr branch name during pr builds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4021
https://github.com/broadinstitute/cromwell/pull/4021:287,Testability,test,test,287,"- As sentry may drop some metadata, print the metadata to slf4j.; - For centaur-restarting-cromwell, remember if cromwell was alive, and log more of the connection status.; - Pass more jenkins variables through docker.; - Increase papi v2 cwl conformance test timeout due to problematic test 55.; - Use pr branch name during pr builds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4021
https://github.com/broadinstitute/cromwell/pull/4022:229,Availability,heartbeat,heartbeats,229,"This change attempts to fix the deadlock between [starting workflows](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L65) and [writing heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L75) by removing transaction semantics from the heartbeat write query. This way, the second query no longer locks multiple rows at once. I am using Slick's [`withPinnedSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.19",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:430,Availability,heartbeat,heartbeat,430,"This change attempts to fix the deadlock between [starting workflows](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L65) and [writing heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L75) by removing transaction semantics from the heartbeat write query. This way, the second query no longer locks multiple rows at once. I am using Slick's [`withPinnedSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.19",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:985,Deployability,update,update,985,"This change attempts to fix the deadlock between [starting workflows](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L65) and [writing heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L75) by removing transaction semantics from the heartbeat write query. This way, the second query no longer locks multiple rows at once. I am using Slick's [`withPinnedSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.19",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:1172,Deployability,update,update,1172,"ick/WorkflowStoreSlickDatabase.scala#L65) and [writing heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L75) by removing transaction semantics from the heartbeat write query. This way, the second query no longer locks multiple rows at once. I am using Slick's [`withPinnedSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:1359,Deployability,update,update,1359,"eSlickDatabase.scala#L75) by removing transaction semantics from the heartbeat write query. This way, the second query no longer locks multiple rows at once. I am using Slick's [`withPinnedSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:1546,Deployability,update,update,1546,"edSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:1733,Deployability,update,update,1733," log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:1920,Deployability,update,update,1920,(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-8,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:2200,Deployability,update,update,2200,TBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLO,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:2387,Deployability,update,update,2387,set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:2574,Deployability,update,update,2574,set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:2761,Deployability,update,update,2761,set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c3994549-e8e1-4478-9bf3-ef46cc16f505'; Query update ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:2948,Deployability,update,update,2948,set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c3994549-e8e1-4478-9bf3-ef46cc16f505'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '8d535b7f-4832-429c-9144-c3c39eeb7006'; Query update ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:3196,Deployability,update,update,3196,_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c3994549-e8e1-4478-9bf3-ef46cc16f505'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '8d535b7f-4832-429c-9144-c3c39eeb7006'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd89fe8c2-3803-407d-99c6-3b77443ff20b'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `W,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:3383,Deployability,update,update,3383,ORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c3994549-e8e1-4478-9bf3-ef46cc16f505'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '8d535b7f-4832-429c-9144-c3c39eeb7006'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd89fe8c2-3803-407d-99c6-3b77443ff20b'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '768dfe30-7072-4430-bb31-355499ca1443'; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:3570,Deployability,update,update,3570,ORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c3994549-e8e1-4478-9bf3-ef46cc16f505'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '8d535b7f-4832-429c-9144-c3c39eeb7006'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd89fe8c2-3803-407d-99c6-3b77443ff20b'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '768dfe30-7072-4430-bb31-355499ca1443'; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:3757,Deployability,update,update,3757,ORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c3994549-e8e1-4478-9bf3-ef46cc16f505'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '8d535b7f-4832-429c-9144-c3c39eeb7006'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd89fe8c2-3803-407d-99c6-3b77443ff20b'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '768dfe30-7072-4430-bb31-355499ca1443'; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:3944,Deployability,update,update,3944,ORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c3994549-e8e1-4478-9bf3-ef46cc16f505'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '8d535b7f-4832-429c-9144-c3c39eeb7006'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd89fe8c2-3803-407d-99c6-3b77443ff20b'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '768dfe30-7072-4430-bb31-355499ca1443'; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:4131,Deployability,update,update,4131,ORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c3994549-e8e1-4478-9bf3-ef46cc16f505'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '8d535b7f-4832-429c-9144-c3c39eeb7006'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd89fe8c2-3803-407d-99c6-3b77443ff20b'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '768dfe30-7072-4430-bb31-355499ca1443'; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:883,Modifiability,variab,variable,883,"This change attempts to fix the deadlock between [starting workflows](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L65) and [writing heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L75) by removing transaction semantics from the heartbeat write query. This way, the second query no longer locks multiple rows at once. I am using Slick's [`withPinnedSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.19",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/pull/4022:736,Testability,log,log,736,"This change attempts to fix the deadlock between [starting workflows](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L65) and [writing heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L75) by removing transaction semantics from the heartbeat write query. This way, the second query no longer locks multiple rows at once. I am using Slick's [`withPinnedSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.19",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022
https://github.com/broadinstitute/cromwell/issues/4023:654,Availability,error,error,654,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:857,Availability,error,error,857,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:593,Deployability,pipeline,pipeline,593,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:2117,Energy Efficiency,adapt,adapted,2117,xecution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResultsActor.scala:32); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1$adapted(FetchCachedResultsActor.scala:30); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; I,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:109,Modifiability,variab,variable,109,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:2117,Modifiability,adapt,adapted,2117,xecution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResultsActor.scala:32); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1$adapted(FetchCachedResultsActor.scala:30); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; I,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:70,Performance,cache,cache,70,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:638,Performance,cache,cache,638,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:2169,Performance,concurren,concurrent,2169,nfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResultsActor.scala:32); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1$adapted(FetchCachedResultsActor.scala:30); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; I can reproduce with this test CWL workflow (https,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:2490,Performance,concurren,concurrent,2490,"ion.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResultsActor.scala:32); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1$adapted(FetchCachedResultsActor.scala:30); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; I can reproduce with this test CWL workflow (https://github.com/bcbio/test_bcbio_cwl/tree/master/prealign). The first time running will work fine, and then if you re-run using the cache from the previous run it fails as above. Thanks for any thoughts/tips about what is going wrong here and how best to fix.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:3305,Performance,cache,cache,3305,"ion.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResultsActor.scala:32); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1$adapted(FetchCachedResultsActor.scala:30); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; I can reproduce with this test CWL workflow (https://github.com/bcbio/test_bcbio_cwl/tree/master/prealign). The first time running will work fine, and then if you re-run using the cache from the previous run it fails as above. Thanks for any thoughts/tips about what is going wrong here and how best to fix.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:3151,Testability,test,test,3151,"ion.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResultsActor.scala:32); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1$adapted(FetchCachedResultsActor.scala:30); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; I can reproduce with this test CWL workflow (https://github.com/bcbio/test_bcbio_cwl/tree/master/prealign). The first time running will work fine, and then if you re-run using the cache from the previous run it fails as above. Thanks for any thoughts/tips about what is going wrong here and how best to fix.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:877,Usability,simpl,simpleton,877,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:944,Usability,simpl,simpleton,944,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:982,Usability,Simpl,Simpletons,982,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:1006,Usability,Simpl,Simpletons,1006,"I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResultsActor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:1040,Usability,Simpl,Simpletons,1040,"sue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResultsActor.scala:32); at cromwell.engine.workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/issues/4023:1064,Usability,Simpl,Simpletons,1064,"from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResultsActor.scala:32); at cromwell.engine.workflow.lifecycle.executio",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023
https://github.com/broadinstitute/cromwell/pull/4025:66,Testability,test,testing,66,"Since this is all in flux at the moment at the behest of external testing, I'm going to just merge this unless someone raises a good reason not to by the time I loop back around in the AM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4025
https://github.com/broadinstitute/cromwell/issues/4026:559,Availability,failure,failures,559,"Ensure that PAPI v2 stdout/stderr logs today contain information about localization and delocalization stages as well. This is very different from PAPI v1 which annotated and separated the three main stages of a job's life:; 1. Localization logs; 2. Exec.sh stdout/stderr; 3. Delocalization logs . While it's advantageous to have information about all three stages in one log file, it can be hard to distinguish which portions of the logs has to do with the actual tool a user is trying to learn. In order to make it easier for a user to be able to debug job failures -- the PAPI v2 logs will need to make it very obvious to distinguish between the three stages listed above. AC: Ensure that the three stages (Localization, User command, Delocalization) are annotated in both the stdout/stderr file so that it's obvious to a user at what stage their job failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4026
https://github.com/broadinstitute/cromwell/issues/4026:34,Testability,log,logs,34,"Ensure that PAPI v2 stdout/stderr logs today contain information about localization and delocalization stages as well. This is very different from PAPI v1 which annotated and separated the three main stages of a job's life:; 1. Localization logs; 2. Exec.sh stdout/stderr; 3. Delocalization logs . While it's advantageous to have information about all three stages in one log file, it can be hard to distinguish which portions of the logs has to do with the actual tool a user is trying to learn. In order to make it easier for a user to be able to debug job failures -- the PAPI v2 logs will need to make it very obvious to distinguish between the three stages listed above. AC: Ensure that the three stages (Localization, User command, Delocalization) are annotated in both the stdout/stderr file so that it's obvious to a user at what stage their job failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4026
https://github.com/broadinstitute/cromwell/issues/4026:241,Testability,log,logs,241,"Ensure that PAPI v2 stdout/stderr logs today contain information about localization and delocalization stages as well. This is very different from PAPI v1 which annotated and separated the three main stages of a job's life:; 1. Localization logs; 2. Exec.sh stdout/stderr; 3. Delocalization logs . While it's advantageous to have information about all three stages in one log file, it can be hard to distinguish which portions of the logs has to do with the actual tool a user is trying to learn. In order to make it easier for a user to be able to debug job failures -- the PAPI v2 logs will need to make it very obvious to distinguish between the three stages listed above. AC: Ensure that the three stages (Localization, User command, Delocalization) are annotated in both the stdout/stderr file so that it's obvious to a user at what stage their job failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4026
https://github.com/broadinstitute/cromwell/issues/4026:291,Testability,log,logs,291,"Ensure that PAPI v2 stdout/stderr logs today contain information about localization and delocalization stages as well. This is very different from PAPI v1 which annotated and separated the three main stages of a job's life:; 1. Localization logs; 2. Exec.sh stdout/stderr; 3. Delocalization logs . While it's advantageous to have information about all three stages in one log file, it can be hard to distinguish which portions of the logs has to do with the actual tool a user is trying to learn. In order to make it easier for a user to be able to debug job failures -- the PAPI v2 logs will need to make it very obvious to distinguish between the three stages listed above. AC: Ensure that the three stages (Localization, User command, Delocalization) are annotated in both the stdout/stderr file so that it's obvious to a user at what stage their job failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4026
https://github.com/broadinstitute/cromwell/issues/4026:372,Testability,log,log,372,"Ensure that PAPI v2 stdout/stderr logs today contain information about localization and delocalization stages as well. This is very different from PAPI v1 which annotated and separated the three main stages of a job's life:; 1. Localization logs; 2. Exec.sh stdout/stderr; 3. Delocalization logs . While it's advantageous to have information about all three stages in one log file, it can be hard to distinguish which portions of the logs has to do with the actual tool a user is trying to learn. In order to make it easier for a user to be able to debug job failures -- the PAPI v2 logs will need to make it very obvious to distinguish between the three stages listed above. AC: Ensure that the three stages (Localization, User command, Delocalization) are annotated in both the stdout/stderr file so that it's obvious to a user at what stage their job failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4026
https://github.com/broadinstitute/cromwell/issues/4026:434,Testability,log,logs,434,"Ensure that PAPI v2 stdout/stderr logs today contain information about localization and delocalization stages as well. This is very different from PAPI v1 which annotated and separated the three main stages of a job's life:; 1. Localization logs; 2. Exec.sh stdout/stderr; 3. Delocalization logs . While it's advantageous to have information about all three stages in one log file, it can be hard to distinguish which portions of the logs has to do with the actual tool a user is trying to learn. In order to make it easier for a user to be able to debug job failures -- the PAPI v2 logs will need to make it very obvious to distinguish between the three stages listed above. AC: Ensure that the three stages (Localization, User command, Delocalization) are annotated in both the stdout/stderr file so that it's obvious to a user at what stage their job failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4026
https://github.com/broadinstitute/cromwell/issues/4026:583,Testability,log,logs,583,"Ensure that PAPI v2 stdout/stderr logs today contain information about localization and delocalization stages as well. This is very different from PAPI v1 which annotated and separated the three main stages of a job's life:; 1. Localization logs; 2. Exec.sh stdout/stderr; 3. Delocalization logs . While it's advantageous to have information about all three stages in one log file, it can be hard to distinguish which portions of the logs has to do with the actual tool a user is trying to learn. In order to make it easier for a user to be able to debug job failures -- the PAPI v2 logs will need to make it very obvious to distinguish between the three stages listed above. AC: Ensure that the three stages (Localization, User command, Delocalization) are annotated in both the stdout/stderr file so that it's obvious to a user at what stage their job failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4026
https://github.com/broadinstitute/cromwell/issues/4026:490,Usability,learn,learn,490,"Ensure that PAPI v2 stdout/stderr logs today contain information about localization and delocalization stages as well. This is very different from PAPI v1 which annotated and separated the three main stages of a job's life:; 1. Localization logs; 2. Exec.sh stdout/stderr; 3. Delocalization logs . While it's advantageous to have information about all three stages in one log file, it can be hard to distinguish which portions of the logs has to do with the actual tool a user is trying to learn. In order to make it easier for a user to be able to debug job failures -- the PAPI v2 logs will need to make it very obvious to distinguish between the three stages listed above. AC: Ensure that the three stages (Localization, User command, Delocalization) are annotated in both the stdout/stderr file so that it's obvious to a user at what stage their job failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4026
https://github.com/broadinstitute/cromwell/issues/4027:181,Performance,cache,cached,181,"Today, since Cromwell utilizes the backend name as a part of the caching algorithm, when a user has both PAPI v1 and the PAPI v2 backends enabled -- the jobs between them cannot be cached to each other. It's quite possible that users will want both backends enabled, or be able to rename their backends without losing caching history. . AC: To accomodate for both of those cases, Cromwell will need to drop backend name from its caching algorithm. Testing Criteria: . - [ ] - Ensure that when both v1 & v2 backends are enabled, outputs from the v1 backend are cacheable from the v2 backend.; - [ ] - Ensure that renaming a backend from ""JES"" to ""PAPIv1"" doesn't break call caching",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4027
https://github.com/broadinstitute/cromwell/issues/4027:560,Performance,cache,cacheable,560,"Today, since Cromwell utilizes the backend name as a part of the caching algorithm, when a user has both PAPI v1 and the PAPI v2 backends enabled -- the jobs between them cannot be cached to each other. It's quite possible that users will want both backends enabled, or be able to rename their backends without losing caching history. . AC: To accomodate for both of those cases, Cromwell will need to drop backend name from its caching algorithm. Testing Criteria: . - [ ] - Ensure that when both v1 & v2 backends are enabled, outputs from the v1 backend are cacheable from the v2 backend.; - [ ] - Ensure that renaming a backend from ""JES"" to ""PAPIv1"" doesn't break call caching",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4027
https://github.com/broadinstitute/cromwell/issues/4027:448,Testability,Test,Testing,448,"Today, since Cromwell utilizes the backend name as a part of the caching algorithm, when a user has both PAPI v1 and the PAPI v2 backends enabled -- the jobs between them cannot be cached to each other. It's quite possible that users will want both backends enabled, or be able to rename their backends without losing caching history. . AC: To accomodate for both of those cases, Cromwell will need to drop backend name from its caching algorithm. Testing Criteria: . - [ ] - Ensure that when both v1 & v2 backends are enabled, outputs from the v1 backend are cacheable from the v2 backend.; - [ ] - Ensure that renaming a backend from ""JES"" to ""PAPIv1"" doesn't break call caching",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4027
https://github.com/broadinstitute/cromwell/issues/4029:75,Deployability,Pipeline,Pipelines,75,"It seems that the HealthMonitor relies on a specific type of response from Pipelines API v2 to determine health of the sub-services. The PAPI response about operation metadata is different between v1 and v2, and the HealthMonitor expectations are tied to the type of response returned from v1. This essentially prevents one from running a HealthMonitor against the v2 backend --which is going to be a requirement of enabling Pipelines API v2 in FireCloud production. AC: Be able to perform HealthMonitoring on both the PAPI v1 and PAPI v2 google backend. Testing criteria: Enable HealthMonitoring for PAPI v1 and PAPI v2 centaur tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4029
https://github.com/broadinstitute/cromwell/issues/4029:425,Deployability,Pipeline,Pipelines,425,"It seems that the HealthMonitor relies on a specific type of response from Pipelines API v2 to determine health of the sub-services. The PAPI response about operation metadata is different between v1 and v2, and the HealthMonitor expectations are tied to the type of response returned from v1. This essentially prevents one from running a HealthMonitor against the v2 backend --which is going to be a requirement of enabling Pipelines API v2 in FireCloud production. AC: Be able to perform HealthMonitoring on both the PAPI v1 and PAPI v2 google backend. Testing criteria: Enable HealthMonitoring for PAPI v1 and PAPI v2 centaur tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4029
https://github.com/broadinstitute/cromwell/issues/4029:482,Performance,perform,perform,482,"It seems that the HealthMonitor relies on a specific type of response from Pipelines API v2 to determine health of the sub-services. The PAPI response about operation metadata is different between v1 and v2, and the HealthMonitor expectations are tied to the type of response returned from v1. This essentially prevents one from running a HealthMonitor against the v2 backend --which is going to be a requirement of enabling Pipelines API v2 in FireCloud production. AC: Be able to perform HealthMonitoring on both the PAPI v1 and PAPI v2 google backend. Testing criteria: Enable HealthMonitoring for PAPI v1 and PAPI v2 centaur tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4029
https://github.com/broadinstitute/cromwell/issues/4029:555,Testability,Test,Testing,555,"It seems that the HealthMonitor relies on a specific type of response from Pipelines API v2 to determine health of the sub-services. The PAPI response about operation metadata is different between v1 and v2, and the HealthMonitor expectations are tied to the type of response returned from v1. This essentially prevents one from running a HealthMonitor against the v2 backend --which is going to be a requirement of enabling Pipelines API v2 in FireCloud production. AC: Be able to perform HealthMonitoring on both the PAPI v1 and PAPI v2 google backend. Testing criteria: Enable HealthMonitoring for PAPI v1 and PAPI v2 centaur tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4029
https://github.com/broadinstitute/cromwell/issues/4029:629,Testability,test,tests,629,"It seems that the HealthMonitor relies on a specific type of response from Pipelines API v2 to determine health of the sub-services. The PAPI response about operation metadata is different between v1 and v2, and the HealthMonitor expectations are tied to the type of response returned from v1. This essentially prevents one from running a HealthMonitor against the v2 backend --which is going to be a requirement of enabling Pipelines API v2 in FireCloud production. AC: Be able to perform HealthMonitoring on both the PAPI v1 and PAPI v2 google backend. Testing criteria: Enable HealthMonitoring for PAPI v1 and PAPI v2 centaur tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4029
https://github.com/broadinstitute/cromwell/issues/4031:330,Availability,error,error,330,"User reported issue: https://gatkforums.broadinstitute.org/gatk/discussion/12759/womtool-should-tell-me-to-not-use-the-reserved-keyword-output-upon-validate. AC: Ensure that Womtool returns the true cause for an invalid workflow (in this case its using the ""output"" keyword as a variable name instead of a misleading/unrelated(?) error. Testing Criteria: Add a test for certain reserved keywords, such as `input`, `output`, `command` and possibly others to ensure that using them in unexpected ways in a WDL yields the appropriate error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4031
https://github.com/broadinstitute/cromwell/issues/4031:531,Availability,error,error,531,"User reported issue: https://gatkforums.broadinstitute.org/gatk/discussion/12759/womtool-should-tell-me-to-not-use-the-reserved-keyword-output-upon-validate. AC: Ensure that Womtool returns the true cause for an invalid workflow (in this case its using the ""output"" keyword as a variable name instead of a misleading/unrelated(?) error. Testing Criteria: Add a test for certain reserved keywords, such as `input`, `output`, `command` and possibly others to ensure that using them in unexpected ways in a WDL yields the appropriate error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4031
https://github.com/broadinstitute/cromwell/issues/4031:279,Modifiability,variab,variable,279,"User reported issue: https://gatkforums.broadinstitute.org/gatk/discussion/12759/womtool-should-tell-me-to-not-use-the-reserved-keyword-output-upon-validate. AC: Ensure that Womtool returns the true cause for an invalid workflow (in this case its using the ""output"" keyword as a variable name instead of a misleading/unrelated(?) error. Testing Criteria: Add a test for certain reserved keywords, such as `input`, `output`, `command` and possibly others to ensure that using them in unexpected ways in a WDL yields the appropriate error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4031
https://github.com/broadinstitute/cromwell/issues/4031:148,Security,validat,validate,148,"User reported issue: https://gatkforums.broadinstitute.org/gatk/discussion/12759/womtool-should-tell-me-to-not-use-the-reserved-keyword-output-upon-validate. AC: Ensure that Womtool returns the true cause for an invalid workflow (in this case its using the ""output"" keyword as a variable name instead of a misleading/unrelated(?) error. Testing Criteria: Add a test for certain reserved keywords, such as `input`, `output`, `command` and possibly others to ensure that using them in unexpected ways in a WDL yields the appropriate error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4031
https://github.com/broadinstitute/cromwell/issues/4031:337,Testability,Test,Testing,337,"User reported issue: https://gatkforums.broadinstitute.org/gatk/discussion/12759/womtool-should-tell-me-to-not-use-the-reserved-keyword-output-upon-validate. AC: Ensure that Womtool returns the true cause for an invalid workflow (in this case its using the ""output"" keyword as a variable name instead of a misleading/unrelated(?) error. Testing Criteria: Add a test for certain reserved keywords, such as `input`, `output`, `command` and possibly others to ensure that using them in unexpected ways in a WDL yields the appropriate error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4031
https://github.com/broadinstitute/cromwell/issues/4031:361,Testability,test,test,361,"User reported issue: https://gatkforums.broadinstitute.org/gatk/discussion/12759/womtool-should-tell-me-to-not-use-the-reserved-keyword-output-upon-validate. AC: Ensure that Womtool returns the true cause for an invalid workflow (in this case its using the ""output"" keyword as a variable name instead of a misleading/unrelated(?) error. Testing Criteria: Add a test for certain reserved keywords, such as `input`, `output`, `command` and possibly others to ensure that using them in unexpected ways in a WDL yields the appropriate error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4031
https://github.com/broadinstitute/cromwell/pull/4033:202,Availability,error,error,202,Closes #4023 . I hijacked one of the steps in `cwl_cache_between_workflows.cwl` to test `Long` alongside `Float` - the second commit [fails](https://api.travis-ci.org/v3/job/419242749/log.txt) with the error the user saw.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4033
https://github.com/broadinstitute/cromwell/pull/4033:83,Testability,test,test,83,Closes #4023 . I hijacked one of the steps in `cwl_cache_between_workflows.cwl` to test `Long` alongside `Float` - the second commit [fails](https://api.travis-ci.org/v3/job/419242749/log.txt) with the error the user saw.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4033
https://github.com/broadinstitute/cromwell/pull/4033:184,Testability,log,log,184,Closes #4023 . I hijacked one of the steps in `cwl_cache_between_workflows.cwl` to test `Long` alongside `Float` - the second commit [fails](https://api.travis-ci.org/v3/job/419242749/log.txt) with the error the user saw.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4033
https://github.com/broadinstitute/cromwell/pull/4036:30,Availability,error,error,30,We've seen a few times the IO error `Failed to evaluate job outputs [...] Futures timed out after [10 seconds]` in tests. I hypothesize that file read times from buckets may suffer from occasional outliers due to 🌩. I know this is the right timeout to change thanks to [this branch](https://github.com/broadinstitute/cromwell/compare/aen_make_it_timeout?expand=1) where I induced error `Failed to evaluate job outputs [...] Futures timed out after [10 microseconds]`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036
https://github.com/broadinstitute/cromwell/pull/4036:380,Availability,error,error,380,We've seen a few times the IO error `Failed to evaluate job outputs [...] Futures timed out after [10 seconds]` in tests. I hypothesize that file read times from buckets may suffer from occasional outliers due to 🌩. I know this is the right timeout to change thanks to [this branch](https://github.com/broadinstitute/cromwell/compare/aen_make_it_timeout?expand=1) where I induced error `Failed to evaluate job outputs [...] Futures timed out after [10 microseconds]`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036
https://github.com/broadinstitute/cromwell/pull/4036:241,Safety,timeout,timeout,241,We've seen a few times the IO error `Failed to evaluate job outputs [...] Futures timed out after [10 seconds]` in tests. I hypothesize that file read times from buckets may suffer from occasional outliers due to 🌩. I know this is the right timeout to change thanks to [this branch](https://github.com/broadinstitute/cromwell/compare/aen_make_it_timeout?expand=1) where I induced error `Failed to evaluate job outputs [...] Futures timed out after [10 microseconds]`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036
https://github.com/broadinstitute/cromwell/pull/4036:115,Testability,test,tests,115,We've seen a few times the IO error `Failed to evaluate job outputs [...] Futures timed out after [10 seconds]` in tests. I hypothesize that file read times from buckets may suffer from occasional outliers due to 🌩. I know this is the right timeout to change thanks to [this branch](https://github.com/broadinstitute/cromwell/compare/aen_make_it_timeout?expand=1) where I induced error `Failed to evaluate job outputs [...] Futures timed out after [10 microseconds]`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036
https://github.com/broadinstitute/cromwell/pull/4037:604,Deployability,Update,Update,604,"This PR introduces two new config settings: `max-scatter-width-per-scatter` and `total-max-jobs-per-root-workflow`. Their purpose is:. - max-scatter-width-per-scatter: Maximum scatter width per scatter node. If a workflow has a scatter width more than this number, Cromwell will fail the workflow; - total-max-jobs-per-root-workflow: Total max. jobs that can be created per root workflow. If workflow tries to create jobs more than this number, Cromwell will fail the workflow by: no longer creating new jobs and let the jobs that have already been started finish, and then fail the workflow. Remaining: Update the default values of these two configurations in `reference.conf` based on threshold determined from Performance Tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4037
https://github.com/broadinstitute/cromwell/pull/4037:643,Deployability,configurat,configurations,643,"This PR introduces two new config settings: `max-scatter-width-per-scatter` and `total-max-jobs-per-root-workflow`. Their purpose is:. - max-scatter-width-per-scatter: Maximum scatter width per scatter node. If a workflow has a scatter width more than this number, Cromwell will fail the workflow; - total-max-jobs-per-root-workflow: Total max. jobs that can be created per root workflow. If workflow tries to create jobs more than this number, Cromwell will fail the workflow by: no longer creating new jobs and let the jobs that have already been started finish, and then fail the workflow. Remaining: Update the default values of these two configurations in `reference.conf` based on threshold determined from Performance Tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4037
https://github.com/broadinstitute/cromwell/pull/4037:27,Modifiability,config,config,27,"This PR introduces two new config settings: `max-scatter-width-per-scatter` and `total-max-jobs-per-root-workflow`. Their purpose is:. - max-scatter-width-per-scatter: Maximum scatter width per scatter node. If a workflow has a scatter width more than this number, Cromwell will fail the workflow; - total-max-jobs-per-root-workflow: Total max. jobs that can be created per root workflow. If workflow tries to create jobs more than this number, Cromwell will fail the workflow by: no longer creating new jobs and let the jobs that have already been started finish, and then fail the workflow. Remaining: Update the default values of these two configurations in `reference.conf` based on threshold determined from Performance Tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4037
https://github.com/broadinstitute/cromwell/pull/4037:643,Modifiability,config,configurations,643,"This PR introduces two new config settings: `max-scatter-width-per-scatter` and `total-max-jobs-per-root-workflow`. Their purpose is:. - max-scatter-width-per-scatter: Maximum scatter width per scatter node. If a workflow has a scatter width more than this number, Cromwell will fail the workflow; - total-max-jobs-per-root-workflow: Total max. jobs that can be created per root workflow. If workflow tries to create jobs more than this number, Cromwell will fail the workflow by: no longer creating new jobs and let the jobs that have already been started finish, and then fail the workflow. Remaining: Update the default values of these two configurations in `reference.conf` based on threshold determined from Performance Tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4037
https://github.com/broadinstitute/cromwell/pull/4037:713,Performance,Perform,Performance,713,"This PR introduces two new config settings: `max-scatter-width-per-scatter` and `total-max-jobs-per-root-workflow`. Their purpose is:. - max-scatter-width-per-scatter: Maximum scatter width per scatter node. If a workflow has a scatter width more than this number, Cromwell will fail the workflow; - total-max-jobs-per-root-workflow: Total max. jobs that can be created per root workflow. If workflow tries to create jobs more than this number, Cromwell will fail the workflow by: no longer creating new jobs and let the jobs that have already been started finish, and then fail the workflow. Remaining: Update the default values of these two configurations in `reference.conf` based on threshold determined from Performance Tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4037
https://github.com/broadinstitute/cromwell/pull/4037:725,Testability,Test,Tests,725,"This PR introduces two new config settings: `max-scatter-width-per-scatter` and `total-max-jobs-per-root-workflow`. Their purpose is:. - max-scatter-width-per-scatter: Maximum scatter width per scatter node. If a workflow has a scatter width more than this number, Cromwell will fail the workflow; - total-max-jobs-per-root-workflow: Total max. jobs that can be created per root workflow. If workflow tries to create jobs more than this number, Cromwell will fail the workflow by: no longer creating new jobs and let the jobs that have already been started finish, and then fail the workflow. Remaining: Update the default values of these two configurations in `reference.conf` based on threshold determined from Performance Tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4037
https://github.com/broadinstitute/cromwell/pull/4038:13,Security,secur,secure,13,Don't render secure resources unless build script can use them.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4038
https://github.com/broadinstitute/cromwell/pull/4039:1875,Availability,heartbeat,heartbeat,1875,"2-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:1939,Availability,heartbeat,heartbeatInterval,1939,"2-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:5182,Availability,echo,echo,5182,"gnments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Starting wgbs.flatten_; [2018-08-27 02:04:13,48] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: ; mkdir -p mapping; cat /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/write_lines_8f61fd340a04ccd930e243709dfb1bed.tmp | xargs -I % ln -s % mapping; ls mapping; [2018-08-27 02:04:13,50] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: executing: chmod u+x /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script && \; singularity \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b04e-204674333468""; }; [2018-08-27 02:04:26,91] [info] Workflow polling stopped; [2018-08-27 02:04:26,91] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-27 02:04:26,92] [info] Aborting all running workflows.; [2018-08-27 02:04:26,9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6007,Availability,down,down,6007,"ty \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b04e-204674333468""; }; [2018-08-27 02:04:26,91] [info] Workflow polling stopped; [2018-08-27 02:04:26,91] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-27 02:04:26,92] [info] Aborting all running workflows.; [2018-08-27 02:04:26,92] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6220,Availability,down,down,6220,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6311,Availability,down,down,6311,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6475,Availability,down,down,6475,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6783,Availability,down,down,6783,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6830,Availability,down,down,6830,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6924,Availability,down,down,6924,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7010,Availability,down,down,7010,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7122,Availability,down,down,7122,"-08-27 02:04:26,92] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the asy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7199,Availability,down,down,7199,"3] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry thr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7311,Availability,down,down,7311,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7401,Availability,down,down,7401,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7478,Availability,down,down,7478,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7628,Availability,down,down,7628,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:8058,Availability,down,down,8058,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:221,Deployability,configurat,configuration,221,"hey cromwell! I know the other PR is still being reviewed (and you are busy, no worries!) but I just wanted to put this example here of (one idea) for running a singularity container, as an instance or binary, just via a configuration. I don't think any special changes are needed here to the backend of cromwell, it's really just executing commands to a container with a particular set of inputs (container name, etc.) Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""wri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:1474,Deployability,pipeline,pipelines,1474,"ny user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 sec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:1493,Deployability,Pipeline,PipelinesApiLifecycleActorFactory,1493,"nds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:1566,Deployability,pipeline,pipelines,1566,"don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 pe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:1585,Deployability,Pipeline,PipelinesApiLifecycleActorFactory,1585,"ts out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:1885,Deployability,configurat,configuration,1885,"2-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:4481,Deployability,pipeline,pipelines,4481,"nerActor: received unexpected message: Done in state RunningSwraData; [2018-08-27 02:04:08,07] [info] WorkflowManagerActor Successfully started WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,07] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-08-27 02:04:08,09] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-08-27 02:04:08,17] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Parsing workflow as WDL draft-2; [2018-08-27 02:04:08,86] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Call-to-Backend assignments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Starting wgbs.flatten_; [2018-08-27 02:04:13,48] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: ; mkdir -p mapping; cat /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/write_lines_8f61fd340a04ccd930e243709dfb1bed.tmp | xargs -I % ln -s % mapping; ls mapping; [2018-08-27 02:04:13,50] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: executing: chmod u+x /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script && \; singularity \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:4496,Deployability,pipeline,pipeline,4496,"nerActor: received unexpected message: Done in state RunningSwraData; [2018-08-27 02:04:08,07] [info] WorkflowManagerActor Successfully started WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,07] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-08-27 02:04:08,09] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-08-27 02:04:08,17] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Parsing workflow as WDL draft-2; [2018-08-27 02:04:08,86] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Call-to-Backend assignments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Starting wgbs.flatten_; [2018-08-27 02:04:13,48] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: ; mkdir -p mapping; cat /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/write_lines_8f61fd340a04ccd930e243709dfb1bed.tmp | xargs -I % ln -s % mapping; ls mapping; [2018-08-27 02:04:13,50] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: executing: chmod u+x /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script && \; singularity \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:4853,Deployability,pipeline,pipelines,4853,"ch size 10000 and process rate 2 minutes.; [2018-08-27 02:04:08,17] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Parsing workflow as WDL draft-2; [2018-08-27 02:04:08,86] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Call-to-Backend assignments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Starting wgbs.flatten_; [2018-08-27 02:04:13,48] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: ; mkdir -p mapping; cat /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/write_lines_8f61fd340a04ccd930e243709dfb1bed.tmp | xargs -I % ln -s % mapping; ls mapping; [2018-08-27 02:04:13,50] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: executing: chmod u+x /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script && \; singularity \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:4868,Deployability,pipeline,pipeline,4868,"ch size 10000 and process rate 2 minutes.; [2018-08-27 02:04:08,17] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Parsing workflow as WDL draft-2; [2018-08-27 02:04:08,86] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Call-to-Backend assignments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Starting wgbs.flatten_; [2018-08-27 02:04:13,48] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: ; mkdir -p mapping; cat /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/write_lines_8f61fd340a04ccd930e243709dfb1bed.tmp | xargs -I % ln -s % mapping; ls mapping; [2018-08-27 02:04:13,50] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: executing: chmod u+x /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script && \; singularity \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:5062,Deployability,pipeline,pipelines,5062,"gnments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Starting wgbs.flatten_; [2018-08-27 02:04:13,48] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: ; mkdir -p mapping; cat /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/write_lines_8f61fd340a04ccd930e243709dfb1bed.tmp | xargs -I % ln -s % mapping; ls mapping; [2018-08-27 02:04:13,50] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: executing: chmod u+x /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script && \; singularity \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b04e-204674333468""; }; [2018-08-27 02:04:26,91] [info] Workflow polling stopped; [2018-08-27 02:04:26,91] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-27 02:04:26,92] [info] Aborting all running workflows.; [2018-08-27 02:04:26,9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:5077,Deployability,pipeline,pipeline,5077,"gnments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Starting wgbs.flatten_; [2018-08-27 02:04:13,48] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: ; mkdir -p mapping; cat /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/write_lines_8f61fd340a04ccd930e243709dfb1bed.tmp | xargs -I % ln -s % mapping; ls mapping; [2018-08-27 02:04:13,50] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: executing: chmod u+x /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script && \; singularity \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b04e-204674333468""; }; [2018-08-27 02:04:26,91] [info] Workflow polling stopped; [2018-08-27 02:04:26,91] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-27 02:04:26,92] [info] Aborting all running workflows.; [2018-08-27 02:04:26,9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:8657,Deployability,pipeline,pipelines,8657,"o] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.; [2018-08-27 02:04:27,06] [info] Shutdown finished.; ```; I don't see any strings in the outputs above, but I do see files in the ""mapping"" folder of execution:. ```bash; $ tree cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution; cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution; ├── mapping; │   ├── flowcell_1_1_1.fastq.gz -> /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/inputs/-1629611677/flowcell_1_1_1.fastq.gz; │   └── flowcell_1_1_2.fastq.gz -> /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/inputs/-1629611677/flowcell_1_1_2.fastq.gz; ├── rc; ├── script; ├── script.submit; ├── stderr; ├── stderr.submit; ├── stdout; ├── stdout.submit; └── write_lines_b474737d24db6afd91c22a47b2b69e4f.tmp. 1 directory, 10 files; ```; and the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files:. ```; $ cat cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution/stdout; flowcell_1_1_1.fastq.gz; flowcell_1_1_2.fastq.gz; ```; Let me know if this looks correct? What you are looking for? Completely off base? I think this will be easier with respect to getting the process ids for singularity instances with the next version, when there",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:8672,Deployability,pipeline,pipeline,8672,"o] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.; [2018-08-27 02:04:27,06] [info] Shutdown finished.; ```; I don't see any strings in the outputs above, but I do see files in the ""mapping"" folder of execution:. ```bash; $ tree cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution; cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution; ├── mapping; │   ├── flowcell_1_1_1.fastq.gz -> /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/inputs/-1629611677/flowcell_1_1_1.fastq.gz; │   └── flowcell_1_1_2.fastq.gz -> /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/inputs/-1629611677/flowcell_1_1_2.fastq.gz; ├── rc; ├── script; ├── script.submit; ├── stderr; ├── stderr.submit; ├── stdout; ├── stdout.submit; └── write_lines_b474737d24db6afd91c22a47b2b69e4f.tmp. 1 directory, 10 files; ```; and the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files:. ```; $ cat cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution/stdout; flowcell_1_1_1.fastq.gz; flowcell_1_1_2.fastq.gz; ```; Let me know if this looks correct? What you are looking for? Completely off base? I think this will be easier with respect to getting the process ids for singularity instances with the next version, when there",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:8885,Deployability,pipeline,pipelines,8885,"-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.; [2018-08-27 02:04:27,06] [info] Shutdown finished.; ```; I don't see any strings in the outputs above, but I do see files in the ""mapping"" folder of execution:. ```bash; $ tree cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution; cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution; ├── mapping; │   ├── flowcell_1_1_1.fastq.gz -> /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/inputs/-1629611677/flowcell_1_1_1.fastq.gz; │   └── flowcell_1_1_2.fastq.gz -> /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/inputs/-1629611677/flowcell_1_1_2.fastq.gz; ├── rc; ├── script; ├── script.submit; ├── stderr; ├── stderr.submit; ├── stdout; ├── stdout.submit; └── write_lines_b474737d24db6afd91c22a47b2b69e4f.tmp. 1 directory, 10 files; ```; and the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files:. ```; $ cat cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution/stdout; flowcell_1_1_1.fastq.gz; flowcell_1_1_2.fastq.gz; ```; Let me know if this looks correct? What you are looking for? Completely off base? I think this will be easier with respect to getting the process ids for singularity instances with the next version, when there (I think) will be better control commands. Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:8900,Deployability,pipeline,pipeline,8900,"-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.; [2018-08-27 02:04:27,06] [info] Shutdown finished.; ```; I don't see any strings in the outputs above, but I do see files in the ""mapping"" folder of execution:. ```bash; $ tree cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution; cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution; ├── mapping; │   ├── flowcell_1_1_1.fastq.gz -> /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/inputs/-1629611677/flowcell_1_1_1.fastq.gz; │   └── flowcell_1_1_2.fastq.gz -> /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/inputs/-1629611677/flowcell_1_1_2.fastq.gz; ├── rc; ├── script; ├── script.submit; ├── stderr; ├── stderr.submit; ├── stdout; ├── stdout.submit; └── write_lines_b474737d24db6afd91c22a47b2b69e4f.tmp. 1 directory, 10 files; ```; and the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files:. ```; $ cat cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution/stdout; flowcell_1_1_1.fastq.gz; flowcell_1_1_2.fastq.gz; ```; Let me know if this looks correct? What you are looking for? Completely off base? I think this will be easier with respect to getting the process ids for singularity instances with the next version, when there (I think) will be better control commands. Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:3544,Integrability,message,message,3544,"7 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,93] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,98] [info] Unspecified type (Unspecified version) workflow 967af8b6-0d68-44c4-b04e-204674333468 submitted; [2018-08-27 02:04:08,05] [info] SingleWorkflowRunnerActor: Workflow submitted 967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,05] [info] 1 new workflows fetched; [2018-08-27 02:04:08,05] [info] WorkflowManagerActor Starting workflow 967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,06] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-08-27 02:04:08,07] [info] WorkflowManagerActor Successfully started WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,07] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-08-27 02:04:08,09] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-08-27 02:04:08,17] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Parsing workflow as WDL draft-2; [2018-08-27 02:04:08,86] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Call-to-Backend assignments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7137,Integrability,message,messages,7137,"2] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7326,Integrability,message,messages,7326,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7416,Integrability,message,messages,7416,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:221,Modifiability,config,configuration,221,"hey cromwell! I know the other PR is still being reviewed (and you are busy, no worries!) but I just wanted to put this example here of (one idea) for running a singularity container, as an instance or binary, just via a configuration. I don't think any special changes are needed here to the backend of cromwell, it's really just executing commands to a container with a particular set of inputs (container name, etc.) Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""wri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:458,Modifiability,variab,variable,458,"hey cromwell! I know the other PR is still being reviewed (and you are busy, no worries!) but I just wanted to put this example here of (one idea) for running a singularity container, as an instance or binary, just via a configuration. I don't think any special changes are needed here to the backend of cromwell, it's really just executing commands to a container with a particular set of inputs (container name, etc.) Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""wri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:1885,Modifiability,config,configuration,1885,"2-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 3",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:2176,Modifiability,config,configured,2176,"[RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,93] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,98] [info] Unspecified type (Unspecified version) workflow 967af8b6-0d68-44c4-b04e-204674333468 submitted; [20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:2296,Modifiability,config,configured,2296,"m:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,93] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,98] [info] Unspecified type (Unspecified version) workflow 967af8b6-0d68-44c4-b04e-204674333468 submitted; [2018-08-27 02:04:08,05] [info] SingleWorkflowRunnerActor: Workflow submitted 967af8b6-0d68-44c4-b04e-204674333468; [201",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:2417,Modifiability,config,configured,2417," is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,93] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,98] [info] Unspecified type (Unspecified version) workflow 967af8b6-0d68-44c4-b04e-204674333468 submitted; [2018-08-27 02:04:08,05] [info] SingleWorkflowRunnerActor: Workflow submitted 967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,05] [info] 1 new workflows fetched; [2018-08-27 02:04:08,05] [info] WorkflowManagerActor Starting workfl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:3858,Modifiability,config,configured,3858,"7 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,93] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,98] [info] Unspecified type (Unspecified version) workflow 967af8b6-0d68-44c4-b04e-204674333468 submitted; [2018-08-27 02:04:08,05] [info] SingleWorkflowRunnerActor: Workflow submitted 967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,05] [info] 1 new workflows fetched; [2018-08-27 02:04:08,05] [info] WorkflowManagerActor Starting workflow 967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,06] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-08-27 02:04:08,07] [info] WorkflowManagerActor Successfully started WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,07] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-08-27 02:04:08,09] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-08-27 02:04:08,17] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Parsing workflow as WDL draft-2; [2018-08-27 02:04:08,86] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Call-to-Backend assignments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7130,Performance,queue,queued,7130,":04:26,92] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7319,Performance,queue,queued,7319,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7409,Performance,queue,queued,7409,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6033,Safety,Timeout,Timeout,6033,"ty \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b04e-204674333468""; }; [2018-08-27 02:04:26,91] [info] Workflow polling stopped; [2018-08-27 02:04:26,91] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-27 02:04:26,92] [info] Aborting all running workflows.; [2018-08-27 02:04:26,92] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6086,Safety,Abort,Aborting,6086,"ty \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b04e-204674333468""; }; [2018-08-27 02:04:26,91] [info] Workflow polling stopped; [2018-08-27 02:04:26,91] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-27 02:04:26,92] [info] Aborting all running workflows.; [2018-08-27 02:04:26,92] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6249,Safety,Timeout,Timeout,6249,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6345,Safety,Timeout,Timeout,6345,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6503,Safety,Timeout,Timeout,6503,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6859,Safety,Timeout,Timeout,6859,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:6945,Safety,Timeout,Timeout,6945,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7037,Safety,Timeout,Timeout,7037,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7227,Safety,Timeout,Timeout,7227,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7501,Safety,Timeout,Timeout,7501,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:7643,Safety,Timeout,Timeout,7643,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:640,Testability,test,test,640,"hey cromwell! I know the other PR is still being reviewed (and you are busy, no worries!) but I just wanted to put this example here of (one idea) for running a singularity container, as an instance or binary, just via a configuration. I don't think any special changes are needed here to the backend of cromwell, it's really just executing commands to a container with a particular set of inputs (container name, etc.) Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""wri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:783,Testability,test,test,783,"hey cromwell! I know the other PR is still being reviewed (and you are busy, no worries!) but I just wanted to put this example here of (one idea) for running a singularity container, as an instance or binary, just via a configuration. I don't think any special changes are needed here to the backend of cromwell, it's really just executing commands to a container with a particular set of inputs (container name, etc.) Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""wri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/pull/4039:800,Testability,TEST,TEST-YEAST,800,"hey cromwell! I know the other PR is still being reviewed (and you are busy, no worries!) but I just wanted to put this example here of (one idea) for running a singularity container, as an instance or binary, just via a configuration. I don't think any special changes are needed here to the backend of cromwell, it's really just executing commands to a container with a particular set of inputs (container name, etc.) Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""wri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039
https://github.com/broadinstitute/cromwell/issues/4040:1125,Deployability,configurat,configuration,1125,"Feedback from today's workshop: `womtool validate` uselessly prints a few newlines on successful exit. Various workshop participants thought:; 1. It should exit with no output to harmonize with Unix CLI tool conventions; 2. It should confirm success in a way that is obvious to users who are e.g. biologists first, programmers second. Either would be an improvement over the current state. cc @rmeffan @ndbolliger ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4040
https://github.com/broadinstitute/cromwell/issues/4040:1125,Modifiability,config,configuration,1125,"Feedback from today's workshop: `womtool validate` uselessly prints a few newlines on successful exit. Various workshop participants thought:; 1. It should exit with no output to harmonize with Unix CLI tool conventions; 2. It should confirm success in a way that is obvious to users who are e.g. biologists first, programmers second. Either would be an improvement over the current state. cc @rmeffan @ndbolliger ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4040
https://github.com/broadinstitute/cromwell/issues/4040:41,Security,validat,validate,41,"Feedback from today's workshop: `womtool validate` uselessly prints a few newlines on successful exit. Various workshop participants thought:; 1. It should exit with no output to harmonize with Unix CLI tool conventions; 2. It should confirm success in a way that is obvious to users who are e.g. biologists first, programmers second. Either would be an improvement over the current state. cc @rmeffan @ndbolliger ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4040
https://github.com/broadinstitute/cromwell/issues/4040:1170,Security,PASSWORD,PASSWORDS,1170,"Feedback from today's workshop: `womtool validate` uselessly prints a few newlines on successful exit. Various workshop participants thought:; 1. It should exit with no output to harmonize with Unix CLI tool conventions; 2. It should confirm success in a way that is obvious to users who are e.g. biologists first, programmers second. Either would be an improvement over the current state. cc @rmeffan @ndbolliger ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4040
https://github.com/broadinstitute/cromwell/issues/4040:0,Usability,Feedback,Feedback,0,"Feedback from today's workshop: `womtool validate` uselessly prints a few newlines on successful exit. Various workshop participants thought:; 1. It should exit with no output to harmonize with Unix CLI tool conventions; 2. It should confirm success in a way that is obvious to users who are e.g. biologists first, programmers second. Either would be an improvement over the current state. cc @rmeffan @ndbolliger ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4040
https://github.com/broadinstitute/cromwell/issues/4040:463,Usability,feedback,feedback,463,"Feedback from today's workshop: `womtool validate` uselessly prints a few newlines on successful exit. Various workshop participants thought:; 1. It should exit with no output to harmonize with Unix CLI tool conventions; 2. It should confirm success in a way that is obvious to users who are e.g. biologists first, programmers second. Either would be an improvement over the current state. cc @rmeffan @ndbolliger ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4040
https://github.com/broadinstitute/cromwell/issues/4041:24,Availability,error,error,24,"Found during workshop - error formatter caret points to the wrong thing. Looks like a tabs vs. spaces thing.; ```; workflow HelloWorld {. 	call WriteGreetings; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; ERROR: Call references a task (WriteGreetings) that doesn't exist (line 3, col 7). 	call WriteGreetings; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4041
https://github.com/broadinstitute/cromwell/issues/4041:199,Availability,echo,echo,199,"Found during workshop - error formatter caret points to the wrong thing. Looks like a tabs vs. spaces thing.; ```; workflow HelloWorld {. 	call WriteGreetings; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; ERROR: Call references a task (WriteGreetings) that doesn't exist (line 3, col 7). 	call WriteGreetings; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4041
https://github.com/broadinstitute/cromwell/issues/4041:413,Availability,ERROR,ERROR,413,"Found during workshop - error formatter caret points to the wrong thing. Looks like a tabs vs. spaces thing.; ```; workflow HelloWorld {. 	call WriteGreetings; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; ERROR: Call references a task (WriteGreetings) that doesn't exist (line 3, col 7). 	call WriteGreetings; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4041
https://github.com/broadinstitute/cromwell/issues/4041:372,Security,validat,validate,372,"Found during workshop - error formatter caret points to the wrong thing. Looks like a tabs vs. spaces thing.; ```; workflow HelloWorld {. 	call WriteGreetings; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; ERROR: Call references a task (WriteGreetings) that doesn't exist (line 3, col 7). 	call WriteGreetings; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4041
https://github.com/broadinstitute/cromwell/issues/4042:41,Availability,error,error,41,"Found during workshop - incomprehensible error output.; ```; workflow HelloWorld {. 	call WriteGreeting; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = asdf(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; wdl.draft2.model.expression.WdlStandardLibraryFunctionsType.asdf(scala.collection.Seq); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4042
https://github.com/broadinstitute/cromwell/issues/4042:144,Availability,echo,echo,144,"Found during workshop - incomprehensible error output.; ```; workflow HelloWorld {. 	call WriteGreeting; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = asdf(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; wdl.draft2.model.expression.WdlStandardLibraryFunctionsType.asdf(scala.collection.Seq); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4042
https://github.com/broadinstitute/cromwell/issues/4042:315,Security,validat,validate,315,"Found during workshop - incomprehensible error output.; ```; workflow HelloWorld {. 	call WriteGreeting; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = asdf(); 	}; }; ```; ```; (gatk) root@5721c54d094c:/gatk/workshop_bundle/workshop_bundle# java -jar jars/womtool-34.jar validate hello_world/hello_world_0.wdl ; wdl.draft2.model.expression.WdlStandardLibraryFunctionsType.asdf(scala.collection.Seq); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4042
https://github.com/broadinstitute/cromwell/pull/4043:226,Integrability,message,message,226,"I know the I/O actor is not very trendy these days, but even if it ends up going away I thought this was a small change that could help with handling IO pressure in a better way.; Currently if an actor receives a backpressure message it waits more or less 5 seconds and retries. This uses configurable exponential backoff instead with a higher randomization of waiting times to avoid spikes as much as possible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4043
https://github.com/broadinstitute/cromwell/pull/4043:289,Modifiability,config,configurable,289,"I know the I/O actor is not very trendy these days, but even if it ends up going away I thought this was a small change that could help with handling IO pressure in a better way.; Currently if an actor receives a backpressure message it waits more or less 5 seconds and retries. This uses configurable exponential backoff instead with a higher randomization of waiting times to avoid spikes as much as possible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4043
https://github.com/broadinstitute/cromwell/pull/4043:378,Safety,avoid,avoid,378,"I know the I/O actor is not very trendy these days, but even if it ends up going away I thought this was a small change that could help with handling IO pressure in a better way.; Currently if an actor receives a backpressure message it waits more or less 5 seconds and retries. This uses configurable exponential backoff instead with a higher randomization of waiting times to avoid spikes as much as possible.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4043
https://github.com/broadinstitute/cromwell/pull/4044:144,Integrability,rout,routes,144,"Definitely not for prod but could be useful for dev purposes when one wants to spin up a few Cromwells on the same DB. A load balancer in front routes requests to underlying Cromwells talking to a MySQL instance. For example:. ```; $ docker-compose -f scripts/docker-compose-mysql/docker-compose-cloudwell.yml up --scale cromwell=3 -d; Starting docker-compose-mysql_mysql-db_1 ... done; Starting docker-compose-mysql_cromwell_1 ... done; Starting docker-compose-mysql_cromwell_2 ... done; Starting docker-compose-mysql_cromwell_3 ... done; Starting docker-compose-mysql_lb_1 ... done. $ docker ps; CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES; 9311d2f053d6 broadinstitute/cromwell:develop ""/bin/bash -c 'java …"" 6 minutes ago Up 15 seconds 8000/tcp docker-compose-mysql_cromwell_3; e040065fe3ba broadinstitute/cromwell:develop ""/bin/bash -c 'java …"" 6 minutes ago Up 15 seconds 8000/tcp docker-compose-mysql_cromwell_2; aff5007b8d26 dockercloud/haproxy ""/sbin/tini -- docke…"" 7 minutes ago Up 14 seconds 443/tcp, 1936/tcp, 0.0.0.0:8000->80/tcp docker-compose-mysql_lb_1; b991a7c412a5 broadinstitute/cromwell:develop ""/bin/bash -c 'java …"" 7 minutes ago Up 15 seconds 8000/tcp docker-compose-mysql_cromwell_1; 30892dce18b2 mysql:5.7 ""docker-entrypoint.s…"" 17 minutes ago Up 18 seconds (healthy) 0.0.0.0:3306->3306/tcp docker-compose-mysql_mysql-db_1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4044
https://github.com/broadinstitute/cromwell/pull/4044:121,Performance,load,load,121,"Definitely not for prod but could be useful for dev purposes when one wants to spin up a few Cromwells on the same DB. A load balancer in front routes requests to underlying Cromwells talking to a MySQL instance. For example:. ```; $ docker-compose -f scripts/docker-compose-mysql/docker-compose-cloudwell.yml up --scale cromwell=3 -d; Starting docker-compose-mysql_mysql-db_1 ... done; Starting docker-compose-mysql_cromwell_1 ... done; Starting docker-compose-mysql_cromwell_2 ... done; Starting docker-compose-mysql_cromwell_3 ... done; Starting docker-compose-mysql_lb_1 ... done. $ docker ps; CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES; 9311d2f053d6 broadinstitute/cromwell:develop ""/bin/bash -c 'java …"" 6 minutes ago Up 15 seconds 8000/tcp docker-compose-mysql_cromwell_3; e040065fe3ba broadinstitute/cromwell:develop ""/bin/bash -c 'java …"" 6 minutes ago Up 15 seconds 8000/tcp docker-compose-mysql_cromwell_2; aff5007b8d26 dockercloud/haproxy ""/sbin/tini -- docke…"" 7 minutes ago Up 14 seconds 443/tcp, 1936/tcp, 0.0.0.0:8000->80/tcp docker-compose-mysql_lb_1; b991a7c412a5 broadinstitute/cromwell:develop ""/bin/bash -c 'java …"" 7 minutes ago Up 15 seconds 8000/tcp docker-compose-mysql_cromwell_1; 30892dce18b2 mysql:5.7 ""docker-entrypoint.s…"" 17 minutes ago Up 18 seconds (healthy) 0.0.0.0:3306->3306/tcp docker-compose-mysql_mysql-db_1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4044
https://github.com/broadinstitute/cromwell/issues/4046:36,Availability,failure,failures,36,"Today -- there seems to be a lot of failures on both the v1 and v2 backend related to the caching tests. Many call caching tests are structured in a way that a workflow runs one time --and second run of the workflow caches to the results of the first run. The issue here is that if Cromwell restarts during the first run --then its possible the first workflow caches to itself, which causes the test to fail. . AC: To prevent this from happening, we should change test expectations such that the first time a call caching test is run --read_from_cache is set to false so regardless of when the Cromwell restart occurs, the jobs of the first workflow aren't call cached but forced to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046
https://github.com/broadinstitute/cromwell/issues/4046:216,Performance,cache,caches,216,"Today -- there seems to be a lot of failures on both the v1 and v2 backend related to the caching tests. Many call caching tests are structured in a way that a workflow runs one time --and second run of the workflow caches to the results of the first run. The issue here is that if Cromwell restarts during the first run --then its possible the first workflow caches to itself, which causes the test to fail. . AC: To prevent this from happening, we should change test expectations such that the first time a call caching test is run --read_from_cache is set to false so regardless of when the Cromwell restart occurs, the jobs of the first workflow aren't call cached but forced to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046
https://github.com/broadinstitute/cromwell/issues/4046:360,Performance,cache,caches,360,"Today -- there seems to be a lot of failures on both the v1 and v2 backend related to the caching tests. Many call caching tests are structured in a way that a workflow runs one time --and second run of the workflow caches to the results of the first run. The issue here is that if Cromwell restarts during the first run --then its possible the first workflow caches to itself, which causes the test to fail. . AC: To prevent this from happening, we should change test expectations such that the first time a call caching test is run --read_from_cache is set to false so regardless of when the Cromwell restart occurs, the jobs of the first workflow aren't call cached but forced to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046
https://github.com/broadinstitute/cromwell/issues/4046:662,Performance,cache,cached,662,"Today -- there seems to be a lot of failures on both the v1 and v2 backend related to the caching tests. Many call caching tests are structured in a way that a workflow runs one time --and second run of the workflow caches to the results of the first run. The issue here is that if Cromwell restarts during the first run --then its possible the first workflow caches to itself, which causes the test to fail. . AC: To prevent this from happening, we should change test expectations such that the first time a call caching test is run --read_from_cache is set to false so regardless of when the Cromwell restart occurs, the jobs of the first workflow aren't call cached but forced to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046
https://github.com/broadinstitute/cromwell/issues/4046:98,Testability,test,tests,98,"Today -- there seems to be a lot of failures on both the v1 and v2 backend related to the caching tests. Many call caching tests are structured in a way that a workflow runs one time --and second run of the workflow caches to the results of the first run. The issue here is that if Cromwell restarts during the first run --then its possible the first workflow caches to itself, which causes the test to fail. . AC: To prevent this from happening, we should change test expectations such that the first time a call caching test is run --read_from_cache is set to false so regardless of when the Cromwell restart occurs, the jobs of the first workflow aren't call cached but forced to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046
https://github.com/broadinstitute/cromwell/issues/4046:123,Testability,test,tests,123,"Today -- there seems to be a lot of failures on both the v1 and v2 backend related to the caching tests. Many call caching tests are structured in a way that a workflow runs one time --and second run of the workflow caches to the results of the first run. The issue here is that if Cromwell restarts during the first run --then its possible the first workflow caches to itself, which causes the test to fail. . AC: To prevent this from happening, we should change test expectations such that the first time a call caching test is run --read_from_cache is set to false so regardless of when the Cromwell restart occurs, the jobs of the first workflow aren't call cached but forced to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046
https://github.com/broadinstitute/cromwell/issues/4046:395,Testability,test,test,395,"Today -- there seems to be a lot of failures on both the v1 and v2 backend related to the caching tests. Many call caching tests are structured in a way that a workflow runs one time --and second run of the workflow caches to the results of the first run. The issue here is that if Cromwell restarts during the first run --then its possible the first workflow caches to itself, which causes the test to fail. . AC: To prevent this from happening, we should change test expectations such that the first time a call caching test is run --read_from_cache is set to false so regardless of when the Cromwell restart occurs, the jobs of the first workflow aren't call cached but forced to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046
https://github.com/broadinstitute/cromwell/issues/4046:464,Testability,test,test,464,"Today -- there seems to be a lot of failures on both the v1 and v2 backend related to the caching tests. Many call caching tests are structured in a way that a workflow runs one time --and second run of the workflow caches to the results of the first run. The issue here is that if Cromwell restarts during the first run --then its possible the first workflow caches to itself, which causes the test to fail. . AC: To prevent this from happening, we should change test expectations such that the first time a call caching test is run --read_from_cache is set to false so regardless of when the Cromwell restart occurs, the jobs of the first workflow aren't call cached but forced to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046
https://github.com/broadinstitute/cromwell/issues/4046:522,Testability,test,test,522,"Today -- there seems to be a lot of failures on both the v1 and v2 backend related to the caching tests. Many call caching tests are structured in a way that a workflow runs one time --and second run of the workflow caches to the results of the first run. The issue here is that if Cromwell restarts during the first run --then its possible the first workflow caches to itself, which causes the test to fail. . AC: To prevent this from happening, we should change test expectations such that the first time a call caching test is run --read_from_cache is set to false so regardless of when the Cromwell restart occurs, the jobs of the first workflow aren't call cached but forced to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046
https://github.com/broadinstitute/cromwell/issues/4048:326,Availability,ERROR,ERROR,326,"Example WDL taken directly from the spec: https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#workflow-level-resolution; ```; version 1.0. workflow wf {; input {; String s = ""wf_s""; String t = ""t""; }; call my_task {; String s = ""my_task_s""; input: in0 = s+""-suffix"", in1 = t+""-suffix""; }; }; ```; results in; ```; ERROR: Unexpected symbol (line 9, col 5) when parsing 'call_body'. Expected input, got ""String"". String s = ""my_task_s""; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4048
https://github.com/broadinstitute/cromwell/issues/4049:150,Availability,Down,Downstream,150,"I have a WDL with command block `samtools view -s ${frac} -b ${bampath} -o ${outname}.bam`, where `frac` is a `Float` type. I'm running in FireCloud. Downstream, in the PAPI script, this had turned into `samtools view -s 5.0E-5 -b ...<etc>`, which rather confused samtools. Someone somewhere decided to render `0.00005` as `5.0E-5`, and it wasn't me. Can you help?. (Adam N said to reference WDL draft-2, so I am hereby doing that.). WDL follows:. ```; task downsample {; File bampath; File baipath; File refpath; File refipath; Float frac; String outname; ; Int disk_size = ceil((size(bampath, ""GB"") + size(refpath, ""GB"") + size(refipath, ""GB"")) * 3). command {; samtools view -s ${frac} -b ${bampath} -o ${outname}.bam; }; ; output {; 	File outcram = ""${outname}.bam""; }. runtime {; docker : ""mshand/genomesinthecloud:samtools.1.8""; disks: ""local-disk ${disk_size} HDD""; memory: ""4GB""; }; }. workflow dswf {; call downsample; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4049
https://github.com/broadinstitute/cromwell/issues/4049:458,Availability,down,downsample,458,"I have a WDL with command block `samtools view -s ${frac} -b ${bampath} -o ${outname}.bam`, where `frac` is a `Float` type. I'm running in FireCloud. Downstream, in the PAPI script, this had turned into `samtools view -s 5.0E-5 -b ...<etc>`, which rather confused samtools. Someone somewhere decided to render `0.00005` as `5.0E-5`, and it wasn't me. Can you help?. (Adam N said to reference WDL draft-2, so I am hereby doing that.). WDL follows:. ```; task downsample {; File bampath; File baipath; File refpath; File refipath; Float frac; String outname; ; Int disk_size = ceil((size(bampath, ""GB"") + size(refpath, ""GB"") + size(refipath, ""GB"")) * 3). command {; samtools view -s ${frac} -b ${bampath} -o ${outname}.bam; }; ; output {; 	File outcram = ""${outname}.bam""; }. runtime {; docker : ""mshand/genomesinthecloud:samtools.1.8""; disks: ""local-disk ${disk_size} HDD""; memory: ""4GB""; }; }. workflow dswf {; call downsample; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4049
https://github.com/broadinstitute/cromwell/issues/4049:916,Availability,down,downsample,916,"I have a WDL with command block `samtools view -s ${frac} -b ${bampath} -o ${outname}.bam`, where `frac` is a `Float` type. I'm running in FireCloud. Downstream, in the PAPI script, this had turned into `samtools view -s 5.0E-5 -b ...<etc>`, which rather confused samtools. Someone somewhere decided to render `0.00005` as `5.0E-5`, and it wasn't me. Can you help?. (Adam N said to reference WDL draft-2, so I am hereby doing that.). WDL follows:. ```; task downsample {; File bampath; File baipath; File refpath; File refipath; Float frac; String outname; ; Int disk_size = ceil((size(bampath, ""GB"") + size(refpath, ""GB"") + size(refipath, ""GB"")) * 3). command {; samtools view -s ${frac} -b ${bampath} -o ${outname}.bam; }; ; output {; 	File outcram = ""${outname}.bam""; }. runtime {; docker : ""mshand/genomesinthecloud:samtools.1.8""; disks: ""local-disk ${disk_size} HDD""; memory: ""4GB""; }; }. workflow dswf {; call downsample; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4049
https://github.com/broadinstitute/cromwell/issues/4050:246,Availability,echo,echo,246,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050
https://github.com/broadinstitute/cromwell/issues/4050:608,Availability,Echo,Echo,608,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050
https://github.com/broadinstitute/cromwell/issues/4050:616,Availability,echo,echo,616,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050
https://github.com/broadinstitute/cromwell/issues/4050:656,Availability,Echo,Echo,656,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050
https://github.com/broadinstitute/cromwell/issues/4050:723,Availability,echo,echo,723,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050
https://github.com/broadinstitute/cromwell/issues/4050:309,Testability,test,tested,309,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050
https://github.com/broadinstitute/cromwell/issues/4050:455,Testability,test,tested,455,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050
https://github.com/broadinstitute/cromwell/issues/4050:520,Testability,test,testing,520,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050
https://github.com/broadinstitute/cromwell/issues/4050:583,Testability,Test,Test,583,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050
https://github.com/broadinstitute/cromwell/issues/4050:728,Testability,test,test,728,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050
https://github.com/broadinstitute/cromwell/issues/4051:63,Availability,error,error,63,"When I try to use tabs as whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:102,Availability,error,error,102,"When I try to use tabs as whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:366,Availability,Error,Error,366,"When I try to use tabs as whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:736,Availability,Echo,Echo,736,"When I try to use tabs as whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:744,Availability,echo,echo,744,"When I try to use tabs as whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:804,Availability,Echo,Echo,804,"When I try to use tabs as whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:882,Availability,echo,echo,882,"When I try to use tabs as whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:981,Availability,error,error,981,"s whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1245,Availability,Error,Error,1245,"lowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1686,Availability,failure,failure,1686,"ab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1739,Availability,failure,failure,1739,"o {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1813,Availability,failure,failure,1813,"t {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(Batching",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:2036,Energy Efficiency,adapt,adapted,2036,a18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:2036,Modifiability,adapt,adapted,2036,a18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1258,Performance,concurren,concurrent,1258,"e.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.eff",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1317,Performance,concurren,concurrent,1317,"1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workfl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1348,Performance,concurren,concurrent,1348,"rror; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWork",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1409,Performance,concurren,concurrent,1409,"e, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1486,Performance,concurren,concurrent,1486,"md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1539,Performance,concurren,concurrent,1539,"ested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurren",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1593,Performance,concurren,concurrent,1593,"a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1667,Performance,concurren,concurrent,1667,"0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.di",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1720,Performance,concurren,concurrent,1720," <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(Ba",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1774,Performance,concurren,concurrent,1774,"utput {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:2474,Performance,concurren,concurrent,2474, scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinW,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:2536,Performance,concurren,concurrent,2536,ncurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.e,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:2610,Performance,concurren,concurrent,2610,e$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:2919,Performance,concurren,concurrent,2919,er.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:206); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorAc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1865,Safety,unsafe,unsafeToFuture,1865,"; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:1963,Safety,unsafe,unsafeToFuture,1963,"9:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockCon",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:2019,Safety,unsafe,unsafeToFuture,2019,a18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:2226,Safety,unsafe,unsafeRunAsync,2226,d:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:2271,Safety,unsafe,unsafeToFuture,2271,e$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:543,Testability,test,tested,543,"When I try to use tabs as whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:684,Testability,Test,Test,684,"When I try to use tabs as whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:887,Testability,test,test,887,"When I try to use tabs as whitespace in cromwell I'm getting a error:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; ```. The specs say this should be possible, 0x9 == tab:; https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. I have tested cromwell 34 and the current develop branch (ce27a93). wdl file that I used (replaced '\t' with '\<tab\>'); ```; version 1.0. workflow Test {; <tab>input {; <tab><tab>; <tab>}. <tab>call Echo as echo {; <tab>input:; <tab>}. <tab>output {; <tab>}; }. task Echo {; <tab>input {; <tab>}. <tab>command {; <tab><tab>kill -9 $$; <tab><tab>echo test; <tab>}. <tab>output {; <tab>}; }; ```. Full stacktrace:; ```; [2018-08-29 09:25:20,10] [error] WorkflowManagerActor Workflow 1ed0e19c-fa18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:4382,Testability,Log,LoggingFSM,4382,ool.runWorker(ForkJoinPool.java:1979); akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:206); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:179); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:171); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:670); 	at akka.actor.FSM.processEvent$(FSM.scala:667); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:130); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:806); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:788); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:130); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:664); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:658); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:130); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:4475,Testability,Log,LoggingFSM,4475,orkerThread.run(ForkJoinWorkerThread.java:107); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:206); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:179); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:171); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:670); 	at akka.actor.FSM.processEvent$(FSM.scala:667); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:130); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:806); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:788); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:130); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:664); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:658); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:130); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/issues/4051:4530,Testability,Log,LoggingFSM,4530,mwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:206); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:179); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:171); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:670); 	at akka.actor.FSM.processEvent$(FSM.scala:667); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:130); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:806); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:788); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:130); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:664); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:658); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:130); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051
https://github.com/broadinstitute/cromwell/pull/4054:1,Deployability,Pipeline,Pipeline,1,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054
https://github.com/broadinstitute/cromwell/pull/4054:38,Deployability,Pipeline,Pipeline,38,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054
https://github.com/broadinstitute/cromwell/pull/4054:425,Deployability,Pipeline,Pipeline,425,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054
https://github.com/broadinstitute/cromwell/pull/4054:654,Safety,unsafe,unsafe,654,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054
https://github.com/broadinstitute/cromwell/pull/4054:318,Security,validat,validating,318,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054
https://github.com/broadinstitute/cromwell/pull/4054:464,Security,validat,validate,464,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054
https://github.com/broadinstitute/cromwell/pull/4054:445,Testability,test,tests,445,"""Pipeline"" Scopes are added only for ""Pipeline"" Credentials.; Otherwise scopes must be requested when asking for credentials.; `Credential` generator (vs. `Credentials`, the former an older API) still returns an unscoped Credential.; Renamed methods returning Credentials from `credential` to `credentials`.; Now also validating USA Credentials before returning.; Credentials lookups from workflow options are only done for ""Pipeline"" creds and tests.; Removed a `validate(WorkflowOptions)` that wasn't in use since commit 6fbeadc.; Removed scope declarations no longer in use.; Using scope-constants as-much-as-possible from the Google SDKs.; Added an `unsafe` to replace `toTry.get`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4054
https://github.com/broadinstitute/cromwell/pull/4055:24,Availability,failure,failures,24,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055
https://github.com/broadinstitute/cromwell/pull/4055:78,Deployability,integrat,integration,78,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055
https://github.com/broadinstitute/cromwell/pull/4055:78,Integrability,integrat,integration,78,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055
https://github.com/broadinstitute/cromwell/pull/4055:273,Modifiability,config,config,273,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055
https://github.com/broadinstitute/cromwell/pull/4055:383,Modifiability,variab,variables,383,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055
https://github.com/broadinstitute/cromwell/pull/4055:427,Modifiability,Refactor,Refactored,427,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055
https://github.com/broadinstitute/cromwell/pull/4055:453,Modifiability,config,config,453,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055
https://github.com/broadinstitute/cromwell/pull/4055:372,Security,secur,secure,372,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055
https://github.com/broadinstitute/cromwell/pull/4055:446,Security,secur,secure,446,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055
https://github.com/broadinstitute/cromwell/pull/4055:90,Testability,test,tests,90,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055
https://github.com/broadinstitute/cromwell/issues/4056:65,Availability,mainten,maintenance,65,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056
https://github.com/broadinstitute/cromwell/issues/4056:146,Availability,error,errors,146,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056
https://github.com/broadinstitute/cromwell/issues/4056:218,Availability,resilien,resilient,218,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056
https://github.com/broadinstitute/cromwell/issues/4056:231,Availability,outage,outages,231,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056
https://github.com/broadinstitute/cromwell/issues/4056:313,Availability,down,down,313,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056
https://github.com/broadinstitute/cromwell/issues/4056:347,Availability,degraded,degraded,347,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056
https://github.com/broadinstitute/cromwell/issues/4056:572,Availability,down,down,572,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056
https://github.com/broadinstitute/cromwell/issues/4056:55,Energy Efficiency,schedul,scheduled,55,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056
https://github.com/broadinstitute/cromwell/issues/4056:246,Integrability,depend,dependencies,246,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056
https://github.com/broadinstitute/cromwell/issues/4056:40,Performance,perform,performed,40,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056
https://github.com/broadinstitute/cromwell/issues/4057:31,Availability,failure,failures,31,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057
https://github.com/broadinstitute/cromwell/issues/4057:907,Availability,failure,failure,907,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057
https://github.com/broadinstitute/cromwell/issues/4057:925,Integrability,Depend,Depending,925,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057
https://github.com/broadinstitute/cromwell/issues/4057:646,Performance,queue,queued,646,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057
https://github.com/broadinstitute/cromwell/issues/4057:89,Safety,timeout,timeouts,89,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057
https://github.com/broadinstitute/cromwell/issues/4057:699,Safety,timeout,timeout,699,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057
https://github.com/broadinstitute/cromwell/issues/4057:754,Safety,timeout,timeout,754,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057
https://github.com/broadinstitute/cromwell/issues/4057:1018,Safety,timeout,timeout,1018,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057
https://github.com/broadinstitute/cromwell/issues/4057:1100,Safety,timeout,timeout,1100,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057
https://github.com/broadinstitute/cromwell/issues/4057:26,Testability,test,test,26,"A recent review of Travis test failures revealed that some workflows were failing due to timeouts on functions like read_lines() or read_int() timing out:. ```cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'int_reader.int': Failed to read_int(""""gs://cloud-cromwell-dev/cromwell_execution/travis/globs/57f6e677-c2aa-4d96-bf33-9591fce20da7/call-int_reader/shard-3/stdout"""") (reason 1 of 1): Futures timed out after [10 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:851)```. It's possible that being queued in the I/O actor can take longer than the 10s timeout and thus that is the issue. It's possible this timeout needs to be raised or output evaluation needs to be retried, but this needs a fix as the outputs being evaluated already exist, so this is a bad failure mode. AC: Depending on the potential causes for such behavior, either retry this evaluation, raise the timeout or explore another solution to ensure that jobs dont fail because of this timeout.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057
https://github.com/broadinstitute/cromwell/issues/4058:25,Availability,failure,failures,25,"Based on the travis test failures -- it seems the `write_lines` centaur tests has a draft3 and a draft2 version and they can end up caching to each other. AC: Either change the test cases so they won't cache to each other, or ensure that `read_from_cache()` is set to false.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4058
https://github.com/broadinstitute/cromwell/issues/4058:202,Performance,cache,cache,202,"Based on the travis test failures -- it seems the `write_lines` centaur tests has a draft3 and a draft2 version and they can end up caching to each other. AC: Either change the test cases so they won't cache to each other, or ensure that `read_from_cache()` is set to false.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4058
https://github.com/broadinstitute/cromwell/issues/4058:20,Testability,test,test,20,"Based on the travis test failures -- it seems the `write_lines` centaur tests has a draft3 and a draft2 version and they can end up caching to each other. AC: Either change the test cases so they won't cache to each other, or ensure that `read_from_cache()` is set to false.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4058
https://github.com/broadinstitute/cromwell/issues/4058:72,Testability,test,tests,72,"Based on the travis test failures -- it seems the `write_lines` centaur tests has a draft3 and a draft2 version and they can end up caching to each other. AC: Either change the test cases so they won't cache to each other, or ensure that `read_from_cache()` is set to false.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4058
https://github.com/broadinstitute/cromwell/issues/4058:177,Testability,test,test,177,"Based on the travis test failures -- it seems the `write_lines` centaur tests has a draft3 and a draft2 version and they can end up caching to each other. AC: Either change the test cases so they won't cache to each other, or ensure that `read_from_cache()` is set to false.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4058
https://github.com/broadinstitute/cromwell/pull/4059:172,Testability,test,test,172,"Related to (but does not close) https://github.com/broadinstitute/cromwell/issues/4051. In the issue above, doubt was raised whether we correctly support tabs. I wanted to test whether we do and add a test, and this change does both.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4059
https://github.com/broadinstitute/cromwell/pull/4059:201,Testability,test,test,201,"Related to (but does not close) https://github.com/broadinstitute/cromwell/issues/4051. In the issue above, doubt was raised whether we correctly support tabs. I wanted to test whether we do and add a test, and this change does both.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4059
https://github.com/broadinstitute/cromwell/issues/4060:727,Availability,ERROR,ERROR,727,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; Output:; ```; [2018-08-30 17:36:02,67] [info] Running with database db.url = jdbc:hsqldb:mem:6713284f-67ff-4eb9-9fd6-3fde0a4cc0ce;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our oth",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060
https://github.com/broadinstitute/cromwell/issues/4060:1657,Energy Efficiency,adapt,adapted,1657,"e;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the stacktrace information is likely overwhelming—that is, the signal-to-noise ratio of this output can likely be improved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060
https://github.com/broadinstitute/cromwell/issues/4060:2009,Integrability,interface,interfaces,2009,"e;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the stacktrace information is likely overwhelming—that is, the signal-to-noise ratio of this output can likely be improved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060
https://github.com/broadinstitute/cromwell/issues/4060:1657,Modifiability,adapt,adapted,1657,"e;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the stacktrace information is likely overwhelming—that is, the signal-to-noise ratio of this output can likely be improved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060
https://github.com/broadinstitute/cromwell/issues/4060:938,Security,Validat,Validated,938,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; Output:; ```; [2018-08-30 17:36:02,67] [info] Running with database db.url = jdbc:hsqldb:mem:6713284f-67ff-4eb9-9fd6-3fde0a4cc0ce;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our oth",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060
https://github.com/broadinstitute/cromwell/issues/4060:956,Security,Validat,Validated,956,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; Output:; ```; [2018-08-30 17:36:02,67] [info] Running with database db.url = jdbc:hsqldb:mem:6713284f-67ff-4eb9-9fd6-3fde0a4cc0ce;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our oth",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060
https://github.com/broadinstitute/cromwell/issues/4060:1096,Security,validat,validateRunArguments,1096,"67] [info] Running with database db.url = jdbc:hsqldb:mem:6713284f-67ff-4eb9-9fd6-3fde0a4cc0ce;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the stacktrace information is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060
https://github.com/broadinstitute/cromwell/issues/4060:1952,Usability,usab,usability,1952,"e;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the stacktrace information is likely overwhelming—that is, the signal-to-noise ratio of this output can likely be improved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060
https://github.com/broadinstitute/cromwell/issues/4061:243,Availability,failure,failure,243,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; The output shows a stacktrace and then hangs. It should likely exit with a non-zero status, following the convention of other command-line tools and allowing for failure detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4061
https://github.com/broadinstitute/cromwell/issues/4061:251,Safety,detect,detection,251,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl; ```; The output shows a stacktrace and then hangs. It should likely exit with a non-zero status, following the convention of other command-line tools and allowing for failure detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4061
https://github.com/broadinstitute/cromwell/issues/4062:715,Availability,heartbeat,heartbeat,715,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run hello_world/hello_world_0.wdl; ```; Output:; ```; [2018-08-30 17:53:11,17] [info] Running with database db.url = jdbc:hsqldb:mem:4fbaa426-09e6-4c70-9a1a-15469c4d77a0;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:19,24] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:53:19,26] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:53:19,39] [info] Running with database db.url = jdbc:hsqldb:mem:146c8707-d56e-4f58-a2de-df327f328109;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:20,13] [info] Slf4jLogger started; [2018-08-30 17:53:20,68] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-232861f"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-30 17:53:20,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:779,Availability,heartbeat,heartbeatInterval,779,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run hello_world/hello_world_0.wdl; ```; Output:; ```; [2018-08-30 17:53:11,17] [info] Running with database db.url = jdbc:hsqldb:mem:4fbaa426-09e6-4c70-9a1a-15469c4d77a0;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:19,24] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:53:19,26] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:53:19,39] [info] Running with database db.url = jdbc:hsqldb:mem:146c8707-d56e-4f58-a2de-df327f328109;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:20,13] [info] Slf4jLogger started; [2018-08-30 17:53:20,68] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-232861f"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-30 17:53:20,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:3105,Availability,echo,echo,3105,"ved 1 workflows from the WorkflowStoreActor; [2018-08-30 17:53:22,18] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-08-30 17:53:22,20] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-30 17:53:22,21] [info] Using noop to send events.; [2018-08-30 17:53:22,25] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-08-30 17:53:22,30] [info] MaterializeWorkflowDescriptorActor [4dbd7d1c]: Parsing workflow as WDL draft-2; [2018-08-30 17:53:23,48] [info] MaterializeWorkflowDescriptorActor [4dbd7d1c]: Call-to-Backend assignments: HelloWorld.WriteGreeting -> Local; [2018-08-30 17:53:24,95] [info] WorkflowExecutionActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc [4dbd7d1c]: Starting HelloWorld.WriteGreeting; [2018-08-30 17:53:26,42] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: echo ""Hello World""; [2018-08-30 17:53:26,49] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: executing: /bin/bash /gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/script; [2018-08-30 17:53:31,09] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: job id: 115; [2018-08-30 17:53:31,10] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: Status change from - to Done; [2018-08-30 17:53:33,13] [info] WorkflowExecutionActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc [4dbd7d1c]: Workflow HelloWorld complete. Final Outputs:; {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; }; [2018-08-30 17:53:33,18] [info] WorkflowManagerActor WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc is in a terminal state: WorkflowSucceededState; [2018-08-30 17:53:36,13] [info] SingleWorkflowRun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:4525,Availability,down,down,4525,"tor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: job id: 115; [2018-08-30 17:53:31,10] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: Status change from - to Done; [2018-08-30 17:53:33,13] [info] WorkflowExecutionActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc [4dbd7d1c]: Workflow HelloWorld complete. Final Outputs:; {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; }; [2018-08-30 17:53:33,18] [info] WorkflowManagerActor WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc is in a terminal state: WorkflowSucceededState; [2018-08-30 17:53:36,13] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; },; ""id"": ""4dbd7d1c-e7e8-4f83-9750-5c638d1567bc""; }; [2018-08-30 17:53:41,12] [info] Workflow polling stopped; [2018-08-30 17:53:41,13] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-30 17:53:41,14] [info] Aborting all running workflows.; [2018-08-30 17:53:41,15] [info] WorkflowStoreActor stopped; [2018-08-30 17:53:41,15] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:4738,Availability,down,down,4738,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:4829,Availability,down,down,4829,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5056,Availability,down,down,5056,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5301,Availability,down,down,5301,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5348,Availability,down,down,5348,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5505,Availability,down,down,5505,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5591,Availability,down,down,5591,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5703,Availability,down,down,5703,"-30 17:53:41,15] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutd",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5841,Availability,down,down,5841,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5934,Availability,down,down,5934,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:6035,Availability,down,down,6035,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:6131,Availability,down,down,6131,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:6265,Availability,down,down,6265,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:6576,Availability,down,down,6576,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:7508,Availability,echo,echo,7508,"] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.; [2018-08-30 17:53:41,29] [info] Shutdown finished.; ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the class names in the above output are misleading. For example, in the line:; ```; CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; ```; a user may want to investigate the terms ""batch size"" and ""process rate"" through the documentation or forums, which will likely have meaning. CallCacheWriteActor, on the other hand, is likely an implementation detail that probably adds confusion rather than clarity. hello_world_0.wdl:; ```wdl; workflow HelloWorld {. 	call WriteGreeting; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:725,Deployability,configurat,configuration,725,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run hello_world/hello_world_0.wdl; ```; Output:; ```; [2018-08-30 17:53:11,17] [info] Running with database db.url = jdbc:hsqldb:mem:4fbaa426-09e6-4c70-9a1a-15469c4d77a0;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:19,24] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:53:19,26] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:53:19,39] [info] Running with database db.url = jdbc:hsqldb:mem:146c8707-d56e-4f58-a2de-df327f328109;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:20,13] [info] Slf4jLogger started; [2018-08-30 17:53:20,68] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-232861f"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-30 17:53:20,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:2274,Integrability,message,message,2274,"0,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] WorkflowManagerActor Successfully started WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-08-30 17:53:22,18] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-08-30 17:53:22,20] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-30 17:53:22,21] [info] Using noop to send events.; [2018-08-30 17:53:22,25] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-08-30 17:53:22,30] [info] MaterializeWorkflowDescriptorActor [4dbd7d1c]: Parsing workflow as WDL draft-2; [2018-08-30 17:53:23,48] [info] MaterializeWorkflowDescriptorActor [4dbd7d1c]: Call-to-Backend assignments: HelloWorld.WriteGreeting -> Local; [2018-08-30 17:53:24,95] [info] WorkflowExecutionActor-4db",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5718,Integrability,message,messages,5718,"[info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:6050,Integrability,message,messages,6050,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:6146,Integrability,message,messages,6146,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:6875,Integrability,interface,interfaces,6875,"] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.; [2018-08-30 17:53:41,29] [info] Shutdown finished.; ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the class names in the above output are misleading. For example, in the line:; ```; CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; ```; a user may want to investigate the terms ""batch size"" and ""process rate"" through the documentation or forums, which will likely have meaning. CallCacheWriteActor, on the other hand, is likely an implementation detail that probably adds confusion rather than clarity. hello_world_0.wdl:; ```wdl; workflow HelloWorld {. 	call WriteGreeting; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:725,Modifiability,config,configuration,725,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run hello_world/hello_world_0.wdl; ```; Output:; ```; [2018-08-30 17:53:11,17] [info] Running with database db.url = jdbc:hsqldb:mem:4fbaa426-09e6-4c70-9a1a-15469c4d77a0;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:19,24] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:53:19,26] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:53:19,39] [info] Running with database db.url = jdbc:hsqldb:mem:146c8707-d56e-4f58-a2de-df327f328109;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:20,13] [info] Slf4jLogger started; [2018-08-30 17:53:20,68] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-232861f"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-30 17:53:20,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:1023,Modifiability,config,configured,1023,"r jars/cromwell-34.jar run hello_world/hello_world_0.wdl; ```; Output:; ```; [2018-08-30 17:53:11,17] [info] Running with database db.url = jdbc:hsqldb:mem:4fbaa426-09e6-4c70-9a1a-15469c4d77a0;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:19,24] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:53:19,26] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:53:19,39] [info] Running with database db.url = jdbc:hsqldb:mem:146c8707-d56e-4f58-a2de-df327f328109;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:20,13] [info] Slf4jLogger started; [2018-08-30 17:53:20,68] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-232861f"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-30 17:53:20,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] WorkflowMana",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:1137,Modifiability,config,configured,1137,"ith database db.url = jdbc:hsqldb:mem:4fbaa426-09e6-4c70-9a1a-15469c4d77a0;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:19,24] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:53:19,26] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:53:19,39] [info] Running with database db.url = jdbc:hsqldb:mem:146c8707-d56e-4f58-a2de-df327f328109;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:20,13] [info] Slf4jLogger started; [2018-08-30 17:53:20,68] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-232861f"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-30 17:53:20,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] WorkflowManagerActor Successfully started WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] Ret",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:1257,Modifiability,config,configured,1257," 17:53:19,24] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:53:19,26] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:53:19,39] [info] Running with database db.url = jdbc:hsqldb:mem:146c8707-d56e-4f58-a2de-df327f328109;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:53:20,13] [info] Slf4jLogger started; [2018-08-30 17:53:20,68] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-232861f"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-30 17:53:20,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] WorkflowManagerActor Successfully started WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-08-30 17:53:22,18] [warn] SingleWorkflowRunnerActor: received u",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:2528,Modifiability,config,configured,2528,"ingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] WorkflowManagerActor Successfully started WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-08-30 17:53:22,18] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-08-30 17:53:22,20] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-30 17:53:22,21] [info] Using noop to send events.; [2018-08-30 17:53:22,25] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-08-30 17:53:22,30] [info] MaterializeWorkflowDescriptorActor [4dbd7d1c]: Parsing workflow as WDL draft-2; [2018-08-30 17:53:23,48] [info] MaterializeWorkflowDescriptorActor [4dbd7d1c]: Call-to-Backend assignments: HelloWorld.WriteGreeting -> Local; [2018-08-30 17:53:24,95] [info] WorkflowExecutionActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc [4dbd7d1c]: Starting HelloWorld.WriteGreeting; [2018-08-30 17:53:26,42] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: echo ""Hello World""; [2018-08-30 17:53:26,49] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: executing: /bin/bash /gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/script; [2018-08-30 17:53:31,09] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: job id: 115; [2018-08-30 17:53:31,10] [info]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:7056,Modifiability,config,configured,7056,"] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.; [2018-08-30 17:53:41,29] [info] Shutdown finished.; ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the class names in the above output are misleading. For example, in the line:; ```; CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; ```; a user may want to investigate the terms ""batch size"" and ""process rate"" through the documentation or forums, which will likely have meaning. CallCacheWriteActor, on the other hand, is likely an implementation detail that probably adds confusion rather than clarity. hello_world_0.wdl:; ```wdl; workflow HelloWorld {. 	call WriteGreeting; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5711,Performance,queue,queued,5711,":41,15] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:6043,Performance,queue,queued,6043,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:6139,Performance,queue,queued,6139,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:4551,Safety,Timeout,Timeout,4551,"tor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: job id: 115; [2018-08-30 17:53:31,10] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: Status change from - to Done; [2018-08-30 17:53:33,13] [info] WorkflowExecutionActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc [4dbd7d1c]: Workflow HelloWorld complete. Final Outputs:; {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; }; [2018-08-30 17:53:33,18] [info] WorkflowManagerActor WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc is in a terminal state: WorkflowSucceededState; [2018-08-30 17:53:36,13] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; },; ""id"": ""4dbd7d1c-e7e8-4f83-9750-5c638d1567bc""; }; [2018-08-30 17:53:41,12] [info] Workflow polling stopped; [2018-08-30 17:53:41,13] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-30 17:53:41,14] [info] Aborting all running workflows.; [2018-08-30 17:53:41,15] [info] WorkflowStoreActor stopped; [2018-08-30 17:53:41,15] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:4604,Safety,Abort,Aborting,4604,"tor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: job id: 115; [2018-08-30 17:53:31,10] [info] BackgroundConfigAsyncJobExecutionActor [4dbd7d1cHelloWorld.WriteGreeting:NA:1]: Status change from - to Done; [2018-08-30 17:53:33,13] [info] WorkflowExecutionActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc [4dbd7d1c]: Workflow HelloWorld complete. Final Outputs:; {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; }; [2018-08-30 17:53:33,18] [info] WorkflowManagerActor WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc is in a terminal state: WorkflowSucceededState; [2018-08-30 17:53:36,13] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {; ""HelloWorld.WriteGreeting.outfile"": ""/gatk/wsb/cromwell-executions/HelloWorld/4dbd7d1c-e7e8-4f83-9750-5c638d1567bc/call-WriteGreeting/execution/stdout""; },; ""id"": ""4dbd7d1c-e7e8-4f83-9750-5c638d1567bc""; }; [2018-08-30 17:53:41,12] [info] Workflow polling stopped; [2018-08-30 17:53:41,13] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-30 17:53:41,14] [info] Aborting all running workflows.; [2018-08-30 17:53:41,15] [info] WorkflowStoreActor stopped; [2018-08-30 17:53:41,15] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:4767,Safety,Timeout,Timeout,4767,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:4863,Safety,Timeout,Timeout,4863,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5084,Safety,Timeout,Timeout,5084,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5377,Safety,Timeout,Timeout,5377,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5526,Safety,Timeout,Timeout,5526,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5618,Safety,Timeout,Timeout,5618,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5869,Safety,Timeout,Timeout,5869,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:5957,Safety,Timeout,Timeout,5957,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:6280,Safety,Timeout,Timeout,6280,"nfo] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-30 17:53:41,15] [info] JobExecutionTokenDispenser stopped; [2018-08-30 17:53:41,17] [info] WorkflowLogCopyRouter stopped; [2018-08-30 17:53:41,17] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor All workflows finished; [2018-08-30 17:53:41,17] [info] WorkflowManagerActor stopped; [2018-08-30 17:53:41,17] [info] Connection pools shut down; [2018-08-30 17:53:41,18] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] SubWorkflowStoreActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,18] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4062:6818,Usability,usab,usability,6818,"] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] CallCacheWriteActor stopped; [2018-08-30 17:53:41,18] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-30 17:53:41,19] [info] DockerHashActor stopped; [2018-08-30 17:53:41,19] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-30 17:53:41,19] [info] JobStoreActor stopped; [2018-08-30 17:53:41,19] [info] IoProxy stopped; [2018-08-30 17:53:41,19] [info] ServiceRegistryActor stopped; [2018-08-30 17:53:41,23] [info] Database closed; [2018-08-30 17:53:41,23] [info] Stream materializer shut down; [2018-08-30 17:53:41,29] [info] Automatic shutdown of the async connection; [2018-08-30 17:53:41,29] [info] Gracefully shutdown sentry threads.; [2018-08-30 17:53:41,29] [info] Shutdown finished.; ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the class names in the above output are misleading. For example, in the line:; ```; CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; ```; a user may want to investigate the terms ""batch size"" and ""process rate"" through the documentation or forums, which will likely have meaning. CallCacheWriteActor, on the other hand, is likely an implementation detail that probably adds confusion rather than clarity. hello_world_0.wdl:; ```wdl; workflow HelloWorld {. 	call WriteGreeting; }. task WriteGreeting {. 	command {; 		echo ""Hello World""; 	}; 	output {; 		File outfile = stdout(); 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062
https://github.com/broadinstitute/cromwell/issues/4063:184,Availability,error,error,184,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl > /dev/null; ```; This produces no output. Running the same command without redirecting STDOUT results in an error message. This message should likely be reported on STDERR, not STDOUT, to be consistent with common standards around command-line tools.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4063
https://github.com/broadinstitute/cromwell/issues/4063:190,Integrability,message,message,190,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl > /dev/null; ```; This produces no output. Running the same command without redirecting STDOUT results in an error message. This message should likely be reported on STDERR, not STDOUT, to be consistent with common standards around command-line tools.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4063
https://github.com/broadinstitute/cromwell/issues/4063:204,Integrability,message,message,204,"Command:; ```bash; $ java -jar jars/cromwell-34.jar run does-not-exist.wdl > /dev/null; ```; This produces no output. Running the same command without redirecting STDOUT results in an error message. This message should likely be reported on STDERR, not STDOUT, to be consistent with common standards around command-line tools.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4063
https://github.com/broadinstitute/cromwell/issues/4064:2314,Deployability,pipeline,pipeline,2314,"d outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for the purpose of making caching more feasible for us. Suppose I took the two tasks I spoke of, one of which “passes” a large file to the second, and made them into a sub-workflow. And I mark the large files as “too big to keep” so they Cromwell would strip them out of the execution folder after the run completed. If caching were to work by looking at the inputs and outputs of the sub-workflow, and not at each task one by one, then it would be possible to cache the entire sub-workflow. Right?. Let’s say this sounds theoretically possible. Wouldn’t it be possible then, to skip making an actual sub-workflow at all to bracket the trashed intermediates, but just have clever Cromwell analyse the execution tree and the presence of “too big to keep” vaca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/issues/4064:2349,Energy Efficiency,reduce,reduce,2349,"d outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for the purpose of making caching more feasible for us. Suppose I took the two tasks I spoke of, one of which “passes” a large file to the second, and made them into a sub-workflow. And I mark the large files as “too big to keep” so they Cromwell would strip them out of the execution folder after the run completed. If caching were to work by looking at the inputs and outputs of the sub-workflow, and not at each task one by one, then it would be possible to cache the entire sub-workflow. Right?. Let’s say this sounds theoretically possible. Wouldn’t it be possible then, to skip making an actual sub-workflow at all to bracket the trashed intermediates, but just have clever Cromwell analyse the execution tree and the presence of “too big to keep” vaca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/issues/4064:845,Performance,cache,cache,845,"I would like to allow caching, although the size of the saved execution folder makes that prohibitive. I am suggesting a scheme in which large intermediate objects (files) can be removed by Cromwell at our suggestion, yet permit intact caching. In this scheme it would be possible to mark a task output as ""too big to keep, please remove when no longer referenced"" (or something more pithy). The object would be left in the execution folder until after its last mention and then removed (or at the very end of the workflow). Caching would then need to be modified to ""bracket"" the first task in which the object is mentioned as an output and the last task for which it is an input. This group of tasks would be considered as a group. The group of tasks would be skipped if the inputs to the first task and the outputs from the last task are all cache hits. (In case the group is not linear but some other DAG the scheme could be abandoned, or else perhaps the method would generalize with more complicated rules--I'm not sure.). Following is a motivation of the need for this feature, and sequence of arguments for why it can work.; ---------------------; We have a certain amount of input and output objects that we keep in their own buckets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/issues/4064:1340,Performance,cache,cache,1340," more pithy). The object would be left in the execution folder until after its last mention and then removed (or at the very end of the workflow). Caching would then need to be modified to ""bracket"" the first task in which the object is mentioned as an output and the last task for which it is an input. This group of tasks would be considered as a group. The group of tasks would be skipped if the inputs to the first task and the outputs from the last task are all cache hits. (In case the group is not linear but some other DAG the scheme could be abandoned, or else perhaps the method would generalize with more complicated rules--I'm not sure.). Following is a motivation of the need for this feature, and sequence of arguments for why it can work.; ---------------------; We have a certain amount of input and output objects that we keep in their own buckets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermedia",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/issues/4064:1575,Performance,cache,cached,1575," the object is mentioned as an output and the last task for which it is an input. This group of tasks would be considered as a group. The group of tasks would be skipped if the inputs to the first task and the outputs from the last task are all cache hits. (In case the group is not linear but some other DAG the scheme could be abandoned, or else perhaps the method would generalize with more complicated rules--I'm not sure.). Following is a motivation of the need for this feature, and sequence of arguments for why it can work.; ---------------------; We have a certain amount of input and output objects that we keep in their own buckets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/issues/4064:1655,Performance,cache,cache,1655,"ich it is an input. This group of tasks would be considered as a group. The group of tasks would be skipped if the inputs to the first task and the outputs from the last task are all cache hits. (In case the group is not linear but some other DAG the scheme could be abandoned, or else perhaps the method would generalize with more complicated rules--I'm not sure.). Following is a motivation of the need for this feature, and sequence of arguments for why it can work.; ---------------------; We have a certain amount of input and output objects that we keep in their own buckets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/issues/4064:1920,Performance,cache,cache,1920," the group is not linear but some other DAG the scheme could be abandoned, or else perhaps the method would generalize with more complicated rules--I'm not sure.). Following is a motivation of the need for this feature, and sequence of arguments for why it can work.; ---------------------; We have a certain amount of input and output objects that we keep in their own buckets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for the purpose of making caching more feasible for us. Suppose I took the two tasks I spoke of, one of which “passes” a large file to the second, and made them into a sub-workflow. And I mark the large files ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/issues/4064:2190,Performance,cache,cached,2190,"kets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for the purpose of making caching more feasible for us. Suppose I took the two tasks I spoke of, one of which “passes” a large file to the second, and made them into a sub-workflow. And I mark the large files as “too big to keep” so they Cromwell would strip them out of the execution folder after the run completed. If caching were to work by looking at the inputs and outputs of the sub-workflow, and not at each task one by one, then it would be possible to cache the entire sub-workflow. Right?. Let’s say this sounds theoretically possible. Wouldn’t it be possible then, to sk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/issues/4064:2241,Performance,cache,cached,2241,"kets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for the purpose of making caching more feasible for us. Suppose I took the two tasks I spoke of, one of which “passes” a large file to the second, and made them into a sub-workflow. And I mark the large files as “too big to keep” so they Cromwell would strip them out of the execution folder after the run completed. If caching were to work by looking at the inputs and outputs of the sub-workflow, and not at each task one by one, then it would be possible to cache the entire sub-workflow. Right?. Let’s say this sounds theoretically possible. Wouldn’t it be possible then, to sk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/issues/4064:3118,Performance,cache,cache,3118,"ntermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for the purpose of making caching more feasible for us. Suppose I took the two tasks I spoke of, one of which “passes” a large file to the second, and made them into a sub-workflow. And I mark the large files as “too big to keep” so they Cromwell would strip them out of the execution folder after the run completed. If caching were to work by looking at the inputs and outputs of the sub-workflow, and not at each task one by one, then it would be possible to cache the entire sub-workflow. Right?. Let’s say this sounds theoretically possible. Wouldn’t it be possible then, to skip making an actual sub-workflow at all to bracket the trashed intermediates, but just have clever Cromwell analyse the execution tree and the presence of “too big to keep” vacancies and allow the same optimization automatically.?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/issues/4064:3440,Performance,optimiz,optimization,3440,"ntermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for the purpose of making caching more feasible for us. Suppose I took the two tasks I spoke of, one of which “passes” a large file to the second, and made them into a sub-workflow. And I mark the large files as “too big to keep” so they Cromwell would strip them out of the execution folder after the run completed. If caching were to work by looking at the inputs and outputs of the sub-workflow, and not at each task one by one, then it would be possible to cache the entire sub-workflow. Right?. Let’s say this sounds theoretically possible. Wouldn’t it be possible then, to skip making an actual sub-workflow at all to bracket the trashed intermediates, but just have clever Cromwell analyse the execution tree and the presence of “too big to keep” vacancies and allow the same optimization automatically.?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/issues/4064:2205,Security,access,accessed,2205,"kets. The outputs are copied out of the execution folder after the workflow completes. These files we cache by reference, and that works fine, and we want to keep the inputs and outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. We’ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for the purpose of making caching more feasible for us. Suppose I took the two tasks I spoke of, one of which “passes” a large file to the second, and made them into a sub-workflow. And I mark the large files as “too big to keep” so they Cromwell would strip them out of the execution folder after the run completed. If caching were to work by looking at the inputs and outputs of the sub-workflow, and not at each task one by one, then it would be possible to cache the entire sub-workflow. Right?. Let’s say this sounds theoretically possible. Wouldn’t it be possible then, to sk",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064
https://github.com/broadinstitute/cromwell/pull/4065:50,Availability,error,errors,50,Closes #4051 . Also checks for and produces clear errors when tabs and spaces are mixed in a cmd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4065
https://github.com/broadinstitute/cromwell/pull/4065:44,Usability,clear,clear,44,Closes #4051 . Also checks for and produces clear errors when tabs and spaces are mixed in a cmd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4065
https://github.com/broadinstitute/cromwell/issues/4066:328,Energy Efficiency,reduce,reduce,328,"Today, there's a max workflow concurrency limit and by design it helps restrict how much work is being taken on by a single instance of Cromwell. However, when a root level workflow spawns a lot of subworkflows, those workflows don't count against the max workflow concurrency limit, and that can nullify the work being done to reduce load by putting in a max concurrency limit on workflows in the first place. AC: From a performance perspective, it would be great if sub-workflows were accounted for in the max workflow concurrency limit. So when Cromwell sweeps for new workflows, sub-workflows are accounted for in the max concurrent workflow counter before starting any new root workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4066
https://github.com/broadinstitute/cromwell/issues/4066:30,Performance,concurren,concurrency,30,"Today, there's a max workflow concurrency limit and by design it helps restrict how much work is being taken on by a single instance of Cromwell. However, when a root level workflow spawns a lot of subworkflows, those workflows don't count against the max workflow concurrency limit, and that can nullify the work being done to reduce load by putting in a max concurrency limit on workflows in the first place. AC: From a performance perspective, it would be great if sub-workflows were accounted for in the max workflow concurrency limit. So when Cromwell sweeps for new workflows, sub-workflows are accounted for in the max concurrent workflow counter before starting any new root workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4066
https://github.com/broadinstitute/cromwell/issues/4066:265,Performance,concurren,concurrency,265,"Today, there's a max workflow concurrency limit and by design it helps restrict how much work is being taken on by a single instance of Cromwell. However, when a root level workflow spawns a lot of subworkflows, those workflows don't count against the max workflow concurrency limit, and that can nullify the work being done to reduce load by putting in a max concurrency limit on workflows in the first place. AC: From a performance perspective, it would be great if sub-workflows were accounted for in the max workflow concurrency limit. So when Cromwell sweeps for new workflows, sub-workflows are accounted for in the max concurrent workflow counter before starting any new root workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4066
https://github.com/broadinstitute/cromwell/issues/4066:335,Performance,load,load,335,"Today, there's a max workflow concurrency limit and by design it helps restrict how much work is being taken on by a single instance of Cromwell. However, when a root level workflow spawns a lot of subworkflows, those workflows don't count against the max workflow concurrency limit, and that can nullify the work being done to reduce load by putting in a max concurrency limit on workflows in the first place. AC: From a performance perspective, it would be great if sub-workflows were accounted for in the max workflow concurrency limit. So when Cromwell sweeps for new workflows, sub-workflows are accounted for in the max concurrent workflow counter before starting any new root workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4066
https://github.com/broadinstitute/cromwell/issues/4066:360,Performance,concurren,concurrency,360,"Today, there's a max workflow concurrency limit and by design it helps restrict how much work is being taken on by a single instance of Cromwell. However, when a root level workflow spawns a lot of subworkflows, those workflows don't count against the max workflow concurrency limit, and that can nullify the work being done to reduce load by putting in a max concurrency limit on workflows in the first place. AC: From a performance perspective, it would be great if sub-workflows were accounted for in the max workflow concurrency limit. So when Cromwell sweeps for new workflows, sub-workflows are accounted for in the max concurrent workflow counter before starting any new root workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4066
https://github.com/broadinstitute/cromwell/issues/4066:422,Performance,perform,performance,422,"Today, there's a max workflow concurrency limit and by design it helps restrict how much work is being taken on by a single instance of Cromwell. However, when a root level workflow spawns a lot of subworkflows, those workflows don't count against the max workflow concurrency limit, and that can nullify the work being done to reduce load by putting in a max concurrency limit on workflows in the first place. AC: From a performance perspective, it would be great if sub-workflows were accounted for in the max workflow concurrency limit. So when Cromwell sweeps for new workflows, sub-workflows are accounted for in the max concurrent workflow counter before starting any new root workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4066
https://github.com/broadinstitute/cromwell/issues/4066:521,Performance,concurren,concurrency,521,"Today, there's a max workflow concurrency limit and by design it helps restrict how much work is being taken on by a single instance of Cromwell. However, when a root level workflow spawns a lot of subworkflows, those workflows don't count against the max workflow concurrency limit, and that can nullify the work being done to reduce load by putting in a max concurrency limit on workflows in the first place. AC: From a performance perspective, it would be great if sub-workflows were accounted for in the max workflow concurrency limit. So when Cromwell sweeps for new workflows, sub-workflows are accounted for in the max concurrent workflow counter before starting any new root workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4066
https://github.com/broadinstitute/cromwell/issues/4066:626,Performance,concurren,concurrent,626,"Today, there's a max workflow concurrency limit and by design it helps restrict how much work is being taken on by a single instance of Cromwell. However, when a root level workflow spawns a lot of subworkflows, those workflows don't count against the max workflow concurrency limit, and that can nullify the work being done to reduce load by putting in a max concurrency limit on workflows in the first place. AC: From a performance perspective, it would be great if sub-workflows were accounted for in the max workflow concurrency limit. So when Cromwell sweeps for new workflows, sub-workflows are accounted for in the max concurrent workflow counter before starting any new root workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4066
https://github.com/broadinstitute/cromwell/issues/4069:147,Availability,failure,failure,147,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069
https://github.com/broadinstitute/cromwell/issues/4069:392,Availability,failure,failure,392,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069
https://github.com/broadinstitute/cromwell/issues/4069:37,Deployability,integrat,integration,37,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069
https://github.com/broadinstitute/cromwell/issues/4069:443,Deployability,configurat,configuration,443,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069
https://github.com/broadinstitute/cromwell/issues/4069:37,Integrability,integrat,integration,37,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069
https://github.com/broadinstitute/cromwell/issues/4069:400,Integrability,message,message,400,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069
https://github.com/broadinstitute/cromwell/issues/4069:443,Modifiability,config,configuration,443,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069
https://github.com/broadinstitute/cromwell/issues/4069:49,Testability,test,tests,49,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069
https://github.com/broadinstitute/cromwell/issues/4069:333,Testability,Log,Log,333,As of recently-- some of the nightly integration tests have been failing with issues evaluating the contents of the RC file. . This is a transient failure that should already be retried -- and a part of the problem here is the inability to confirm if the operation was retried as expected. So there were two discussed solutions:; 1. Log the number of attempts to read a file as a part of the failure message for a job.; 2. There is a Cromwell configuration for the number of times an IO operation should be retried -- raise that number as a way to retry cloud hiccups.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4069
https://github.com/broadinstitute/cromwell/issues/4070:226,Availability,avail,available,226,"Edit (by @cjllanwarne) in light of #4806:. Following #4806 we will be able to read Google project metadata to specify a VPC network and subnet. Therefore what will remain for ***this*** ticket is making the same functionality available on a per-workflow basis... eg an ability to supply the same network/subnetwork information via workflow-options?. ---. ### Original issue text:. https://cloud.google.com/vpc/docs/vpc -- for a primer on GCP Subnets. Users should be able to tell Cromwell to launch nodes into a subnet. For environments like Firecloud, we should have some mechanism (like maybe SAM) to make sure the user actually has the right to use a particular subnet. . The main reason to do this is https://cloud.google.com/vpc/docs/using-flow-logs -- we want to be able to monitor traffic in and out of the network for more significant audited environments. So the driver is ultimately ""compliance"". But it's probably a good idea anyhow. After this is done, please work with FC team to make sure they can take advantage of this. I'm not sure who to tag to make sure this cross-team work is done.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070
https://github.com/broadinstitute/cromwell/issues/4070:780,Energy Efficiency,monitor,monitor,780,"Edit (by @cjllanwarne) in light of #4806:. Following #4806 we will be able to read Google project metadata to specify a VPC network and subnet. Therefore what will remain for ***this*** ticket is making the same functionality available on a per-workflow basis... eg an ability to supply the same network/subnetwork information via workflow-options?. ---. ### Original issue text:. https://cloud.google.com/vpc/docs/vpc -- for a primer on GCP Subnets. Users should be able to tell Cromwell to launch nodes into a subnet. For environments like Firecloud, we should have some mechanism (like maybe SAM) to make sure the user actually has the right to use a particular subnet. . The main reason to do this is https://cloud.google.com/vpc/docs/using-flow-logs -- we want to be able to monitor traffic in and out of the network for more significant audited environments. So the driver is ultimately ""compliance"". But it's probably a good idea anyhow. After this is done, please work with FC team to make sure they can take advantage of this. I'm not sure who to tag to make sure this cross-team work is done.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070
https://github.com/broadinstitute/cromwell/issues/4070:843,Security,audit,audited,843,"Edit (by @cjllanwarne) in light of #4806:. Following #4806 we will be able to read Google project metadata to specify a VPC network and subnet. Therefore what will remain for ***this*** ticket is making the same functionality available on a per-workflow basis... eg an ability to supply the same network/subnetwork information via workflow-options?. ---. ### Original issue text:. https://cloud.google.com/vpc/docs/vpc -- for a primer on GCP Subnets. Users should be able to tell Cromwell to launch nodes into a subnet. For environments like Firecloud, we should have some mechanism (like maybe SAM) to make sure the user actually has the right to use a particular subnet. . The main reason to do this is https://cloud.google.com/vpc/docs/using-flow-logs -- we want to be able to monitor traffic in and out of the network for more significant audited environments. So the driver is ultimately ""compliance"". But it's probably a good idea anyhow. After this is done, please work with FC team to make sure they can take advantage of this. I'm not sure who to tag to make sure this cross-team work is done.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070
https://github.com/broadinstitute/cromwell/issues/4070:750,Testability,log,logs,750,"Edit (by @cjllanwarne) in light of #4806:. Following #4806 we will be able to read Google project metadata to specify a VPC network and subnet. Therefore what will remain for ***this*** ticket is making the same functionality available on a per-workflow basis... eg an ability to supply the same network/subnetwork information via workflow-options?. ---. ### Original issue text:. https://cloud.google.com/vpc/docs/vpc -- for a primer on GCP Subnets. Users should be able to tell Cromwell to launch nodes into a subnet. For environments like Firecloud, we should have some mechanism (like maybe SAM) to make sure the user actually has the right to use a particular subnet. . The main reason to do this is https://cloud.google.com/vpc/docs/using-flow-logs -- we want to be able to monitor traffic in and out of the network for more significant audited environments. So the driver is ultimately ""compliance"". But it's probably a good idea anyhow. After this is done, please work with FC team to make sure they can take advantage of this. I'm not sure who to tag to make sure this cross-team work is done.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070
https://github.com/broadinstitute/cromwell/pull/4074:707,Deployability,configurat,configuration,707,"This is a PR to the already existing TESK PR to add some docs and a ""global filesystem config for ftp"". The reason is this:. - FTP servers can decide to limit how many connections they allow per IP address per user; - Cromwell Filesystems are defined as ""PathBuilderFactories"". A factory is configurable at the backend level and the engine level. Meaning there will be one instance of the factory per backend that chooses to enable a given filesystem, + one for the engine if the engine wants to support it too.; - However since we need to manage a pool of connections to the FTP server over the entire Cromwell instance, we can't create a new pool per factory.; - This introduces a ""global"" or ""singleton"" configuration object that can be defined in the root filesystem configuration (as java class path) and will be shared among all factories.; - A factory can choose to use it or not, if not then everything is as it was. If yes, a single instance of this singleton object will be created and passed to the factories when they're instantiated",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4074
https://github.com/broadinstitute/cromwell/pull/4074:771,Deployability,configurat,configuration,771,"This is a PR to the already existing TESK PR to add some docs and a ""global filesystem config for ftp"". The reason is this:. - FTP servers can decide to limit how many connections they allow per IP address per user; - Cromwell Filesystems are defined as ""PathBuilderFactories"". A factory is configurable at the backend level and the engine level. Meaning there will be one instance of the factory per backend that chooses to enable a given filesystem, + one for the engine if the engine wants to support it too.; - However since we need to manage a pool of connections to the FTP server over the entire Cromwell instance, we can't create a new pool per factory.; - This introduces a ""global"" or ""singleton"" configuration object that can be defined in the root filesystem configuration (as java class path) and will be shared among all factories.; - A factory can choose to use it or not, if not then everything is as it was. If yes, a single instance of this singleton object will be created and passed to the factories when they're instantiated",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4074
https://github.com/broadinstitute/cromwell/pull/4074:87,Modifiability,config,config,87,"This is a PR to the already existing TESK PR to add some docs and a ""global filesystem config for ftp"". The reason is this:. - FTP servers can decide to limit how many connections they allow per IP address per user; - Cromwell Filesystems are defined as ""PathBuilderFactories"". A factory is configurable at the backend level and the engine level. Meaning there will be one instance of the factory per backend that chooses to enable a given filesystem, + one for the engine if the engine wants to support it too.; - However since we need to manage a pool of connections to the FTP server over the entire Cromwell instance, we can't create a new pool per factory.; - This introduces a ""global"" or ""singleton"" configuration object that can be defined in the root filesystem configuration (as java class path) and will be shared among all factories.; - A factory can choose to use it or not, if not then everything is as it was. If yes, a single instance of this singleton object will be created and passed to the factories when they're instantiated",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4074
https://github.com/broadinstitute/cromwell/pull/4074:291,Modifiability,config,configurable,291,"This is a PR to the already existing TESK PR to add some docs and a ""global filesystem config for ftp"". The reason is this:. - FTP servers can decide to limit how many connections they allow per IP address per user; - Cromwell Filesystems are defined as ""PathBuilderFactories"". A factory is configurable at the backend level and the engine level. Meaning there will be one instance of the factory per backend that chooses to enable a given filesystem, + one for the engine if the engine wants to support it too.; - However since we need to manage a pool of connections to the FTP server over the entire Cromwell instance, we can't create a new pool per factory.; - This introduces a ""global"" or ""singleton"" configuration object that can be defined in the root filesystem configuration (as java class path) and will be shared among all factories.; - A factory can choose to use it or not, if not then everything is as it was. If yes, a single instance of this singleton object will be created and passed to the factories when they're instantiated",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4074
https://github.com/broadinstitute/cromwell/pull/4074:707,Modifiability,config,configuration,707,"This is a PR to the already existing TESK PR to add some docs and a ""global filesystem config for ftp"". The reason is this:. - FTP servers can decide to limit how many connections they allow per IP address per user; - Cromwell Filesystems are defined as ""PathBuilderFactories"". A factory is configurable at the backend level and the engine level. Meaning there will be one instance of the factory per backend that chooses to enable a given filesystem, + one for the engine if the engine wants to support it too.; - However since we need to manage a pool of connections to the FTP server over the entire Cromwell instance, we can't create a new pool per factory.; - This introduces a ""global"" or ""singleton"" configuration object that can be defined in the root filesystem configuration (as java class path) and will be shared among all factories.; - A factory can choose to use it or not, if not then everything is as it was. If yes, a single instance of this singleton object will be created and passed to the factories when they're instantiated",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4074
https://github.com/broadinstitute/cromwell/pull/4074:771,Modifiability,config,configuration,771,"This is a PR to the already existing TESK PR to add some docs and a ""global filesystem config for ftp"". The reason is this:. - FTP servers can decide to limit how many connections they allow per IP address per user; - Cromwell Filesystems are defined as ""PathBuilderFactories"". A factory is configurable at the backend level and the engine level. Meaning there will be one instance of the factory per backend that chooses to enable a given filesystem, + one for the engine if the engine wants to support it too.; - However since we need to manage a pool of connections to the FTP server over the entire Cromwell instance, we can't create a new pool per factory.; - This introduces a ""global"" or ""singleton"" configuration object that can be defined in the root filesystem configuration (as java class path) and will be shared among all factories.; - A factory can choose to use it or not, if not then everything is as it was. If yes, a single instance of this singleton object will be created and passed to the factories when they're instantiated",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4074
https://github.com/broadinstitute/cromwell/pull/4076:67,Testability,log,logic,67,wanted to get rid of . * thread.sleep; * exponential backoff retry logic,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4076
https://github.com/broadinstitute/cromwell/issues/4077:171,Security,hash,hash,171,"I do not see a good explanation in docs on how soft/hard links work in cromwell.; For instance, you say:; ```; # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path""; ```; here it is not clear how to make it check BOTH file content and the path. Or if path implies that both path and file is schecked. It is also not clear what will be the difference (other than this option) between hard and soft link localization.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077
https://github.com/broadinstitute/cromwell/issues/4077:227,Security,hash,hash,227,"I do not see a good explanation in docs on how soft/hard links work in cromwell.; For instance, you say:; ```; # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path""; ```; here it is not clear how to make it check BOTH file content and the path. Or if path implies that both path and file is schecked. It is also not clear what will be the difference (other than this option) between hard and soft link localization.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077
https://github.com/broadinstitute/cromwell/issues/4077:400,Security,hash,hashed,400,"I do not see a good explanation in docs on how soft/hard links work in cromwell.; For instance, you say:; ```; # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path""; ```; here it is not clear how to make it check BOTH file content and the path. Or if path implies that both path and file is schecked. It is also not clear what will be the difference (other than this option) between hard and soft link localization.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077
https://github.com/broadinstitute/cromwell/issues/4077:409,Security,hash,hashing-strategy,409,"I do not see a good explanation in docs on how soft/hard links work in cromwell.; For instance, you say:; ```; # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path""; ```; here it is not clear how to make it check BOTH file content and the path. Or if path implies that both path and file is schecked. It is also not clear what will be the difference (other than this option) between hard and soft link localization.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077
https://github.com/broadinstitute/cromwell/issues/4077:455,Usability,clear,clear,455,"I do not see a good explanation in docs on how soft/hard links work in cromwell.; For instance, you say:; ```; # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path""; ```; here it is not clear how to make it check BOTH file content and the path. Or if path implies that both path and file is schecked. It is also not clear what will be the difference (other than this option) between hard and soft link localization.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077
https://github.com/broadinstitute/cromwell/issues/4077:585,Usability,clear,clear,585,"I do not see a good explanation in docs on how soft/hard links work in cromwell.; For instance, you say:; ```; # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path""; ```; here it is not clear how to make it check BOTH file content and the path. Or if path implies that both path and file is schecked. It is also not clear what will be the difference (other than this option) between hard and soft link localization.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4077
https://github.com/broadinstitute/cromwell/pull/4078:15,Availability,error,error,15,"Bypass Mockito error ""Cannot cast to primitive type"" on classes also modified due to use of Refined.; Dependencies still reported out of date via `sbt dependencyUpdates`:; - com.aliyun:aliyun-java-sdk-core : 3.6.0 -> 3.7.1 -> 4.0.8; Waiting for https://github.com/aliyun/aliyun-openapi-java-sdk/issues/54; - com.github.pathikrit:better-files : 2.17.1 -> 3.6.0; Unstable API would probably require multiple changes; - com.google.apis:google-api-services-cloudkms : v1-rev63-1.25.0 -> InvalidVersion(v1beta1-rev6-1.22.0); False positive due to version not being SemVer; - com.google.apis:google-api-services-genomics : v2alpha1-rev31-1.25.0 -> InvalidVersion(v2alpha1-rev9-1.23.0); False positive due to version not being SemVer; - mysql:mysql-connector-java : 5.1.47 -> 8.0.12; See notes in Dependencies.scala on changes that would be required by users.; - org.broadinstitute.dsde.workbench:workbench-google : 0.15-2fc79a3 -> 0.15-ff73de5-SNAP -> 0.100-f9bd914-SNAP -> 1.0-e8e6ff0-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-model : 0.10-6800f3a -> 0.10-ff73de5-SNAP -> 0.12-e24d5a6-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-util : 0.3-f3ce961 -> 0.3-ff937c4-SNAP; Need more research to know what changed; - org.liquibase:liquibase-core : 3.5.5 -> 3.6.2; Waiting for https://liquibase.jira.com/browse/CORE-3311; - org.webjars:swagger-ui : 3.2.2 -> 3.18.2; Unstable API would probably require multiple changes; - software.amazon.awssdk:aws-sdk-java : 2.0.0-preview-9 -> 2.0.1; Waiting for https://github.com/broadinstitute/cromwell/issues/3909",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4078
https://github.com/broadinstitute/cromwell/pull/4078:102,Integrability,Depend,Dependencies,102,"Bypass Mockito error ""Cannot cast to primitive type"" on classes also modified due to use of Refined.; Dependencies still reported out of date via `sbt dependencyUpdates`:; - com.aliyun:aliyun-java-sdk-core : 3.6.0 -> 3.7.1 -> 4.0.8; Waiting for https://github.com/aliyun/aliyun-openapi-java-sdk/issues/54; - com.github.pathikrit:better-files : 2.17.1 -> 3.6.0; Unstable API would probably require multiple changes; - com.google.apis:google-api-services-cloudkms : v1-rev63-1.25.0 -> InvalidVersion(v1beta1-rev6-1.22.0); False positive due to version not being SemVer; - com.google.apis:google-api-services-genomics : v2alpha1-rev31-1.25.0 -> InvalidVersion(v2alpha1-rev9-1.23.0); False positive due to version not being SemVer; - mysql:mysql-connector-java : 5.1.47 -> 8.0.12; See notes in Dependencies.scala on changes that would be required by users.; - org.broadinstitute.dsde.workbench:workbench-google : 0.15-2fc79a3 -> 0.15-ff73de5-SNAP -> 0.100-f9bd914-SNAP -> 1.0-e8e6ff0-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-model : 0.10-6800f3a -> 0.10-ff73de5-SNAP -> 0.12-e24d5a6-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-util : 0.3-f3ce961 -> 0.3-ff937c4-SNAP; Need more research to know what changed; - org.liquibase:liquibase-core : 3.5.5 -> 3.6.2; Waiting for https://liquibase.jira.com/browse/CORE-3311; - org.webjars:swagger-ui : 3.2.2 -> 3.18.2; Unstable API would probably require multiple changes; - software.amazon.awssdk:aws-sdk-java : 2.0.0-preview-9 -> 2.0.1; Waiting for https://github.com/broadinstitute/cromwell/issues/3909",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4078
https://github.com/broadinstitute/cromwell/pull/4078:151,Integrability,depend,dependencyUpdates,151,"Bypass Mockito error ""Cannot cast to primitive type"" on classes also modified due to use of Refined.; Dependencies still reported out of date via `sbt dependencyUpdates`:; - com.aliyun:aliyun-java-sdk-core : 3.6.0 -> 3.7.1 -> 4.0.8; Waiting for https://github.com/aliyun/aliyun-openapi-java-sdk/issues/54; - com.github.pathikrit:better-files : 2.17.1 -> 3.6.0; Unstable API would probably require multiple changes; - com.google.apis:google-api-services-cloudkms : v1-rev63-1.25.0 -> InvalidVersion(v1beta1-rev6-1.22.0); False positive due to version not being SemVer; - com.google.apis:google-api-services-genomics : v2alpha1-rev31-1.25.0 -> InvalidVersion(v2alpha1-rev9-1.23.0); False positive due to version not being SemVer; - mysql:mysql-connector-java : 5.1.47 -> 8.0.12; See notes in Dependencies.scala on changes that would be required by users.; - org.broadinstitute.dsde.workbench:workbench-google : 0.15-2fc79a3 -> 0.15-ff73de5-SNAP -> 0.100-f9bd914-SNAP -> 1.0-e8e6ff0-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-model : 0.10-6800f3a -> 0.10-ff73de5-SNAP -> 0.12-e24d5a6-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-util : 0.3-f3ce961 -> 0.3-ff937c4-SNAP; Need more research to know what changed; - org.liquibase:liquibase-core : 3.5.5 -> 3.6.2; Waiting for https://liquibase.jira.com/browse/CORE-3311; - org.webjars:swagger-ui : 3.2.2 -> 3.18.2; Unstable API would probably require multiple changes; - software.amazon.awssdk:aws-sdk-java : 2.0.0-preview-9 -> 2.0.1; Waiting for https://github.com/broadinstitute/cromwell/issues/3909",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4078
https://github.com/broadinstitute/cromwell/pull/4078:790,Integrability,Depend,Dependencies,790,"Bypass Mockito error ""Cannot cast to primitive type"" on classes also modified due to use of Refined.; Dependencies still reported out of date via `sbt dependencyUpdates`:; - com.aliyun:aliyun-java-sdk-core : 3.6.0 -> 3.7.1 -> 4.0.8; Waiting for https://github.com/aliyun/aliyun-openapi-java-sdk/issues/54; - com.github.pathikrit:better-files : 2.17.1 -> 3.6.0; Unstable API would probably require multiple changes; - com.google.apis:google-api-services-cloudkms : v1-rev63-1.25.0 -> InvalidVersion(v1beta1-rev6-1.22.0); False positive due to version not being SemVer; - com.google.apis:google-api-services-genomics : v2alpha1-rev31-1.25.0 -> InvalidVersion(v2alpha1-rev9-1.23.0); False positive due to version not being SemVer; - mysql:mysql-connector-java : 5.1.47 -> 8.0.12; See notes in Dependencies.scala on changes that would be required by users.; - org.broadinstitute.dsde.workbench:workbench-google : 0.15-2fc79a3 -> 0.15-ff73de5-SNAP -> 0.100-f9bd914-SNAP -> 1.0-e8e6ff0-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-model : 0.10-6800f3a -> 0.10-ff73de5-SNAP -> 0.12-e24d5a6-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-util : 0.3-f3ce961 -> 0.3-ff937c4-SNAP; Need more research to know what changed; - org.liquibase:liquibase-core : 3.5.5 -> 3.6.2; Waiting for https://liquibase.jira.com/browse/CORE-3311; - org.webjars:swagger-ui : 3.2.2 -> 3.18.2; Unstable API would probably require multiple changes; - software.amazon.awssdk:aws-sdk-java : 2.0.0-preview-9 -> 2.0.1; Waiting for https://github.com/broadinstitute/cromwell/issues/3909",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4078
https://github.com/broadinstitute/cromwell/pull/4078:7,Testability,Mock,Mockito,7,"Bypass Mockito error ""Cannot cast to primitive type"" on classes also modified due to use of Refined.; Dependencies still reported out of date via `sbt dependencyUpdates`:; - com.aliyun:aliyun-java-sdk-core : 3.6.0 -> 3.7.1 -> 4.0.8; Waiting for https://github.com/aliyun/aliyun-openapi-java-sdk/issues/54; - com.github.pathikrit:better-files : 2.17.1 -> 3.6.0; Unstable API would probably require multiple changes; - com.google.apis:google-api-services-cloudkms : v1-rev63-1.25.0 -> InvalidVersion(v1beta1-rev6-1.22.0); False positive due to version not being SemVer; - com.google.apis:google-api-services-genomics : v2alpha1-rev31-1.25.0 -> InvalidVersion(v2alpha1-rev9-1.23.0); False positive due to version not being SemVer; - mysql:mysql-connector-java : 5.1.47 -> 8.0.12; See notes in Dependencies.scala on changes that would be required by users.; - org.broadinstitute.dsde.workbench:workbench-google : 0.15-2fc79a3 -> 0.15-ff73de5-SNAP -> 0.100-f9bd914-SNAP -> 1.0-e8e6ff0-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-model : 0.10-6800f3a -> 0.10-ff73de5-SNAP -> 0.12-e24d5a6-SNAP; Need more research to know what changed; - org.broadinstitute.dsde.workbench:workbench-util : 0.3-f3ce961 -> 0.3-ff937c4-SNAP; Need more research to know what changed; - org.liquibase:liquibase-core : 3.5.5 -> 3.6.2; Waiting for https://liquibase.jira.com/browse/CORE-3311; - org.webjars:swagger-ui : 3.2.2 -> 3.18.2; Unstable API would probably require multiple changes; - software.amazon.awssdk:aws-sdk-java : 2.0.0-preview-9 -> 2.0.1; Waiting for https://github.com/broadinstitute/cromwell/issues/3909",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4078
https://github.com/broadinstitute/cromwell/issues/4079:83,Availability,Echo,Echo,83,"When running workflow below I would expect to able to use this value:; `Test.test2.Echo.text`. But cromwell / womtools does use this value:; `Test.test2.Test2.Echo.text`. There is a double nested value there. Does not seems to be happening or was this intended?. tested version: d9b9262. test.wdl; ```; version 1.0. import ""test2.wdl"" as test2. workflow Test {; input {; }. call test2.Test2 as test2 {; input:; }. output {; }; }; ```. test2.wdl; ```; version 1.0. workflow Test2 {; input {; }. call Echo {; input:; }. output {; }; }. task Echo {; input {; String? text; }. command {; echo ~{text}; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079
https://github.com/broadinstitute/cromwell/issues/4079:159,Availability,Echo,Echo,159,"When running workflow below I would expect to able to use this value:; `Test.test2.Echo.text`. But cromwell / womtools does use this value:; `Test.test2.Test2.Echo.text`. There is a double nested value there. Does not seems to be happening or was this intended?. tested version: d9b9262. test.wdl; ```; version 1.0. import ""test2.wdl"" as test2. workflow Test {; input {; }. call test2.Test2 as test2 {; input:; }. output {; }; }; ```. test2.wdl; ```; version 1.0. workflow Test2 {; input {; }. call Echo {; input:; }. output {; }; }. task Echo {; input {; String? text; }. command {; echo ~{text}; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079
https://github.com/broadinstitute/cromwell/issues/4079:499,Availability,Echo,Echo,499,"When running workflow below I would expect to able to use this value:; `Test.test2.Echo.text`. But cromwell / womtools does use this value:; `Test.test2.Test2.Echo.text`. There is a double nested value there. Does not seems to be happening or was this intended?. tested version: d9b9262. test.wdl; ```; version 1.0. import ""test2.wdl"" as test2. workflow Test {; input {; }. call test2.Test2 as test2 {; input:; }. output {; }; }; ```. test2.wdl; ```; version 1.0. workflow Test2 {; input {; }. call Echo {; input:; }. output {; }; }. task Echo {; input {; String? text; }. command {; echo ~{text}; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079
https://github.com/broadinstitute/cromwell/issues/4079:539,Availability,Echo,Echo,539,"When running workflow below I would expect to able to use this value:; `Test.test2.Echo.text`. But cromwell / womtools does use this value:; `Test.test2.Test2.Echo.text`. There is a double nested value there. Does not seems to be happening or was this intended?. tested version: d9b9262. test.wdl; ```; version 1.0. import ""test2.wdl"" as test2. workflow Test {; input {; }. call test2.Test2 as test2 {; input:; }. output {; }; }; ```. test2.wdl; ```; version 1.0. workflow Test2 {; input {; }. call Echo {; input:; }. output {; }; }. task Echo {; input {; String? text; }. command {; echo ~{text}; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079
https://github.com/broadinstitute/cromwell/issues/4079:584,Availability,echo,echo,584,"When running workflow below I would expect to able to use this value:; `Test.test2.Echo.text`. But cromwell / womtools does use this value:; `Test.test2.Test2.Echo.text`. There is a double nested value there. Does not seems to be happening or was this intended?. tested version: d9b9262. test.wdl; ```; version 1.0. import ""test2.wdl"" as test2. workflow Test {; input {; }. call test2.Test2 as test2 {; input:; }. output {; }; }; ```. test2.wdl; ```; version 1.0. workflow Test2 {; input {; }. call Echo {; input:; }. output {; }; }. task Echo {; input {; String? text; }. command {; echo ~{text}; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079
https://github.com/broadinstitute/cromwell/issues/4079:72,Testability,Test,Test,72,"When running workflow below I would expect to able to use this value:; `Test.test2.Echo.text`. But cromwell / womtools does use this value:; `Test.test2.Test2.Echo.text`. There is a double nested value there. Does not seems to be happening or was this intended?. tested version: d9b9262. test.wdl; ```; version 1.0. import ""test2.wdl"" as test2. workflow Test {; input {; }. call test2.Test2 as test2 {; input:; }. output {; }; }; ```. test2.wdl; ```; version 1.0. workflow Test2 {; input {; }. call Echo {; input:; }. output {; }; }. task Echo {; input {; String? text; }. command {; echo ~{text}; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079
https://github.com/broadinstitute/cromwell/issues/4079:142,Testability,Test,Test,142,"When running workflow below I would expect to able to use this value:; `Test.test2.Echo.text`. But cromwell / womtools does use this value:; `Test.test2.Test2.Echo.text`. There is a double nested value there. Does not seems to be happening or was this intended?. tested version: d9b9262. test.wdl; ```; version 1.0. import ""test2.wdl"" as test2. workflow Test {; input {; }. call test2.Test2 as test2 {; input:; }. output {; }; }; ```. test2.wdl; ```; version 1.0. workflow Test2 {; input {; }. call Echo {; input:; }. output {; }; }. task Echo {; input {; String? text; }. command {; echo ~{text}; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079
https://github.com/broadinstitute/cromwell/issues/4079:263,Testability,test,tested,263,"When running workflow below I would expect to able to use this value:; `Test.test2.Echo.text`. But cromwell / womtools does use this value:; `Test.test2.Test2.Echo.text`. There is a double nested value there. Does not seems to be happening or was this intended?. tested version: d9b9262. test.wdl; ```; version 1.0. import ""test2.wdl"" as test2. workflow Test {; input {; }. call test2.Test2 as test2 {; input:; }. output {; }; }; ```. test2.wdl; ```; version 1.0. workflow Test2 {; input {; }. call Echo {; input:; }. output {; }; }. task Echo {; input {; String? text; }. command {; echo ~{text}; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079
https://github.com/broadinstitute/cromwell/issues/4079:288,Testability,test,test,288,"When running workflow below I would expect to able to use this value:; `Test.test2.Echo.text`. But cromwell / womtools does use this value:; `Test.test2.Test2.Echo.text`. There is a double nested value there. Does not seems to be happening or was this intended?. tested version: d9b9262. test.wdl; ```; version 1.0. import ""test2.wdl"" as test2. workflow Test {; input {; }. call test2.Test2 as test2 {; input:; }. output {; }; }; ```. test2.wdl; ```; version 1.0. workflow Test2 {; input {; }. call Echo {; input:; }. output {; }; }. task Echo {; input {; String? text; }. command {; echo ~{text}; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079
https://github.com/broadinstitute/cromwell/issues/4079:354,Testability,Test,Test,354,"When running workflow below I would expect to able to use this value:; `Test.test2.Echo.text`. But cromwell / womtools does use this value:; `Test.test2.Test2.Echo.text`. There is a double nested value there. Does not seems to be happening or was this intended?. tested version: d9b9262. test.wdl; ```; version 1.0. import ""test2.wdl"" as test2. workflow Test {; input {; }. call test2.Test2 as test2 {; input:; }. output {; }; }; ```. test2.wdl; ```; version 1.0. workflow Test2 {; input {; }. call Echo {; input:; }. output {; }; }. task Echo {; input {; String? text; }. command {; echo ~{text}; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079
https://github.com/broadinstitute/cromwell/issues/4080:592,Availability,error,error,592,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080
https://github.com/broadinstitute/cromwell/issues/4080:664,Availability,avail,available,664,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080
https://github.com/broadinstitute/cromwell/issues/4080:750,Availability,error,errors,750,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080
https://github.com/broadinstitute/cromwell/issues/4080:643,Deployability,configurat,configuration,643,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080
https://github.com/broadinstitute/cromwell/issues/4080:1813,Deployability,release,release,1813,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080
https://github.com/broadinstitute/cromwell/issues/4080:643,Modifiability,config,configuration,643,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080
https://github.com/broadinstitute/cromwell/issues/4080:1634,Modifiability,config,config,1634,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080
https://github.com/broadinstitute/cromwell/issues/4080:1862,Modifiability,config,config,1862,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080
https://github.com/broadinstitute/cromwell/issues/4080:685,Testability,test,tests,685,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080
https://github.com/broadinstitute/cromwell/issues/4081:298,Availability,error,error,298,Based on this report on the forums: https://gatkforums.broadinstitute.org/wdl/discussion/12878/exception-in-thread-main-scala-matcherror-null-validating-my-wdl. In this case the mistake was using `if (is_exome !=) {` instead of `if (!is_exome)` - but that should be nicely turned into a reportable error... rather than throwing up some obtuse scala-internals error message.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4081
https://github.com/broadinstitute/cromwell/issues/4081:359,Availability,error,error,359,Based on this report on the forums: https://gatkforums.broadinstitute.org/wdl/discussion/12878/exception-in-thread-main-scala-matcherror-null-validating-my-wdl. In this case the mistake was using `if (is_exome !=) {` instead of `if (!is_exome)` - but that should be nicely turned into a reportable error... rather than throwing up some obtuse scala-internals error message.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4081
https://github.com/broadinstitute/cromwell/issues/4081:365,Integrability,message,message,365,Based on this report on the forums: https://gatkforums.broadinstitute.org/wdl/discussion/12878/exception-in-thread-main-scala-matcherror-null-validating-my-wdl. In this case the mistake was using `if (is_exome !=) {` instead of `if (!is_exome)` - but that should be nicely turned into a reportable error... rather than throwing up some obtuse scala-internals error message.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4081
https://github.com/broadinstitute/cromwell/issues/4081:142,Security,validat,validating-my-wdl,142,Based on this report on the forums: https://gatkforums.broadinstitute.org/wdl/discussion/12878/exception-in-thread-main-scala-matcherror-null-validating-my-wdl. In this case the mistake was using `if (is_exome !=) {` instead of `if (!is_exome)` - but that should be nicely turned into a reportable error... rather than throwing up some obtuse scala-internals error message.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4081
https://github.com/broadinstitute/cromwell/issues/4082:891,Availability,error,error,891,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082
https://github.com/broadinstitute/cromwell/issues/4082:1181,Availability,Error,Error,1181,"our question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode); ```. Any idea, what is going wrong here?; Thanks a lot!. Cheers,; Flo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082
https://github.com/broadinstitute/cromwell/issues/4082:709,Deployability,configurat,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082
https://github.com/broadinstitute/cromwell/issues/4082:709,Modifiability,config,configuration,709,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082
https://github.com/broadinstitute/cromwell/issues/4082:754,Security,PASSWORD,PASSWORDS,754,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082
https://github.com/broadinstitute/cromwell/issues/4082:1100,Security,Hash,HashMap,1100,"eedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode); ```. Any ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082
https://github.com/broadinstitute/cromwell/issues/4082:1108,Security,Hash,HashTrieMap,1108,"eedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode); ```. Any ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082
https://github.com/broadinstitute/cromwell/issues/4082:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Hi folks,. I try to launch cromwell in its server mode, however I get the following error:. ```; java -jar ./cromwell-34.jar server; Exception in thread ""main"" java.lang.VerifyError: Uninitialized object exists on backward branch 209; Exception Details:; Location:; scala/collection/immutable/HashMap$HashTrieMap.split()Lscala/collection/immutable/Seq; @249: goto; Reason:; Error exists in the bytecode; Bytecode:; 0x0000000: 2ab6 0060 04a0 001e b200 b8b2 00bd 04bd; 0x0000010: 0002 5903 2a53 c000 bfb6 00c3 b600 c7c0; 0x0000020: 00c9 b02a b600 36b8 0040 3c1b 04a4 015e; 0x0000030: 1b05 6c3d 2a1b 056c 2ab6 0036 b700 cb3e; 0x0000040: 2ab6 0036 021d 787e 3604 2ab6 0036 0210; 0x0000050: 201d 647c 7e36 05bb 0019 59b2 00bd 2ab6; 0x0000060: 0038 c000 bfb6 00cf b700 d21c b600 d63a; 0x0000070: 0619 06c6 001a 1906 b600 dac0 0086 3a07; 0x0000080: 1906 b600 ddc0 0086 3a08 a700 0dbb 00df; 0x0000090: 5919 06b7 00e2 bf19 073a 0919 083a 0abb; 0x00000a0: 0002 5915 0419 09bb 0019 59b2 00bd 1909; 0x00000b0: c000 bfb6 00cf b700 d203 b800 e83a 0e3a. ```. OS: redhat 6.9 ; Java: ; ```; java -version; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082
https://github.com/broadinstitute/cromwell/pull/4083:411,Modifiability,evolve,evolve,411,"@danbills I was looking at the EJEA and on second thoughts it seemed a lot easier to just have it push the state transition to statsD directly.; I was thinking we could still use the ammonite script to aggregate transition time statistics about a single test run (which we can't really easily do in grafana) in a SQL table like you suggested before.; This way we could compare test by test how the measurements evolve, while grafana would show us ""real time"" how the timings are moving (which may even be useful in prod). Let me know what you think. Example from a test run (Y axis is ms):. <img width=""1416"" alt=""screen shot 2018-09-11 at 9 27 48 am"" src=""https://user-images.githubusercontent.com/2978948/45363248-02712180-b5a5-11e8-987f-f128e8bd3789.png"">. ~~Unrelated but I also took the opportunity to make execution events be pushed as they happen (the EJEA ones, not the backend ones - yet). This will hopefully give users more visibility on what's going on with their job in the timing diagram~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4083
https://github.com/broadinstitute/cromwell/pull/4083:254,Testability,test,test,254,"@danbills I was looking at the EJEA and on second thoughts it seemed a lot easier to just have it push the state transition to statsD directly.; I was thinking we could still use the ammonite script to aggregate transition time statistics about a single test run (which we can't really easily do in grafana) in a SQL table like you suggested before.; This way we could compare test by test how the measurements evolve, while grafana would show us ""real time"" how the timings are moving (which may even be useful in prod). Let me know what you think. Example from a test run (Y axis is ms):. <img width=""1416"" alt=""screen shot 2018-09-11 at 9 27 48 am"" src=""https://user-images.githubusercontent.com/2978948/45363248-02712180-b5a5-11e8-987f-f128e8bd3789.png"">. ~~Unrelated but I also took the opportunity to make execution events be pushed as they happen (the EJEA ones, not the backend ones - yet). This will hopefully give users more visibility on what's going on with their job in the timing diagram~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4083
https://github.com/broadinstitute/cromwell/pull/4083:377,Testability,test,test,377,"@danbills I was looking at the EJEA and on second thoughts it seemed a lot easier to just have it push the state transition to statsD directly.; I was thinking we could still use the ammonite script to aggregate transition time statistics about a single test run (which we can't really easily do in grafana) in a SQL table like you suggested before.; This way we could compare test by test how the measurements evolve, while grafana would show us ""real time"" how the timings are moving (which may even be useful in prod). Let me know what you think. Example from a test run (Y axis is ms):. <img width=""1416"" alt=""screen shot 2018-09-11 at 9 27 48 am"" src=""https://user-images.githubusercontent.com/2978948/45363248-02712180-b5a5-11e8-987f-f128e8bd3789.png"">. ~~Unrelated but I also took the opportunity to make execution events be pushed as they happen (the EJEA ones, not the backend ones - yet). This will hopefully give users more visibility on what's going on with their job in the timing diagram~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4083
https://github.com/broadinstitute/cromwell/pull/4083:385,Testability,test,test,385,"@danbills I was looking at the EJEA and on second thoughts it seemed a lot easier to just have it push the state transition to statsD directly.; I was thinking we could still use the ammonite script to aggregate transition time statistics about a single test run (which we can't really easily do in grafana) in a SQL table like you suggested before.; This way we could compare test by test how the measurements evolve, while grafana would show us ""real time"" how the timings are moving (which may even be useful in prod). Let me know what you think. Example from a test run (Y axis is ms):. <img width=""1416"" alt=""screen shot 2018-09-11 at 9 27 48 am"" src=""https://user-images.githubusercontent.com/2978948/45363248-02712180-b5a5-11e8-987f-f128e8bd3789.png"">. ~~Unrelated but I also took the opportunity to make execution events be pushed as they happen (the EJEA ones, not the backend ones - yet). This will hopefully give users more visibility on what's going on with their job in the timing diagram~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4083
https://github.com/broadinstitute/cromwell/pull/4083:565,Testability,test,test,565,"@danbills I was looking at the EJEA and on second thoughts it seemed a lot easier to just have it push the state transition to statsD directly.; I was thinking we could still use the ammonite script to aggregate transition time statistics about a single test run (which we can't really easily do in grafana) in a SQL table like you suggested before.; This way we could compare test by test how the measurements evolve, while grafana would show us ""real time"" how the timings are moving (which may even be useful in prod). Let me know what you think. Example from a test run (Y axis is ms):. <img width=""1416"" alt=""screen shot 2018-09-11 at 9 27 48 am"" src=""https://user-images.githubusercontent.com/2978948/45363248-02712180-b5a5-11e8-987f-f128e8bd3789.png"">. ~~Unrelated but I also took the opportunity to make execution events be pushed as they happen (the EJEA ones, not the backend ones - yet). This will hopefully give users more visibility on what's going on with their job in the timing diagram~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4083
https://github.com/broadinstitute/cromwell/issues/4084:395,Deployability,configurat,configuration,395,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:465,Deployability,configurat,configuration,465,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:923,Deployability,configurat,configuration,923,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:977,Deployability,configurat,configuration,977,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:1060,Deployability,configurat,configuration,1060,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:1800,Deployability,configurat,configuration,1800,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:395,Modifiability,config,configuration,395,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:465,Modifiability,config,configuration,465,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:923,Modifiability,config,configuration,923,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:977,Modifiability,config,configuration,977,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:1060,Modifiability,config,configuration,1060,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:1800,Modifiability,config,configuration,1800,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:237,Security,access,access,237,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:1845,Security,PASSWORD,PASSWORDS,1845,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/issues/4084:1138,Usability,feedback,feedback,1138,"I'm trying to run a WDL file which specifies a Docker container in the runtime attributes of a task. I'm trying to do so on an SGE HPC with a shared file system. Running this with docker is not an option because I don't have root rights/access. Instead, I'm trying to run it using Singularity. This works fine when using a custom runtime attribute to define the container and using the `submit` configuration. However, if the `docker` attribute and `submit-docker` configuration are used I run into a problem:; Cromwell will use `docker_cwd` (instead of `cwd`) in the call's script. `docker_cwd` however does not exist in the container and can therefore not be mounted (due to a lack of sudo rights), like you would normally do when using Docker. The result is that the job will fail because it can't find a folder that is referenced in the script. Is there some way for me to override the `docker_cwd` value in my backend configuration? I would prefer not to use the `submit` configuration, as not all tasks currently list a container, so I need the `submit` configuration for those tasks. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4084
https://github.com/broadinstitute/cromwell/pull/4085:95,Safety,sanity check,sanity check,95,"@Horneth I think this was originally added by you (but git blame might be lying...), could you sanity check this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4085
https://github.com/broadinstitute/cromwell/issues/4086:215,Availability,error,error,215,<!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Submitting an empty file as a `workflowInputs` causes cromwell to return an unhelpful ; `There was an internal server error.%`. <!-- Which backend are you running? -->; It happens on both PAPIv1 and PAPIv2 (tested on cromwell 33 and 34) . <!-- Paste/Attach your workflow if possible: -->; running the following:. ` ``; curl -s -v -F workflowSource=empty -F workflowInputs=empty https://cromwell-v34.dsde-methods.broadinstitute.org/api/workflows/v1; ```; results in:; ```; There was an internal server error.%; ```. The file `empty` is an empty file. It would be nice if it returned a more useful error message to tell you what the problem was.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4086
https://github.com/broadinstitute/cromwell/issues/4086:598,Availability,error,error,598,<!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Submitting an empty file as a `workflowInputs` causes cromwell to return an unhelpful ; `There was an internal server error.%`. <!-- Which backend are you running? -->; It happens on both PAPIv1 and PAPIv2 (tested on cromwell 33 and 34) . <!-- Paste/Attach your workflow if possible: -->; running the following:. ` ``; curl -s -v -F workflowSource=empty -F workflowInputs=empty https://cromwell-v34.dsde-methods.broadinstitute.org/api/workflows/v1; ```; results in:; ```; There was an internal server error.%; ```. The file `empty` is an empty file. It would be nice if it returned a more useful error message to tell you what the problem was.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4086
https://github.com/broadinstitute/cromwell/issues/4086:693,Availability,error,error,693,<!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Submitting an empty file as a `workflowInputs` causes cromwell to return an unhelpful ; `There was an internal server error.%`. <!-- Which backend are you running? -->; It happens on both PAPIv1 and PAPIv2 (tested on cromwell 33 and 34) . <!-- Paste/Attach your workflow if possible: -->; running the following:. ` ``; curl -s -v -F workflowSource=empty -F workflowInputs=empty https://cromwell-v34.dsde-methods.broadinstitute.org/api/workflows/v1; ```; results in:; ```; There was an internal server error.%; ```. The file `empty` is an empty file. It would be nice if it returned a more useful error message to tell you what the problem was.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4086
https://github.com/broadinstitute/cromwell/issues/4086:699,Integrability,message,message,699,<!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Submitting an empty file as a `workflowInputs` causes cromwell to return an unhelpful ; `There was an internal server error.%`. <!-- Which backend are you running? -->; It happens on both PAPIv1 and PAPIv2 (tested on cromwell 33 and 34) . <!-- Paste/Attach your workflow if possible: -->; running the following:. ` ``; curl -s -v -F workflowSource=empty -F workflowInputs=empty https://cromwell-v34.dsde-methods.broadinstitute.org/api/workflows/v1; ```; results in:; ```; There was an internal server error.%; ```. The file `empty` is an empty file. It would be nice if it returned a more useful error message to tell you what the problem was.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4086
https://github.com/broadinstitute/cromwell/issues/4086:304,Testability,test,tested,304,<!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Submitting an empty file as a `workflowInputs` causes cromwell to return an unhelpful ; `There was an internal server error.%`. <!-- Which backend are you running? -->; It happens on both PAPIv1 and PAPIv2 (tested on cromwell 33 and 34) . <!-- Paste/Attach your workflow if possible: -->; running the following:. ` ``; curl -s -v -F workflowSource=empty -F workflowInputs=empty https://cromwell-v34.dsde-methods.broadinstitute.org/api/workflows/v1; ```; results in:; ```; There was an internal server error.%; ```. The file `empty` is an empty file. It would be nice if it returned a more useful error message to tell you what the problem was.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4086
https://github.com/broadinstitute/cromwell/pull/4088:126,Availability,avail,available,126,"Fixes #4084 ; For singularity users it is nice that `dockerRoot` can be set, as they do not have control over which paths are available in the image and `/cromwell-executions` cannot be automatically created. Other paths could be used, BioContainers for example have a `/data` folder meant specifically for these use cases. But this requires dockerRoot to be configurable. The fallback value is still `/cromwell-executions` so nothing changes if the value is not specified in the config.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088
https://github.com/broadinstitute/cromwell/pull/4088:359,Modifiability,config,configurable,359,"Fixes #4084 ; For singularity users it is nice that `dockerRoot` can be set, as they do not have control over which paths are available in the image and `/cromwell-executions` cannot be automatically created. Other paths could be used, BioContainers for example have a `/data` folder meant specifically for these use cases. But this requires dockerRoot to be configurable. The fallback value is still `/cromwell-executions` so nothing changes if the value is not specified in the config.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088
https://github.com/broadinstitute/cromwell/pull/4088:480,Modifiability,config,config,480,"Fixes #4084 ; For singularity users it is nice that `dockerRoot` can be set, as they do not have control over which paths are available in the image and `/cromwell-executions` cannot be automatically created. Other paths could be used, BioContainers for example have a `/data` folder meant specifically for these use cases. But this requires dockerRoot to be configurable. The fallback value is still `/cromwell-executions` so nothing changes if the value is not specified in the config.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088
https://github.com/broadinstitute/cromwell/issues/4089:99,Availability,error,error,99,"I was writing a wdl with this line in the task definition. `Float vaf = .01`. and was getting this error when validating . `Error: Invalid WDL: ERROR: Unexpected symbol (line 28, col 16) when parsing 'e'. Expected identifier, got 01. Float vaf = .01 ^ $e = :identifier <=> :dot :identifier -> MemberAccess( lhs=$0, rhs=$2 )`. if I change it to; ; `Float vaf = 0.01`. it's fine but I think looking at the spec the first iteration should work too - https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. `$float = (([0-9]+)?\.([0-9]+)|[0-9]+\.|[0-9]+)([eE][-+]?[0-9]+)?`. Easily worked around just a little annoying. This was run on a FireCloud method so I assume its one of the more recent versions? (cromwell 34) but 🤷‍♂️",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4089
https://github.com/broadinstitute/cromwell/issues/4089:124,Availability,Error,Error,124,"I was writing a wdl with this line in the task definition. `Float vaf = .01`. and was getting this error when validating . `Error: Invalid WDL: ERROR: Unexpected symbol (line 28, col 16) when parsing 'e'. Expected identifier, got 01. Float vaf = .01 ^ $e = :identifier <=> :dot :identifier -> MemberAccess( lhs=$0, rhs=$2 )`. if I change it to; ; `Float vaf = 0.01`. it's fine but I think looking at the spec the first iteration should work too - https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. `$float = (([0-9]+)?\.([0-9]+)|[0-9]+\.|[0-9]+)([eE][-+]?[0-9]+)?`. Easily worked around just a little annoying. This was run on a FireCloud method so I assume its one of the more recent versions? (cromwell 34) but 🤷‍♂️",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4089
https://github.com/broadinstitute/cromwell/issues/4089:144,Availability,ERROR,ERROR,144,"I was writing a wdl with this line in the task definition. `Float vaf = .01`. and was getting this error when validating . `Error: Invalid WDL: ERROR: Unexpected symbol (line 28, col 16) when parsing 'e'. Expected identifier, got 01. Float vaf = .01 ^ $e = :identifier <=> :dot :identifier -> MemberAccess( lhs=$0, rhs=$2 )`. if I change it to; ; `Float vaf = 0.01`. it's fine but I think looking at the spec the first iteration should work too - https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. `$float = (([0-9]+)?\.([0-9]+)|[0-9]+\.|[0-9]+)([eE][-+]?[0-9]+)?`. Easily worked around just a little annoying. This was run on a FireCloud method so I assume its one of the more recent versions? (cromwell 34) but 🤷‍♂️",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4089
https://github.com/broadinstitute/cromwell/issues/4089:110,Security,validat,validating,110,"I was writing a wdl with this line in the task definition. `Float vaf = .01`. and was getting this error when validating . `Error: Invalid WDL: ERROR: Unexpected symbol (line 28, col 16) when parsing 'e'. Expected identifier, got 01. Float vaf = .01 ^ $e = :identifier <=> :dot :identifier -> MemberAccess( lhs=$0, rhs=$2 )`. if I change it to; ; `Float vaf = 0.01`. it's fine but I think looking at the spec the first iteration should work too - https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. `$float = (([0-9]+)?\.([0-9]+)|[0-9]+\.|[0-9]+)([eE][-+]?[0-9]+)?`. Easily worked around just a little annoying. This was run on a FireCloud method so I assume its one of the more recent versions? (cromwell 34) but 🤷‍♂️",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4089
https://github.com/broadinstitute/cromwell/pull/4090:136,Availability,down,download,136,"This PR adds the ability to launch a workflow as part of the Jenkins job. To do this you need to:. - create a workflow script which can download the necessary workflow files (source, input, options,..) and make `curl POST` request to Cromwell; - put the script inside cromwell -> scripts -> perf -> vm_scripts -> workflow_scripts folder; - pass the name of this script through `WORKFLOW_SCRIPT` param while building the Jenkins job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4090
https://github.com/broadinstitute/cromwell/pull/4091:266,Deployability,Update,Update,266,"Part II of the ""unexpected `JobFailedNonRetryable` during cache output copying"" saga. - Stop using the confusing `JobFailedNonRetryable` response when an output copy fails (it's not like `JobSucceeded` which correctly tells us that the job has already succeeded); - Update the tests to reflect this; - Add some sanity checks to the EJEA's handlers (specifically - did we copy the right outputs, and did we fetch the right outputs from the database)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4091
https://github.com/broadinstitute/cromwell/pull/4091:58,Performance,cache,cache,58,"Part II of the ""unexpected `JobFailedNonRetryable` during cache output copying"" saga. - Stop using the confusing `JobFailedNonRetryable` response when an output copy fails (it's not like `JobSucceeded` which correctly tells us that the job has already succeeded); - Update the tests to reflect this; - Add some sanity checks to the EJEA's handlers (specifically - did we copy the right outputs, and did we fetch the right outputs from the database)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4091
https://github.com/broadinstitute/cromwell/pull/4091:311,Safety,sanity check,sanity checks,311,"Part II of the ""unexpected `JobFailedNonRetryable` during cache output copying"" saga. - Stop using the confusing `JobFailedNonRetryable` response when an output copy fails (it's not like `JobSucceeded` which correctly tells us that the job has already succeeded); - Update the tests to reflect this; - Add some sanity checks to the EJEA's handlers (specifically - did we copy the right outputs, and did we fetch the right outputs from the database)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4091
https://github.com/broadinstitute/cromwell/pull/4091:277,Testability,test,tests,277,"Part II of the ""unexpected `JobFailedNonRetryable` during cache output copying"" saga. - Stop using the confusing `JobFailedNonRetryable` response when an output copy fails (it's not like `JobSucceeded` which correctly tells us that the job has already succeeded); - Update the tests to reflect this; - Add some sanity checks to the EJEA's handlers (specifically - did we copy the right outputs, and did we fetch the right outputs from the database)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4091
https://github.com/broadinstitute/cromwell/issues/4092:439,Availability,error,error,439,"I have a command that creates an output file, and prints the name of that file to a known file. EG `./program` puts its output in 1234.txt, and prints '1234' into output_name. I thought I could do this in my task:. ```; command <<<; ./program; >>>; String filename = read_string('output_name'); output {; File output = ""~{filename}.txt""; }; ```. However this fails, because apparently read_string is executed before command. (no such file error, cromwell never calls the backend to execute the command). I tried replacing `File output = ""~{filename}.txt""` with `File output = ""~{read_string('filename')}.txt""`. This does not work as arbitrary expressions are not yet allowed in string interpolation:. `Unable to start job due to: No implementation of FileEvaluator[StringExpression]`. Is there anything I am doing wrong? Can I make read_string execute after command?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4092
https://github.com/broadinstitute/cromwell/issues/4093:161,Availability,recover,recovered,161,"**What happened:**; On 9/12/18 at 3:41 pm, the Cromwell servers' (801 and 802) CPU were pegged. 801 was restarted (4:11 pm) and immediately got pegged again but recovered 35 min later without further intervention. As for 802, the CPU remained pegged so, at 5:21 pm, 802 was restarted and recovered right away.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093
https://github.com/broadinstitute/cromwell/issues/4093:288,Availability,recover,recovered,288,"**What happened:**; On 9/12/18 at 3:41 pm, the Cromwell servers' (801 and 802) CPU were pegged. 801 was restarted (4:11 pm) and immediately got pegged again but recovered 35 min later without further intervention. As for 802, the CPU remained pegged so, at 5:21 pm, 802 was restarted and recovered right away.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093
https://github.com/broadinstitute/cromwell/issues/4093:161,Safety,recover,recovered,161,"**What happened:**; On 9/12/18 at 3:41 pm, the Cromwell servers' (801 and 802) CPU were pegged. 801 was restarted (4:11 pm) and immediately got pegged again but recovered 35 min later without further intervention. As for 802, the CPU remained pegged so, at 5:21 pm, 802 was restarted and recovered right away.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093
https://github.com/broadinstitute/cromwell/issues/4093:288,Safety,recover,recovered,288,"**What happened:**; On 9/12/18 at 3:41 pm, the Cromwell servers' (801 and 802) CPU were pegged. 801 was restarted (4:11 pm) and immediately got pegged again but recovered 35 min later without further intervention. As for 802, the CPU remained pegged so, at 5:21 pm, 802 was restarted and recovered right away.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4093
https://github.com/broadinstitute/cromwell/issues/4094:124,Availability,recover,recovered,124,"**What Happened**; On 9/12/18 5:40 pm, after a Firecloud release, Cromwell 402 stopped responding to status checks. It only recovered after being restarted at 10pm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4094
https://github.com/broadinstitute/cromwell/issues/4094:57,Deployability,release,release,57,"**What Happened**; On 9/12/18 5:40 pm, after a Firecloud release, Cromwell 402 stopped responding to status checks. It only recovered after being restarted at 10pm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4094
https://github.com/broadinstitute/cromwell/issues/4094:124,Safety,recover,recovered,124,"**What Happened**; On 9/12/18 5:40 pm, after a Firecloud release, Cromwell 402 stopped responding to status checks. It only recovered after being restarted at 10pm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4094
https://github.com/broadinstitute/cromwell/pull/4095:108,Security,access,access,108,"Closes #3811 . Use a hint to the resolver to ignore the local scope of a `WdlTaskCall` and start the member access search in the next higher scope. ---. Deleted earlier comment about allowing search only in earlier lines within the same scope because it wouldn't help with; ```; call y as shouldntBeProblematic {; input:; cram = ""asdf"",; slam = cram.scram; }; ```; which is equally as problematic as; ```; call y as shouldntBeProblematic {; input:; cram = cram.scram; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4095
https://github.com/broadinstitute/cromwell/issues/4096:253,Performance,cache,cache,253,Probably not as horrible as it sounds. Use BQ to determine how many files appear as inputs to calls for a $5 workflow run and how many times each appears over the course of the entire workflow. If files are appearing multiple times we could implement a cache of file hashes to prevent needlessly requesting the hashes for files multiple times.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4096
https://github.com/broadinstitute/cromwell/issues/4096:267,Security,hash,hashes,267,Probably not as horrible as it sounds. Use BQ to determine how many files appear as inputs to calls for a $5 workflow run and how many times each appears over the course of the entire workflow. If files are appearing multiple times we could implement a cache of file hashes to prevent needlessly requesting the hashes for files multiple times.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4096
https://github.com/broadinstitute/cromwell/issues/4096:311,Security,hash,hashes,311,Probably not as horrible as it sounds. Use BQ to determine how many files appear as inputs to calls for a $5 workflow run and how many times each appears over the course of the entire workflow. If files are appearing multiple times we could implement a cache of file hashes to prevent needlessly requesting the hashes for files multiple times.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4096
https://github.com/broadinstitute/cromwell/issues/4099:686,Availability,error,error,686,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099
https://github.com/broadinstitute/cromwell/issues/4099:754,Availability,error,error,754,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099
https://github.com/broadinstitute/cromwell/issues/4099:819,Availability,error,error,819,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099
https://github.com/broadinstitute/cromwell/issues/4099:97,Deployability,Upgrade,Upgrade,97,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099
https://github.com/broadinstitute/cromwell/issues/4099:314,Deployability,upgrade,upgrade,314,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099
https://github.com/broadinstitute/cromwell/issues/4099:373,Deployability,Upgrade,Upgrade,373,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099
https://github.com/broadinstitute/cromwell/issues/4099:438,Modifiability,config,config,438,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099
https://github.com/broadinstitute/cromwell/issues/4099:300,Performance,cache,cached,300,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099
https://github.com/broadinstitute/cromwell/issues/4099:524,Testability,Log,Log,524,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099
https://github.com/broadinstitute/cromwell/issues/4099:656,Testability,log,logs,656,"Basic Expectations:; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade →; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; - ~Log names (detritus files) don’t change (before and after), and if yes, then print a warning~ Split into Issue: #4188; - ~Streaming logs (before and after), else error~ Split into Issue: #4187; - ~Caching (before and after), else error~ PR merged: #4178; - ~Job success (before and after), else error~ PR merged: #4132",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4099
https://github.com/broadinstitute/cromwell/issues/4100:65,Modifiability,Config,Configuring,65,"at https://github.com/broadinstitute/cromwell/blame/develop/docs/Configuring.md#L184; add a note that the local backend should not run dockerized tasks, per; https://gatkforums.broadinstitute.org/wdl/discussion/9581/whats-the-best-practice-way-to-run-cromwell-server-in-docker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4100
https://github.com/broadinstitute/cromwell/issues/4101:657,Availability,error,error,657,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101
https://github.com/broadinstitute/cromwell/issues/4101:697,Availability,error,error,697,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101
https://github.com/broadinstitute/cromwell/issues/4101:741,Availability,error,error,741,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101
https://github.com/broadinstitute/cromwell/issues/4101:98,Deployability,Upgrade,Upgrade,98,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101
https://github.com/broadinstitute/cromwell/issues/4101:316,Deployability,upgrade,upgrade,316,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101
https://github.com/broadinstitute/cromwell/issues/4101:375,Deployability,Upgrade,Upgrade,375,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101
https://github.com/broadinstitute/cromwell/issues/4101:440,Modifiability,config,config,440,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101
https://github.com/broadinstitute/cromwell/issues/4101:302,Performance,cache,cached,302,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101
https://github.com/broadinstitute/cromwell/issues/4101:523,Testability,Log,Log,523,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101
https://github.com/broadinstitute/cromwell/issues/4101:627,Testability,log,logs,627,"Basic Expectations: ; Run a bunch of workflows (call caching turned off) → mid-way to completion, Upgrade → ; a) All running workflows succeed; b) Cromwell can connect to pre-existing operation ids (aka it completed within attempt 1). Run a bunch of workflows → Run them again to see they successfully cached once → upgrade → run them again to ensure they’re still caching. “Upgrade” consists of a new Cromwell Jar, and also a new Cromwell config (with the latest additions being used). Key features that shouldn’t break:; Log names (detritus files) don’t change (before and after), and if yes, then print a warning; Streaming logs (before and after), else error; Caching (before and after), else error; Job success (before and after), else error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4101
https://github.com/broadinstitute/cromwell/issues/4103:1329,Availability,heartbeat,heartbeat,1329,"cd36fb1c.yml --type CWL --workflow-root main; [2018-09-14 13:19:10,55] [info] Running with database db.url = jdbc:hsqldb:mem:d07a09a8-8d20-4095-967b-c6f375a3f309;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:19,53] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-09-14 13:19:19,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-09-14 13:19:19,67] [info] Running with database db.url = jdbc:hsqldb:mem:e41fe9de-508c-4f49-aeaa-ce7474d7c1e2;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:20,18] [info] Slf4jLogger started; [2018-09-14 13:19:20,25] [info] Pre Processing Workflow...; [2018-09-14 13:19:20,65] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl; [2018-09-14 13:19:54,70] [info] Pre Processing Inputs...; [2018-09-14 13:19:54,94] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-89ab52b"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:19:55,04] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:1393,Availability,heartbeat,heartbeatInterval,1393,"cd36fb1c.yml --type CWL --workflow-root main; [2018-09-14 13:19:10,55] [info] Running with database db.url = jdbc:hsqldb:mem:d07a09a8-8d20-4095-967b-c6f375a3f309;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:19,53] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-09-14 13:19:19,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-09-14 13:19:19,67] [info] Running with database db.url = jdbc:hsqldb:mem:e41fe9de-508c-4f49-aeaa-ce7474d7c1e2;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:20,18] [info] Slf4jLogger started; [2018-09-14 13:19:20,25] [info] Pre Processing Workflow...; [2018-09-14 13:19:20,65] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl; [2018-09-14 13:19:54,70] [info] Pre Processing Inputs...; [2018-09-14 13:19:54,94] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-89ab52b"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:19:55,04] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:3199,Availability,error,error,3199,"ersion) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,91] [info] 1 new workflows fetched; [2018-09-14 13:19:57,92] [info] WorkflowManagerActor Starting workflow caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,93] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:19:57,96] [info] WorkflowManagerActor Successfully started WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,96] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-09-14 13:19:57,96] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-09-14 13:19:58,11] [info] MaterializeWorkflowDescriptorActor [caab4283]: Parsing workflow as CWL v1.0; [2018-09-14 13:20:00,08] [error] WorkflowManagerActor Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl#readgroup.yml/readgroup_fastq_pe_file was referred to but not found in schema def Inl(InputRecordSchema(file:///home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl#amplicon_kit.yml/amplicon_kit_set_file,Some([Lcwl.InputRecordField;@5f7affaf),record,None)), Inl(InputRecordSchema(file:///home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl#amplicon_kit.yml/amplicon_kit_set_uuid,Some([Lcwl.InputRecordField;@4e2399d2),record,None)), Inl(InputRecordSchema(file:///home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl#capture_kit.yml/capture_kit_set_file,Some([Lcwl.InputRecordField;@74284741),record,None)), Inl(InputRecordSchema(file:///home/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:8278,Availability,down,down,8278,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-09-14 13:20:00,08] [info] WorkflowManagerActor WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00 is in a terminal state: WorkflowFailedState; [2018-09-14 13:20:00,92] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-09-14 13:20:05,34] [info] Workflow polling stopped; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-09-14 13:20:05,37] [info] Aborting all running workflows.; [2018-09-14 13:20:05,39] [info] WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:8366,Availability,down,down,8366,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-09-14 13:20:00,08] [info] WorkflowManagerActor WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00 is in a terminal state: WorkflowFailedState; [2018-09-14 13:20:00,92] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-09-14 13:20:05,34] [info] Workflow polling stopped; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-09-14 13:20:05,37] [info] Aborting all running workflows.; [2018-09-14 13:20:05,39] [info] WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:8582,Availability,down,down,8582,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:8809,Availability,down,down,8809,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9054,Availability,down,down,9054,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9101,Availability,down,down,9101,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9195,Availability,down,down,9195,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9344,Availability,down,down,9344,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9436,Availability,down,down,9436,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9529,Availability,down,down,9529,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9672,Availability,down,down,9672,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9772,Availability,down,down,9772,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9868,Availability,down,down,9868,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:10125,Availability,down,down,10125,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:10329,Availability,down,down,10329,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:19155,Availability,heartbeat,heartbeat,19155,"/home/jeremiah/gdc-dnaseq-cwl/tools/samtools_idxstats_to_sqlite.cwl; [2018-09-14 13:21:50,17] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats.cwl; [2018-09-14 13:21:50,38] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats_to_sqlite.cwl; [2018-09-14 13:21:50,46] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/integrity.cwl; [2018-09-14 13:21:50,64] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:19219,Availability,heartbeat,heartbeatInterval,19219,"/home/jeremiah/gdc-dnaseq-cwl/tools/samtools_idxstats_to_sqlite.cwl; [2018-09-14 13:21:50,17] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats.cwl; [2018-09-14 13:21:50,38] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats_to_sqlite.cwl; [2018-09-14 13:21:50,46] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/integrity.cwl; [2018-09-14 13:21:50,64] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:1339,Deployability,configurat,configuration,1339,"cd36fb1c.yml --type CWL --workflow-root main; [2018-09-14 13:19:10,55] [info] Running with database db.url = jdbc:hsqldb:mem:d07a09a8-8d20-4095-967b-c6f375a3f309;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:19,53] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-09-14 13:19:19,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-09-14 13:19:19,67] [info] Running with database db.url = jdbc:hsqldb:mem:e41fe9de-508c-4f49-aeaa-ce7474d7c1e2;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:20,18] [info] Slf4jLogger started; [2018-09-14 13:19:20,25] [info] Pre Processing Workflow...; [2018-09-14 13:19:20,65] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl; [2018-09-14 13:19:54,70] [info] Pre Processing Inputs...; [2018-09-14 13:19:54,94] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-89ab52b"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:19:55,04] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:19165,Deployability,configurat,configuration,19165,"/home/jeremiah/gdc-dnaseq-cwl/tools/samtools_idxstats_to_sqlite.cwl; [2018-09-14 13:21:50,17] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats.cwl; [2018-09-14 13:21:50,38] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats_to_sqlite.cwl; [2018-09-14 13:21:50,46] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/integrity.cwl; [2018-09-14 13:21:50,64] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:2679,Integrability,message,message,2679,"ummary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,91] [info] 1 new workflows fetched; [2018-09-14 13:19:57,92] [info] WorkflowManagerActor Starting workflow caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,93] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:19:57,96] [info] WorkflowManagerActor Successfully started WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,96] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-09-14 13:19:57,96] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-09-14 13:19:58,11] [info] MaterializeWorkflowDescriptorActor [caab4283]: Parsing workflow as CWL v1.0; [2018-09-14 13:20:00,08] [error] WorkflowManagerActor Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl#",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9787,Integrability,message,messages,9787,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9883,Integrability,message,messages,9883,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:10140,Integrability,message,messages,10140,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:20505,Integrability,message,message,20505,"ummary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:52,43] [info] 1 new workflows fetched; [2018-09-14 13:21:53,05] [info] WorkflowManagerActor Starting workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,06] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:21:53,09] [info] WorkflowManagerActor Successfully started WorkflowActor-6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,09] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-09-14 13:21:53,10] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-09-14 13:21:53,21] [info] MaterializeWorkflowDescriptorActor [6f311835]: Parsing workflow as CWL v1.0; [2018-09-14 13:21:55,91] [info] MaterializeWorkflowDescriptorActor [6f311835]: Call-to-Backend assignments: bam_ls_l -> Local, bam_readgroup_to_json -> Local, fastqc -> Local, json_to_sqlite -> Local, merge_readgroup_json_db -> Local, fastq_cleaner_se -> Local, picard_collecttargetedpcrmetrics_to_sqlite -> Local, biobambam_bamtofastq -> Local, readgroup_json_db -> Local, pic",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:1339,Modifiability,config,configuration,1339,"cd36fb1c.yml --type CWL --workflow-root main; [2018-09-14 13:19:10,55] [info] Running with database db.url = jdbc:hsqldb:mem:d07a09a8-8d20-4095-967b-c6f375a3f309;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:19,53] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-09-14 13:19:19,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-09-14 13:19:19,67] [info] Running with database db.url = jdbc:hsqldb:mem:e41fe9de-508c-4f49-aeaa-ce7474d7c1e2;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:20,18] [info] Slf4jLogger started; [2018-09-14 13:19:20,25] [info] Pre Processing Workflow...; [2018-09-14 13:19:20,65] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl; [2018-09-14 13:19:54,70] [info] Pre Processing Inputs...; [2018-09-14 13:19:54,94] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-89ab52b"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:19:55,04] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:1630,Modifiability,config,configured,1630,"3:19:19,53] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-09-14 13:19:19,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-09-14 13:19:19,67] [info] Running with database db.url = jdbc:hsqldb:mem:e41fe9de-508c-4f49-aeaa-ce7474d7c1e2;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:20,18] [info] Slf4jLogger started; [2018-09-14 13:19:20,25] [info] Pre Processing Workflow...; [2018-09-14 13:19:20,65] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl; [2018-09-14 13:19:54,70] [info] Pre Processing Inputs...; [2018-09-14 13:19:54,94] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-89ab52b"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:19:55,04] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,91] [info] 1 new workflows fetched; [2018-09-14 13:19:57,92] [info] WorkflowManagerActor Starting workflow caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,93] [warn] SingleWo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:1750,Modifiability,config,configured,1750,"ch size of 100000; [2018-09-14 13:19:19,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-09-14 13:19:19,67] [info] Running with database db.url = jdbc:hsqldb:mem:e41fe9de-508c-4f49-aeaa-ce7474d7c1e2;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:20,18] [info] Slf4jLogger started; [2018-09-14 13:19:20,25] [info] Pre Processing Workflow...; [2018-09-14 13:19:20,65] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl; [2018-09-14 13:19:54,70] [info] Pre Processing Inputs...; [2018-09-14 13:19:54,94] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-89ab52b"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:19:55,04] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,91] [info] 1 new workflows fetched; [2018-09-14 13:19:57,92] [info] WorkflowManagerActor Starting workflow caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,93] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:19:57,96] [info] Workfl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:1871,Modifiability,config,configured,1871,"o] Running with database db.url = jdbc:hsqldb:mem:e41fe9de-508c-4f49-aeaa-ce7474d7c1e2;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:20,18] [info] Slf4jLogger started; [2018-09-14 13:19:20,25] [info] Pre Processing Workflow...; [2018-09-14 13:19:20,65] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl; [2018-09-14 13:19:54,70] [info] Pre Processing Inputs...; [2018-09-14 13:19:54,94] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-89ab52b"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:19:55,04] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,91] [info] 1 new workflows fetched; [2018-09-14 13:19:57,92] [info] WorkflowManagerActor Starting workflow caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,93] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:19:57,96] [info] WorkflowManagerActor Successfully started WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,96] [info] R",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:2993,Modifiability,config,configured,2993,"ummary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,91] [info] 1 new workflows fetched; [2018-09-14 13:19:57,92] [info] WorkflowManagerActor Starting workflow caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,93] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:19:57,96] [info] WorkflowManagerActor Successfully started WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00; [2018-09-14 13:19:57,96] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-09-14 13:19:57,96] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-09-14 13:19:58,11] [info] MaterializeWorkflowDescriptorActor [caab4283]: Parsing workflow as CWL v1.0; [2018-09-14 13:20:00,08] [error] WorkflowManagerActor Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Custom type file:///home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl#",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:19165,Modifiability,config,configuration,19165,"/home/jeremiah/gdc-dnaseq-cwl/tools/samtools_idxstats_to_sqlite.cwl; [2018-09-14 13:21:50,17] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats.cwl; [2018-09-14 13:21:50,38] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats_to_sqlite.cwl; [2018-09-14 13:21:50,46] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/integrity.cwl; [2018-09-14 13:21:50,64] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:19462,Modifiability,config,configured,19462,"rocessing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats_to_sqlite.cwl; [2018-09-14 13:21:50,46] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/integrity.cwl; [2018-09-14 13:21:50,64] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:52,43] [info] 1 new workflows fetched; [2018-09-14 13:21:53,05] [info] WorkflowManagerActor Starting workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,06] [warn] SingleWorkf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:19576,Modifiability,config,configured,19576,"ssing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/integrity.cwl; [2018-09-14 13:21:50,64] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:52,43] [info] 1 new workflows fetched; [2018-09-14 13:21:53,05] [info] WorkflowManagerActor Starting workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,06] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:21:53,09] [info] WorkflowM",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:19697,Modifiability,config,configured,19697,"ssing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:52,43] [info] 1 new workflows fetched; [2018-09-14 13:21:53,05] [info] WorkflowManagerActor Starting workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,06] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:21:53,09] [info] WorkflowManagerActor Successfully started WorkflowActor-6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,09] [info] R",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:20819,Modifiability,config,configured,20819,"ummary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,51] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:21:52,32] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:21:52,35] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:21:52,36] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:21:52,42] [info] CWL (Unspecified version) workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039 submitted; [2018-09-14 13:21:52,43] [info] SingleWorkflowRunnerActor: Workflow submitted 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:52,43] [info] 1 new workflows fetched; [2018-09-14 13:21:53,05] [info] WorkflowManagerActor Starting workflow 6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,06] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-09-14 13:21:53,09] [info] WorkflowManagerActor Successfully started WorkflowActor-6f311835-f1fe-4bbd-8fbb-c5543373d039; [2018-09-14 13:21:53,09] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-09-14 13:21:53,10] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-09-14 13:21:53,21] [info] MaterializeWorkflowDescriptorActor [6f311835]: Parsing workflow as CWL v1.0; [2018-09-14 13:21:55,91] [info] MaterializeWorkflowDescriptorActor [6f311835]: Call-to-Backend assignments: bam_ls_l -> Local, bam_readgroup_to_json -> Local, fastqc -> Local, json_to_sqlite -> Local, merge_readgroup_json_db -> Local, fastq_cleaner_se -> Local, picard_collecttargetedpcrmetrics_to_sqlite -> Local, biobambam_bamtofastq -> Local, readgroup_json_db -> Local, pic",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9780,Performance,queue,queued,9780,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9876,Performance,queue,queued,9876,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:10133,Performance,queue,queued,10133,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:8304,Safety,Timeout,Timeout,8304,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-09-14 13:20:00,08] [info] WorkflowManagerActor WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00 is in a terminal state: WorkflowFailedState; [2018-09-14 13:20:00,92] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-09-14 13:20:05,34] [info] Workflow polling stopped; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-09-14 13:20:05,37] [info] Aborting all running workflows.; [2018-09-14 13:20:05,39] [info] WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:8395,Safety,Timeout,Timeout,8395,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-09-14 13:20:00,08] [info] WorkflowManagerActor WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00 is in a terminal state: WorkflowFailedState; [2018-09-14 13:20:00,92] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-09-14 13:20:05,34] [info] Workflow polling stopped; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-09-14 13:20:05,37] [info] Aborting all running workflows.; [2018-09-14 13:20:05,39] [info] WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:8448,Safety,Abort,Aborting,8448,"scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2018-09-14 13:20:00,08] [info] WorkflowManagerActor WorkflowActor-caab4283-a3d4-4966-85ba-56d0992c8f00 is in a terminal state: WorkflowFailedState; [2018-09-14 13:20:00,92] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2018-09-14 13:20:05,34] [info] Workflow polling stopped; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-09-14 13:20:05,36] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-09-14 13:20:05,37] [info] Aborting all running workflows.; [2018-09-14 13:20:05,39] [info] WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:8616,Safety,Timeout,Timeout,8616,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:8837,Safety,Timeout,Timeout,8837,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9130,Safety,Timeout,Timeout,9130,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9216,Safety,Timeout,Timeout,9216,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9371,Safety,Timeout,Timeout,9371,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9464,Safety,Timeout,Timeout,9464,,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9552,Safety,Timeout,Timeout,9552,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:9687,Safety,Timeout,Timeout,9687,"WorkflowStoreActor stopped; [2018-09-14 13:20:05,36] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-09-14 13:20:05,39] [info] JobExecutionTokenDispenser stopped; [2018-09-14 13:20:05,40] [info] WorkflowLogCopyRouter stopped; [2018-09-14 13:20:05,40] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor All workflows finished; [2018-09-14 13:20:05,40] [info] WorkflowManagerActor stopped; [2018-09-14 13:20:05,40] [info] Connection pools shut down; [2018-09-14 13:20:05,41] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] SubWorkflowStoreActor stopped; [2018-09-14 13:20:05,41] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,41] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] JobStoreActor stopped; [2018-09-14 13:20:05,42] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-09-14 13:20:05,42] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,42] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] CallCacheWriteActor stopped; [2018-09-14 13:20:05,43] [info] DockerHashActor stopped; [2018-09-14 13:20:05,43] [info] IoProxy stopped; [2018-09-14 13:20:05,43] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-09-14 13:20:05,43] [info] ServiceRegistryActor stopped; [2018-09-14 13:20:05,47] [info] Database closed; [2018-09-14 13:20:05,47] [info] Stream materializer shut down; [2018-09-14 13:20:05,48] [info] WDL HTTP import resolver closed; Workflow caab4283-a3d4-4966-85ba-56d0992c8f00 transitioned to state Failed; (p3cwl) [jeremiah@localhost ~]$ ; ```.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:18647,Security,integrity,integrity,18647,"cwl/tools/picard_collectoxogmetrics_to_sqlite.cwl; [2018-09-14 13:21:49,69] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/picard_collectwgsmetrics.cwl; [2018-09-14 13:21:49,78] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/picard_collectwgsmetrics_to_sqlite.cwl; [2018-09-14 13:21:49,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_flagstat.cwl; [2018-09-14 13:21:49,94] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_flagstat_to_sqlite.cwl; [2018-09-14 13:21:50,02] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_idxstats.cwl; [2018-09-14 13:21:50,08] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_idxstats_to_sqlite.cwl; [2018-09-14 13:21:50,17] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats.cwl; [2018-09-14 13:21:50,38] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/samtools_stats_to_sqlite.cwl; [2018-09-14 13:21:50,46] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/integrity.cwl; [2018-09-14 13:21:50,64] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/ls_l.cwl; [2018-09-14 13:21:50,71] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/md5sum.cwl; [2018-09-14 13:21:50,81] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/sha256sum.cwl; [2018-09-14 13:21:50,87] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/tools/integrity_to_sqlite.cwl; [2018-09-14 13:21:51,02] [info] Pre Processing Inputs...; [2018-09-14 13:21:51,27] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6d01716"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:21:51,38] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:21:51,44] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:21:51,45] [info] KvWriteActor configured to flush wit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:6615,Testability,Log,LoggingFSM,6615,"astq_align/transform_pack.cwl#readgroup.yml/readgroups_bam_uuid,Some([Lcwl.InputRecordField;@775ffa16),record,None)).; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:6708,Testability,Log,LoggingFSM,6708,"cwl.InputRecordField;@775ffa16),record,None)).; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/issues/4103:6763,Testability,Log,LoggingFSM,6763,mwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103
https://github.com/broadinstitute/cromwell/pull/4104:18,Testability,test,test,18,There's a Centaur test here that's not quite as exacting as I'd like it to be. Really we should make sure Cromwell never even looks at hits from calls with non-matching prefixes. We can TT how best to have automated assertions for this and other perfy sorts of features.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4104
https://github.com/broadinstitute/cromwell/pull/4104:216,Testability,assert,assertions,216,There's a Centaur test here that's not quite as exacting as I'd like it to be. Really we should make sure Cromwell never even looks at hits from calls with non-matching prefixes. We can TT how best to have automated assertions for this and other perfy sorts of features.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4104
https://github.com/broadinstitute/cromwell/issues/4105:716,Availability,Reboot,Rebooted,716,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105
https://github.com/broadinstitute/cromwell/issues/4105:1319,Availability,Error,Error,1319,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105
https://github.com/broadinstitute/cromwell/issues/4105:1494,Availability,error,error,1494,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105
https://github.com/broadinstitute/cromwell/issues/4105:1569,Availability,Error,Error,1569,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105
https://github.com/broadinstitute/cromwell/issues/4105:1770,Availability,error,error,1770,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105
https://github.com/broadinstitute/cromwell/issues/4105:435,Integrability,message,messages,435,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105
https://github.com/broadinstitute/cromwell/issues/4105:1325,Integrability,Message,Message,1325,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105
https://github.com/broadinstitute/cromwell/issues/4105:423,Safety,timeout,timeout,423,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105
https://github.com/broadinstitute/cromwell/issues/4105:1071,Safety,timeout,timeout,1071,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105
https://github.com/broadinstitute/cromwell/issues/4105:431,Testability,log,log,431,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105
https://github.com/broadinstitute/cromwell/issues/4106:360,Deployability,configurat,configuration,360,"A flexible way of capturing the data involved in a performance run for later analysis:. Capture the following into files and copy them into GCS somewhere, perhaps [perf bucket]/[perf name]/[cromwell version]. where perf name is a small enumerated list of things we want to perf test, e.g. (5_genome, TJeandet_Call_Cache). * reported metrics; * all metadata; * configuration. Once this is done, the user may delete the instance running cromwell, the DB. It may also choose not to report metrics to Perf Grafana",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4106
https://github.com/broadinstitute/cromwell/issues/4106:2,Modifiability,flexible,flexible,2,"A flexible way of capturing the data involved in a performance run for later analysis:. Capture the following into files and copy them into GCS somewhere, perhaps [perf bucket]/[perf name]/[cromwell version]. where perf name is a small enumerated list of things we want to perf test, e.g. (5_genome, TJeandet_Call_Cache). * reported metrics; * all metadata; * configuration. Once this is done, the user may delete the instance running cromwell, the DB. It may also choose not to report metrics to Perf Grafana",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4106
https://github.com/broadinstitute/cromwell/issues/4106:360,Modifiability,config,configuration,360,"A flexible way of capturing the data involved in a performance run for later analysis:. Capture the following into files and copy them into GCS somewhere, perhaps [perf bucket]/[perf name]/[cromwell version]. where perf name is a small enumerated list of things we want to perf test, e.g. (5_genome, TJeandet_Call_Cache). * reported metrics; * all metadata; * configuration. Once this is done, the user may delete the instance running cromwell, the DB. It may also choose not to report metrics to Perf Grafana",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4106
https://github.com/broadinstitute/cromwell/issues/4106:51,Performance,perform,performance,51,"A flexible way of capturing the data involved in a performance run for later analysis:. Capture the following into files and copy them into GCS somewhere, perhaps [perf bucket]/[perf name]/[cromwell version]. where perf name is a small enumerated list of things we want to perf test, e.g. (5_genome, TJeandet_Call_Cache). * reported metrics; * all metadata; * configuration. Once this is done, the user may delete the instance running cromwell, the DB. It may also choose not to report metrics to Perf Grafana",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4106
https://github.com/broadinstitute/cromwell/issues/4106:278,Testability,test,test,278,"A flexible way of capturing the data involved in a performance run for later analysis:. Capture the following into files and copy them into GCS somewhere, perhaps [perf bucket]/[perf name]/[cromwell version]. where perf name is a small enumerated list of things we want to perf test, e.g. (5_genome, TJeandet_Call_Cache). * reported metrics; * all metadata; * configuration. Once this is done, the user may delete the instance running cromwell, the DB. It may also choose not to report metrics to Perf Grafana",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4106
https://github.com/broadinstitute/cromwell/pull/4110:281,Availability,error,error,281,"This PR addresses an issue found during testing of the creation of the custom AMI for the AWS backend. The stderr redirect to /dev/null was put in place to eliminate duplicate logging (proxy and task container). However, redirecting stderr to /dev/null also suppresses some docker error messages; most notably instances where the command executable does not exist in the image. It is not possible to segregate the docker error messages from stderr output from the task. This PR errs on the side of potential duplicate logging in order to catch docker-based errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4110
https://github.com/broadinstitute/cromwell/pull/4110:421,Availability,error,error,421,"This PR addresses an issue found during testing of the creation of the custom AMI for the AWS backend. The stderr redirect to /dev/null was put in place to eliminate duplicate logging (proxy and task container). However, redirecting stderr to /dev/null also suppresses some docker error messages; most notably instances where the command executable does not exist in the image. It is not possible to segregate the docker error messages from stderr output from the task. This PR errs on the side of potential duplicate logging in order to catch docker-based errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4110
https://github.com/broadinstitute/cromwell/pull/4110:557,Availability,error,errors,557,"This PR addresses an issue found during testing of the creation of the custom AMI for the AWS backend. The stderr redirect to /dev/null was put in place to eliminate duplicate logging (proxy and task container). However, redirecting stderr to /dev/null also suppresses some docker error messages; most notably instances where the command executable does not exist in the image. It is not possible to segregate the docker error messages from stderr output from the task. This PR errs on the side of potential duplicate logging in order to catch docker-based errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4110
https://github.com/broadinstitute/cromwell/pull/4110:287,Integrability,message,messages,287,"This PR addresses an issue found during testing of the creation of the custom AMI for the AWS backend. The stderr redirect to /dev/null was put in place to eliminate duplicate logging (proxy and task container). However, redirecting stderr to /dev/null also suppresses some docker error messages; most notably instances where the command executable does not exist in the image. It is not possible to segregate the docker error messages from stderr output from the task. This PR errs on the side of potential duplicate logging in order to catch docker-based errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4110
https://github.com/broadinstitute/cromwell/pull/4110:427,Integrability,message,messages,427,"This PR addresses an issue found during testing of the creation of the custom AMI for the AWS backend. The stderr redirect to /dev/null was put in place to eliminate duplicate logging (proxy and task container). However, redirecting stderr to /dev/null also suppresses some docker error messages; most notably instances where the command executable does not exist in the image. It is not possible to segregate the docker error messages from stderr output from the task. This PR errs on the side of potential duplicate logging in order to catch docker-based errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4110
https://github.com/broadinstitute/cromwell/pull/4110:40,Testability,test,testing,40,"This PR addresses an issue found during testing of the creation of the custom AMI for the AWS backend. The stderr redirect to /dev/null was put in place to eliminate duplicate logging (proxy and task container). However, redirecting stderr to /dev/null also suppresses some docker error messages; most notably instances where the command executable does not exist in the image. It is not possible to segregate the docker error messages from stderr output from the task. This PR errs on the side of potential duplicate logging in order to catch docker-based errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4110
https://github.com/broadinstitute/cromwell/pull/4110:176,Testability,log,logging,176,"This PR addresses an issue found during testing of the creation of the custom AMI for the AWS backend. The stderr redirect to /dev/null was put in place to eliminate duplicate logging (proxy and task container). However, redirecting stderr to /dev/null also suppresses some docker error messages; most notably instances where the command executable does not exist in the image. It is not possible to segregate the docker error messages from stderr output from the task. This PR errs on the side of potential duplicate logging in order to catch docker-based errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4110
https://github.com/broadinstitute/cromwell/pull/4110:518,Testability,log,logging,518,"This PR addresses an issue found during testing of the creation of the custom AMI for the AWS backend. The stderr redirect to /dev/null was put in place to eliminate duplicate logging (proxy and task container). However, redirecting stderr to /dev/null also suppresses some docker error messages; most notably instances where the command executable does not exist in the image. It is not possible to segregate the docker error messages from stderr output from the task. This PR errs on the side of potential duplicate logging in order to catch docker-based errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4110
https://github.com/broadinstitute/cromwell/issues/4111:383,Availability,error,error,383,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:650,Availability,Echo,Echo,650,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:658,Availability,echo,echo,658,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:686,Availability,echo,echo,686,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:705,Availability,Echo,Echo,705,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:736,Availability,echo,echo,736,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:886,Availability,error,error,886,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:925,Availability,error,error,925,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:1214,Availability,echo,echo,1214,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:1137,Deployability,pipeline,pipelines,1137,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:67,Integrability,depend,depending,67,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:625,Testability,Test,Test,625,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/issues/4111:1167,Testability,Test,Test,1167,"When having a tool that sometimes creates a group of files or not (depending on the data sometimes even) a optional struct in the output would be nice to have. As a user This is nice to have but as a developer I also see some issues here. For me best combination would be to to create Some(file) when all files are existing, a None when all files are not existing and still raise an error when 1 File is found but an other is not found.; Of course this only count for required files in the structs, optional files inside the structs already works. . Example below does not work on version d9b9262; ```; version 1.0. workflow Test {; input {; }. call Echo as echo. output {; Out? bla2 = echo.s; }; }. task Echo {; input {; }. command {; echo bla; }. output {; Out? s = object {; file: ""some_none_existing_file"",; index: ""some_none_existing_index""; }; }; }. struct Out {; File f; }; ```. error:; ```; [2018-09-18 11:11:06,16] [error] WorkflowManagerActor Workflow 1beb17d5-1c5d-489b-9750-6c54d17a77de failed (during ExecutingWorkflowState): java.io.FileNotFoundException: Could not process output, file not found: /home/pjvan_thof/src/all-pipelines/cromwell-executions/Test/1beb17d5-1c5d-489b-9750-6c54d17a77de/call-echo/execution/some_none_existing_file; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4111
https://github.com/broadinstitute/cromwell/pull/4112:105,Availability,alive,alive,105,Fixing https://github.com/broadinstitute/cromwell/issues/4050. While doing this I notice that this check alive command is not used at all. First did a restructure of the statuses and now there is also a status Running. - [x] Add timeout on `WaitingForReturnCode` step; - [x] Make timeout a config value. Meanwhile please give already feed on this of course ;),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112
https://github.com/broadinstitute/cromwell/pull/4112:290,Modifiability,config,config,290,Fixing https://github.com/broadinstitute/cromwell/issues/4050. While doing this I notice that this check alive command is not used at all. First did a restructure of the statuses and now there is also a status Running. - [x] Add timeout on `WaitingForReturnCode` step; - [x] Make timeout a config value. Meanwhile please give already feed on this of course ;),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112
https://github.com/broadinstitute/cromwell/pull/4112:229,Safety,timeout,timeout,229,Fixing https://github.com/broadinstitute/cromwell/issues/4050. While doing this I notice that this check alive command is not used at all. First did a restructure of the statuses and now there is also a status Running. - [x] Add timeout on `WaitingForReturnCode` step; - [x] Make timeout a config value. Meanwhile please give already feed on this of course ;),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112
https://github.com/broadinstitute/cromwell/pull/4112:280,Safety,timeout,timeout,280,Fixing https://github.com/broadinstitute/cromwell/issues/4050. While doing this I notice that this check alive command is not used at all. First did a restructure of the statuses and now there is also a status Running. - [x] Add timeout on `WaitingForReturnCode` step; - [x] Make timeout a config value. Meanwhile please give already feed on this of course ;),MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112
https://github.com/broadinstitute/cromwell/issues/4117:54,Availability,failure,failure,54,"While scale testing a pipeline last week, we saw this failure message in Cromwell-as-a-Service on 3/999 workflows:. `Failed to import workflow SmartSeq2SingleSample.wdl.:\nBad import SmartSeq2SingleSample.wdl: Failed to resolve 'SmartSeq2SingleSample.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path""`. All 999 workflows are the same and were started programmatically, but the three that have this import error are missing ""imports"" in the Cromwell metadata. We are providing a zipped directory containing all of the workflow dependencies instead of using http imports. It's unclear whether there was an issue submitting these workflows to Cromwell, or if the zip file was received by Cromwell but was not passed on to the workflow. . Here are the workflow IDs for the failed workflows:; - 8eb0f933-5e9c-477c-9de8-6ef03049aafa; - 96ca1d93-78c6-4eae-ba03-b540728353ca; - c548deda-e518-4a15-8191-97e73b68cd4b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117
https://github.com/broadinstitute/cromwell/issues/4117:450,Availability,error,error,450,"While scale testing a pipeline last week, we saw this failure message in Cromwell-as-a-Service on 3/999 workflows:. `Failed to import workflow SmartSeq2SingleSample.wdl.:\nBad import SmartSeq2SingleSample.wdl: Failed to resolve 'SmartSeq2SingleSample.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path""`. All 999 workflows are the same and were started programmatically, but the three that have this import error are missing ""imports"" in the Cromwell metadata. We are providing a zipped directory containing all of the workflow dependencies instead of using http imports. It's unclear whether there was an issue submitting these workflows to Cromwell, or if the zip file was received by Cromwell but was not passed on to the workflow. . Here are the workflow IDs for the failed workflows:; - 8eb0f933-5e9c-477c-9de8-6ef03049aafa; - 96ca1d93-78c6-4eae-ba03-b540728353ca; - c548deda-e518-4a15-8191-97e73b68cd4b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117
https://github.com/broadinstitute/cromwell/issues/4117:22,Deployability,pipeline,pipeline,22,"While scale testing a pipeline last week, we saw this failure message in Cromwell-as-a-Service on 3/999 workflows:. `Failed to import workflow SmartSeq2SingleSample.wdl.:\nBad import SmartSeq2SingleSample.wdl: Failed to resolve 'SmartSeq2SingleSample.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path""`. All 999 workflows are the same and were started programmatically, but the three that have this import error are missing ""imports"" in the Cromwell metadata. We are providing a zipped directory containing all of the workflow dependencies instead of using http imports. It's unclear whether there was an issue submitting these workflows to Cromwell, or if the zip file was received by Cromwell but was not passed on to the workflow. . Here are the workflow IDs for the failed workflows:; - 8eb0f933-5e9c-477c-9de8-6ef03049aafa; - 96ca1d93-78c6-4eae-ba03-b540728353ca; - c548deda-e518-4a15-8191-97e73b68cd4b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117
https://github.com/broadinstitute/cromwell/issues/4117:62,Integrability,message,message,62,"While scale testing a pipeline last week, we saw this failure message in Cromwell-as-a-Service on 3/999 workflows:. `Failed to import workflow SmartSeq2SingleSample.wdl.:\nBad import SmartSeq2SingleSample.wdl: Failed to resolve 'SmartSeq2SingleSample.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path""`. All 999 workflows are the same and were started programmatically, but the three that have this import error are missing ""imports"" in the Cromwell metadata. We are providing a zipped directory containing all of the workflow dependencies instead of using http imports. It's unclear whether there was an issue submitting these workflows to Cromwell, or if the zip file was received by Cromwell but was not passed on to the workflow. . Here are the workflow IDs for the failed workflows:; - 8eb0f933-5e9c-477c-9de8-6ef03049aafa; - 96ca1d93-78c6-4eae-ba03-b540728353ca; - c548deda-e518-4a15-8191-97e73b68cd4b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117
https://github.com/broadinstitute/cromwell/issues/4117:571,Integrability,depend,dependencies,571,"While scale testing a pipeline last week, we saw this failure message in Cromwell-as-a-Service on 3/999 workflows:. `Failed to import workflow SmartSeq2SingleSample.wdl.:\nBad import SmartSeq2SingleSample.wdl: Failed to resolve 'SmartSeq2SingleSample.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path""`. All 999 workflows are the same and were started programmatically, but the three that have this import error are missing ""imports"" in the Cromwell metadata. We are providing a zipped directory containing all of the workflow dependencies instead of using http imports. It's unclear whether there was an issue submitting these workflows to Cromwell, or if the zip file was received by Cromwell but was not passed on to the workflow. . Here are the workflow IDs for the failed workflows:; - 8eb0f933-5e9c-477c-9de8-6ef03049aafa; - 96ca1d93-78c6-4eae-ba03-b540728353ca; - c548deda-e518-4a15-8191-97e73b68cd4b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117
https://github.com/broadinstitute/cromwell/issues/4117:12,Testability,test,testing,12,"While scale testing a pipeline last week, we saw this failure message in Cromwell-as-a-Service on 3/999 workflows:. `Failed to import workflow SmartSeq2SingleSample.wdl.:\nBad import SmartSeq2SingleSample.wdl: Failed to resolve 'SmartSeq2SingleSample.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path""`. All 999 workflows are the same and were started programmatically, but the three that have this import error are missing ""imports"" in the Cromwell metadata. We are providing a zipped directory containing all of the workflow dependencies instead of using http imports. It's unclear whether there was an issue submitting these workflows to Cromwell, or if the zip file was received by Cromwell but was not passed on to the workflow. . Here are the workflow IDs for the failed workflows:; - 8eb0f933-5e9c-477c-9de8-6ef03049aafa; - 96ca1d93-78c6-4eae-ba03-b540728353ca; - c548deda-e518-4a15-8191-97e73b68cd4b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117
https://github.com/broadinstitute/cromwell/issues/4118:428,Availability,error,error,428,"Today, during Job preparation (prior to localization), when Cromwell calls Martha to get information on a dos url, there is a schema (defined in the Cromwell config) which statically extracts an element from an array, which is no longer guaranteed to be a GS url. Ideally, when Martha returns URL info, Cromwell sorts through the array of urls to *extract the first gcs url*, and when there is no GCS url, the job fails with an error message explaining how the DOS url couldn't be resolved into a GCS url. IN addition, Cromwell can also the print the dos url + actual urls associated to the dos url for a user -- so they can easily take both pieces of info for further debugging.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4118
https://github.com/broadinstitute/cromwell/issues/4118:434,Integrability,message,message,434,"Today, during Job preparation (prior to localization), when Cromwell calls Martha to get information on a dos url, there is a schema (defined in the Cromwell config) which statically extracts an element from an array, which is no longer guaranteed to be a GS url. Ideally, when Martha returns URL info, Cromwell sorts through the array of urls to *extract the first gcs url*, and when there is no GCS url, the job fails with an error message explaining how the DOS url couldn't be resolved into a GCS url. IN addition, Cromwell can also the print the dos url + actual urls associated to the dos url for a user -- so they can easily take both pieces of info for further debugging.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4118
https://github.com/broadinstitute/cromwell/issues/4118:158,Modifiability,config,config,158,"Today, during Job preparation (prior to localization), when Cromwell calls Martha to get information on a dos url, there is a schema (defined in the Cromwell config) which statically extracts an element from an array, which is no longer guaranteed to be a GS url. Ideally, when Martha returns URL info, Cromwell sorts through the array of urls to *extract the first gcs url*, and when there is no GCS url, the job fails with an error message explaining how the DOS url couldn't be resolved into a GCS url. IN addition, Cromwell can also the print the dos url + actual urls associated to the dos url for a user -- so they can easily take both pieces of info for further debugging.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4118
https://github.com/broadinstitute/cromwell/pull/4121:69,Energy Efficiency,reduce,reduces,69,"Perf testing has shown that removing this query improves CC time and reduces DB load (see last row in CC google doc); Unclear if it's worth keeping it as a configurable thing ?; This keeps storing the individual hashes, it just stops using them for ""fast"" cache miss detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121
https://github.com/broadinstitute/cromwell/pull/4121:156,Modifiability,config,configurable,156,"Perf testing has shown that removing this query improves CC time and reduces DB load (see last row in CC google doc); Unclear if it's worth keeping it as a configurable thing ?; This keeps storing the individual hashes, it just stops using them for ""fast"" cache miss detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121
https://github.com/broadinstitute/cromwell/pull/4121:80,Performance,load,load,80,"Perf testing has shown that removing this query improves CC time and reduces DB load (see last row in CC google doc); Unclear if it's worth keeping it as a configurable thing ?; This keeps storing the individual hashes, it just stops using them for ""fast"" cache miss detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121
https://github.com/broadinstitute/cromwell/pull/4121:256,Performance,cache,cache,256,"Perf testing has shown that removing this query improves CC time and reduces DB load (see last row in CC google doc); Unclear if it's worth keeping it as a configurable thing ?; This keeps storing the individual hashes, it just stops using them for ""fast"" cache miss detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121
https://github.com/broadinstitute/cromwell/pull/4121:267,Safety,detect,detection,267,"Perf testing has shown that removing this query improves CC time and reduces DB load (see last row in CC google doc); Unclear if it's worth keeping it as a configurable thing ?; This keeps storing the individual hashes, it just stops using them for ""fast"" cache miss detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121
https://github.com/broadinstitute/cromwell/pull/4121:212,Security,hash,hashes,212,"Perf testing has shown that removing this query improves CC time and reduces DB load (see last row in CC google doc); Unclear if it's worth keeping it as a configurable thing ?; This keeps storing the individual hashes, it just stops using them for ""fast"" cache miss detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121
https://github.com/broadinstitute/cromwell/pull/4121:5,Testability,test,testing,5,"Perf testing has shown that removing this query improves CC time and reduces DB load (see last row in CC google doc); Unclear if it's worth keeping it as a configurable thing ?; This keeps storing the individual hashes, it just stops using them for ""fast"" cache miss detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121
https://github.com/broadinstitute/cromwell/pull/4123:420,Availability,down,down,420,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:508,Deployability,configurat,configuration,508,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:549,Deployability,configurat,configuration,549,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:508,Modifiability,config,configuration,508,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:549,Modifiability,config,configuration,549,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:768,Modifiability,config,configured,768,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:14,Testability,test,test,14,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:67,Testability,Test,Tests,67,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:98,Testability,test,tests,98,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:146,Testability,test,test,146,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:195,Testability,log,logs,195,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:223,Testability,log,logs,223,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:291,Testability,test,test,291,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:526,Testability,test,test,526,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:567,Testability,test,test,567,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:727,Testability,test,test,727,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:831,Testability,test,test,831,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:911,Testability,test,test-reporting,911,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/pull/4123:953,Testability,test,test-,953,"- Runs a perf test automatically as supplied in the jenkins job; - Tests are described as centaur tests and run with centaur; - At the end of the test run, pushes the workflow metadata, crowmell logs, statsd metrics and VM logs up to GCS; - Destroys the VM and associated CloudSQL after the test has run; - Adds a proxy in the docker compose that will redirect statsd metrics to the hosted grafana as well as write them down to a file that will be pushed to gcs at the end of the workflow; - Custom cromwell configuration per test; - Custom centaur configuration per test. As side effects on centaur:; - Now accepts `http(s)` and `gs` urls in the workflow / inputs / options section; - Can push metadata to GCS at the end of a test; - Metadata query parameters can be configured to accommodate for very large workflows. Example of test run output: https://console.cloud.google.com/storage/browser/cromwell-perf-test-reporting/hello/35-1632b40-SNAP/perf-test-130/?project=broad-dsde-cromwell-perf&organizationId=548622027621",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4123
https://github.com/broadinstitute/cromwell/issues/4124:1750,Performance,perform,performant,1750,"# What happens. When a large workflow is queried for metadata, cromwell spends a considerable amount of time preparing the repsonse. **This usually results in a timeout for the caller.** In some cases, the preparation is so expensive that Cromwell either runs out of memory or enters a zombie-like state(#4105). # What should happen. The caller should receive a timely response, and Cromwell should not be endangered by operations on large workflows. # Speculation: Construction of result. The result is constructed in a two-phase manner: gather all the data, then produce a structured response. This is done for two reasons:. 1. Unstructured metadata is difficult for a human to understand.; 1. There are possibly many duplicates due to the way restarts are handled. ## Recommendation. ~Stream results (using doobie SQL library?) and construct response while gathering data. This should mean that a large pool of data is never present in memory, only the current result set and the partial response.~. Not streaming for now. Instead going to [`foldMap`](https://typelevel.org/cats/typeclasses/foldable.html) large sequence into `Map` monoid, then combine all those maps together into a final result. . There is some manipulation to be done after combining a result. 1. Sort calls by time; 1. Prune duplicates by taking the most recent. [This has some special cases](https://github.com/broadinstitute/cromwell/blob/3d68421b025db26ac3ab53972f69497e90601a47/engine/src/main/scala/cromwell/webservice/metadata/MetadataComponent.scala#L93) that need to be considered. # Speculation: Database table. The metadata table is currently an unindexed monster, comprising 10^6 - 10^9 rows and between 2-3 TB of data. The query has historically been surprisingly performant but is likely going to degrade over time. ## Recommendation . **punt on DB changes**. Believe to be related to #4093 and #4105",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124
https://github.com/broadinstitute/cromwell/issues/4124:161,Safety,timeout,timeout,161,"# What happens. When a large workflow is queried for metadata, cromwell spends a considerable amount of time preparing the repsonse. **This usually results in a timeout for the caller.** In some cases, the preparation is so expensive that Cromwell either runs out of memory or enters a zombie-like state(#4105). # What should happen. The caller should receive a timely response, and Cromwell should not be endangered by operations on large workflows. # Speculation: Construction of result. The result is constructed in a two-phase manner: gather all the data, then produce a structured response. This is done for two reasons:. 1. Unstructured metadata is difficult for a human to understand.; 1. There are possibly many duplicates due to the way restarts are handled. ## Recommendation. ~Stream results (using doobie SQL library?) and construct response while gathering data. This should mean that a large pool of data is never present in memory, only the current result set and the partial response.~. Not streaming for now. Instead going to [`foldMap`](https://typelevel.org/cats/typeclasses/foldable.html) large sequence into `Map` monoid, then combine all those maps together into a final result. . There is some manipulation to be done after combining a result. 1. Sort calls by time; 1. Prune duplicates by taking the most recent. [This has some special cases](https://github.com/broadinstitute/cromwell/blob/3d68421b025db26ac3ab53972f69497e90601a47/engine/src/main/scala/cromwell/webservice/metadata/MetadataComponent.scala#L93) that need to be considered. # Speculation: Database table. The metadata table is currently an unindexed monster, comprising 10^6 - 10^9 rows and between 2-3 TB of data. The query has historically been surprisingly performant but is likely going to degrade over time. ## Recommendation . **punt on DB changes**. Believe to be related to #4093 and #4105",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4124
https://github.com/broadinstitute/cromwell/pull/4125:4,Testability,test,tests,4,All tests passed on the first try. 😮,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4125
https://github.com/broadinstitute/cromwell/issues/4128:452,Availability,failure,failures,452,"This workflow `20d61ca9-0cc8-4dc1-9c20-85dbf9146de4`, running in the mint-dev Cromwell instance (version 35-fe8c4bf-SNAP) had a failed task where there is no start time in the metadata, but there is an end time. Was this task technically not started yet? If that's the case, then why would there be an end time?. ```; ""Adapter10xCount.inputs_for_submit.other_inputs"": [; {; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""shardIndex"": -1,; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Failed to evaluate 'Adapter10xCount.inputs_for_submit.other_inputs' (reason 1 of 1): Evaluating [{\""name\"": \""sample_id\"", \""value\"": GetInputs.sample_id}, {\""name\"": \""reference_name\"", \""value\"": reference_name}, {\""name\"": \""transcriptome_tar_gz\"", \""value\"": transcriptome_tar_gz}, {\""name\"": \""expect_cells\"", \""value\"": expect_cells}] failed: :\nFailed to coerce one or more keys or values for creating a Map[String, Int?]:\nList(java.lang.NumberFormatException: For input string: \""expect_cells\""\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:301)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:301)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n\tat wom.types.WomIntegerType$$anonfun$coercion$1.applyOrElse(WomIntegerType.scala:20)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34)\n\tat wom.types.WomType.$anonfun$coerceRawValue$2(WomType.scala:37)\n\tat scala.util.Try$.apply(Try.scala:209)\n\tat wom.types.WomType.coerceRawValue(WomType.scala:37)\n\tat wom.types.WomType.coerceRawValue$(WomType.scala:27)\n\tat wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9)\n\tat wom.values.WomOptionalValue.coerceAndSetNestingLevel(WomOptionalValue.scala:127)\n\tat wom.types.WomOptionalType$$anonfun$co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4128
https://github.com/broadinstitute/cromwell/issues/4128:487,Integrability,message,message,487,"This workflow `20d61ca9-0cc8-4dc1-9c20-85dbf9146de4`, running in the mint-dev Cromwell instance (version 35-fe8c4bf-SNAP) had a failed task where there is no start time in the metadata, but there is an end time. Was this task technically not started yet? If that's the case, then why would there be an end time?. ```; ""Adapter10xCount.inputs_for_submit.other_inputs"": [; {; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""shardIndex"": -1,; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Failed to evaluate 'Adapter10xCount.inputs_for_submit.other_inputs' (reason 1 of 1): Evaluating [{\""name\"": \""sample_id\"", \""value\"": GetInputs.sample_id}, {\""name\"": \""reference_name\"", \""value\"": reference_name}, {\""name\"": \""transcriptome_tar_gz\"", \""value\"": transcriptome_tar_gz}, {\""name\"": \""expect_cells\"", \""value\"": expect_cells}] failed: :\nFailed to coerce one or more keys or values for creating a Map[String, Int?]:\nList(java.lang.NumberFormatException: For input string: \""expect_cells\""\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:301)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:301)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n\tat wom.types.WomIntegerType$$anonfun$coercion$1.applyOrElse(WomIntegerType.scala:20)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34)\n\tat wom.types.WomType.$anonfun$coerceRawValue$2(WomType.scala:37)\n\tat scala.util.Try$.apply(Try.scala:209)\n\tat wom.types.WomType.coerceRawValue(WomType.scala:37)\n\tat wom.types.WomType.coerceRawValue$(WomType.scala:27)\n\tat wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9)\n\tat wom.values.WomOptionalValue.coerceAndSetNestingLevel(WomOptionalValue.scala:127)\n\tat wom.types.WomOptionalType$$anonfun$co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4128
https://github.com/broadinstitute/cromwell/issues/4128:7361,Testability,Log,LoggingFSM,7361,$1.traverse(list.scala:72)\n\tat cats.instances.ListInstances$$anon$1.traverse(list.scala:12)\n\tat cats.Traverse$Ops.traverse(Traverse.scala:19)\n\tat cats.Traverse$Ops.traverse$(Traverse.scala:19)\n\tat cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:447)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:152)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:150)\n\tat scala.PartialFunction$OrElse.apply(PartialFunction.scala:168)\n\tat akka.actor.FSM.processEvent(FSM.scala:673)\n\tat akka.actor.FSM.processEvent$(FSM.scala:667)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:43)\n\tat akka.actor.LoggingFSM.processEvent(FSM.scala:806)\n\tat akka.actor.LoggingFSM.processEvent$(FSM.scala:788)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:43)\n\tat akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:664)\n\tat akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:658)\n\tat akka.actor.Actor.aroundReceive(Actor.scala:517)\n\tat akka.actor.Actor.aroundReceive$(Actor.scala:515)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:43)\n\tat akka.actor.Timers.aroundReceive(Timers.scala:51)\n\tat akka.actor.Timers.aroundReceive$(Timers.scala:40)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:43)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:588)\n\tat akka.actor.ActorCell.invoke(ActorCell.s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4128
https://github.com/broadinstitute/cromwell/issues/4128:7442,Testability,Log,LoggingFSM,7442,on$1.traverse(list.scala:12)\n\tat cats.Traverse$Ops.traverse(Traverse.scala:19)\n\tat cats.Traverse$Ops.traverse$(Traverse.scala:19)\n\tat cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:447)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:152)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:150)\n\tat scala.PartialFunction$OrElse.apply(PartialFunction.scala:168)\n\tat akka.actor.FSM.processEvent(FSM.scala:673)\n\tat akka.actor.FSM.processEvent$(FSM.scala:667)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:43)\n\tat akka.actor.LoggingFSM.processEvent(FSM.scala:806)\n\tat akka.actor.LoggingFSM.processEvent$(FSM.scala:788)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:43)\n\tat akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:664)\n\tat akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:658)\n\tat akka.actor.Actor.aroundReceive(Actor.scala:517)\n\tat akka.actor.Actor.aroundReceive$(Actor.scala:515)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:43)\n\tat akka.actor.Timers.aroundReceive(Timers.scala:51)\n\tat akka.actor.Timers.aroundReceive$(Timers.scala:40)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:43)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:588)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:557)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4128
https://github.com/broadinstitute/cromwell/issues/4128:7498,Testability,Log,LoggingFSM,7498,verse(Traverse.scala:19)\n\tat cats.Traverse$Ops.traverse$(Traverse.scala:19)\n\tat cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.cromwell$engine$workflow$lifecycle$execution$WorkflowExecutionActor$$startRunnableNodes(WorkflowExecutionActor.scala:447)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:152)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor$$anonfun$5.applyOrElse(WorkflowExecutionActor.scala:150)\n\tat scala.PartialFunction$OrElse.apply(PartialFunction.scala:168)\n\tat akka.actor.FSM.processEvent(FSM.scala:673)\n\tat akka.actor.FSM.processEvent$(FSM.scala:667)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowExecutionActor.scala:43)\n\tat akka.actor.LoggingFSM.processEvent(FSM.scala:806)\n\tat akka.actor.LoggingFSM.processEvent$(FSM.scala:788)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processEvent(WorkflowExecutionActor.scala:43)\n\tat akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:664)\n\tat akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:658)\n\tat akka.actor.Actor.aroundReceive(Actor.scala:517)\n\tat akka.actor.Actor.aroundReceive$(Actor.scala:515)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:43)\n\tat akka.actor.Timers.aroundReceive(Timers.scala:51)\n\tat akka.actor.Timers.aroundReceive$(Timers.scala:40)\n\tat cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:43)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:588)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:557)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:225,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4128
https://github.com/broadinstitute/cromwell/issues/4129:98,Testability,test,tested,98,https://github.com/broadinstitute/cromwell/pull/4109 adds support for globbing; Make sure it gets tested at some point. Could just be turning on the corresponding centaur test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4129
https://github.com/broadinstitute/cromwell/issues/4129:171,Testability,test,test,171,https://github.com/broadinstitute/cromwell/pull/4109 adds support for globbing; Make sure it gets tested at some point. Could just be turning on the corresponding centaur test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4129
https://github.com/broadinstitute/cromwell/pull/4130:0,Deployability,Update,Update,0,Update: [link](https://cromwell.readthedocs.io/en/cjl_hog_factor_doc/cromwell_features/HogFactors/) to the RTD page!. - [x] ~~Wait for #4108 before merging~~,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4130
https://github.com/broadinstitute/cromwell/issues/4131:15,Availability,error,error,15,"I'm getting an error from Cromwell `Failed to coerce one or more keys or values for creating a Map[String, Int?]` when defining an object as an input to a task that consists of a string, file and int. I would expect the Int to get converted into a string, but the error makes it seem as if Cromwell is trying to convert all of the values into integers. Here is the part of the WDL that's throwing the error: https://github.com/HumanCellAtlas/pipeline-tools/blob/c949cb5ffa2df8f2a7fc7d7a4c34478e8eadbf34/adapter_pipelines/cellranger/adapter.wdl#L222",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131
https://github.com/broadinstitute/cromwell/issues/4131:264,Availability,error,error,264,"I'm getting an error from Cromwell `Failed to coerce one or more keys or values for creating a Map[String, Int?]` when defining an object as an input to a task that consists of a string, file and int. I would expect the Int to get converted into a string, but the error makes it seem as if Cromwell is trying to convert all of the values into integers. Here is the part of the WDL that's throwing the error: https://github.com/HumanCellAtlas/pipeline-tools/blob/c949cb5ffa2df8f2a7fc7d7a4c34478e8eadbf34/adapter_pipelines/cellranger/adapter.wdl#L222",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131
https://github.com/broadinstitute/cromwell/issues/4131:401,Availability,error,error,401,"I'm getting an error from Cromwell `Failed to coerce one or more keys or values for creating a Map[String, Int?]` when defining an object as an input to a task that consists of a string, file and int. I would expect the Int to get converted into a string, but the error makes it seem as if Cromwell is trying to convert all of the values into integers. Here is the part of the WDL that's throwing the error: https://github.com/HumanCellAtlas/pipeline-tools/blob/c949cb5ffa2df8f2a7fc7d7a4c34478e8eadbf34/adapter_pipelines/cellranger/adapter.wdl#L222",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131
https://github.com/broadinstitute/cromwell/issues/4131:442,Deployability,pipeline,pipeline-tools,442,"I'm getting an error from Cromwell `Failed to coerce one or more keys or values for creating a Map[String, Int?]` when defining an object as an input to a task that consists of a string, file and int. I would expect the Int to get converted into a string, but the error makes it seem as if Cromwell is trying to convert all of the values into integers. Here is the part of the WDL that's throwing the error: https://github.com/HumanCellAtlas/pipeline-tools/blob/c949cb5ffa2df8f2a7fc7d7a4c34478e8eadbf34/adapter_pipelines/cellranger/adapter.wdl#L222",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131
https://github.com/broadinstitute/cromwell/issues/4131:532,Energy Efficiency,adapt,adapter,532,"I'm getting an error from Cromwell `Failed to coerce one or more keys or values for creating a Map[String, Int?]` when defining an object as an input to a task that consists of a string, file and int. I would expect the Int to get converted into a string, but the error makes it seem as if Cromwell is trying to convert all of the values into integers. Here is the part of the WDL that's throwing the error: https://github.com/HumanCellAtlas/pipeline-tools/blob/c949cb5ffa2df8f2a7fc7d7a4c34478e8eadbf34/adapter_pipelines/cellranger/adapter.wdl#L222",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131
https://github.com/broadinstitute/cromwell/issues/4131:532,Integrability,adapter,adapter,532,"I'm getting an error from Cromwell `Failed to coerce one or more keys or values for creating a Map[String, Int?]` when defining an object as an input to a task that consists of a string, file and int. I would expect the Int to get converted into a string, but the error makes it seem as if Cromwell is trying to convert all of the values into integers. Here is the part of the WDL that's throwing the error: https://github.com/HumanCellAtlas/pipeline-tools/blob/c949cb5ffa2df8f2a7fc7d7a4c34478e8eadbf34/adapter_pipelines/cellranger/adapter.wdl#L222",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131
https://github.com/broadinstitute/cromwell/issues/4131:532,Modifiability,adapt,adapter,532,"I'm getting an error from Cromwell `Failed to coerce one or more keys or values for creating a Map[String, Int?]` when defining an object as an input to a task that consists of a string, file and int. I would expect the Int to get converted into a string, but the error makes it seem as if Cromwell is trying to convert all of the values into integers. Here is the part of the WDL that's throwing the error: https://github.com/HumanCellAtlas/pipeline-tools/blob/c949cb5ffa2df8f2a7fc7d7a4c34478e8eadbf34/adapter_pipelines/cellranger/adapter.wdl#L222",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131
https://github.com/broadinstitute/cromwell/pull/4132:272,Availability,down,down,272,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4132:20,Deployability,upgrade,upgrade,20,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4132:129,Deployability,integrat,integration,129,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4132:185,Deployability,upgrade,upgrade,185,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4132:307,Deployability,upgrade,upgrade,307,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4132:129,Integrability,integrat,integration,129,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4132:56,Modifiability,Refactor,Refactored,56,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4132:29,Testability,test,tests,29,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4132:193,Testability,test,tests,193,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4132:315,Testability,test,tests,315,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4132:407,Testability,log,logs,407,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4132:431,Testability,Log,Logging,431,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132
https://github.com/broadinstitute/cromwell/pull/4133:264,Availability,error,errors,264,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133
https://github.com/broadinstitute/cromwell/pull/4133:159,Integrability,rout,routing,159,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133
https://github.com/broadinstitute/cromwell/pull/4133:209,Integrability,rout,routing,209,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133
https://github.com/broadinstitute/cromwell/pull/4133:28,Testability,log,logged-but-expected,28,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133
https://github.com/broadinstitute/cromwell/pull/4133:48,Testability,test,test,48,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133
https://github.com/broadinstitute/cromwell/pull/4133:72,Testability,log,logback,72,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133
https://github.com/broadinstitute/cromwell/pull/4133:92,Testability,test,test,92,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133
https://github.com/broadinstitute/cromwell/pull/4133:167,Testability,log,logs,167,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133
https://github.com/broadinstitute/cromwell/pull/4133:217,Testability,log,logs,217,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133
https://github.com/broadinstitute/cromwell/pull/4135:360,Availability,toler,tolerant,360,Closes #3990. ~Awaiting accompanying OpenWDL PR with the underlying grammar change (after lunch).~. OpenWDL PR: https://github.com/openwdl/wdl/pull/253. ---. Reviewers: is it worth a changelog note to tell people `version draft-3` went away?. I think the change itself is OK because it seems like anyone who signed up to use a draft version is pretty breakage-tolerant.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4135
https://github.com/broadinstitute/cromwell/issues/4136:1472,Availability,Failure,Failure,1472," https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Im using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterIntervals; 2018-09-21 02:12:52,275 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a072",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136
https://github.com/broadinstitute/cromwell/issues/4136:1341,Deployability,configurat,configuration,1341,"ecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Im using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterInterva",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136
https://github.com/broadinstitute/cromwell/issues/4136:1461,Energy Efficiency,monitor,monitor,1461," https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Im using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterIntervals; 2018-09-21 02:12:52,275 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a072",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136
https://github.com/broadinstitute/cromwell/issues/4136:1573,Energy Efficiency,monitor,monitor,1573,"m using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterIntervals; 2018-09-21 02:12:52,275 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.DetermineGermlineContigPloidyCohortMode; 2018-09-21 02:31:18,476 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136
https://github.com/broadinstitute/cromwell/issues/4136:1729,Integrability,depend,dependent,1729,"m using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterIntervals; 2018-09-21 02:12:52,275 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.DetermineGermlineContigPloidyCohortMode; 2018-09-21 02:31:18,476 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136
https://github.com/broadinstitute/cromwell/issues/4136:1341,Modifiability,config,configuration,1341,"ecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Im using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterInterva",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136
https://github.com/broadinstitute/cromwell/issues/4136:1386,Security,PASSWORD,PASSWORDS,1386,"ecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Im using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterInterva",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136
https://github.com/broadinstitute/cromwell/issues/4136:1945,Testability,log,log,1945,"/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterIntervals; 2018-09-21 02:12:52,275 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.DetermineGermlineContigPloidyCohortMode; 2018-09-21 02:31:18,476 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PostprocessGermlineCNVCalls (95 shards) ; ```; The beginning of the sam log confirms that the skipped task was initially parsed in:; ```; 2018-09-20 22:45:09,504 INFO - MaterializeWorkflowDescriptorActor [UUID(6d980272)]: Parsing workflow as WDL draft-2; 2018-09-20 22:45:09,877 INFO - MaterializeWorkflowDescriptorActor [UUID",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136
https://github.com/broadinstitute/cromwell/issues/4136:2790,Testability,log,log,2790,"80272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterIntervals; 2018-09-21 02:12:52,275 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.DetermineGermlineContigPloidyCohortMode; 2018-09-21 02:31:18,476 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PostprocessGermlineCNVCalls (95 shards) ; ```; The beginning of the sam log confirms that the skipped task was initially parsed in:; ```; 2018-09-20 22:45:09,504 INFO - MaterializeWorkflowDescriptorActor [UUID(6d980272)]: Parsing workflow as WDL draft-2; 2018-09-20 22:45:09,877 INFO - MaterializeWorkflowDescriptorActor [UUID(6d980272)]: Call-to-Backend assignments: ; CNVGermlineCohortWorkflow.GermlineCNVCallerCohortMode -> JES, ; CNVGermlineCohortWorkflow.DetermineGermlineContigPloidyCohortMode -> JES, ; CNVGermlineCohortWorkflow.PreprocessIntervals -> JES, ; CNVGermlineCohortWorkflow.ScatterIntervals -> JES, ; CNVGermlineCohortWorkflow.PostprocessGermlineCNVCalls -> JES, ; CNVGermlineCohortWorkflow.AnnotateIntervals -> JES, ; CNVGermlineCohortWorkflow.CollectCounts -> JES; ```; <i>Notice that in the log extract above I had added new-lines and 4-blank indents to make them more readable.</i>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136
https://github.com/broadinstitute/cromwell/issues/4136:3530,Testability,log,log,3530,"80272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterIntervals; 2018-09-21 02:12:52,275 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.DetermineGermlineContigPloidyCohortMode; 2018-09-21 02:31:18,476 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PostprocessGermlineCNVCalls (95 shards) ; ```; The beginning of the sam log confirms that the skipped task was initially parsed in:; ```; 2018-09-20 22:45:09,504 INFO - MaterializeWorkflowDescriptorActor [UUID(6d980272)]: Parsing workflow as WDL draft-2; 2018-09-20 22:45:09,877 INFO - MaterializeWorkflowDescriptorActor [UUID(6d980272)]: Call-to-Backend assignments: ; CNVGermlineCohortWorkflow.GermlineCNVCallerCohortMode -> JES, ; CNVGermlineCohortWorkflow.DetermineGermlineContigPloidyCohortMode -> JES, ; CNVGermlineCohortWorkflow.PreprocessIntervals -> JES, ; CNVGermlineCohortWorkflow.ScatterIntervals -> JES, ; CNVGermlineCohortWorkflow.PostprocessGermlineCNVCalls -> JES, ; CNVGermlineCohortWorkflow.AnnotateIntervals -> JES, ; CNVGermlineCohortWorkflow.CollectCounts -> JES; ```; <i>Notice that in the log extract above I had added new-lines and 4-blank indents to make them more readable.</i>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136
https://github.com/broadinstitute/cromwell/issues/4136:47,Usability,feedback,feedback,47,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Im using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136
https://github.com/broadinstitute/cromwell/issues/4138:627,Availability,resilien,resiliency,627,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138
https://github.com/broadinstitute/cromwell/issues/4138:641,Availability,failure,failures,641,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138
https://github.com/broadinstitute/cromwell/issues/4138:548,Energy Efficiency,power,powerful,548,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138
https://github.com/broadinstitute/cromwell/issues/4138:481,Integrability,depend,dependent,481,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138
https://github.com/broadinstitute/cromwell/issues/4138:653,Integrability,depend,dependent,653,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138
https://github.com/broadinstitute/cromwell/issues/4138:1309,Integrability,depend,dependent,1309,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138
https://github.com/broadinstitute/cromwell/issues/4138:1196,Performance,concurren,concurrent,1196,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138
https://github.com/broadinstitute/cromwell/issues/4139:164,Availability,error,error,164,"So I've got a workflow with tasks A, B, C, D. The outputs of the workflow are the outputs of the tasks. I ran the workflow on a bunch of samples, and because of an error in one of my tools, the task D failed one some (but not all) samples: 6/8 workflows succeeded, 2/8 failed on task D. FireCloud writes workflow outputs back to its data model when the workflow completes. This means that for 2/8 of my samples, _none_ of the intermediates were written back to the data model. It would be nice if FireCloud could say ""this workflow failed, but this subset of workflow outputs were successfully computed, so I'll write those back for you."" However, FireCloud can't do this, because calling Cromwell's `/outputs` endpoint only provides the list of workflow outputs if the workflow succeeds. If the workflow fails or is still running, it returns `""outputs"" : {}`. Could Cromwell perhaps determine ""this workflow output has been computed to its final state, and thus I can tell you about it even though the workflow hasn't completed [successfully]""?. Note that the Swagger documentation for the `/outputs` endpoint says:. > Retrieve the outputs for the specified workflow. Cromwell will return any outputs which currently exist even if a workflow has not successfully completed. This does not seem to be the case.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4139
https://github.com/broadinstitute/cromwell/issues/4140:199,Availability,down,downstream,199,"Based on a conversation with the Epigenomics group that is using Cromwell 34 on PAPI v2 --; it seems that a task can fail, not delocalize an expected output and the workflow will continue to start a downstream task but fail at running the command (as an expected input is missing). It's been confirmed that Cromwell was running in a Fail fast mode. There seem to be two main requirements here:; 1. Cromwell should be evaluating if all expected outputs for a task exist before marking the task as a success.; 2. Cromwell should fail a job if it failed localize a specific input file. ; 3. Cromwell should fail a job if it failed to delocalize any expected output file. AC: Ensure that the PAPI v2 backend fulfills requirements #1-3.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4140
https://github.com/broadinstitute/cromwell/issues/4141:306,Deployability,update,updated,306,"From user reports, this is upsetting cost estimation (and is scary anyway wrt re-writing history!. Needs validation and a reproducible case, but presumably something like:. - Run a long workflow, one shard fails; - Re-run the same workflow, most shard call cache; - The shards in the original workflow get updated to have the call-cache timings, rather than the original long timings.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4141
https://github.com/broadinstitute/cromwell/issues/4141:257,Performance,cache,cache,257,"From user reports, this is upsetting cost estimation (and is scary anyway wrt re-writing history!. Needs validation and a reproducible case, but presumably something like:. - Run a long workflow, one shard fails; - Re-run the same workflow, most shard call cache; - The shards in the original workflow get updated to have the call-cache timings, rather than the original long timings.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4141
https://github.com/broadinstitute/cromwell/issues/4141:331,Performance,cache,cache,331,"From user reports, this is upsetting cost estimation (and is scary anyway wrt re-writing history!. Needs validation and a reproducible case, but presumably something like:. - Run a long workflow, one shard fails; - Re-run the same workflow, most shard call cache; - The shards in the original workflow get updated to have the call-cache timings, rather than the original long timings.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4141
https://github.com/broadinstitute/cromwell/issues/4141:105,Security,validat,validation,105,"From user reports, this is upsetting cost estimation (and is scary anyway wrt re-writing history!. Needs validation and a reproducible case, but presumably something like:. - Run a long workflow, one shard fails; - Re-run the same workflow, most shard call cache; - The shards in the original workflow get updated to have the call-cache timings, rather than the original long timings.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4141
https://github.com/broadinstitute/cromwell/issues/4142:217,Safety,abort,abort,217,"At the moment there's a single Cromwell server underlying the CromIAM server, and all Cromwell traffic is directed there. Modify CromIAM to allow for a second Cromwell address (defaulting to the main one), and direct abort requests to that Cromwell. NB: This is an intermediate term hack, so err on the side of quick & pragmatic instead of perfect & beautiful",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4142
https://github.com/broadinstitute/cromwell/issues/4147:1187,Availability,echo,echo,1187,"es), Cromwell will mark that as SUCCESS, and delocalize what ever is produced by the command. -----------; Expected behavior:. The job should fail. -----------; Actually happen:. The job is marked `Success`. -----------. the backend is the methods cromwell server v 34, the actual workflow-ID is ; `17faf5b5-be67-4756-b168-130450081cfb`; The bucket is here:; `gs://broad-dsde-methods/cromwell-execution-34/TestOutputMultipleFiles/17faf5b5-be67-4756-b168-130450081cfb/call-PrintsToFileTest`. JSON input. ```json; {; ""TestOutputMultipleFiles.dummy_array"": [""chr1"", ""chr2""]; }. ```; And the WDL script; ```wdl; workflow TestOutputMultipleFiles {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as PrintsToFileTest {; input:; out_prefix = ele,; to_print = ele; }; }. output {; Array[Array[File]] matrix = [PrintsToFileTest.out_txt, ; PrintsToFileTest.out_md]; }; }. task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147
https://github.com/broadinstitute/cromwell/issues/4147:1315,Availability,failure,failure,1315,"he command. -----------; Expected behavior:. The job should fail. -----------; Actually happen:. The job is marked `Success`. -----------. the backend is the methods cromwell server v 34, the actual workflow-ID is ; `17faf5b5-be67-4756-b168-130450081cfb`; The bucket is here:; `gs://broad-dsde-methods/cromwell-execution-34/TestOutputMultipleFiles/17faf5b5-be67-4756-b168-130450081cfb/call-PrintsToFileTest`. JSON input. ```json; {; ""TestOutputMultipleFiles.dummy_array"": [""chr1"", ""chr2""]; }. ```; And the WDL script; ```wdl; workflow TestOutputMultipleFiles {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as PrintsToFileTest {; input:; out_prefix = ele,; to_print = ele; }; }. output {; Array[Array[File]] matrix = [PrintsToFileTest.out_txt, ; PrintsToFileTest.out_md]; }; }. task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call Do",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147
https://github.com/broadinstitute/cromwell/issues/4147:1352,Availability,echo,echo,1352,"ually happen:. The job is marked `Success`. -----------. the backend is the methods cromwell server v 34, the actual workflow-ID is ; `17faf5b5-be67-4756-b168-130450081cfb`; The bucket is here:; `gs://broad-dsde-methods/cromwell-execution-34/TestOutputMultipleFiles/17faf5b5-be67-4756-b168-130450081cfb/call-PrintsToFileTest`. JSON input. ```json; {; ""TestOutputMultipleFiles.dummy_array"": [""chr1"", ""chr2""]; }. ```; And the WDL script; ```wdl; workflow TestOutputMultipleFiles {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as PrintsToFileTest {; input:; out_prefix = ele,; to_print = ele; }; }. output {; Array[Array[File]] matrix = [PrintsToFileTest.out_txt, ; PrintsToFileTest.out_md]; }; }. task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call DownstreamConsumer {; input:; txt_array = UpstreamPrintToFile.out_txt,; md_array = Up",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147
https://github.com/broadinstitute/cromwell/issues/4147:1643,Availability,down,downstream,1643,"b5-be67-4756-b168-130450081cfb/call-PrintsToFileTest`. JSON input. ```json; {; ""TestOutputMultipleFiles.dummy_array"": [""chr1"", ""chr2""]; }. ```; And the WDL script; ```wdl; workflow TestOutputMultipleFiles {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as PrintsToFileTest {; input:; out_prefix = ele,; to_print = ele; }; }. output {; Array[Array[File]] matrix = [PrintsToFileTest.out_txt, ; PrintsToFileTest.out_md]; }; }. task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call DownstreamConsumer {; input:; txt_array = UpstreamPrintToFile.out_txt,; md_array = UpstreamPrintToFile.out_md; }. output {; File merged_txt = DownstreamConsumer.cat_txt; File merged_md = DownstreamConsumer.cat_md; }; }. # upstream task that supposed to be producing 2 out files; task PrintsToFile {. String out_prefix; String to_print. command {; touch ${o",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147
https://github.com/broadinstitute/cromwell/issues/4147:2282,Availability,Down,DownstreamConsumer,2282,"port failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call DownstreamConsumer {; input:; txt_array = UpstreamPrintToFile.out_txt,; md_array = UpstreamPrintToFile.out_md; }. output {; File merged_txt = DownstreamConsumer.cat_txt; File merged_md = DownstreamConsumer.cat_md; }; }. # upstream task that supposed to be producing 2 out files; task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. # downstream task that depends on upstream task outputing all files; task DownstreamConsumer {; Array[File] txt_array; Array[File] md_array. command {; cat ${sep="" ""} txt_array > merged.txt; cat ${sep="" ""} md_array > merged.md; }. runtime {; docker: ""ubuntu:tr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147
https://github.com/broadinstitute/cromwell/issues/4147:2424,Availability,Down,DownstreamConsumer,2424,"u:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call DownstreamConsumer {; input:; txt_array = UpstreamPrintToFile.out_txt,; md_array = UpstreamPrintToFile.out_md; }. output {; File merged_txt = DownstreamConsumer.cat_txt; File merged_md = DownstreamConsumer.cat_md; }; }. # upstream task that supposed to be producing 2 out files; task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. # downstream task that depends on upstream task outputing all files; task DownstreamConsumer {; Array[File] txt_array; Array[File] md_array. command {; cat ${sep="" ""} txt_array > merged.txt; cat ${sep="" ""} md_array > merged.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""50"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File cat_txt = ""merged.tx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147
