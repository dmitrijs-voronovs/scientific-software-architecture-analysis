id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md:9602,Modifiability,config,configuration,9602," considered successful. ```; runtime {; continueOnReturnCode: true; }; ```. When set to an integer, or an array of integers, only those integers will be considered as successful return codes. ```; runtime {; continueOnReturnCode: 1; }; ```. ```; runtime {; continueOnReturnCode: [0, 1]; }; ```. ### `failOnStderr`. *Default: _false_*. Some programs write to the standard error stream when there is an error, but still return a zero exit code. Set `failOnStderr` to true for these tasks, and it will be considered a failure if anything is written to the standard error stream. ```; runtime {; failOnStderr: true; }; ```. ### `zones`. The ordered list of zone preference (see [Region and Zones](https://cloud.google.com/compute/docs/zones) documentation for specifics). *The zones are specified as a space separated list, with no commas:*. ```; runtime {; zones: ""us-central1-c us-central1-b""; }; ```. Defaults to the configuration setting `genomics.default-zones` in the Google Cloud configuration block, which in turn defaults to using `us-central1-b`. ### `preemptible`. *Default: _0_*. Passed to Google Cloud: ""If applicable, preemptible machines may be used for the run."". Take an Int as a value that indicates the maximum number of times Cromwell should request a preemptible machine for this task before defaulting back to a non-preemptible one. ; *eg. With a value of 1, Cromwell will request a preemptible VM, if the VM is preempted, the task will be retried with a non-preemptible VM.*. ```; runtime {; preemptible: 1; }; ```. ### `bootDiskSizeGb`. In addition to working disks, Google Cloud allows specification of a boot disk size. This is the disk where the docker image itself is booted (**not the working directory of your task on the VM**).; Its primary purpose is to ensure that larger docker images can fit on the boot disk.; ```; runtime {; # Yikes, we have a big OS in this docker image! Allow 50GB to hold it:; bootDiskSizeGb: 50; }; ```. Since no `local-disk` entry is specified, ",MatchSource.DOCS,docs/RuntimeAttributes.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md
https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md:14264,Modifiability,config,configured,14264,"umentation](https://cloud.google.com/compute/docs/gpus/)).; Make sure to choose a zone for which the type of GPU you want to attach is available. The types of compute GPU supported are:. * `nvidia-tesla-k80` ; * `nvidia-tesla-v100`; * `nvidia-tesla-p100`; * `nvidia-tesla-p4`; * `nvidia-tesla-t4`. For the latest list of supported GPU's, please visit [Google's GPU documentation](nvidia-drivers-us-public). The default driver is `418.87.00`, you may specify your own via the `nvidiaDriverVersion` key. Make sure that driver exists in the `nvidia-drivers-us-public` beforehand, per the [Google Pipelines API documentation](https://cloud.google.com/genomics/reference/rest/Shared.Types/Metadata#VirtualMachine). . ```; runtime {; gpuType: ""nvidia-tesla-k80""; gpuCount: 2; nvidiaDriverVersion: ""418.87.00""; zones: [""us-central1-c""]; }; ```. ### `cpuPlatform`. This option is specific to the Google Cloud backend, specifically [this](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform) feature when a certain minimum CPU platform is desired. A usage example:. ```; runtime {; cpu: 2; cpuPlatform: ""Intel Cascade Lake""; }; ```; Note that when this options is specified, make sure the requested CPU platform is [available](https://cloud.google.com/compute/docs/regions-zones/#available) in the `zones` you selected. The following CPU platforms are currently supported by the Google Cloud backend:; - `Intel Ice Lake`; - `Intel Cascade Lake`; - `Intel Skylake` ; - `Intel Broadwell` ; - `Intel Haswell` ; - `Intel Ivy Bridge` ; - `Intel Sandy Bridge`; - `AMD Rome`. ### 'useDockerImageCache'. This option is specific to the Google Cloud backend, moreover it is only supported by Google Life Sciences API starting from version v2 beta.; In order to use this feature Cromwell has to have PAPI v2 backend configured with this feature enabled. ; More information about this feature and it's configuration can be found [in the Google backend section of documentation](backends/Google.md).; ",MatchSource.DOCS,docs/RuntimeAttributes.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md
https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md:14349,Modifiability,config,configuration,14349,"umentation](https://cloud.google.com/compute/docs/gpus/)).; Make sure to choose a zone for which the type of GPU you want to attach is available. The types of compute GPU supported are:. * `nvidia-tesla-k80` ; * `nvidia-tesla-v100`; * `nvidia-tesla-p100`; * `nvidia-tesla-p4`; * `nvidia-tesla-t4`. For the latest list of supported GPU's, please visit [Google's GPU documentation](nvidia-drivers-us-public). The default driver is `418.87.00`, you may specify your own via the `nvidiaDriverVersion` key. Make sure that driver exists in the `nvidia-drivers-us-public` beforehand, per the [Google Pipelines API documentation](https://cloud.google.com/genomics/reference/rest/Shared.Types/Metadata#VirtualMachine). . ```; runtime {; gpuType: ""nvidia-tesla-k80""; gpuCount: 2; nvidiaDriverVersion: ""418.87.00""; zones: [""us-central1-c""]; }; ```. ### `cpuPlatform`. This option is specific to the Google Cloud backend, specifically [this](https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform) feature when a certain minimum CPU platform is desired. A usage example:. ```; runtime {; cpu: 2; cpuPlatform: ""Intel Cascade Lake""; }; ```; Note that when this options is specified, make sure the requested CPU platform is [available](https://cloud.google.com/compute/docs/regions-zones/#available) in the `zones` you selected. The following CPU platforms are currently supported by the Google Cloud backend:; - `Intel Ice Lake`; - `Intel Cascade Lake`; - `Intel Skylake` ; - `Intel Broadwell` ; - `Intel Haswell` ; - `Intel Ivy Bridge` ; - `Intel Sandy Bridge`; - `AMD Rome`. ### 'useDockerImageCache'. This option is specific to the Google Cloud backend, moreover it is only supported by Google Life Sciences API starting from version v2 beta.; In order to use this feature Cromwell has to have PAPI v2 backend configured with this feature enabled. ; More information about this feature and it's configuration can be found [in the Google backend section of documentation](backends/Google.md).; ",MatchSource.DOCS,docs/RuntimeAttributes.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md
https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md:10981,Performance,load,load,10981,"t a preemptible VM, if the VM is preempted, the task will be retried with a non-preemptible VM.*. ```; runtime {; preemptible: 1; }; ```. ### `bootDiskSizeGb`. In addition to working disks, Google Cloud allows specification of a boot disk size. This is the disk where the docker image itself is booted (**not the working directory of your task on the VM**).; Its primary purpose is to ensure that larger docker images can fit on the boot disk.; ```; runtime {; # Yikes, we have a big OS in this docker image! Allow 50GB to hold it:; bootDiskSizeGb: 50; }; ```. Since no `local-disk` entry is specified, Cromwell will automatically add `local-disk 10 SSD` to this list. ### `noAddress`. This runtime attribute adds support to disable assigning external IP addresses to VMs provisioned by the Google backend. If set to true, the VM will NOT be provided with a public IP address, and only contain an internal IP. If this option is enabled, the associated job can only load docker images from Google Container Registry, and the job executable cannot use external services other than Google APIs. Note well! You must enable ""Private Google Access"" for this feature to work. See ""How To Setup"" below. For example, the task below will succeed:; ```; command {; echo ""hello!""; ; }. runtime {; docker: ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest""; noAddress: true; }; ```. The task below will fail for two reasons:; 1. The command is accessing an external service, in this case GitHub.; 2. The docker image is available in DockerHub and not the Google Container Registry. ; ```; command {; git clone https://github.com/broadinstitute/cromwell.git; ; }. runtime {; docker: ""docker.io/alpine/git:latest""; noAddress: true; }; ```. #### How to Setup. Configure your Google network to use ""Private Google Access"". This will allow your VMs to access Google Services including Google Container Registry, as well as Dockerhub images. 1. Using `gcloud compute networks subnets list`, identify the subnet and region you w",MatchSource.DOCS,docs/RuntimeAttributes.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md
https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md:7462,Safety,timeout,timeout,7462,"SD""; }; ```. *Example 2: Mounting an Additional Two Disks*. ```; runtime {; disks: ""/mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""; }; ```. ### `disk`. Specific to the TES backend, sets the `disk_gb` resource. ```; runtime {; disk: ""25 GB""; }; ```. ### `docker`. When specified, Cromwell will run your task within the specified Docker image. . ```; runtime {; docker: ""ubuntu:latest""; }; ```. - Local: Cromwell will automatically run the docker container.; - SFS: When a docker container exists within a task, the `submit-docker` method is called. See the [Getting started with containers](/tutorials/Containers/) guide for more information.; - GCP: This attribute is mandatory when submitting tasks to Google Cloud.; - AWS Batch: This attribute is mandatory when submitting tasks to AWS Batch. ### `maxRetries`. *Default: _0_*. This retry option is introduced to provide a method for tackling transient job failures. For example, if a task fails due to a timeout from accessing an external service, then this option helps re-run the failed the task without having to re-run the entire workflow. It takes an Int as a value that indicates the maximum number of times Cromwell should retry a failed task. This retry is applied towards jobs that fail while executing the task command. This method only applies to transient job failures and is a feeble attempt to retry a job, that is it cannot be used to increase memory in out-of-memory situations. If using the Google backend, it's important to note that The `maxRetries` count is independent from the [preemptible](#preemptible) count. For example, the task below can be retried up to 6 times if it's preempted 3 times AND the command execution fails 3 times. ```; runtime {; preemptible: 3; maxRetries: 3; }; ```. ### `continueOnReturnCode`; *Default: _0_*. When each task finishes it returns a code. Normally, a non-zero return code indicates a failure. However you can override this behavior by specifying the `continueOnReturnCode` attribute. When set ",MatchSource.DOCS,docs/RuntimeAttributes.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md
https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md:1790,Security,expose,exposed,1790,"(/backends/Backends). See the table below for common attributes that apply to _most_ backends. | Runtime Attribute | Local | Google Cloud | TES | AWS Batch | HPC |; |-------------------------------------------------|:-----:|:------------:|-----------|:---------:|:-------------------------:|; | [`cpu`](#cpu) | | ✅ | | ✅ | `cpu` |; | [`memory`](#memory) | | ✅ | | ✅ | `memory_mb` / `memory_gb` |; | [`disks`](#disks) | | ✅ | ⚠️ Note 1 | ⚠️ Note 2 | ℹ️ Note 3 |; | [`disk`](#disk) | | | ✅ | | |; | [`docker`](#docker) | ✅ | ✅ | | ✅ | `docker` ℹ️ Note 3 |; | [`maxRetries`](#maxretries) | ✅ | ✅ | | ✅ | ℹ️ Note 3 |; | [`continueOnReturnCode`](#continueonreturncode) | ✅ | ✅ | | ✅ | ℹ️ Note 3 |; | [`failOnStderr`](#failonstderr) | ✅ | ✅ | | ✅ | ℹ️ Note 3 |. > **Note 1**; > ; > Partial support. See [TES documentation](/backends/TES) for details. ; ; > **Note 2**; >; > Partial support. See [`disks`](#disks) for details. > **Note 3**; > ; > The HPC [Shared Filesystem backend](/backends/HPC#shared-filesystem) (SFS) is fully configurable and any number of attributes can be exposed. Cromwell recognizes some of these attributes (`cpu`, `memory` and `docker`) and parses them into the attribute listed in the table which can be used within the HPC backend configuration. ### Google Cloud Specific Attributes; There are a number of additional runtime attributes that apply to the Google Cloud Platform:. - [zones](#zones); - [preemptible](#preemptible); - [bootDiskSizeGb](#bootdisksizegb); - [noAddress](#noaddress); - [gpuCount, gpuType, and nvidiaDriverVersion](#gpucount-gputype-and-nvidiadriverversion); - [cpuPlatform](#cpuplatform); - [useDockerImageCache](#usedockerimagecache). ## Expression support. Runtime attribute values are interpreted as expressions. This means that it has the ability to express the value of a runtime attribute as a function of one of the task's inputs. ; _For example:_. ```; task runtime_test {; String ubuntu_tag; Int memory_gb. command {; ./my_binary; }. runtime {",MatchSource.DOCS,docs/RuntimeAttributes.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md
https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md:7475,Security,access,accessing,7475,"SD""; }; ```. *Example 2: Mounting an Additional Two Disks*. ```; runtime {; disks: ""/mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""; }; ```. ### `disk`. Specific to the TES backend, sets the `disk_gb` resource. ```; runtime {; disk: ""25 GB""; }; ```. ### `docker`. When specified, Cromwell will run your task within the specified Docker image. . ```; runtime {; docker: ""ubuntu:latest""; }; ```. - Local: Cromwell will automatically run the docker container.; - SFS: When a docker container exists within a task, the `submit-docker` method is called. See the [Getting started with containers](/tutorials/Containers/) guide for more information.; - GCP: This attribute is mandatory when submitting tasks to Google Cloud.; - AWS Batch: This attribute is mandatory when submitting tasks to AWS Batch. ### `maxRetries`. *Default: _0_*. This retry option is introduced to provide a method for tackling transient job failures. For example, if a task fails due to a timeout from accessing an external service, then this option helps re-run the failed the task without having to re-run the entire workflow. It takes an Int as a value that indicates the maximum number of times Cromwell should retry a failed task. This retry is applied towards jobs that fail while executing the task command. This method only applies to transient job failures and is a feeble attempt to retry a job, that is it cannot be used to increase memory in out-of-memory situations. If using the Google backend, it's important to note that The `maxRetries` count is independent from the [preemptible](#preemptible) count. For example, the task below can be retried up to 6 times if it's preempted 3 times AND the command execution fails 3 times. ```; runtime {; preemptible: 3; maxRetries: 3; }; ```. ### `continueOnReturnCode`; *Default: _0_*. When each task finishes it returns a code. Normally, a non-zero return code indicates a failure. However you can override this behavior by specifying the `continueOnReturnCode` attribute. When set ",MatchSource.DOCS,docs/RuntimeAttributes.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md
https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md:11439,Security,access,accessing,11439,"sk.; ```; runtime {; # Yikes, we have a big OS in this docker image! Allow 50GB to hold it:; bootDiskSizeGb: 50; }; ```. Since no `local-disk` entry is specified, Cromwell will automatically add `local-disk 10 SSD` to this list. ### `noAddress`. This runtime attribute adds support to disable assigning external IP addresses to VMs provisioned by the Google backend. If set to true, the VM will NOT be provided with a public IP address, and only contain an internal IP. If this option is enabled, the associated job can only load docker images from Google Container Registry, and the job executable cannot use external services other than Google APIs. Note well! You must enable ""Private Google Access"" for this feature to work. See ""How To Setup"" below. For example, the task below will succeed:; ```; command {; echo ""hello!""; ; }. runtime {; docker: ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest""; noAddress: true; }; ```. The task below will fail for two reasons:; 1. The command is accessing an external service, in this case GitHub.; 2. The docker image is available in DockerHub and not the Google Container Registry. ; ```; command {; git clone https://github.com/broadinstitute/cromwell.git; ; }. runtime {; docker: ""docker.io/alpine/git:latest""; noAddress: true; }; ```. #### How to Setup. Configure your Google network to use ""Private Google Access"". This will allow your VMs to access Google Services including Google Container Registry, as well as Dockerhub images. 1. Using `gcloud compute networks subnets list`, identify the subnet and region you will be using with Cromwell. If multiple, run the next step for each region and subnet you wish to use.; 1. `gcloud compute networks subnets update [SUBNET-NAME] --region [REGION] --enable-private-ip-google-access`. That's it! You can now run with `noAddress` runtime attribute and it will work as expected. ### `gpuCount`, `gpuType`, and `nvidiaDriverVersion`. Attach GPUs to the instance when running on the Pipelines API([GPU documentation",MatchSource.DOCS,docs/RuntimeAttributes.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md
https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md:11842,Security,access,access,11842," a public IP address, and only contain an internal IP. If this option is enabled, the associated job can only load docker images from Google Container Registry, and the job executable cannot use external services other than Google APIs. Note well! You must enable ""Private Google Access"" for this feature to work. See ""How To Setup"" below. For example, the task below will succeed:; ```; command {; echo ""hello!""; ; }. runtime {; docker: ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest""; noAddress: true; }; ```. The task below will fail for two reasons:; 1. The command is accessing an external service, in this case GitHub.; 2. The docker image is available in DockerHub and not the Google Container Registry. ; ```; command {; git clone https://github.com/broadinstitute/cromwell.git; ; }. runtime {; docker: ""docker.io/alpine/git:latest""; noAddress: true; }; ```. #### How to Setup. Configure your Google network to use ""Private Google Access"". This will allow your VMs to access Google Services including Google Container Registry, as well as Dockerhub images. 1. Using `gcloud compute networks subnets list`, identify the subnet and region you will be using with Cromwell. If multiple, run the next step for each region and subnet you wish to use.; 1. `gcloud compute networks subnets update [SUBNET-NAME] --region [REGION] --enable-private-ip-google-access`. That's it! You can now run with `noAddress` runtime attribute and it will work as expected. ### `gpuCount`, `gpuType`, and `nvidiaDriverVersion`. Attach GPUs to the instance when running on the Pipelines API([GPU documentation](https://cloud.google.com/compute/docs/gpus/)).; Make sure to choose a zone for which the type of GPU you want to attach is available. The types of compute GPU supported are:. * `nvidia-tesla-k80` ; * `nvidia-tesla-v100`; * `nvidia-tesla-p100`; * `nvidia-tesla-p4`; * `nvidia-tesla-t4`. For the latest list of supported GPU's, please visit [Google's GPU documentation](nvidia-drivers-us-public). The default driver",MatchSource.DOCS,docs/RuntimeAttributes.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md
https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md:12222,Security,access,access,12222,"to work. See ""How To Setup"" below. For example, the task below will succeed:; ```; command {; echo ""hello!""; ; }. runtime {; docker: ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest""; noAddress: true; }; ```. The task below will fail for two reasons:; 1. The command is accessing an external service, in this case GitHub.; 2. The docker image is available in DockerHub and not the Google Container Registry. ; ```; command {; git clone https://github.com/broadinstitute/cromwell.git; ; }. runtime {; docker: ""docker.io/alpine/git:latest""; noAddress: true; }; ```. #### How to Setup. Configure your Google network to use ""Private Google Access"". This will allow your VMs to access Google Services including Google Container Registry, as well as Dockerhub images. 1. Using `gcloud compute networks subnets list`, identify the subnet and region you will be using with Cromwell. If multiple, run the next step for each region and subnet you wish to use.; 1. `gcloud compute networks subnets update [SUBNET-NAME] --region [REGION] --enable-private-ip-google-access`. That's it! You can now run with `noAddress` runtime attribute and it will work as expected. ### `gpuCount`, `gpuType`, and `nvidiaDriverVersion`. Attach GPUs to the instance when running on the Pipelines API([GPU documentation](https://cloud.google.com/compute/docs/gpus/)).; Make sure to choose a zone for which the type of GPU you want to attach is available. The types of compute GPU supported are:. * `nvidia-tesla-k80` ; * `nvidia-tesla-v100`; * `nvidia-tesla-p100`; * `nvidia-tesla-p4`; * `nvidia-tesla-t4`. For the latest list of supported GPU's, please visit [Google's GPU documentation](nvidia-drivers-us-public). The default driver is `418.87.00`, you may specify your own via the `nvidiaDriverVersion` key. Make sure that driver exists in the `nvidia-drivers-us-public` beforehand, per the [Google Pipelines API documentation](https://cloud.google.com/genomics/reference/rest/Shared.Types/Metadata#VirtualMachine). . ```; runtime {; g",MatchSource.DOCS,docs/RuntimeAttributes.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md
https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md:7120,Usability,guid,guide,7120,"ill be ignored. If provided, the mount point will be verified at runtime. The Disk type must be one of ""LOCAL"", ""SSD"", or ""HDD"". When set to ""LOCAL"", the size of the drive is constrained to 375 GB intervals so intermediate values will be rounded up to the next 375 GB. All disks are set to auto-delete after the job completes. *Example 1: Changing the Localization Disk*. ```; runtime {; disks: ""local-disk 100 SSD""; }; ```. *Example 2: Mounting an Additional Two Disks*. ```; runtime {; disks: ""/mnt/my_mnt 3 SSD, /mnt/my_mnt2 500 HDD""; }; ```. ### `disk`. Specific to the TES backend, sets the `disk_gb` resource. ```; runtime {; disk: ""25 GB""; }; ```. ### `docker`. When specified, Cromwell will run your task within the specified Docker image. . ```; runtime {; docker: ""ubuntu:latest""; }; ```. - Local: Cromwell will automatically run the docker container.; - SFS: When a docker container exists within a task, the `submit-docker` method is called. See the [Getting started with containers](/tutorials/Containers/) guide for more information.; - GCP: This attribute is mandatory when submitting tasks to Google Cloud.; - AWS Batch: This attribute is mandatory when submitting tasks to AWS Batch. ### `maxRetries`. *Default: _0_*. This retry option is introduced to provide a method for tackling transient job failures. For example, if a task fails due to a timeout from accessing an external service, then this option helps re-run the failed the task without having to re-run the entire workflow. It takes an Int as a value that indicates the maximum number of times Cromwell should retry a failed task. This retry is applied towards jobs that fail while executing the task command. This method only applies to transient job failures and is a feeble attempt to retry a job, that is it cannot be used to increase memory in out-of-memory situations. If using the Google backend, it's important to note that The `maxRetries` count is independent from the [preemptible](#preemptible) count. For exam",MatchSource.DOCS,docs/RuntimeAttributes.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/RuntimeAttributes.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:404,Availability,resilien,resiliency,404,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:2330,Availability,heartbeat,heartbeats,2330,"** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configuration changes to operate in the front end role. If a particular; Cromwell instance is not intended to operate in the front end role then requests should not be directed to; that instance. If there are multiple front end instances then it may be desirable to configure a load balancer; in front of these instances to direct requests to the front end instances only. ",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:2406,Availability,heartbeat,heartbeats,2406,"** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configuration changes to operate in the front end role. If a particular; Cromwell instance is not intended to operate in the front end role then requests should not be directed to; that instance. If there are multiple front end instances then it may be desirable to configure a load balancer; in front of these instances to direct requests to the front end instances only. ",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:221,Deployability,deploy,deployment,221,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:339,Deployability,deploy,deployments,339,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:615,Deployability,deploy,deployment,615,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:721,Deployability,deploy,deployment,721,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:878,Deployability,configurat,configuration,878,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:920,Deployability,configurat,configuration,920,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:978,Deployability,configurat,configuration,978,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:1071,Deployability,configurat,configuration,1071,"e fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, re",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:1416,Deployability,configurat,configuration,1416,"scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) d",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:1570,Deployability,configurat,configuration,1570,"lti-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configurati",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:2531,Deployability,configurat,configuration,2531,"** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configuration changes to operate in the front end role. If a particular; Cromwell instance is not intended to operate in the front end role then requests should not be directed to; that instance. If there are multiple front end instances then it may be desirable to configure a load balancer; in front of these instances to direct requests to the front end instances only. ",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:2591,Deployability,configurat,configuration,2591,"** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configuration changes to operate in the front end role. If a particular; Cromwell instance is not intended to operate in the front end role then requests should not be directed to; that instance. If there are multiple front end instances then it may be desirable to configure a load balancer; in front of these instances to direct requests to the front end instances only. ",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:878,Modifiability,config,configuration,878,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:920,Modifiability,config,configuration,920,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:978,Modifiability,config,configuration,978,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:1071,Modifiability,config,configuration,1071,"e fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, re",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:1116,Modifiability,config,config,1116,"er - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a no",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:1416,Modifiability,config,configuration,1416,"scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) d",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:1570,Modifiability,config,configuration,1570,"lti-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configurati",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:2531,Modifiability,config,configuration,2531,"** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configuration changes to operate in the front end role. If a particular; Cromwell instance is not intended to operate in the front end role then requests should not be directed to; that instance. If there are multiple front end instances then it may be desirable to configure a load balancer; in front of these instances to direct requests to the front end instances only. ",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:2591,Modifiability,config,configuration,2591,"** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configuration changes to operate in the front end role. If a particular; Cromwell instance is not intended to operate in the front end role then requests should not be directed to; that instance. If there are multiple front end instances then it may be desirable to configure a load balancer; in front of these instances to direct requests to the front end instances only. ",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:2857,Modifiability,config,configure,2857,"** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configuration changes to operate in the front end role. If a particular; Cromwell instance is not intended to operate in the front end role then requests should not be directed to; that instance. If there are multiple front end instances then it may be desirable to configure a load balancer; in front of these instances to direct requests to the front end instances only. ",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:32,Performance,perform,performing,32,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:260,Performance,perform,performs,260,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:419,Performance,scalab,scalability,419,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:517,Performance,perform,performing,517,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:630,Performance,perform,perform,630,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:1262,Performance,perform,performing,1262," is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:1338,Performance,perform,performing,1338," is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:2203,Performance,concurren,concurrently,2203,"** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configuration changes to operate in the front end role. If a particular; Cromwell instance is not intended to operate in the front end role then requests should not be directed to; that instance. If there are multiple front end instances then it may be desirable to configure a load balancer; in front of these instances to direct requests to the front end instances only. ",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:2257,Performance,concurren,concurrent-workflows,2257,"** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configuration changes to operate in the front end role. If a particular; Cromwell instance is not intended to operate in the front end role then requests should not be directed to; that instance. If there are multiple front end instances then it may be desirable to configure a load balancer; in front of these instances to direct requests to the front end instances only. ",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:2869,Performance,load,load,2869,"** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; # Set this to 0 for a non-runner.; max-workflow-launch-count = 1. # The maximum number of workflows to run concurrently.; # Set this to 0 for a non-runner.; max-concurrent-workflows = 5000; ...; }; ```. The documentation on [workflow heartbeats](https://cromwell.readthedocs.io/en/stable/Configuring/#workflow-heartbeats) describes how multiple Cromwell; runners collaborate to run workflows from a single workflow store. ** Front end configuration **. Cromwell instances should not require any configuration changes to operate in the front end role. If a particular; Cromwell instance is not intended to operate in the front end role then requests should not be directed to; that instance. If there are multiple front end instances then it may be desirable to configure a load balancer; in front of these instances to direct requests to the front end instances only. ",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md:203,Usability,simpl,simplest,203,"; Cromwell can be thought of as performing three fundamental roles:. 1. Front end - handling all REST requests; 1. Runner - picking up and running workflows; 1. Summarizer - summarizing metadata. In the simplest Cromwell deployment, a single Cromwell instance performs all three of these roles.; But it is also possible to create Cromwell deployments with many Cromwell instances, gaining advantages in; resiliency and scalability. There is only one restriction on roles: there must be exactly one Cromwell; instance performing the role of summarizer. Apart from this restriction, any instance in a multi-Cromwell; deployment can perform one or more of these roles. As a real world example, the current production Terra; deployment uses 7 Cromwell instances, each dedicated to a single role: 3 front ends, 3 runners, and one summarizer. These roles are not explicit in Cromwell configuration, rather they are implied by configuration settings as; described below. ** Summarizer configuration **. The frequency of metadata summarization is determined by the value for the configuration key; `services.MetadataService.config.metadata-summary-refresh-interval`, which has a default value of `1 second`.; As stated above, there must be exactly one Cromwell instance performing the role of summarizer, so; all Cromwell instances which are not performing the summarizer role should specify `Inf` for this value. ** Runner configuration **. Cromwell instances in the runner role should periodically scan the workflow store to pick up and run; unclaimed workflows. The relevant configuration parameters are described below along with their default values.; The default values may be adequate for instances in the runner role, but will need to be overridden for; non-runner instances to effectively turn running off. ```hocon; system {; ...; # Number of seconds between polls of the workflow store.; # Set this to a very large value for non-runners (e.g. 999999); new-workflow-poll-rate = 20. # Cromwell will l",MatchSource.DOCS,docs/Scaling.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/Scaling.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:1386,Availability,echo,echo,1386,"ion of such workflows. However, a single WDL file can contain only a single workflow definition. In order to reference a sub-workflow, the import directive can be used to bring that sub-workflow into existence and referenced by it's alias and name. See the [documentation on Imports](Imports.md) for more details of how to declare and reference tasks and workflows via imports. **Execution**. A sub-workflows is executed exactly as a task would be.; *This means that if another call depends on an output of a sub-workflow, this call will run when the whole sub-workflow completes (successfully).*; For example, in the following case :. `main.wdl`; ```; import ""sub_wdl.wdl"" as sub. workflow main_workflow {. call sub.hello_and_goodbye { input: hello_and_goodbye_input = ""sub world"" }; ; # call myTask { input: hello_and_goodbye.hello_output }; ; output {; String main_output = hello_and_goodbye.hello_output; }; }; ```. `sub_wdl.wdl`; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }. task goodbye {; String addressee; command {; echo ""Goodbye ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }. workflow hello_and_goodbye {; String hello_and_goodbye_input; ; call hello {input: addressee = hello_and_goodbye_input }; call goodbye {input: addressee = hello_and_goodbye_input }; ; output {; String hello_output = hello.salutation; String goodbye_output = goodbye.salutation; }; }; ```. `myTask` will start only when hello_and_goodbye completes (which means all of its calls are done), even though `myTask` only needs the output of hello in the hello_and_goodbye sub-workflow. ; If hello_and_goodbye fails, then `myTask` won't be executed.; Only workflow outputs are visible outside a workflow, which means that references to outputs produced by a sub-workflow will only be valid if those outputs are exposed in the workflow output section. Sub-workflows are executed in the context of a m",MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:1521,Availability,echo,echo,1521,"mport directive can be used to bring that sub-workflow into existence and referenced by it's alias and name. See the [documentation on Imports](Imports.md) for more details of how to declare and reference tasks and workflows via imports. **Execution**. A sub-workflows is executed exactly as a task would be.; *This means that if another call depends on an output of a sub-workflow, this call will run when the whole sub-workflow completes (successfully).*; For example, in the following case :. `main.wdl`; ```; import ""sub_wdl.wdl"" as sub. workflow main_workflow {. call sub.hello_and_goodbye { input: hello_and_goodbye_input = ""sub world"" }; ; # call myTask { input: hello_and_goodbye.hello_output }; ; output {; String main_output = hello_and_goodbye.hello_output; }; }; ```. `sub_wdl.wdl`; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }. task goodbye {; String addressee; command {; echo ""Goodbye ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }. workflow hello_and_goodbye {; String hello_and_goodbye_input; ; call hello {input: addressee = hello_and_goodbye_input }; call goodbye {input: addressee = hello_and_goodbye_input }; ; output {; String hello_output = hello.salutation; String goodbye_output = goodbye.salutation; }; }; ```. `myTask` will start only when hello_and_goodbye completes (which means all of its calls are done), even though `myTask` only needs the output of hello in the hello_and_goodbye sub-workflow. ; If hello_and_goodbye fails, then `myTask` won't be executed.; Only workflow outputs are visible outside a workflow, which means that references to outputs produced by a sub-workflow will only be valid if those outputs are exposed in the workflow output section. Sub-workflows are executed in the context of a main workflow, which means that operations that are normally executed once per workflow (set up, clean up, outputs copying, log copying, etc.",MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:886,Integrability,depend,depends,886,"In order to support the composition and reuse of workflows, WDL allows the execution of an entire workflow as a step in a larger workflow. When a workflow calls another workflow, that second workflow is referred to as a sub-workflow. Note that sub-workflows can themselves contain sub-workflows and so on, and there is no explicit limit as to how deeply workflows can be nested. Cromwell supports execution of such workflows. However, a single WDL file can contain only a single workflow definition. In order to reference a sub-workflow, the import directive can be used to bring that sub-workflow into existence and referenced by it's alias and name. See the [documentation on Imports](Imports.md) for more details of how to declare and reference tasks and workflows via imports. **Execution**. A sub-workflows is executed exactly as a task would be.; *This means that if another call depends on an output of a sub-workflow, this call will run when the whole sub-workflow completes (successfully).*; For example, in the following case :. `main.wdl`; ```; import ""sub_wdl.wdl"" as sub. workflow main_workflow {. call sub.hello_and_goodbye { input: hello_and_goodbye_input = ""sub world"" }; ; # call myTask { input: hello_and_goodbye.hello_output }; ; output {; String main_output = hello_and_goodbye.hello_output; }; }; ```. `sub_wdl.wdl`; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }. task goodbye {; String addressee; command {; echo ""Goodbye ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }. workflow hello_and_goodbye {; String hello_and_goodbye_input; ; call hello {input: addressee = hello_and_goodbye_input }; call goodbye {input: addressee = hello_and_goodbye_input }; ; output {; String hello_output = hello.salutation; String goodbye_output = goodbye.salutation; }; }; ```. `myTask` will start only when hello_and_goodbye completes (which means all of its calls are done), ",MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:3036,Performance,cache,cached,3036,"nly needs the output of hello in the hello_and_goodbye sub-workflow. ; If hello_and_goodbye fails, then `myTask` won't be executed.; Only workflow outputs are visible outside a workflow, which means that references to outputs produced by a sub-workflow will only be valid if those outputs are exposed in the workflow output section. Sub-workflows are executed in the context of a main workflow, which means that operations that are normally executed once per workflow (set up, clean up, outputs copying, log copying, etc...); will NOT be re-executed for each sub-workflow. For instance if a resource is created during workflow initialization, sub-workflows will need to share this same resource.; Workflow outputs will be copied for the main root workflow but not for intermediate sub-workflows. Restarts, aborts, and call-caching work exactly as they would with tasks. ; All tasks run by a sub-workflow are eligible for call caching under the same rules as any other task.; However, workflows themselves are not cached as such. Which means that running the exact same workflow twice with call caching on will trigger each task to cache individually,; but not the workflow itself. The root path for sub-workflow execution files (scripts, output files, logs) will be under the parent workflow call directory.; For example, the execution directory for the above main workflow would look like the following:. ```; cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/ <- main workflow id; └── call-hello_and_goodbye <- call directory for call hello_and_goodbye in the main workflow; └── hello_and_goodbye <- name of the sub-workflow ; └── a6365f91-c807-465a-9186-a5d3da98fe11 <- sub-workflow id; ├── call-goodbye; │ └── execution; │ ├── rc; │ ├── script; │ ├── script.background; │ ├── script.submit; │ ├── stderr; │ ├── stderr.background; │ ├── stdout; │ └── stdout.background; └── call-hello; └── execution; ├── rc; ├── script; ├── script.background; ├── script.submit; ├── stderr; ├─",MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:3154,Performance,cache,cache,3154,"`myTask` won't be executed.; Only workflow outputs are visible outside a workflow, which means that references to outputs produced by a sub-workflow will only be valid if those outputs are exposed in the workflow output section. Sub-workflows are executed in the context of a main workflow, which means that operations that are normally executed once per workflow (set up, clean up, outputs copying, log copying, etc...); will NOT be re-executed for each sub-workflow. For instance if a resource is created during workflow initialization, sub-workflows will need to share this same resource.; Workflow outputs will be copied for the main root workflow but not for intermediate sub-workflows. Restarts, aborts, and call-caching work exactly as they would with tasks. ; All tasks run by a sub-workflow are eligible for call caching under the same rules as any other task.; However, workflows themselves are not cached as such. Which means that running the exact same workflow twice with call caching on will trigger each task to cache individually,; but not the workflow itself. The root path for sub-workflow execution files (scripts, output files, logs) will be under the parent workflow call directory.; For example, the execution directory for the above main workflow would look like the following:. ```; cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/ <- main workflow id; └── call-hello_and_goodbye <- call directory for call hello_and_goodbye in the main workflow; └── hello_and_goodbye <- name of the sub-workflow ; └── a6365f91-c807-465a-9186-a5d3da98fe11 <- sub-workflow id; ├── call-goodbye; │ └── execution; │ ├── rc; │ ├── script; │ ├── script.background; │ ├── script.submit; │ ├── stderr; │ ├── stderr.background; │ ├── stdout; │ └── stdout.background; └── call-hello; └── execution; ├── rc; ├── script; ├── script.background; ├── script.submit; ├── stderr; ├── stderr.background; ├── stdout; └── stdout.background. ```. **Metadata**. Each sub-workflow will have ",MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:6267,Performance,cache,cache,6267,"; ""attempt"": 1,; ""start"": ""2016-11-17T14:13:39.236-05:00"",; ""subWorkflowId"": ""a6365f91-c807-465a-9186-a5d3da98fe11""; }; ]; },; ""outputs"": {; ""main_output"": ""Hello sub world!""; },; ""workflowRoot"": ""/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd"",; ""id"": ""1d919bd4-d046-43b0-9918-9964509689dd"",; ""inputs"": {},; ""submission"": ""2016-11-17T14:13:39.104-05:00"",; ""status"": ""Succeeded"",; ""end"": ""2016-11-17T14:13:41.120-05:00"",; ""start"": ""2016-11-17T14:13:39.204-05:00""; }; ```. The sub-workflow ID can be queried separately:. `GET /api/workflows/v2/a6365f91-c807-465a-9186-a5d3da98fe11/metadata`. ```; {; ""workflowName"": ""hello_and_goodbye"",; ""calls"": {; ""sub.hello_and_goodbye.hello"": [; {; ""executionStatus"": ""Done"",; ""stdout"": ""/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-hello/execution/stdout"",; ""shardIndex"": -1,; ""outputs"": {; ""salutation"": ""Hello sub world!""; },; ""runtimeAttributes"": {; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""CallCachingOff"",; ""inputs"": {; ""addressee"": ""sub world""; },; ""returnCode"": 0,; ""jobId"": ""49830"",; ""backend"": ""Local"",; ""end"": ""2016-11-17T14:13:40.712-05:00"",; ""stderr"": ""/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-hello/execution/stderr"",; ""callRoot"": ""/cromwell/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-hello"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""Pending"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; },; {; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; },; {;",MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:8042,Performance,cache,cache,8042,"-05:00"",; ""description"": ""Pending"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; },; {; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; },; {; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-11-17T14:13:39.243-05:00""; },; {; ""startTime"": ""2016-11-17T14:13:39.243-05:00"",; ""description"": ""RunningJob"",; ""endTime"": ""2016-11-17T14:13:40.704-05:00""; },; {; ""startTime"": ""2016-11-17T14:13:40.704-05:00"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2016-11-17T14:13:40.712-05:00""; }; ],; ""start"": ""2016-11-17T14:13:39.239-05:00""; }; ],; ""sub.hello_and_goodbye.goodbye"": [; {; ""executionStatus"": ""Done"",; ""stdout"": ""/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-goodbye/execution/stdout"",; ""shardIndex"": -1,; ""outputs"": {; ""salutation"": ""Goodbye sub world!""; },; ""runtimeAttributes"": {; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""CallCachingOff"",; ""inputs"": {; ""addressee"": ""sub world""; },; ""returnCode"": 0,; ""jobId"": ""49831"",; ""backend"": ""Local"",; ""end"": ""2016-11-17T14:13:41.115-05:00"",; ""stderr"": ""/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-goodbye/execution/stderr"",; ""callRoot"": ""/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-goodbye"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""Pending"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; },; {; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; },; {; """,MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:11149,Performance,cache,cache,11149,"tadata (`false` by default). `GET api/workflows/v2/1d919bd4-d046-43b0-9918-9964509689dd/metadata?expandSubWorkflows=true`. ```; {; ""workflowName"": ""main_workflow"",; ""submittedFiles"": {; ""inputs"": ""{}"",; ""workflow"": ""import \""sub_wdl.wdl\"" as sub\n\nworkflow main_workflow {\n\n call sub.hello_and_goodbye { input: hello_and_goodbye_input = \""sub world\"" }\n \n # call myTask { input: hello_and_goodbye.hello_output }\n \n output {\n String main_output = hello_and_goodbye.hello_output\n }\n}"",; ""options"": ""{\n\n}""; },; ""calls"": {; ""main_workflow.hello_and_goodbye"": [{; ""executionStatus"": ""Done"",; ""subWorkflowMetadata"": {; ""workflowName"": ""hello_and_goodbye"",; ""calls"": {; ""sub.hello_and_goodbye.hello"": [{; ""executionStatus"": ""Done"",; ""stdout"": ""/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-hello/execution/stdout"",; ""shardIndex"": -1,; ""outputs"": {; ""salutation"": ""Hello sub world!""; },; ""runtimeAttributes"": {; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""CallCachingOff"",; ""inputs"": {; ""addressee"": ""sub world""; },; ""returnCode"": 0,; ""jobId"": ""49830"",; ""backend"": ""Local"",; ""end"": ""2016-11-17T14:13:40.712-05:00"",; ""stderr"": ""/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-hello/execution/stderr"",; ""callRoot"": ""cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-hello"",; ""attempt"": 1,; ""executionEvents"": [{; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""Pending"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; }, {; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; }, {; ""startTime"": """,MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:12902,Performance,cache,cache,12902,"13:39.240-05:00"",; ""description"": ""Pending"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; }, {; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; }, {; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""PreparingJob"",; ""endTime"": ""2016-11-17T14:13:39.243-05:00""; }, {; ""startTime"": ""2016-11-17T14:13:39.243-05:00"",; ""description"": ""RunningJob"",; ""endTime"": ""2016-11-17T14:13:40.704-05:00""; }, {; ""startTime"": ""2016-11-17T14:13:40.704-05:00"",; ""description"": ""UpdatingJobStore"",; ""endTime"": ""2016-11-17T14:13:40.712-05:00""; }],; ""start"": ""2016-11-17T14:13:39.239-05:00""; }],; ""sub.hello_and_goodbye.goodbye"": [{; ""executionStatus"": ""Done"",; ""stdout"": ""/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-goodbye/execution/stdout"",; ""shardIndex"": -1,; ""outputs"": {; ""salutation"": ""Goodbye sub world!""; },; ""runtimeAttributes"": {; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""CallCachingOff"",; ""inputs"": {; ""addressee"": ""sub world""; },; ""returnCode"": 0,; ""jobId"": ""49831"",; ""backend"": ""Local"",; ""end"": ""2016-11-17T14:13:41.115-05:00"",; ""stderr"": ""/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-goodbye/execution/stderr"",; ""callRoot"": ""/cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/call-hello_and_goodbye/hello_and_goodbye/a6365f91-c807-465a-9186-a5d3da98fe11/call-goodbye"",; ""attempt"": 1,; ""executionEvents"": [{; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""Pending"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; }, {; ""startTime"": ""2016-11-17T14:13:39.240-05:00"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2016-11-17T14:13:39.240-05:00""; }, {; ""start",MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:2829,Safety,abort,aborts,2829,"oodbye_output = goodbye.salutation; }; }; ```. `myTask` will start only when hello_and_goodbye completes (which means all of its calls are done), even though `myTask` only needs the output of hello in the hello_and_goodbye sub-workflow. ; If hello_and_goodbye fails, then `myTask` won't be executed.; Only workflow outputs are visible outside a workflow, which means that references to outputs produced by a sub-workflow will only be valid if those outputs are exposed in the workflow output section. Sub-workflows are executed in the context of a main workflow, which means that operations that are normally executed once per workflow (set up, clean up, outputs copying, log copying, etc...); will NOT be re-executed for each sub-workflow. For instance if a resource is created during workflow initialization, sub-workflows will need to share this same resource.; Workflow outputs will be copied for the main root workflow but not for intermediate sub-workflows. Restarts, aborts, and call-caching work exactly as they would with tasks. ; All tasks run by a sub-workflow are eligible for call caching under the same rules as any other task.; However, workflows themselves are not cached as such. Which means that running the exact same workflow twice with call caching on will trigger each task to cache individually,; but not the workflow itself. The root path for sub-workflow execution files (scripts, output files, logs) will be under the parent workflow call directory.; For example, the execution directory for the above main workflow would look like the following:. ```; cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/ <- main workflow id; └── call-hello_and_goodbye <- call directory for call hello_and_goodbye in the main workflow; └── hello_and_goodbye <- name of the sub-workflow ; └── a6365f91-c807-465a-9186-a5d3da98fe11 <- sub-workflow id; ├── call-goodbye; │ └── execution; │ ├── rc; │ ├── script; │ ├── script.background; │ ├── script.submit; │ ├── stderr; │ ",MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:2316,Security,expose,exposed,2316,"t {; String main_output = hello_and_goodbye.hello_output; }; }; ```. `sub_wdl.wdl`; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }. task goodbye {; String addressee; command {; echo ""Goodbye ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }. workflow hello_and_goodbye {; String hello_and_goodbye_input; ; call hello {input: addressee = hello_and_goodbye_input }; call goodbye {input: addressee = hello_and_goodbye_input }; ; output {; String hello_output = hello.salutation; String goodbye_output = goodbye.salutation; }; }; ```. `myTask` will start only when hello_and_goodbye completes (which means all of its calls are done), even though `myTask` only needs the output of hello in the hello_and_goodbye sub-workflow. ; If hello_and_goodbye fails, then `myTask` won't be executed.; Only workflow outputs are visible outside a workflow, which means that references to outputs produced by a sub-workflow will only be valid if those outputs are exposed in the workflow output section. Sub-workflows are executed in the context of a main workflow, which means that operations that are normally executed once per workflow (set up, clean up, outputs copying, log copying, etc...); will NOT be re-executed for each sub-workflow. For instance if a resource is created during workflow initialization, sub-workflows will need to share this same resource.; Workflow outputs will be copied for the main root workflow but not for intermediate sub-workflows. Restarts, aborts, and call-caching work exactly as they would with tasks. ; All tasks run by a sub-workflow are eligible for call caching under the same rules as any other task.; However, workflows themselves are not cached as such. Which means that running the exact same workflow twice with call caching on will trigger each task to cache individually,; but not the workflow itself. The root path for sub-workflow execution files (scr",MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:2527,Testability,log,log,2527,"ad_string(stdout()); }; }. task goodbye {; String addressee; command {; echo ""Goodbye ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }. workflow hello_and_goodbye {; String hello_and_goodbye_input; ; call hello {input: addressee = hello_and_goodbye_input }; call goodbye {input: addressee = hello_and_goodbye_input }; ; output {; String hello_output = hello.salutation; String goodbye_output = goodbye.salutation; }; }; ```. `myTask` will start only when hello_and_goodbye completes (which means all of its calls are done), even though `myTask` only needs the output of hello in the hello_and_goodbye sub-workflow. ; If hello_and_goodbye fails, then `myTask` won't be executed.; Only workflow outputs are visible outside a workflow, which means that references to outputs produced by a sub-workflow will only be valid if those outputs are exposed in the workflow output section. Sub-workflows are executed in the context of a main workflow, which means that operations that are normally executed once per workflow (set up, clean up, outputs copying, log copying, etc...); will NOT be re-executed for each sub-workflow. For instance if a resource is created during workflow initialization, sub-workflows will need to share this same resource.; Workflow outputs will be copied for the main root workflow but not for intermediate sub-workflows. Restarts, aborts, and call-caching work exactly as they would with tasks. ; All tasks run by a sub-workflow are eligible for call caching under the same rules as any other task.; However, workflows themselves are not cached as such. Which means that running the exact same workflow twice with call caching on will trigger each task to cache individually,; but not the workflow itself. The root path for sub-workflow execution files (scripts, output files, logs) will be under the parent workflow call directory.; For example, the execution directory for the above main workflow would look like the following:. ```; cromwell-executi",MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md:3275,Testability,log,logs,3275,"-workflow will only be valid if those outputs are exposed in the workflow output section. Sub-workflows are executed in the context of a main workflow, which means that operations that are normally executed once per workflow (set up, clean up, outputs copying, log copying, etc...); will NOT be re-executed for each sub-workflow. For instance if a resource is created during workflow initialization, sub-workflows will need to share this same resource.; Workflow outputs will be copied for the main root workflow but not for intermediate sub-workflows. Restarts, aborts, and call-caching work exactly as they would with tasks. ; All tasks run by a sub-workflow are eligible for call caching under the same rules as any other task.; However, workflows themselves are not cached as such. Which means that running the exact same workflow twice with call caching on will trigger each task to cache individually,; but not the workflow itself. The root path for sub-workflow execution files (scripts, output files, logs) will be under the parent workflow call directory.; For example, the execution directory for the above main workflow would look like the following:. ```; cromwell-executions/main_workflow/1d919bd4-d046-43b0-9918-9964509689dd/ <- main workflow id; └── call-hello_and_goodbye <- call directory for call hello_and_goodbye in the main workflow; └── hello_and_goodbye <- name of the sub-workflow ; └── a6365f91-c807-465a-9186-a5d3da98fe11 <- sub-workflow id; ├── call-goodbye; │ └── execution; │ ├── rc; │ ├── script; │ ├── script.background; │ ├── script.submit; │ ├── stderr; │ ├── stderr.background; │ ├── stdout; │ └── stdout.background; └── call-hello; └── execution; ├── rc; ├── script; ├── script.background; ├── script.submit; ├── stderr; ├── stderr.background; ├── stdout; └── stdout.background. ```. **Metadata**. Each sub-workflow will have its own workflow ID. This ID will appear in the metadata of the parent workflow, in the call section corresponding to the sub-workflow, unde",MatchSource.DOCS,docs/SubWorkflows.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/SubWorkflows.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:85,Availability,down,download,85,"Command line utilities for interacting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prin",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:716,Availability,down,downloading,716,"Command line utilities for interacting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prin",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:1784,Availability,error,error,1784,"u image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is valid, and a syntax error; otherwise. Note that graph currently DOES NOT WORK on; version 1.0 workflows. womgraph <WDL file> [ancillary files]. Reads a WDL file from the first argument and; converts it to a WOM representation then prints out a graph; of the WOM produced.; Any imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist ",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:2056,Availability,error,error,2056,"encies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is valid, and a syntax error; otherwise. Note that graph currently DOES NOT WORK on; version 1.0 workflows. womgraph <WDL file> [ancillary files]. Reads a WDL file from the first argument and; converts it to a WOM representation then prints out a graph; of the WOM produced.; Any imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist (line 22, col 8). call BADps; ^; ```. Error if namespace and task have the same name:. `$ java -jar womtool.jar validate 5.wdl`. ```; ERROR: Task and namespace have the same name:. Task defined here (line 3, col 6):. task ps {; ^. Import statement defined here (line 1, col 20):. import ""ps.wdl"" as ps; ^;",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:2508,Availability,error,errors,2508,"t type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is valid, and a syntax error; otherwise. Note that graph currently DOES NOT WORK on; version 1.0 workflows. womgraph <WDL file> [ancillary files]. Reads a WDL file from the first argument and; converts it to a WOM representation then prints out a graph; of the WOM produced.; Any imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist (line 22, col 8). call BADps; ^; ```. Error if namespace and task have the same name:. `$ java -jar womtool.jar validate 5.wdl`. ```; ERROR: Task and namespace have the same name:. Task defined here (line 3, col 6):. task ps {; ^. Import statement defined here (line 1, col 20):. import ""ps.wdl"" as ps; ^; ```. ##### --list-dependencies or -l flag. For a successful validation, this will output the list of files referenced in import statements in workflows and their subworkflows. `$ java -jar womtool.jar validate -l myWdl.wdl`. ```hocon; Success!; List of Workflow dependencies are:; /path/to/my/import/myImport.wdl; /path/to/another/import/anotherImport.wdl; https://path-to-http-import/httpImport.wdl; ```. ### `inputs`. Examine a WDL file with one workflow in it, compute all ",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:4250,Availability,echo,echo,4250,"l.wdl`. ```hocon; Success!; List of Workflow dependencies are:; /path/to/my/import/myImport.wdl; /path/to/another/import/anotherImport.wdl; https://path-to-http-import/httpImport.wdl; ```. ### `inputs`. Examine a WDL file with one workflow in it, compute all the inputs needed for that workflow and output a JSON template that the user can fill in with values. The keys in this document should remain unchanged. The values tell you what type the parameter is expecting. For example, if the value were `Array[String]`, then it's expecting a JSON array of JSON strings, like this: `[""string1"", ""string2"", ""string3""]`. `$ java -jar womtool.jar inputs 3step.wdl`. ```; {; ""three_step.cgrep.pattern"": ""String""; }; ```. This inputs document is used as input to the `run` subcommand. ### `highlight`. Formats a WDL file and semantically tags it. This takes a second parameter (`html` or `console`) which determines what the output format will be. test.wdl; ```; task abc {; String in; command {; echo ${in}; }; output {; String out = read_string(stdout()); }; }. workflow wf {; call abc; }; ```. ### `parse`. Given a WDL file input, this does grammar level syntax checks and writes out the resulting abstract syntax tree. `$ echo ""workflow wf {}"" | java -jar womtool.jar parse /dev/stdin`. ```; (Document:; imports=[],; definitions=[; (Workflow:; name=<stdin:1:10 identifier ""d2Y="">,; body=[]; ); ]; ); ```. This WDL file can be formatted in HTML as follows:. `$ java -jar womtool.jar highlight test.wdl html`. ```; <span class=""keyword"">task</span> <span class=""name"">abc</span> {; <span class=""type"">String</span> <span class=""variable"">in</span>; <span class=""section"">command</span> {; <span class=""command"">echo ${in}</span>; }; <span class=""section"">output</span> {; <span class=""type"">String</span> <span class=""variable"">out</span> = <span class=""function"">read_string</span>(<span class=""function"">stdout</span>()); }; }. <span class=""keyword"">workflow</span> <span class=""name"">wf</span> {; <span ",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:4479,Availability,echo,echo,4479,"ow in it, compute all the inputs needed for that workflow and output a JSON template that the user can fill in with values. The keys in this document should remain unchanged. The values tell you what type the parameter is expecting. For example, if the value were `Array[String]`, then it's expecting a JSON array of JSON strings, like this: `[""string1"", ""string2"", ""string3""]`. `$ java -jar womtool.jar inputs 3step.wdl`. ```; {; ""three_step.cgrep.pattern"": ""String""; }; ```. This inputs document is used as input to the `run` subcommand. ### `highlight`. Formats a WDL file and semantically tags it. This takes a second parameter (`html` or `console`) which determines what the output format will be. test.wdl; ```; task abc {; String in; command {; echo ${in}; }; output {; String out = read_string(stdout()); }; }. workflow wf {; call abc; }; ```. ### `parse`. Given a WDL file input, this does grammar level syntax checks and writes out the resulting abstract syntax tree. `$ echo ""workflow wf {}"" | java -jar womtool.jar parse /dev/stdin`. ```; (Document:; imports=[],; definitions=[; (Workflow:; name=<stdin:1:10 identifier ""d2Y="">,; body=[]; ); ]; ); ```. This WDL file can be formatted in HTML as follows:. `$ java -jar womtool.jar highlight test.wdl html`. ```; <span class=""keyword"">task</span> <span class=""name"">abc</span> {; <span class=""type"">String</span> <span class=""variable"">in</span>; <span class=""section"">command</span> {; <span class=""command"">echo ${in}</span>; }; <span class=""section"">output</span> {; <span class=""type"">String</span> <span class=""variable"">out</span> = <span class=""function"">read_string</span>(<span class=""function"">stdout</span>()); }; }. <span class=""keyword"">workflow</span> <span class=""name"">wf</span> {; <span class=""keyword"">call</span> <span class=""name"">abc</span>; }; ```. ### `graph`; ; The syntax of the graph command is:; ```; womtool graph [--all] wdlFile.wdl; ```. Given a WDL file input, command generates the data-flow graph through the",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:4966,Availability,echo,echo,4966," This inputs document is used as input to the `run` subcommand. ### `highlight`. Formats a WDL file and semantically tags it. This takes a second parameter (`html` or `console`) which determines what the output format will be. test.wdl; ```; task abc {; String in; command {; echo ${in}; }; output {; String out = read_string(stdout()); }; }. workflow wf {; call abc; }; ```. ### `parse`. Given a WDL file input, this does grammar level syntax checks and writes out the resulting abstract syntax tree. `$ echo ""workflow wf {}"" | java -jar womtool.jar parse /dev/stdin`. ```; (Document:; imports=[],; definitions=[; (Workflow:; name=<stdin:1:10 identifier ""d2Y="">,; body=[]; ); ]; ); ```. This WDL file can be formatted in HTML as follows:. `$ java -jar womtool.jar highlight test.wdl html`. ```; <span class=""keyword"">task</span> <span class=""name"">abc</span> {; <span class=""type"">String</span> <span class=""variable"">in</span>; <span class=""section"">command</span> {; <span class=""command"">echo ${in}</span>; }; <span class=""section"">output</span> {; <span class=""type"">String</span> <span class=""variable"">out</span> = <span class=""function"">read_string</span>(<span class=""function"">stdout</span>()); }; }. <span class=""keyword"">workflow</span> <span class=""name"">wf</span> {; <span class=""keyword"">call</span> <span class=""name"">abc</span>; }; ```. ### `graph`; ; The syntax of the graph command is:; ```; womtool graph [--all] wdlFile.wdl; ```. Given a WDL file input, command generates the data-flow graph through the system in `.dot` format. For example the fork-join WDL:; ```; task mkFile {; command {; for i in `seq 1 1000`; do; echo $i; done; }; output {; File numbers = stdout(); }; runtime {docker: ""ubuntu:latest""}; }. task grep {; String pattern; File in_file; command {; grep '${pattern}' ${in_file} | wc -l; }; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task wc {; File in_file; command {; cat ${in_file} | wc -l; }; output {; Int count = rea",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:5614,Availability,echo,echo,5614,"10 identifier ""d2Y="">,; body=[]; ); ]; ); ```. This WDL file can be formatted in HTML as follows:. `$ java -jar womtool.jar highlight test.wdl html`. ```; <span class=""keyword"">task</span> <span class=""name"">abc</span> {; <span class=""type"">String</span> <span class=""variable"">in</span>; <span class=""section"">command</span> {; <span class=""command"">echo ${in}</span>; }; <span class=""section"">output</span> {; <span class=""type"">String</span> <span class=""variable"">out</span> = <span class=""function"">read_string</span>(<span class=""function"">stdout</span>()); }; }. <span class=""keyword"">workflow</span> <span class=""name"">wf</span> {; <span class=""keyword"">call</span> <span class=""name"">abc</span>; }; ```. ### `graph`; ; The syntax of the graph command is:; ```; womtool graph [--all] wdlFile.wdl; ```. Given a WDL file input, command generates the data-flow graph through the system in `.dot` format. For example the fork-join WDL:; ```; task mkFile {; command {; for i in `seq 1 1000`; do; echo $i; done; }; output {; File numbers = stdout(); }; runtime {docker: ""ubuntu:latest""}; }. task grep {; String pattern; File in_file; command {; grep '${pattern}' ${in_file} | wc -l; }; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task wc {; File in_file; command {; cat ${in_file} | wc -l; }; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task join {; Int grepCount; Int wcCount; command {; expr ${wcCount} / ${grepCount}; }; output {; Int proportion = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. workflow forkjoin {; call mkFile; call grep { input: in_file = mkFile.numbers }; call wc { input: in_file=mkFile.numbers }; call join { input: wcCount = wc.count, grepCount = grep.count }; output {; join.proportion; }; }; ```. Produces the DAG:; ```; digraph forkjoin {; ""call forkjoin.mkFile"" -> ""call forkjoin.wc""; ""call forkjoin.mkFile"" -> ""call forkjoin.grep""; ""call forkjoin.wc"" -> ""call forkjoin.join"";",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:132,Deployability,release,releases,132,"Command line utilities for interacting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prin",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:200,Deployability,release,releases,200,"Command line utilities for interacting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prin",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:909,Integrability,message,message,909,"Command line utilities for interacting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prin",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:1036,Integrability,depend,dependencies,1036,"ting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:1157,Integrability,depend,dependencies,1157,"/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is valid, and a syntax error; otherwise. Note that graph currently DOES NOT WORK on; version 1.0 workflows. womgraph <WDL file> [ancillary files]. Reads a WDL",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:3062,Integrability,depend,dependencies,3062,"rwise. Note that graph currently DOES NOT WORK on; version 1.0 workflows. womgraph <WDL file> [ancillary files]. Reads a WDL file from the first argument and; converts it to a WOM representation then prints out a graph; of the WOM produced.; Any imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist (line 22, col 8). call BADps; ^; ```. Error if namespace and task have the same name:. `$ java -jar womtool.jar validate 5.wdl`. ```; ERROR: Task and namespace have the same name:. Task defined here (line 3, col 6):. task ps {; ^. Import statement defined here (line 1, col 20):. import ""ps.wdl"" as ps; ^; ```. ##### --list-dependencies or -l flag. For a successful validation, this will output the list of files referenced in import statements in workflows and their subworkflows. `$ java -jar womtool.jar validate -l myWdl.wdl`. ```hocon; Success!; List of Workflow dependencies are:; /path/to/my/import/myImport.wdl; /path/to/another/import/anotherImport.wdl; https://path-to-http-import/httpImport.wdl; ```. ### `inputs`. Examine a WDL file with one workflow in it, compute all the inputs needed for that workflow and output a JSON template that the user can fill in with values. The keys in this document should remain unchanged. The values tell you what type the parameter is expecting. For example, if the value were `Array[String]`, then it's expecting a JSON array of JSON strings, like this: `[""string1"", ""string2"", ""string3""]`. `$ java -jar womtool.jar inputs 3step.wdl`. ```; {; ""three_step.cgrep.pattern"": ""String""; }; ```. This inputs document is used as input to the `run` subcommand. ### `highlight`. Formats a WD",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:3306,Integrability,depend,dependencies,3306,"ny imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist (line 22, col 8). call BADps; ^; ```. Error if namespace and task have the same name:. `$ java -jar womtool.jar validate 5.wdl`. ```; ERROR: Task and namespace have the same name:. Task defined here (line 3, col 6):. task ps {; ^. Import statement defined here (line 1, col 20):. import ""ps.wdl"" as ps; ^; ```. ##### --list-dependencies or -l flag. For a successful validation, this will output the list of files referenced in import statements in workflows and their subworkflows. `$ java -jar womtool.jar validate -l myWdl.wdl`. ```hocon; Success!; List of Workflow dependencies are:; /path/to/my/import/myImport.wdl; /path/to/another/import/anotherImport.wdl; https://path-to-http-import/httpImport.wdl; ```. ### `inputs`. Examine a WDL file with one workflow in it, compute all the inputs needed for that workflow and output a JSON template that the user can fill in with values. The keys in this document should remain unchanged. The values tell you what type the parameter is expecting. For example, if the value were `Array[String]`, then it's expecting a JSON array of JSON strings, like this: `[""string1"", ""string2"", ""string3""]`. `$ java -jar womtool.jar inputs 3step.wdl`. ```; {; ""three_step.cgrep.pattern"": ""String""; }; ```. This inputs document is used as input to the `run` subcommand. ### `highlight`. Formats a WDL file and semantically tags it. This takes a second parameter (`html` or `console`) which determines what the output format will be. test.wdl; ```; task abc {; String in; command {; echo ${in}; }; output {; String out = read_string(stdout()); ",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:4883,Modifiability,variab,variable,4883," This inputs document is used as input to the `run` subcommand. ### `highlight`. Formats a WDL file and semantically tags it. This takes a second parameter (`html` or `console`) which determines what the output format will be. test.wdl; ```; task abc {; String in; command {; echo ${in}; }; output {; String out = read_string(stdout()); }; }. workflow wf {; call abc; }; ```. ### `parse`. Given a WDL file input, this does grammar level syntax checks and writes out the resulting abstract syntax tree. `$ echo ""workflow wf {}"" | java -jar womtool.jar parse /dev/stdin`. ```; (Document:; imports=[],; definitions=[; (Workflow:; name=<stdin:1:10 identifier ""d2Y="">,; body=[]; ); ]; ); ```. This WDL file can be formatted in HTML as follows:. `$ java -jar womtool.jar highlight test.wdl html`. ```; <span class=""keyword"">task</span> <span class=""name"">abc</span> {; <span class=""type"">String</span> <span class=""variable"">in</span>; <span class=""section"">command</span> {; <span class=""command"">echo ${in}</span>; }; <span class=""section"">output</span> {; <span class=""type"">String</span> <span class=""variable"">out</span> = <span class=""function"">read_string</span>(<span class=""function"">stdout</span>()); }; }. <span class=""keyword"">workflow</span> <span class=""name"">wf</span> {; <span class=""keyword"">call</span> <span class=""name"">abc</span>; }; ```. ### `graph`; ; The syntax of the graph command is:; ```; womtool graph [--all] wdlFile.wdl; ```. Given a WDL file input, command generates the data-flow graph through the system in `.dot` format. For example the fork-join WDL:; ```; task mkFile {; command {; for i in `seq 1 1000`; do; echo $i; done; }; output {; File numbers = stdout(); }; runtime {docker: ""ubuntu:latest""}; }. task grep {; String pattern; File in_file; command {; grep '${pattern}' ${in_file} | wc -l; }; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task wc {; File in_file; command {; cat ${in_file} | wc -l; }; output {; Int count = rea",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:5073,Modifiability,variab,variable,5073," This inputs document is used as input to the `run` subcommand. ### `highlight`. Formats a WDL file and semantically tags it. This takes a second parameter (`html` or `console`) which determines what the output format will be. test.wdl; ```; task abc {; String in; command {; echo ${in}; }; output {; String out = read_string(stdout()); }; }. workflow wf {; call abc; }; ```. ### `parse`. Given a WDL file input, this does grammar level syntax checks and writes out the resulting abstract syntax tree. `$ echo ""workflow wf {}"" | java -jar womtool.jar parse /dev/stdin`. ```; (Document:; imports=[],; definitions=[; (Workflow:; name=<stdin:1:10 identifier ""d2Y="">,; body=[]; ); ]; ); ```. This WDL file can be formatted in HTML as follows:. `$ java -jar womtool.jar highlight test.wdl html`. ```; <span class=""keyword"">task</span> <span class=""name"">abc</span> {; <span class=""type"">String</span> <span class=""variable"">in</span>; <span class=""section"">command</span> {; <span class=""command"">echo ${in}</span>; }; <span class=""section"">output</span> {; <span class=""type"">String</span> <span class=""variable"">out</span> = <span class=""function"">read_string</span>(<span class=""function"">stdout</span>()); }; }. <span class=""keyword"">workflow</span> <span class=""name"">wf</span> {; <span class=""keyword"">call</span> <span class=""name"">abc</span>; }; ```. ### `graph`; ; The syntax of the graph command is:; ```; womtool graph [--all] wdlFile.wdl; ```. Given a WDL file input, command generates the data-flow graph through the system in `.dot` format. For example the fork-join WDL:; ```; task mkFile {; command {; for i in `seq 1 1000`; do; echo $i; done; }; output {; File numbers = stdout(); }; runtime {docker: ""ubuntu:latest""}; }. task grep {; String pattern; File in_file; command {; grep '${pattern}' ${in_file} | wc -l; }; output {; Int count = read_int(stdout()); }; runtime {docker: ""ubuntu:latest""}; }. task wc {; File in_file; command {; cat ${in_file} | wc -l; }; output {; Int count = rea",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:1019,Security,validat,validate,1019,"ting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:1076,Security,validat,validation,1076,"st WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is valid, and a syntax error; otherwise. Note that graph currently DO",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:1880,Security,validat,validate,1880,"file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is valid, and a syntax error; otherwise. Note that graph currently DOES NOT WORK on; version 1.0 workflows. womgraph <WDL file> [ancillary files]. Reads a WDL file from the first argument and; converts it to a WOM representation then prints out a graph; of the WOM produced.; Any imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist (line 22, col 8). call BADps; ^; ```. Error if namespace and task have the same name:. `$ java -jar womtool.jar validate 5.wdl`. ``",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:1926,Security,validat,validation,1926,"file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is valid, and a syntax error; otherwise. Note that graph currently DOES NOT WORK on; version 1.0 workflows. womgraph <WDL file> [ancillary files]. Reads a WDL file from the first argument and; converts it to a WOM representation then prints out a graph; of the WOM produced.; Any imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist (line 22, col 8). call BADps; ^; ```. Error if namespace and task have the same name:. `$ java -jar womtool.jar validate 5.wdl`. ``",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:2380,Security,validat,validate,2380," to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is valid, and a syntax error; otherwise. Note that graph currently DOES NOT WORK on; version 1.0 workflows. womgraph <WDL file> [ancillary files]. Reads a WDL file from the first argument and; converts it to a WOM representation then prints out a graph; of the WOM produced.; Any imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist (line 22, col 8). call BADps; ^; ```. Error if namespace and task have the same name:. `$ java -jar womtool.jar validate 5.wdl`. ```; ERROR: Task and namespace have the same name:. Task defined here (line 3, col 6):. task ps {; ^. Import statement defined here (line 1, col 20):. import ""ps.wdl"" as ps; ^; ```. ##### --list-dependencies or -l flag. For a successful validation, this will output the list of files referenced in import statements in workflows and their subworkflows. `$ java -jar womtool.jar validate -l myWdl.wdl`. ```hocon; Success!; List of Workflow dependencies are:; /path/to/my/import/myImport.wdl; /path/to/another/import/",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:2659,Security,validat,validate,2659," <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is valid, and a syntax error; otherwise. Note that graph currently DOES NOT WORK on; version 1.0 workflows. womgraph <WDL file> [ancillary files]. Reads a WDL file from the first argument and; converts it to a WOM representation then prints out a graph; of the WOM produced.; Any imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist (line 22, col 8). call BADps; ^; ```. Error if namespace and task have the same name:. `$ java -jar womtool.jar validate 5.wdl`. ```; ERROR: Task and namespace have the same name:. Task defined here (line 3, col 6):. task ps {; ^. Import statement defined here (line 1, col 20):. import ""ps.wdl"" as ps; ^; ```. ##### --list-dependencies or -l flag. For a successful validation, this will output the list of files referenced in import statements in workflows and their subworkflows. `$ java -jar womtool.jar validate -l myWdl.wdl`. ```hocon; Success!; List of Workflow dependencies are:; /path/to/my/import/myImport.wdl; /path/to/another/import/anotherImport.wdl; https://path-to-http-import/httpImport.wdl; ```. ### `inputs`. Examine a WDL file with one workflow in it, compute all the inputs needed for that workflow and output a JSON template that the user can fill in with values. The keys in this document should remain u",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:2850,Security,validat,validate,2850," this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prints out a; .dot of the DAG if it is valid, and a syntax error; otherwise. Note that graph currently DOES NOT WORK on; version 1.0 workflows. womgraph <WDL file> [ancillary files]. Reads a WDL file from the first argument and; converts it to a WOM representation then prints out a graph; of the WOM produced.; Any imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist (line 22, col 8). call BADps; ^; ```. Error if namespace and task have the same name:. `$ java -jar womtool.jar validate 5.wdl`. ```; ERROR: Task and namespace have the same name:. Task defined here (line 3, col 6):. task ps {; ^. Import statement defined here (line 1, col 20):. import ""ps.wdl"" as ps; ^; ```. ##### --list-dependencies or -l flag. For a successful validation, this will output the list of files referenced in import statements in workflows and their subworkflows. `$ java -jar womtool.jar validate -l myWdl.wdl`. ```hocon; Success!; List of Workflow dependencies are:; /path/to/my/import/myImport.wdl; /path/to/another/import/anotherImport.wdl; https://path-to-http-import/httpImport.wdl; ```. ### `inputs`. Examine a WDL file with one workflow in it, compute all the inputs needed for that workflow and output a JSON template that the user can fill in with values. The keys in this document should remain unchanged. The values tell you what type the parameter is expecting. For example, if the value were `Array[String]`, then it's expecting a JSON array of JSON strings, like this: `[""string1"", """,MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:3104,Security,validat,validation,3104,"DL file> [ancillary files]. Reads a WDL file from the first argument and; converts it to a WOM representation then prints out a graph; of the WOM produced.; Any imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist (line 22, col 8). call BADps; ^; ```. Error if namespace and task have the same name:. `$ java -jar womtool.jar validate 5.wdl`. ```; ERROR: Task and namespace have the same name:. Task defined here (line 3, col 6):. task ps {; ^. Import statement defined here (line 1, col 20):. import ""ps.wdl"" as ps; ^; ```. ##### --list-dependencies or -l flag. For a successful validation, this will output the list of files referenced in import statements in workflows and their subworkflows. `$ java -jar womtool.jar validate -l myWdl.wdl`. ```hocon; Success!; List of Workflow dependencies are:; /path/to/my/import/myImport.wdl; /path/to/another/import/anotherImport.wdl; https://path-to-http-import/httpImport.wdl; ```. ### `inputs`. Examine a WDL file with one workflow in it, compute all the inputs needed for that workflow and output a JSON template that the user can fill in with values. The keys in this document should remain unchanged. The values tell you what type the parameter is expecting. For example, if the value were `Array[String]`, then it's expecting a JSON array of JSON strings, like this: `[""string1"", ""string2"", ""string3""]`. `$ java -jar womtool.jar inputs 3step.wdl`. ```; {; ""three_step.cgrep.pattern"": ""String""; }; ```. This inputs document is used as input to the `run` subcommand. ### `highlight`. Formats a WDL file and semantically tags it. This takes a second parameter (`html` or `console`) w",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:3245,Security,validat,validate,3245,"sentation then prints out a graph; of the WOM produced.; Any imported files can be supplied as subsequent arguments.; ```. ### `validate`. Given a WDL file, this runs the full syntax checker over the file and resolves imports in the process. If any syntax errors are found, they are written out. Otherwise the program exits. Error if a `call` references a task that doesn't exist:. `$ java -jar womtool.jar validate 2.wdl`. ```; ERROR: Call references a task (BADps) that doesn't exist (line 22, col 8). call BADps; ^; ```. Error if namespace and task have the same name:. `$ java -jar womtool.jar validate 5.wdl`. ```; ERROR: Task and namespace have the same name:. Task defined here (line 3, col 6):. task ps {; ^. Import statement defined here (line 1, col 20):. import ""ps.wdl"" as ps; ^; ```. ##### --list-dependencies or -l flag. For a successful validation, this will output the list of files referenced in import statements in workflows and their subworkflows. `$ java -jar womtool.jar validate -l myWdl.wdl`. ```hocon; Success!; List of Workflow dependencies are:; /path/to/my/import/myImport.wdl; /path/to/another/import/anotherImport.wdl; https://path-to-http-import/httpImport.wdl; ```. ### `inputs`. Examine a WDL file with one workflow in it, compute all the inputs needed for that workflow and output a JSON template that the user can fill in with values. The keys in this document should remain unchanged. The values tell you what type the parameter is expecting. For example, if the value were `Array[String]`, then it's expecting a JSON array of JSON strings, like this: `[""string1"", ""string2"", ""string3""]`. `$ java -jar womtool.jar inputs 3step.wdl`. ```; {; ""three_step.cgrep.pattern"": ""String""; }; ```. This inputs document is used as input to the `run` subcommand. ### `highlight`. Formats a WDL file and semantically tags it. This takes a second parameter (`html` or `console`) which determines what the output format will be. test.wdl; ```; task abc {; String in; command {; ec",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:633,Testability,test,test,633,"Command line utilities for interacting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prin",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:654,Testability,test,tests,654,"Command line utilities for interacting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prin",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:696,Testability,test,test,696,"Command line utilities for interacting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prin",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:766,Testability,test,tests,766,"Command line utilities for interacting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prin",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:827,Testability,test,test,827,"Command line utilities for interacting with the Workflow Object Model (WOM). You can download the latest WOMtool from the [Cromwell releases page on Github](https://github.com/broadinstitute/cromwell/releases/latest). ## Requirements. The following is the toolchain used for development of womtool. Other versions may work, but these are recommended. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). ## Building. `sbt assembly` will build a runnable JAR in `womtool/target/scala-2.13/`. Tests are run via `sbt test`. Note that the tests do require Docker to be running. To test this out while downloading the Ubuntu image that is required for tests, run `docker pull ubuntu:latest` prior to running `sbt test`. ## Command Line Usage. Run the JAR file with no arguments to get the usage message:. `$ java -jar womtool.jar`. ```bash; java -jar /path/to/womtool.jar <action> <parameters>. Actions:; validate [--list-dependencies] <WDL file>. Performs full validation of the WDL file including syntax; and semantic checking. -l or --list-dependencies is an optional flag to ; list files referenced in import statements. inputs <WDL file>. Print a JSON skeleton file of the inputs needed for this; workflow. Fill in the values in this JSON document and; pass it in to the 'run' subcommand. highlight <WDL file> <html|console>. Reformats and colorizes/tags a WDL file. The second; parameter is the output type. ""html"" will output the WDL; file with <span> tags around elements. ""console"" mode; will output colorized text to the terminal; ; parse <WDL file>. Compares a WDL file against the grammar and writes out an; abstract syntax tree if it is valid, and a syntax error; otherwise. Note that higher-level AST checks are not done; via this sub-command and the 'validate' subcommand should; be used for full validation. graph <WDL file>. Reads a WDL file against the grammar and prin",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:4201,Testability,test,test,4201,"ir subworkflows. `$ java -jar womtool.jar validate -l myWdl.wdl`. ```hocon; Success!; List of Workflow dependencies are:; /path/to/my/import/myImport.wdl; /path/to/another/import/anotherImport.wdl; https://path-to-http-import/httpImport.wdl; ```. ### `inputs`. Examine a WDL file with one workflow in it, compute all the inputs needed for that workflow and output a JSON template that the user can fill in with values. The keys in this document should remain unchanged. The values tell you what type the parameter is expecting. For example, if the value were `Array[String]`, then it's expecting a JSON array of JSON strings, like this: `[""string1"", ""string2"", ""string3""]`. `$ java -jar womtool.jar inputs 3step.wdl`. ```; {; ""three_step.cgrep.pattern"": ""String""; }; ```. This inputs document is used as input to the `run` subcommand. ### `highlight`. Formats a WDL file and semantically tags it. This takes a second parameter (`html` or `console`) which determines what the output format will be. test.wdl; ```; task abc {; String in; command {; echo ${in}; }; output {; String out = read_string(stdout()); }; }. workflow wf {; call abc; }; ```. ### `parse`. Given a WDL file input, this does grammar level syntax checks and writes out the resulting abstract syntax tree. `$ echo ""workflow wf {}"" | java -jar womtool.jar parse /dev/stdin`. ```; (Document:; imports=[],; definitions=[; (Workflow:; name=<stdin:1:10 identifier ""d2Y="">,; body=[]; ); ]; ); ```. This WDL file can be formatted in HTML as follows:. `$ java -jar womtool.jar highlight test.wdl html`. ```; <span class=""keyword"">task</span> <span class=""name"">abc</span> {; <span class=""type"">String</span> <span class=""variable"">in</span>; <span class=""section"">command</span> {; <span class=""command"">echo ${in}</span>; }; <span class=""section"">output</span> {; <span class=""type"">String</span> <span class=""variable"">out</span> = <span class=""function"">read_string</span>(<span class=""function"">stdout</span>()); }; }. <span class=""keywo",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md:4749,Testability,test,test,4749,"if the value were `Array[String]`, then it's expecting a JSON array of JSON strings, like this: `[""string1"", ""string2"", ""string3""]`. `$ java -jar womtool.jar inputs 3step.wdl`. ```; {; ""three_step.cgrep.pattern"": ""String""; }; ```. This inputs document is used as input to the `run` subcommand. ### `highlight`. Formats a WDL file and semantically tags it. This takes a second parameter (`html` or `console`) which determines what the output format will be. test.wdl; ```; task abc {; String in; command {; echo ${in}; }; output {; String out = read_string(stdout()); }; }. workflow wf {; call abc; }; ```. ### `parse`. Given a WDL file input, this does grammar level syntax checks and writes out the resulting abstract syntax tree. `$ echo ""workflow wf {}"" | java -jar womtool.jar parse /dev/stdin`. ```; (Document:; imports=[],; definitions=[; (Workflow:; name=<stdin:1:10 identifier ""d2Y="">,; body=[]; ); ]; ); ```. This WDL file can be formatted in HTML as follows:. `$ java -jar womtool.jar highlight test.wdl html`. ```; <span class=""keyword"">task</span> <span class=""name"">abc</span> {; <span class=""type"">String</span> <span class=""variable"">in</span>; <span class=""section"">command</span> {; <span class=""command"">echo ${in}</span>; }; <span class=""section"">output</span> {; <span class=""type"">String</span> <span class=""variable"">out</span> = <span class=""function"">read_string</span>(<span class=""function"">stdout</span>()); }; }. <span class=""keyword"">workflow</span> <span class=""name"">wf</span> {; <span class=""keyword"">call</span> <span class=""name"">abc</span>; }; ```. ### `graph`; ; The syntax of the graph command is:; ```; womtool graph [--all] wdlFile.wdl; ```. Given a WDL file input, command generates the data-flow graph through the system in `.dot` format. For example the fork-join WDL:; ```; task mkFile {; command {; for i in `seq 1 1000`; do; echo $i; done; }; output {; File numbers = stdout(); }; runtime {docker: ""ubuntu:latest""}; }. task grep {; String pattern; File in_",MatchSource.DOCS,docs/WOMtool.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/WOMtool.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:738,Deployability,install,installed,738,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:51,Integrability,contract,contract,51,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:352,Integrability,depend,dependsOn,352,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:430,Integrability,contract,contract,430,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:793,Integrability,contract,contract,793,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:1104,Integrability,contract,contracts,1104,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:937,Performance,cache,cache,937,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:964,Performance,cache,cache,964,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:60,Testability,test,testing,60,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:370,Testability,test,test,370,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:376,Testability,test,test,376,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:439,Testability,test,tests,439,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:649,Testability,test,test,649,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:802,Testability,test,tests,802,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md:1079,Testability,test,test,1079,"c# pact4s [Under construction]. pact4s is used for contract testing. # Dependencies. ```scala; val pact4sDependencies = Seq(; pact4sScalaTest,; pact4sCirce,; pact4sSpray; http4sEmberClient,; http4sDsl,; http4sEmberServer,; http4sCirce,; circeCore,; typelevelCat,; scalaTest; ). lazy val pact4s = project.in(file(""pact4s"")); .settings(pact4sSettings); .dependsOn(http % ""test->test;compile->compile""); ```. ## Building and running contract tests; Clone the repo.; ```; $ git clone https://github.com/broadinstitute/cromwell.git ; $ cd cromwell; ```. If you are already using OpenJDK 11, run the following command. ; ```; $ sbt ""project pact4s"" clean test ; ```. Otherwise, you can run the command inside a docker container with OpenJDK 11 installed. ; This is especially useful when automating contract tests in a GitHub Action runner which does not guarantee the correct OpenJDK version.; ```; docker run --rm -v $PWD:/working \; -v jar-cache:/root/.ivy \; -v jar-cache:/root/.ivy2 \; -w /working \; sbtscala/scala-sbt:openjdk-11.0.16_1.8.1_2.13.10 \; sbt ""project pact4s"" clean test; ```. The generated contracts can be found in the `./target/pacts` folder; - `cromwell-drshub.json`; - `cromwell-cbas.json`. ",MatchSource.DOCS,pact4s/README.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/pact4s/README.md
https://github.com/broadinstitute/cromwell/tree/87/processes/README.MD:430,Availability,down,download,430,"# Documented Processes. This directory contains a selection of processes which are:. * Best expressed as a `.dot` chart; * Still manual for now; * Under source control to make edits easy yet reviewable (just like code!). ## How to update these processes. Do you have a better idea about how any of these processes should work? ; Make a PR and it'll be reviewed, just like a code change!. * [Install Graphviz](https://graphviz.org/download/); * `brew install graphviz` on Mac; * Modify the appropriate `.dot` file(s); * Navigate to the `processes` directory ; * Run `refresh.sh` to update the png files.; * Add and commit the changed `.dot` and `.png` files to git; * Submit a PR for the change to be reviewed - and hopefully adopted!; ",MatchSource.DOCS,processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/README.MD:231,Deployability,update,update,231,"# Documented Processes. This directory contains a selection of processes which are:. * Best expressed as a `.dot` chart; * Still manual for now; * Under source control to make edits easy yet reviewable (just like code!). ## How to update these processes. Do you have a better idea about how any of these processes should work? ; Make a PR and it'll be reviewed, just like a code change!. * [Install Graphviz](https://graphviz.org/download/); * `brew install graphviz` on Mac; * Modify the appropriate `.dot` file(s); * Navigate to the `processes` directory ; * Run `refresh.sh` to update the png files.; * Add and commit the changed `.dot` and `.png` files to git; * Submit a PR for the change to be reviewed - and hopefully adopted!; ",MatchSource.DOCS,processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/README.MD:450,Deployability,install,install,450,"# Documented Processes. This directory contains a selection of processes which are:. * Best expressed as a `.dot` chart; * Still manual for now; * Under source control to make edits easy yet reviewable (just like code!). ## How to update these processes. Do you have a better idea about how any of these processes should work? ; Make a PR and it'll be reviewed, just like a code change!. * [Install Graphviz](https://graphviz.org/download/); * `brew install graphviz` on Mac; * Modify the appropriate `.dot` file(s); * Navigate to the `processes` directory ; * Run `refresh.sh` to update the png files.; * Add and commit the changed `.dot` and `.png` files to git; * Submit a PR for the change to be reviewed - and hopefully adopted!; ",MatchSource.DOCS,processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/processes/README.MD:581,Deployability,update,update,581,"# Documented Processes. This directory contains a selection of processes which are:. * Best expressed as a `.dot` chart; * Still manual for now; * Under source control to make edits easy yet reviewable (just like code!). ## How to update these processes. Do you have a better idea about how any of these processes should work? ; Make a PR and it'll be reviewed, just like a code change!. * [Install Graphviz](https://graphviz.org/download/); * `brew install graphviz` on Mac; * Modify the appropriate `.dot` file(s); * Navigate to the `processes` directory ; * Run `refresh.sh` to update the png files.; * Add and commit the changed `.dot` and `.png` files to git; * Submit a PR for the change to be reviewed - and hopefully adopted!; ",MatchSource.DOCS,processes/README.MD,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/processes/README.MD
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWS.md:265,Performance,perform,performs,265,"# AWS Batch backend (beta). Check out the [getting started guide](../tutorials/AwsBatch101.md) for the bulk of our documentation. AWS support is fairly new to Cromwell and this reference section will expand as features are added and documented. ### Disks. Cromwell performs automatic disk sizing on your behalf when running with the AWS backend, so attributes like ; ```; disks: ""local-disk""; ```; or; ```; disks: ""/some/mnt""; ```; are adequate to specify a disk that will suffice to complete your task. To facilitate the running of workflows originally authored for Pipelines API on Google Cloud Platform, Cromwell's AWS backend can also interpret attributes like; ```; disks: ""local-disk 20 SSD""; ```; and; ```; disks: ""/some/mnt 20 SSD""; ```; The size information and HDD/SSD have no effect on this backend and Cromwell simply drops them.; ",MatchSource.DOCS,docs/backends/AWS.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWS.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWS.md:59,Usability,guid,guide,59,"# AWS Batch backend (beta). Check out the [getting started guide](../tutorials/AwsBatch101.md) for the bulk of our documentation. AWS support is fairly new to Cromwell and this reference section will expand as features are added and documented. ### Disks. Cromwell performs automatic disk sizing on your behalf when running with the AWS backend, so attributes like ; ```; disks: ""local-disk""; ```; or; ```; disks: ""/some/mnt""; ```; are adequate to specify a disk that will suffice to complete your task. To facilitate the running of workflows originally authored for Pipelines API on Google Cloud Platform, Cromwell's AWS backend can also interpret attributes like; ```; disks: ""local-disk 20 SSD""; ```; and; ```; disks: ""/some/mnt 20 SSD""; ```; The size information and HDD/SSD have no effect on this backend and Cromwell simply drops them.; ",MatchSource.DOCS,docs/backends/AWS.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWS.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWS.md:823,Usability,simpl,simply,823,"# AWS Batch backend (beta). Check out the [getting started guide](../tutorials/AwsBatch101.md) for the bulk of our documentation. AWS support is fairly new to Cromwell and this reference section will expand as features are added and documented. ### Disks. Cromwell performs automatic disk sizing on your behalf when running with the AWS backend, so attributes like ; ```; disks: ""local-disk""; ```; or; ```; disks: ""/some/mnt""; ```; are adequate to specify a disk that will suffice to complete your task. To facilitate the running of workflows originally authored for Pipelines API on Google Cloud Platform, Cromwell's AWS backend can also interpret attributes like; ```; disks: ""local-disk 20 SSD""; ```; and; ```; disks: ""/some/mnt 20 SSD""; ```; The size information and HDD/SSD have no effect on this backend and Cromwell simply drops them.; ",MatchSource.DOCS,docs/backends/AWS.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWS.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:391,Deployability,configurat,configuration,391,"# AWS Batch Backend. AWS Batch is a set of batch management capabilities that dynamically provision the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. This section provides details on how to configure the AWS Batch backend with Cromwell. For instructions on common configuration and deployment tutorial, see [Getting started with AWS Batch](https://cromwell.readthedocs.io/en/develop/tutorials/AwsBatch101/). . ## Resources and Runtime Attributes. Cromwell and AWS Batch recognizes number of runtime attributes, more information can be found in the [customize tasks](/RuntimeAttributes#recognized-runtime-attributes-and-backends) page. ## Running Cromwell on an EC2 instance. Cromwell can be run on an EC2 instance and submit jobs to AWS Batch, AWS provide [CloudFormation stacks and guides](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/) to building the correct IAM permissions. ## Scaling Requirements; For a Cromwell server that will run multiple workflows, or workflows with many steps (e.g. ones with large scatter steps), it is recommended to setup a database to store workflow metadata. The application config file will expect a SQL database location. Follow [these instructions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.create.html) on how to create a serverless Amazon Aurora database. . ## Configuring Cromwell for AWS Batch. Within the `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesy",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:409,Deployability,deploy,deployment,409,"# AWS Batch Backend. AWS Batch is a set of batch management capabilities that dynamically provision the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. This section provides details on how to configure the AWS Batch backend with Cromwell. For instructions on common configuration and deployment tutorial, see [Getting started with AWS Batch](https://cromwell.readthedocs.io/en/develop/tutorials/AwsBatch101/). . ## Resources and Runtime Attributes. Cromwell and AWS Batch recognizes number of runtime attributes, more information can be found in the [customize tasks](/RuntimeAttributes#recognized-runtime-attributes-and-backends) page. ## Running Cromwell on an EC2 instance. Cromwell can be run on an EC2 instance and submit jobs to AWS Batch, AWS provide [CloudFormation stacks and guides](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/) to building the correct IAM permissions. ## Scaling Requirements; For a Cromwell server that will run multiple workflows, or workflows with many steps (e.g. ones with large scatter steps), it is recommended to setup a database to store workflow metadata. The application config file will expect a SQL database location. Follow [these instructions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.create.html) on how to create a serverless Amazon Aurora database. . ## Configuring Cromwell for AWS Batch. Within the `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesy",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:2797,Deployability,configurat,configuration,2797,"e `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter based on how many nodes are in the compute environment.; * `(backend.providers.AWSBatch.config.)root` points to the S3 bucket where workflow outputs are stored. This becomes a path on the root instance, and by default is cromwell_root. This is monitored by preinstalled daemon that expands drive space on the host, ie AWS EBS autoscale. This path is used as the 'local-disk' for containers.; ",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:2961,Deployability,configurat,configuration,2961,"e `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter based on how many nodes are in the compute environment.; * `(backend.providers.AWSBatch.config.)root` points to the S3 bucket where workflow outputs are stored. This becomes a path on the root instance, and by default is cromwell_root. This is monitored by preinstalled daemon that expands drive space on the host, ie AWS EBS autoscale. This path is used as the 'local-disk' for containers.; ",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:3402,Energy Efficiency,monitor,monitored,3402,"e `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter based on how many nodes are in the compute environment.; * `(backend.providers.AWSBatch.config.)root` points to the S3 bucket where workflow outputs are stored. This becomes a path on the root instance, and by default is cromwell_root. This is monitored by preinstalled daemon that expands drive space on the host, ie AWS EBS autoscale. This path is used as the 'local-disk' for containers.; ",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:317,Modifiability,config,configure,317,"# AWS Batch Backend. AWS Batch is a set of batch management capabilities that dynamically provision the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. This section provides details on how to configure the AWS Batch backend with Cromwell. For instructions on common configuration and deployment tutorial, see [Getting started with AWS Batch](https://cromwell.readthedocs.io/en/develop/tutorials/AwsBatch101/). . ## Resources and Runtime Attributes. Cromwell and AWS Batch recognizes number of runtime attributes, more information can be found in the [customize tasks](/RuntimeAttributes#recognized-runtime-attributes-and-backends) page. ## Running Cromwell on an EC2 instance. Cromwell can be run on an EC2 instance and submit jobs to AWS Batch, AWS provide [CloudFormation stacks and guides](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/) to building the correct IAM permissions. ## Scaling Requirements; For a Cromwell server that will run multiple workflows, or workflows with many steps (e.g. ones with large scatter steps), it is recommended to setup a database to store workflow metadata. The application config file will expect a SQL database location. Follow [these instructions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.create.html) on how to create a serverless Amazon Aurora database. . ## Configuring Cromwell for AWS Batch. Within the `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesy",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:391,Modifiability,config,configuration,391,"# AWS Batch Backend. AWS Batch is a set of batch management capabilities that dynamically provision the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. This section provides details on how to configure the AWS Batch backend with Cromwell. For instructions on common configuration and deployment tutorial, see [Getting started with AWS Batch](https://cromwell.readthedocs.io/en/develop/tutorials/AwsBatch101/). . ## Resources and Runtime Attributes. Cromwell and AWS Batch recognizes number of runtime attributes, more information can be found in the [customize tasks](/RuntimeAttributes#recognized-runtime-attributes-and-backends) page. ## Running Cromwell on an EC2 instance. Cromwell can be run on an EC2 instance and submit jobs to AWS Batch, AWS provide [CloudFormation stacks and guides](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/) to building the correct IAM permissions. ## Scaling Requirements; For a Cromwell server that will run multiple workflows, or workflows with many steps (e.g. ones with large scatter steps), it is recommended to setup a database to store workflow metadata. The application config file will expect a SQL database location. Follow [these instructions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.create.html) on how to create a serverless Amazon Aurora database. . ## Configuring Cromwell for AWS Batch. Within the `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesy",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:1277,Modifiability,config,config,1277,"ovides details on how to configure the AWS Batch backend with Cromwell. For instructions on common configuration and deployment tutorial, see [Getting started with AWS Batch](https://cromwell.readthedocs.io/en/develop/tutorials/AwsBatch101/). . ## Resources and Runtime Attributes. Cromwell and AWS Batch recognizes number of runtime attributes, more information can be found in the [customize tasks](/RuntimeAttributes#recognized-runtime-attributes-and-backends) page. ## Running Cromwell on an EC2 instance. Cromwell can be run on an EC2 instance and submit jobs to AWS Batch, AWS provide [CloudFormation stacks and guides](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/) to building the correct IAM permissions. ## Scaling Requirements; For a Cromwell server that will run multiple workflows, or workflows with many steps (e.g. ones with large scatter steps), it is recommended to setup a database to store workflow metadata. The application config file will expect a SQL database location. Follow [these instructions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.create.html) on how to create a serverless Amazon Aurora database. . ## Configuring Cromwell for AWS Batch. Within the `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with th",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:2797,Modifiability,config,configuration,2797,"e `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter based on how many nodes are in the compute environment.; * `(backend.providers.AWSBatch.config.)root` points to the S3 bucket where workflow outputs are stored. This becomes a path on the root instance, and by default is cromwell_root. This is monitored by preinstalled daemon that expands drive space on the host, ie AWS EBS autoscale. This path is used as the 'local-disk' for containers.; ",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:2889,Modifiability,config,config,2889,"e `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter based on how many nodes are in the compute environment.; * `(backend.providers.AWSBatch.config.)root` points to the S3 bucket where workflow outputs are stored. This becomes a path on the root instance, and by default is cromwell_root. This is monitored by preinstalled daemon that expands drive space on the host, ie AWS EBS autoscale. This path is used as the 'local-disk' for containers.; ",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:2961,Modifiability,config,configuration,2961,"e `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter based on how many nodes are in the compute environment.; * `(backend.providers.AWSBatch.config.)root` points to the S3 bucket where workflow outputs are stored. This becomes a path on the root instance, and by default is cromwell_root. This is monitored by preinstalled daemon that expands drive space on the host, ie AWS EBS autoscale. This path is used as the 'local-disk' for containers.; ",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:3015,Modifiability,config,config,3015,"e `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter based on how many nodes are in the compute environment.; * `(backend.providers.AWSBatch.config.)root` points to the S3 bucket where workflow outputs are stored. This becomes a path on the root instance, and by default is cromwell_root. This is monitored by preinstalled daemon that expands drive space on the host, ie AWS EBS autoscale. This path is used as the 'local-disk' for containers.; ",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:3246,Modifiability,config,config,3246,"e `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter based on how many nodes are in the compute environment.; * `(backend.providers.AWSBatch.config.)root` points to the S3 bucket where workflow outputs are stored. This becomes a path on the root instance, and by default is cromwell_root. This is monitored by preinstalled daemon that expands drive space on the host, ie AWS EBS autoscale. This path is used as the 'local-disk' for containers.; ",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:172,Performance,optimiz,optimized,172,"# AWS Batch Backend. AWS Batch is a set of batch management capabilities that dynamically provision the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. This section provides details on how to configure the AWS Batch backend with Cromwell. For instructions on common configuration and deployment tutorial, see [Getting started with AWS Batch](https://cromwell.readthedocs.io/en/develop/tutorials/AwsBatch101/). . ## Resources and Runtime Attributes. Cromwell and AWS Batch recognizes number of runtime attributes, more information can be found in the [customize tasks](/RuntimeAttributes#recognized-runtime-attributes-and-backends) page. ## Running Cromwell on an EC2 instance. Cromwell can be run on an EC2 instance and submit jobs to AWS Batch, AWS provide [CloudFormation stacks and guides](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/) to building the correct IAM permissions. ## Scaling Requirements; For a Cromwell server that will run multiple workflows, or workflows with many steps (e.g. ones with large scatter steps), it is recommended to setup a database to store workflow metadata. The application config file will expect a SQL database location. Follow [these instructions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.create.html) on how to create a serverless Amazon Aurora database. . ## Configuring Cromwell for AWS Batch. Within the `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesy",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:3023,Performance,concurren,concurrent-job-limit,3023,"e `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter based on how many nodes are in the compute environment.; * `(backend.providers.AWSBatch.config.)root` points to the S3 bucket where workflow outputs are stored. This becomes a path on the root instance, and by default is cromwell_root. This is monitored by preinstalled daemon that expands drive space on the host, ie AWS EBS autoscale. This path is used as the 'local-disk' for containers.; ",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:2143,Security,authenticat,authentication,2143,"(e.g. ones with large scatter steps), it is recommended to setup a database to store workflow metadata. The application config file will expect a SQL database location. Follow [these instructions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.create.html) on how to create a serverless Amazon Aurora database. . ## Configuring Cromwell for AWS Batch. Within the `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter ",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:2183,Security,authenticat,authentication,2183,"(e.g. ones with large scatter steps), it is recommended to setup a database to store workflow metadata. The application config file will expect a SQL database location. Follow [these instructions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.create.html) on how to create a serverless Amazon Aurora database. . ## Configuring Cromwell for AWS Batch. Within the `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter ",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:2431,Security,access,accessKeyId,2431,"ml) on how to create a serverless Amazon Aurora database. . ## Configuring Cromwell for AWS Batch. Within the `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter based on how many nodes are in the compute environment.; * `(backend.providers.AWSBatch.config.)root` points to the S3 bucket where workflow outputs are stored. This becomes a path on the root instance, and by default is cromwell_root. This is monitored by preinstalled daemon that ex",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:910,Usability,guid,guides,910,"# AWS Batch Backend. AWS Batch is a set of batch management capabilities that dynamically provision the optimal quantity and type of compute resources (e.g., CPU or memory optimized instances) based on the volume and specific resource requirements of the batch jobs submitted. This section provides details on how to configure the AWS Batch backend with Cromwell. For instructions on common configuration and deployment tutorial, see [Getting started with AWS Batch](https://cromwell.readthedocs.io/en/develop/tutorials/AwsBatch101/). . ## Resources and Runtime Attributes. Cromwell and AWS Batch recognizes number of runtime attributes, more information can be found in the [customize tasks](/RuntimeAttributes#recognized-runtime-attributes-and-backends) page. ## Running Cromwell on an EC2 instance. Cromwell can be run on an EC2 instance and submit jobs to AWS Batch, AWS provide [CloudFormation stacks and guides](https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/) to building the correct IAM permissions. ## Scaling Requirements; For a Cromwell server that will run multiple workflows, or workflows with many steps (e.g. ones with large scatter steps), it is recommended to setup a database to store workflow metadata. The application config file will expect a SQL database location. Follow [these instructions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.create.html) on how to create a serverless Amazon Aurora database. . ## Configuring Cromwell for AWS Batch. Within the `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesy",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md:2262,Usability,guid,guide,2262,"ata. The application config file will expect a SQL database location. Follow [these instructions](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.create.html) on how to create a serverless Amazon Aurora database. . ## Configuring Cromwell for AWS Batch. Within the `*.conf` file, you have a number of options to change the Cromwell's interaction with AWS Batch. ### Filesystems; > More information about filesystems can be found on the [Filesystems page](/filesystems/Filesystems/).; > . Amazon's S3 storage is a supported filesystem in both the engine and backend, this means that S3 files can be referenced at a workflow level, and as input files, provided they are prefixed by `'s3://'`. * filesystems; * filesystems.s3.auth; * filesystems.s3.caching.duplication-strategy. ### Configuring Authentication. To allow Cromwell to talk to AWS, the `default` authentication scheme uses the [default authentication provider](https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html) with the following AWS search paths:; - Environment Variables - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`; - Java system properties - `aws.accessKeyId` and `aws.secretKey`; - Default credential profiles file - Created by the AWS CLI, typically located at `~/.aws/credentials`; - _Instance profile credentials_ - Only relevant on EC2 instances. ### Allowing private Docker containers. AWS Batch allows the use of private Docker containers by providing `dockerhub` credentials. Under the specific backend's configuration, you can provide the following object:. ```hocon; (backend.providers.AWSBatch.config.)dockerhub = {; // account = """"; // token = """"; }; ```. ### More configuration options. * `(backend.providers.AWSBatch.config.)concurrent-job-limit` specifies the number of jobs that Cromwell will allow to be running in AWS at the same time. Tune this parameter based on how many nodes are in the compute environment.; * `(backend.providers.AWSBatch.config.)ro",MatchSource.DOCS,docs/backends/AWSBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/AWSBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Azure.md:516,Energy Efficiency,adapt,adaptations,516,"**Azure**. [Cromwell on Azure](https://github.com/microsoft/CromwellOnAzure) configures all Azure resources needed to run; workflows through Cromwell on the Azure cloud, and uses the [GA4GH TES](TES) backend for; orchestrating the tasks that create a workflow. Check out the [getting started guide](https://github.com/microsoft/CromwellOnAzure#Deploy-your-instance-of-Cromwell-on-Azure) for how to setup your environment. Have an existing WDL file that you want to run on Azure? [Modify your existing WDL with these adaptations for Azure](https://github.com/microsoft/CromwellOnAzure/blob/main/docs/change-existing-WDL-for-Azure.md/#How-to-modify-an-existing-WDL-file-to-run-on-Cromwell-on-Azure); ",MatchSource.DOCS,docs/backends/Azure.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Azure.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Azure.md:77,Modifiability,config,configures,77,"**Azure**. [Cromwell on Azure](https://github.com/microsoft/CromwellOnAzure) configures all Azure resources needed to run; workflows through Cromwell on the Azure cloud, and uses the [GA4GH TES](TES) backend for; orchestrating the tasks that create a workflow. Check out the [getting started guide](https://github.com/microsoft/CromwellOnAzure#Deploy-your-instance-of-Cromwell-on-Azure) for how to setup your environment. Have an existing WDL file that you want to run on Azure? [Modify your existing WDL with these adaptations for Azure](https://github.com/microsoft/CromwellOnAzure/blob/main/docs/change-existing-WDL-for-Azure.md/#How-to-modify-an-existing-WDL-file-to-run-on-Cromwell-on-Azure); ",MatchSource.DOCS,docs/backends/Azure.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Azure.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Azure.md:516,Modifiability,adapt,adaptations,516,"**Azure**. [Cromwell on Azure](https://github.com/microsoft/CromwellOnAzure) configures all Azure resources needed to run; workflows through Cromwell on the Azure cloud, and uses the [GA4GH TES](TES) backend for; orchestrating the tasks that create a workflow. Check out the [getting started guide](https://github.com/microsoft/CromwellOnAzure#Deploy-your-instance-of-Cromwell-on-Azure) for how to setup your environment. Have an existing WDL file that you want to run on Azure? [Modify your existing WDL with these adaptations for Azure](https://github.com/microsoft/CromwellOnAzure/blob/main/docs/change-existing-WDL-for-Azure.md/#How-to-modify-an-existing-WDL-file-to-run-on-Cromwell-on-Azure); ",MatchSource.DOCS,docs/backends/Azure.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Azure.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Azure.md:292,Usability,guid,guide,292,"**Azure**. [Cromwell on Azure](https://github.com/microsoft/CromwellOnAzure) configures all Azure resources needed to run; workflows through Cromwell on the Azure cloud, and uses the [GA4GH TES](TES) backend for; orchestrating the tasks that create a workflow. Check out the [getting started guide](https://github.com/microsoft/CromwellOnAzure#Deploy-your-instance-of-Cromwell-on-Azure) for how to setup your environment. Have an existing WDL file that you want to run on Azure? [Modify your existing WDL with these adaptations for Azure](https://github.com/microsoft/CromwellOnAzure/blob/main/docs/change-existing-WDL-for-Azure.md/#How-to-modify-an-existing-WDL-file-to-run-on-Cromwell-on-Azure); ",MatchSource.DOCS,docs/backends/Azure.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Azure.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1448,Availability,avail,available,1448,"ne](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities can be added depending on the backend.; For instance, an HPC backend can be configured to work with files on Google Cloud Storage. See the [HPC documentation](HPC) for more details.; ",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:895,Deployability,configurat,configuration,895,"#Backends. A backend is a way to run the commands of your workflow. Cromwell allows for backends conforming to; the Cromwell backend specification to be plugged into the Cromwell engine. Additionally, backends are included with the; Cromwell distribution:. * **[Local](Local)**; * **[HPC](HPC)**, including **[Sun Grid Engine](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).;",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1027,Deployability,configurat,configuration,1027,"y to run the commands of your workflow. Cromwell allows for backends conforming to; the Cromwell backend specification to be plugged into the Cromwell engine. Additionally, backends are included with the; Cromwell distribution:. * **[Local](Local)**; * **[HPC](HPC)**, including **[Sun Grid Engine](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1061,Deployability,configurat,configuration,1061,". Cromwell allows for backends conforming to; the Cromwell backend specification to be plugged into the Cromwell engine. Additionally, backends are included with the; Cromwell distribution:. * **[Local](Local)**; * **[HPC](HPC)**, including **[Sun Grid Engine](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Googl",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1363,Deployability,configurat,configuration,1363,"PC)**, including **[Sun Grid Engine](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities can be added depending on the backend.; For instance, an HPC backend can be configured to work with files on Google Cloud Storage. See the [HPC documen",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1606,Deployability,configurat,configuration,1606,"ne](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities can be added depending on the backend.; For instance, an HPC backend can be configured to work with files on Google Cloud Storage. See the [HPC documentation](HPC) for more details.; ",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:2153,Integrability,depend,depending,2153,"ne](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities can be added depending on the backend.; For instance, an HPC backend can be configured to work with files on Google Cloud Storage. See the [HPC documentation](HPC) for more details.; ",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:895,Modifiability,config,configuration,895,"#Backends. A backend is a way to run the commands of your workflow. Cromwell allows for backends conforming to; the Cromwell backend specification to be plugged into the Cromwell engine. Additionally, backends are included with the; Cromwell distribution:. * **[Local](Local)**; * **[HPC](HPC)**, including **[Sun Grid Engine](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).;",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1027,Modifiability,config,configuration,1027,"y to run the commands of your workflow. Cromwell allows for backends conforming to; the Cromwell backend specification to be plugged into the Cromwell engine. Additionally, backends are included with the; Cromwell distribution:. * **[Local](Local)**; * **[HPC](HPC)**, including **[Sun Grid Engine](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1061,Modifiability,config,configuration,1061,". Cromwell allows for backends conforming to; the Cromwell backend specification to be plugged into the Cromwell engine. Additionally, backends are included with the; Cromwell distribution:. * **[Local](Local)**; * **[HPC](HPC)**, including **[Sun Grid Engine](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Googl",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1179,Modifiability,config,config,1179,"ation to be plugged into the Cromwell engine. Additionally, backends are included with the; Cromwell distribution:. * **[Local](Local)**; * **[HPC](HPC)**, including **[Sun Grid Engine](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities c",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1231,Modifiability,config,config,1231,"PC)**, including **[Sun Grid Engine](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities can be added depending on the backend.; For instance, an HPC backend can be configured to work with files on Google Cloud Storage. See the [HPC documen",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1363,Modifiability,config,configuration,1363,"PC)**, including **[Sun Grid Engine](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities can be added depending on the backend.; For instance, an HPC backend can be configured to work with files on Google Cloud Storage. See the [HPC documen",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1606,Modifiability,config,configuration,1606,"ne](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities can be added depending on the backend.; For instance, an HPC backend can be configured to work with files on Google Cloud Storage. See the [HPC documentation](HPC) for more details.; ",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1700,Modifiability,config,config,1700,"ne](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities can be added depending on the backend.; For instance, an HPC backend can be configured to work with files on Google Cloud Storage. See the [HPC documentation](HPC) for more details.; ",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:2216,Modifiability,config,configured,2216,"ne](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities can be added depending on the backend.; For instance, an HPC backend can be configured to work with files on Google Cloud Storage. See the [HPC documentation](HPC) for more details.; ",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1538,Performance,concurren,concurrent,1538,"ne](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities can be added depending on the backend.; For instance, an HPC backend can be configured to work with files on Google Cloud Storage. See the [HPC documentation](HPC) for more details.; ",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md:1710,Performance,concurren,concurrent-job-limit,1710,"ne](SGE), [LSF](LSF), [HTCondor](HTcondor) & [SLURM](SLURM)** ; * Run jobs as subprocesses or via a dispatcher.; * Supports launching in Docker containers.; * Use `bash`, `qsub`, and `bsub` to run scripts.; * **[Google Cloud](Google)** ; * Launch jobs on Google Compute Engine through the Google Genomics Pipelines API.; * **[GA4GH TES](TES)** ; * Launch jobs on servers that support the GA4GH Task Execution Schema (TES).; * **[AWS Batch (beta)](AWS.md)**; * Use Job Queues on AWS Batch. HPC backends are put under the same umbrella because they all use the same generic configuration that can be specialized to fit the need of a particular technology. Backends are specified in the `backend.providers` configuration. Each backend has a configuration that looks like:. ```hocon; BackendName {; actor-factory = ""FQN of BackendLifecycleActorFactory class""; config {; ...; }; }; ```. The structure within the `config` block will vary from one backend to another; it is the backend implementation's responsibility; to be able to interpret its configuration. The providers section can contain multiple backends which will all be available to Cromwell. ## Backend Job Limits. All backends support limiting the number of concurrent jobs by specifying the following option in the backend's configuration; stanza:. ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. ## Backend Filesystems. Each backend will utilize a filesystem to store the directory structure and results of an executed workflow.; The backend/filesystem pairings are as follows:. * Local and HPC backend use the [Shared Local Filesystem](HPC/#filesystems).; * Google backend uses the [Google Cloud Storage Filesystem](Google/#google-cloud-storage-filesystem). Additional filesystems capabilities can be added depending on the backend.; For instance, an HPC backend can be configured to work with files on Google Cloud Storage. See the [HPC documentation](HPC) for more details.; ",MatchSource.DOCS,docs/backends/Backends.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Backends.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:3368,Availability,down,downloaded,3368,".gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s i",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:8062,Availability,avail,available,8062,"pt that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:8578,Availability,down,downloaded,8578,"ct in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|-------",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:16897,Availability,error,error,16897,"ate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data co",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:16939,Availability,down,download,16939,"ate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data co",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17194,Availability,down,downloading,17194,"ernatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17300,Availability,down,downloads,17300,"ernatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17631,Availability,down,download,17631," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17725,Availability,down,download,17725,"arallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:18002,Availability,down,downloading,18002,"ystem without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.goo",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:18160,Availability,down,downloads,18160,"==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:19220,Availability,avail,available,19220,"dea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on Batch. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a Batch backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapien",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:330,Deployability,configurat,configuration,330,"**Google Cloud Batch Backend (alpha)**. [//]:; Google Cloud Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources. Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. This section offers detailed configuration instructions for using Cromwell with the Google Cloud Batch in all supported; authentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-accoun",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:1162,Deployability,configurat,configuration,1162," Cloud resources. Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. This section offers detailed configuration instructions for using Cromwell with the Google Cloud Batch in all supported; authentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_acc",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:1832,Deployability,deploy,deployment,1832,"Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authentic",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:2277,Deployability,configurat,configuration,2277,"; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service acco",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:2378,Deployability,configurat,configuration,2378,"identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:4224,Deployability,configurat,configuration,4224," in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `GCPBATCH.config.batch.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Batch API and not GCS, it's important that this service account, and the service account specified in `GCPBATCH.config.batch.auth` can both read/write the location specified by `GCPBATCH.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `us",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:4565,Deployability,configurat,configuration,4565,"roject_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `GCPBATCH.config.batch.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Batch API and not GCS, it's important that this service account, and the service account specified in `GCPBATCH.config.batch.auth` can both read/write the location specified by `GCPBATCH.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service acc",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:6318,Deployability,configurat,configuration,6318," Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `gcp_batch_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used, Docker Hub credentials must be provided. If the Docker images being used; are public there is no need to add this configuration. For Batch. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; } ; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. GCP Batch also supports the use of Google Secret Manager for storing private Docker Hub credentials as described in; Google Batch documentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.b",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:8780,Deployability,configurat,configuration,8780,"ow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|; | cromwell-workflow-id | The Cromwell ID given to",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:8957,Deployability,pipeline,pipeline,8957,"`. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:9001,Deployability,pipeline,pipeline,9001,"`. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:12341,Deployability,configurat,configuration,12341,"te-cloud` stanza of a Batch backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. Note that in the; PAPI v2 backend `subnetwork-name` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-name` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running GCP Batch. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to GCP Batch. See the documentation for; [GCP Batch](https://cloud.google.com/batch/docs/networking-overview); for more information on the v",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:12700,Deployability,configurat,configuration,12700,"se values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. Note that in the; PAPI v2 backend `subnetwork-name` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-name` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running GCP Batch. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to GCP Batch. See the documentation for; [GCP Batch](https://cloud.google.com/batch/docs/networking-overview); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:14300,Deployability,configurat,configuration,14300,"-overview); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""reference-to-auth-scheme""; }; ...; }; }; }; }; ```. The `network-label-key` and `subnetwork-label-key` should reference the keys in your project's labels whose value is the name of your private network; and subnetwork within that network respectively. `auth` should reference an auth scheme in the `google` stanza which will be used to get the project metadata from Google Cloud.; The `subnetwork-label-key` is an optional config. Note that in the; PAPI v2 backend `subnetwork-label-key` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-label-key` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container. Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cl",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:15857,Deployability,configurat,configuration,15857,"the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container. Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads . Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `batch.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:16400,Deployability,configurat,configuration,16400,"e-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads . Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `batch.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature i",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:16612,Deployability,deploy,deployment,16612,"ads . Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `batch.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod in",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17385,Deployability,install,installed,17385,"akes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17534,Deployability,install,installation,17534," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17663,Deployability,install,installing,17663,"files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:18102,Deployability,install,installed,18102,"te_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecy",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:18884,Deployability,configurat,configuration,18884,"config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on Batch. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a Batch backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hoco",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:18976,Deployability,pipeline,pipelines,18976,"ted during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on Batch. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a Batch backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:19568,Deployability,configurat,configuration,19568," the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on Batch. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a Batch backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" : 407769621; }, {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.sa"",; ""crc32c"" : 1902048083; },; ...; },; ...; ]; ...; }; }; }; }; ```. Reference disk usage is an opt-in feature, so workflow submissions must speci",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:107,Energy Efficiency,schedul,schedule,107,"**Google Cloud Batch Backend (alpha)**. [//]:; Google Cloud Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources. Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. This section offers detailed configuration instructions for using Cromwell with the Google Cloud Batch in all supported; authentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-accoun",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:7696,Energy Efficiency,monitor,monitor,7696," the use of Google Secret Manager for storing private Docker Hub credentials as described in; Google Batch documentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence ov",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:7949,Energy Efficiency,monitor,monitoring,7949,"y). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. G",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:8028,Energy Efficiency,monitor,monitoring,8028,"similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:1404,Integrability,protocol,protocols,1404,"hentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defi",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:16903,Integrability,message,message,16903,"ate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data co",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:18033,Integrability,message,message,18033,"te_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecy",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:330,Modifiability,config,configuration,330,"**Google Cloud Batch Backend (alpha)**. [//]:; Google Cloud Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources. Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. This section offers detailed configuration instructions for using Cromwell with the Google Cloud Batch in all supported; authentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-accoun",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:1162,Modifiability,config,configuration,1162," Cloud resources. Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. This section offers detailed configuration instructions for using Cromwell with the Google Cloud Batch in all supported; authentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_acc",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:2277,Modifiability,config,configuration,2277,"; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service acco",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:2378,Modifiability,config,configuration,2378,"identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:2941,Modifiability,config,config,2941,"scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:4224,Modifiability,config,configuration,4224," in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `GCPBATCH.config.batch.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Batch API and not GCS, it's important that this service account, and the service account specified in `GCPBATCH.config.batch.auth` can both read/write the location specified by `GCPBATCH.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `us",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:4565,Modifiability,config,configuration,4565,"roject_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `GCPBATCH.config.batch.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Batch API and not GCS, it's important that this service account, and the service account specified in `GCPBATCH.config.batch.auth` can both read/write the location specified by `GCPBATCH.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service acc",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:4596,Modifiability,config,config,4596,"\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `GCPBATCH.config.batch.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Batch API and not GCS, it's important that this service account, and the service account specified in `GCPBATCH.config.batch.auth` can both read/write the location specified by `GCPBATCH.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:4971,Modifiability,config,config,4971,"erts"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `GCPBATCH.config.batch.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Batch API and not GCS, it's important that this service account, and the service account specified in `GCPBATCH.config.batch.auth` can both read/write the location specified by `GCPBATCH.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `gcp_batch_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:5046,Modifiability,config,config,5046,"ta/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `GCPBATCH.config.batch.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Batch API and not GCS, it's important that this service account, and the service account specified in `GCPBATCH.config.batch.auth` can both read/write the location specified by `GCPBATCH.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `gcp_batch_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:6318,Modifiability,config,configuration,6318," Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `gcp_batch_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used, Docker Hub credentials must be provided. If the Docker images being used; are public there is no need to add this configuration. For Batch. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; } ; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. GCP Batch also supports the use of Google Secret Manager for storing private Docker Hub credentials as described in; Google Batch documentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.b",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:6491,Modifiability,config,config,6491,"ccess to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `gcp_batch_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used, Docker Hub credentials must be provided. If the Docker images being used; are public there is no need to add this configuration. For Batch. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; } ; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. GCP Batch also supports the use of Google Secret Manager for storing private Docker Hub credentials as described in; Google Batch documentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the com",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:7332,Modifiability,config,config,7332,"; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; } ; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. GCP Batch also supports the use of Google Secret Manager for storing private Docker Hub credentials as described in; Google Batch documentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Stora",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:8760,Modifiability,config,config,8760,"ll Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|; | cromwell-workflow-id | The C",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:8780,Modifiability,config,configuration,8780,"ow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|; | cromwell-workflow-id | The Cromwell ID given to",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:9129,Modifiability,config,config,9129,"a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | cromwell-sub-workflow-id | The Cromwell ID given to this job's sub-workflow (immediate parent workflow) | cromwell-sub-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the req",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:11406,Modifiability,config,config,11406,"if the task is called in a subworkflow. |; | wdl-task-name | The name of the WDL task | my-task | |; | wdl-call-alias | The alias of the WDL call that created this job | my-task-1 | Only present if the task was called with an alias. |; | wdl-attempt | Attempt number for this call | 1 | |; | wdl-shard-index | Index of this job within a scatter, | | Only present if the task was called within a scatter. |. Any custom labels provided as '`google_labels`' in the [workflow options](../wf_options/Google) are also applied to Google resources by GCP Batch. ### Custom Mount Points. Cromwell's GCP Batch backend supports custom mount points as documented [here](../RuntimeAttributes.md#disks), with the caveat that all custom mount points must be specified under `/mnt/disks`. e.g. a GCP Batch custom mount point specification should look like:. ```; runtime {; disks: ""/mnt/disks/my_mnt 30 SSD, /mnt/disks/my_mnt2 500 HDD""; }; ```. ### Virtual Private Network. Cromwell can arrange for jobs to run in specific GCP private networks via the `config.virtual-private-cloud` stanza of a Batch backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. Note that in the; PAPI v2 backend `subnetwork-name` was an optional configuration parameter which",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:11956,Modifiability,config,config,11956,"s. Cromwell's GCP Batch backend supports custom mount points as documented [here](../RuntimeAttributes.md#disks), with the caveat that all custom mount points must be specified under `/mnt/disks`. e.g. a GCP Batch custom mount point specification should look like:. ```; runtime {; disks: ""/mnt/disks/my_mnt 30 SSD, /mnt/disks/my_mnt2 500 HDD""; }; ```. ### Virtual Private Network. Cromwell can arrange for jobs to run in specific GCP private networks via the `config.virtual-private-cloud` stanza of a Batch backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. Note that in the; PAPI v2 backend `subnetwork-name` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-name` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `netwo",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:12265,Modifiability,config,config,12265,"mnt 30 SSD, /mnt/disks/my_mnt2 500 HDD""; }; ```. ### Virtual Private Network. Cromwell can arrange for jobs to run in specific GCP private networks via the `config.virtual-private-cloud` stanza of a Batch backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. Note that in the; PAPI v2 backend `subnetwork-name` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-name` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running GCP Batch. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pas",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:12341,Modifiability,config,configuration,12341,"te-cloud` stanza of a Batch backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. Note that in the; PAPI v2 backend `subnetwork-name` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-name` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running GCP Batch. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to GCP Batch. See the documentation for; [GCP Batch](https://cloud.google.com/batch/docs/networking-overview); for more information on the v",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:12626,Modifiability,config,config,12626,"se values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. Note that in the; PAPI v2 backend `subnetwork-name` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-name` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running GCP Batch. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to GCP Batch. See the documentation for; [GCP Batch](https://cloud.google.com/batch/docs/networking-overview); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:12700,Modifiability,config,configuration,12700,"se values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. Note that in the; PAPI v2 backend `subnetwork-name` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-name` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running GCP Batch. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to GCP Batch. See the documentation for; [GCP Batch](https://cloud.google.com/batch/docs/networking-overview); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:12856,Modifiability,config,config,12856,"H {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. Note that in the; PAPI v2 backend `subnetwork-name` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-name` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running GCP Batch. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to GCP Batch. See the documentation for; [GCP Batch](https://cloud.google.com/batch/docs/networking-overview); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""reference-to-auth-scheme""; }; ...; }; }; }; }; ```. The `network-label-key` ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:13658,Modifiability,config,config,13658," one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running GCP Batch. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to GCP Batch. See the documentation for; [GCP Batch](https://cloud.google.com/batch/docs/networking-overview); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""reference-to-auth-scheme""; }; ...; }; }; }; }; ```. The `network-label-key` and `subnetwork-label-key` should reference the keys in your project's labels whose value is the name of your private network; and subnetwork within that network respectively. `auth` should reference an auth scheme in the `google` stanza which will be used to get the project metadata from Google Cloud.; The `subnetwork-label-key` is an optional config. Note that in the; PAPI v2 backend `subnetwork-label-key` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-label-key` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, and one of the labels in ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:14219,Modifiability,config,config,14219,"ojectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to GCP Batch. See the documentation for; [GCP Batch](https://cloud.google.com/batch/docs/networking-overview); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""reference-to-auth-scheme""; }; ...; }; }; }; }; ```. The `network-label-key` and `subnetwork-label-key` should reference the keys in your project's labels whose value is the name of your private network; and subnetwork within that network respectively. `auth` should reference an auth scheme in the `google` stanza which will be used to get the project metadata from Google Cloud.; The `subnetwork-label-key` is an optional config. Note that in the; PAPI v2 backend `subnetwork-label-key` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-label-key` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container. Cromwell can",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:14300,Modifiability,config,configuration,14300,"-overview); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""reference-to-auth-scheme""; }; ...; }; }; }; }; ```. The `network-label-key` and `subnetwork-label-key` should reference the keys in your project's labels whose value is the name of your private network; and subnetwork within that network respectively. `auth` should reference an auth scheme in the `google` stanza which will be used to get the project metadata from Google Cloud.; The `subnetwork-label-key` is an optional config. Note that in the; PAPI v2 backend `subnetwork-label-key` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-label-key` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container. Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cl",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:14590,Modifiability,config,config,14590,".backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""reference-to-auth-scheme""; }; ...; }; }; }; }; ```. The `network-label-key` and `subnetwork-label-key` should reference the keys in your project's labels whose value is the name of your private network; and subnetwork within that network respectively. `auth` should reference an auth scheme in the `google` stanza which will be used to get the project metadata from Google Cloud.; The `subnetwork-label-key` is an optional config. Note that in the; PAPI v2 backend `subnetwork-label-key` was an optional configuration parameter which accepted a `*` wildcard for choosing the; appropriate subnetwork region, but in GCP Batch the `subnetwork-label-key` specification can be omitted; and GCP Batch will choose the appropriate subnetwork automatically. For example, if your `virtual-private-cloud` config looks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container. Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads . Cromwell can be configured to use GCS parallel composite uploa",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:15558,Modifiability,config,configured,15558,"ks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container. Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads . Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `batch.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:15857,Modifiability,config,configuration,15857,"the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container. Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads . Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `batch.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:16136,Modifiability,config,config,16136," on the default network. ### Custom Google Cloud SDK container. Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads . Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `batch.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composit",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:16400,Modifiability,config,configuration,16400,"e-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads . Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `batch.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature i",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:16651,Modifiability,config,config,16651,"ads . Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `batch.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod in",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17845,Modifiability,config,config,17845,"arallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:18884,Modifiability,config,configuration,18884,"config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on Batch. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a Batch backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hoco",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:19178,Modifiability,config,config,19178,"ds does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on Batch. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a Batch backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {;",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:19568,Modifiability,config,configuration,19568," the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on Batch. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a Batch backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" : 407769621; }, {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.sa"",; ""crc32c"" : 1902048083; },; ...; },; ...; ]; ...; }; }; }; }; ```. Reference disk usage is an opt-in feature, so workflow submissions must speci",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:19705,Modifiability,config,config,19705,"s using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on Batch. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a Batch backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" : 407769621; }, {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.sa"",; ""crc32c"" : 1902048083; },; ...; },; ...; ]; ...; }; }; }; }; ```. Reference disk usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_reference_disks"": true,; ...; }; ```. Using the first file in the manifest above as an example, ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:19973,Modifiability,config,config,19973,"ackend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on Batch. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a Batch backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" : 407769621; }, {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.sa"",; ""crc32c"" : 1902048083; },; ...; },; ...; ]; ...; }; }; }; }; ```. Reference disk usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_reference_disks"": true,; ...; }; ```. Using the first file in the manifest above as an example, assume a Batch backend is configured to use this manifest and the appropriate; `use_reference_disks` workflow option is set to `true` in the workflow submission. If a call in that workflow ; specifi",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:20791,Modifiability,config,configured,20791,"nce disk manifests: . ```hocon; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" : 407769621; }, {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.sa"",; ""crc32c"" : 1902048083; },; ...; },; ...; ]; ...; }; }; }; }; ```. Reference disk usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_reference_disks"": true,; ...; }; ```. Using the first file in the manifest above as an example, assume a Batch backend is configured to use this manifest and the appropriate; `use_reference_disks` workflow option is set to `true` in the workflow submission. If a call in that workflow ; specifies the input `gs://my-references/enormous_reference.bam` and because that input matches the path of a file on the; reference image without the leading `gs://`, Cromwell would; arrange for a reference disk based on this image to be mounted and for the call's input to refer to the ; copy of the file on the reference disk, bypassing localization of the input. . The Cromwell git repository includes a Java-based tool to facilitate the creation of manifests called; [CromwellRefdiskManifestCreatorApp](https://github.com/broadinstitute/cromwell/tree/develop/CromwellRefdiskManifestCreator).; Please see the help command of that tool for more details. Alternatively for public data stored under `gs://gcp-public-data--broad-references` there exists a shell script to; extract reference data to a new disk and then convert that disk to a public image. For m",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:117,Performance,queue,queue,117,"**Google Cloud Batch Backend (alpha)**. [//]:; Google Cloud Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources. Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. This section offers detailed configuration instructions for using Cromwell with the Google Cloud Batch in all supported; authentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-accoun",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:15648,Performance,perform,performance,15648,"ks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container. Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads . Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `batch.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchLifecycleActorFactory""; config {; ...; batch {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17622,Performance,throttle,throttle,17622," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17640,Performance,perform,performance,17640," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17755,Performance,perform,performance,17755,"arallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:18467,Performance,cache,cache,18467," ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cro",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:18555,Performance,cache,cached,18555,"so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on Batch. Please note the configuration of r",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:8924,Safety,timeout,timeout,8924,"_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [for",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:8966,Safety,timeout,timeout,8966,"`. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:9015,Safety,abort,abort,9015,"`. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:9037,Safety,timeout,timeout,9037,"ll be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | cromwell-sub-workflow-id | The Cromwell ID given to this job's sub-workflow (i",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:9145,Safety,timeout,timeout,9145,"a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |----------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | cromwell-sub-workflow-id | The Cromwell ID given to this job's sub-workflow (immediate parent workflow) | cromwell-sub-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the req",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:422,Security,authenticat,authentication,422,"**Google Cloud Batch Backend (alpha)**. [//]:; Google Cloud Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources. Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. This section offers detailed configuration instructions for using Cromwell with the Google Cloud Batch in all supported; authentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-accoun",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:593,Security,authenticat,authentication,593,"**Google Cloud Batch Backend (alpha)**. [//]:; Google Cloud Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources. Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. This section offers detailed configuration instructions for using Cromwell with the Google Cloud Batch in all supported; authentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-accoun",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:669,Security,authenticat,authentication,669,"**Google Cloud Batch Backend (alpha)**. [//]:; Google Cloud Batch is a fully managed service that lets you schedule, queue, and execute batch processing workloads on Google Cloud resources. Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. This section offers detailed configuration instructions for using Cromwell with the Google Cloud Batch in all supported; authentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-accoun",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:1196,Security,authenticat,authenticate,1196," Cloud resources. Batch provisions resources and manages capacity on your behalf, allowing your batch workloads to run at scale. This section offers detailed configuration instructions for using Cromwell with the Google Cloud Batch in all supported; authentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_acc",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:1246,Security,authenticat,authentication,1246,"allowing your batch workloads to run at scale. This section offers detailed configuration instructions for using Cromwell with the Google Cloud Batch in all supported; authentication modes. Before reading further in this section please see the; [Getting started on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:1546,Security,authenticat,authenticate,1546,"tarted on Google Cloud Batch](../tutorials/Batch101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions wit",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:1791,Security,authenticat,authentication,1791,"Google project enabled for the appropriate APIs. *NOTE*: Google Cloud Batch is still in alpha version, this means that there could be breaking changes, be sure to review the [GCP Batch CHANGELOG](https://github.com/broadinstitute/cromwell/blob/develop/CHANGELOG.md#gcp-batch) carefully before upgrading. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authentic",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:2199,Security,authenticat,authentication,2199,"; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service acco",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:2782,Security,authenticat,authenticate,2782,"ell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:4450,Security,authenticat,authentication,4450,"roject_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `GCPBATCH.config.batch.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Batch API and not GCS, it's important that this service account, and the service account specified in `GCPBATCH.config.batch.auth` can both read/write the location specified by `GCPBATCH.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service acc",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:5519,Security,access,access,5519,"te VM is started with via the configuration option `GCPBATCH.config.batch.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Batch API and not GCS, it's important that this service account, and the service account specified in `GCPBATCH.config.batch.auth` can both read/write the location specified by `GCPBATCH.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `gcp_batch_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used, Docker Hub credentials must be provided. If the Docker images being used; are public there is no need to add this configuration. For Batch. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encod",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:5676,Security,access,access,5676,"_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Batch API and not GCS, it's important that this service account, and the service account specified in `GCPBATCH.config.batch.auth` can both read/write the location specified by `GCPBATCH.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `gcp_batch_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used, Docker Hub credentials must be provided. If the Docker images being used; are public there is no need to add this configuration. For Batch. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; } ; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. GCP Batch al",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:6032,Security,access,access,6032,"th` can both read/write the location specified by `GCPBATCH.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `gcp_batch_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used, Docker Hub credentials must be provided. If the Docker images being used; are public there is no need to add this configuration. For Batch. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; } ; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. GCP Batch also supports the use of Google Secret Manager for storing private Docker Hub credentials as described in; Google Batch documentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage o",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:6558,Security,password,password,6558,"ccess to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `gcp_batch_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used, Docker Hub credentials must be provided. If the Docker images being used; are public there is no need to add this configuration. For Batch. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; } ; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. GCP Batch also supports the use of Google Secret Manager for storing private Docker Hub credentials as described in; Google Batch documentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the com",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:6638,Security,password,password,6638,"case that this service account can not access Cromwell's default google bucket, the `gcp_batch_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used, Docker Hub credentials must be provided. If the Docker images being used; are public there is no need to add this configuration. For Batch. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; } ; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. GCP Batch also supports the use of Google Secret Manager for storing private Docker Hub credentials as described in; Google Batch documentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manag",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:7058,Security,password,password,7058,"command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used, Docker Hub credentials must be provided. If the Docker images being used; are public there is no need to add this configuration. For Batch. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; } ; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. GCP Batch also supports the use of Google Secret Manager for storing private Docker Hub credentials as described in; Google Batch documentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:7123,Security,password,password,7123,"command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used, Docker Hub credentials must be provided. If the Docker images being used; are public there is no need to add this configuration. For Batch. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; } ; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. GCP Batch also supports the use of Google Secret Manager for storing private Docker Hub credentials as described in; Google Batch documentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:7434,Security,password,password,7434,"; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; } ; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. GCP Batch also supports the use of Google Secret Manager for storing private Docker Hub credentials as described in; Google Batch documentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Stora",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17485,Security,integrity,integrity,17485," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17593,Security,hash,hash,17593," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17783,Security,integrity,integrity,17783,"arallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:17912,Security,integrity,integrity,17912,"ual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configura",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:18140,Security,integrity,integrity,18140,"==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:18356,Security,hash,hash,18356," ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Life Sciences v2beta to Google Cloud Batch. 1. If you currently run your workflows using Cloud Genomics v2beta and would like to switch to Google Cloud Batch, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.batch.GcpBatchLifecycleActorFactory`. 2. You will need to remove the parameter `genomics.endpoint-url` and generate a new config file. 3. Google Cloud Batch is now available in a variety of regions. Please see the [Batch Locations](https://cloud.google.com/batch/docs/locations) for a list of supported regions. ### Reference Disk Support. Cro",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:2925,Testability,log,login,2925,"scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `GCPBATCH` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `GCPBATCH` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:7895,Testability,log,log,7895,"mentation; [here](https://cloud.google.com/batch/docs/create-run-job-secret-manager#use-secrets-for-docker-registry). In the; Cromwell GCP Batch backend, the usage of this feature is very similar to the regular; base64-encoded `username:password` token, except that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage U",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:8039,Testability,log,log,8039,"pt that it is the GSM paths of username and password that are separated by; a colon and base64; encoded:. ```; backend {; default = GCPBATCH; providers {; GCPBATCH {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md:8261,Testability,log,log,8261,"ogle.batch.GcpBatchBackendLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-GSM-path-to-docker-hub-username:GSM-path-to-docker-hub-password""; }; }; }; }; }; ```. Note that as per the Google Secret Manager docs, the compute service account for the project in which the GCP Batch; jobs will run will need to be assigned the `Secret Manager Secret Accessor` IAM role. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Batch backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `gcp_batch_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Batch timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `batch-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.GCPBATCH.config {; batch-timeout: 14 days; }; ```. #### Google Labels. Every call run on the GCP Batch backend is given certain labels by default, so that Google ",MatchSource.DOCS,docs/backends/GCPBatch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/GCPBatch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:3065,Availability,down,downloaded,3065,".gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s i",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9994,Availability,avail,available,9994,"oken"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a max",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10514,Availability,down,downloaded,10514,"ions it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11349,Availability,error,error,11349,"e Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.provi",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11368,Availability,avail,available,11368,"e Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.provi",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11481,Availability,failure,failures,11481,"; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.providers.Papiv2.config {; genomics {; enable-fuse = true; }; }; ```. There is a list of limitations regarding the usage ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20189,Availability,error,error,20189," parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data co",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20231,Availability,down,download,20231," parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data co",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20486,Availability,down,downloading,20486,"ernatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20592,Availability,down,downloads,20592,"ernatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20923,Availability,down,download,20923," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21017,Availability,down,download,21017,"arallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21294,Availability,down,downloading,21294,"ystem without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be chang",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21452,Availability,down,downloads,21452,"==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-ur",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:22728,Availability,avail,available,22728,"posite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-url` value should be changed from `https://genomics.googleapis.com/` to ; `https://lifesciences.googleapis.com/`.; 3. Also you should add a new mandatory parameter `genomics.location` to your backend configuration. Currently Google Cloud ; Life Sciences API is available only in `us-central1` and `europe-west2` locations. ### Alpha support for WDL optional outputs on PAPI v2. Cromwell 53 adds alpha-quality support for WDL optional outputs on PAPI v2 backends. Constructs such as: . ```; struct MyStruct {; String name; File? file; }; .; .; .; output {; File? file_does_not_exist = ""does_not_exist""; Pair[String, File?] pair_file_does_not_exist = (""this"", ""does_not_exist""); Map[String, File?] map_file_does_not_exist = { ""does_not_exist"": ""does_not_exist"" }; Array[File?] array_file_does_not_exist = [""does_not_exist""]; MyStruct struct_file_does_not_exist = object { name: ""this"", file: ""does_not_exist"" } ; }; ```. will not produce errors if the file `does_not_exist` does not exist. This support for optional files is considered alpha; quality for two reasons:. 1. As seen in the example above, support for optional files extends to complex WDL types but there is a restriction that; all `File` components of non-primitive types must be optional. e.g. Cromwell wo",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:23403,Availability,error,errors,23403,"ActorFactory`.; 2. Parameter `genomics.endpoint-url` value should be changed from `https://genomics.googleapis.com/` to ; `https://lifesciences.googleapis.com/`.; 3. Also you should add a new mandatory parameter `genomics.location` to your backend configuration. Currently Google Cloud ; Life Sciences API is available only in `us-central1` and `europe-west2` locations. ### Alpha support for WDL optional outputs on PAPI v2. Cromwell 53 adds alpha-quality support for WDL optional outputs on PAPI v2 backends. Constructs such as: . ```; struct MyStruct {; String name; File? file; }; .; .; .; output {; File? file_does_not_exist = ""does_not_exist""; Pair[String, File?] pair_file_does_not_exist = (""this"", ""does_not_exist""); Map[String, File?] map_file_does_not_exist = { ""does_not_exist"": ""does_not_exist"" }; Array[File?] array_file_does_not_exist = [""does_not_exist""]; MyStruct struct_file_does_not_exist = object { name: ""this"", file: ""does_not_exist"" } ; }; ```. will not produce errors if the file `does_not_exist` does not exist. This support for optional files is considered alpha; quality for two reasons:. 1. As seen in the example above, support for optional files extends to complex WDL types but there is a restriction that; all `File` components of non-primitive types must be optional. e.g. Cromwell would not allow the assignment of a ; missing file to the right side of a pair of type `Pair[File, File?]` since the left member of the pair is a non-optional; file. This restriction exists solely due to technical limitations in how type evaluation works in Cromwell today and; may be removed in a future Cromwell release. 2. Call caching does not work for calls with empty optional outputs. Cromwell currently does not recognize; that it is okay for optional output files to be missing, will incorrectly claim that any cache hits with missing ; optional output files are unusable, and will proceed to search for more cache hits which if found will also be unusable,; before eventually g",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:27312,Availability,avail,available,27312,"nce disk based on this image to be mounted and for the call's input to refer to the ; copy of the file on the reference disk, bypassing localization of the input. . The Cromwell git repository includes a Java-based tool to facilitate the creation of manifests called; [CromwellRefdiskManifestCreatorApp](https://github.com/broadinstitute/cromwell/tree/develop/CromwellRefdiskManifestCreator).; Please see the help command of that tool for more details. Alternatively for public data stored under `gs://gcp-public-data--broad-references` there exists a shell script to; extract reference data to a new disk and then convert that disk to a public image. For more information see; [create_images.sh](https://github.com/broadinstitute/cromwell/tree/develop/scripts/reference_disks/create_images.sh). ### Docker Image Cache Support. To optimize job execution time, Cromwell 55 and later support the use of Docker image caches on the PAPI v2 lifesciences beta backend. Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can turn off Docker image caching through the use of a runtime attribute:. ```wdl; task my_task {; ...; runti",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:220,Deployability,configurat,configuration,220,"**Google Cloud Backend**. Google Genomics Pipelines API is a Docker-as-a-service from Google. It was formerly called JES (Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For exa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:279,Deployability,configurat,configuration,279,"**Google Cloud Backend**. Google Genomics Pipelines API is a Docker-as-a-service from Google. It was formerly called JES (Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For exa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:859,Deployability,configurat,configuration,859,"**Google Cloud Backend**. Google Genomics Pipelines API is a Docker-as-a-service from Google. It was formerly called JES (Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For exa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:1529,Deployability,deploy,deployment,1529,"tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authentic",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:1974,Deployability,configurat,configuration,1974,"; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service acco",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:2075,Deployability,configurat,configuration,2075,"identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:3921,Deployability,configurat,configuration,3921," in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `JES.config.genomics.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Pipelines API and not GCS, it's important that this service account, and the service account specified in `JES.config.genomics.auth` can both read/write the location specified by `JES.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_se",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:4262,Deployability,configurat,configuration,4262,"""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `JES.config.genomics.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Pipelines API and not GCS, it's important that this service account, and the service account specified in `JES.config.genomics.auth` can both read/write the location specified by `JES.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service accou",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:5912,Deployability,configurat,configuration,5912,"on` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; bac",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6017,Deployability,configurat,configuration,6017,"ng this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6169,Deployability,pipeline,pipelines,6169,"t this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6548,Deployability,pipeline,pipelines,6548,"ocker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6988,Deployability,pipeline,pipelines,6988," is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The co",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7972,Deployability,configurat,configuration,7972,"ult = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; """,MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:8237,Deployability,configurat,configuration,8237,"tion""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are mi",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:8552,Deployability,configurat,configuration,8552,"t and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:8884,Deployability,configurat,configuration,8884,"oken` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"":",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9212,Deployability,configurat,configuration,9212,"n, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens an",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9585,Deployability,configurat,configuration,9585,"s not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root`",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10710,Deployability,configurat,configuration,10710,"flow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE c",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10890,Deployability,pipeline,pipeline,10890,". The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens beca",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10934,Deployability,pipeline,pipeline,10934,". The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens beca",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10964,Deployability,pipeline,pipeline-timeout,10964," be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ;",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11073,Deployability,pipeline,pipeline-timeout,11073,"ript in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so doe",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11424,Deployability,configurat,configuration,11424,"; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.providers.Papiv2.config {; genomics {; enable-fuse = true; }; }; ```. There is a list of limitations regarding the usage ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11999,Deployability,pipeline,pipelines,11999,"end.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.providers.Papiv2.config {; genomics {; enable-fuse = true; }; }; ```. There is a list of limitations regarding the usage of FUSE filesystems:. + Any inputs brought in via a FUSE filesystem will not be considered for call caching.; + Any outputs stored via a FUSE filesystem will not be recreated if a task is replayed from a call-cache hit.; + If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens. #### Google Labels. Every call run on the Pipelines API backend is given certain labels by default, so that Google resources can be queried by these labels l",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:12301,Deployability,configurat,configuration,12301,"are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.providers.Papiv2.config {; genomics {; enable-fuse = true; }; }; ```. There is a list of limitations regarding the usage of FUSE filesystems:. + Any inputs brought in via a FUSE filesystem will not be considered for call caching.; + Any outputs stored via a FUSE filesystem will not be recreated if a task is replayed from a call-cache hit.; + If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens. #### Google Labels. Every call run on the Pipelines API backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |-----|-------|---------|-------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cro",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:14510,Deployability,configurat,configuration,14510,"ask is called in a subworkflow. |; | wdl-task-name | The name of the WDL task | my-task | |; | wdl-call-alias | The alias of the WDL call that created this job | my-task-1 | Only present if the task was called with an alias. |. Any custom labels provided as '`google_labels`' in the [workflow options](../wf_options/Google) are also applied to Google resources by the Pipelines API. ## Using NCBI Sequence Read Archive (SRA) Data. The v2alpha1 and v2beta backends support accessing [NCBI; SRA](https://www.ncbi.nlm.nih.gov/sra) accessions natively. To configure this; support you'll need to enable it in your config file like so:. ```hocon; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. This filesystem has two required configuration options:; * `docker-image`: The [fusera](https://github.com/mitre/fusera) docker image to; use to provide access. This can be a custom image, but using the public; [fusera/fusera:alpine](https://hub.docker.com/r/fusera/fusera/) image is; recommended.; * `ngc`: A base-64 encoded NGC file. This is provided through the NCBI; interface. Please see [the; documentation](https://www.ncbi.nlm.nih.gov/books/NBK63512/#Download.are_downloaded_files_encrypted); for more information on obtaining your NGC. The `ngc` value provided above; is the sample credential file. ### Virtual Private Network. Cromwell can arrange for jobs to run in specific GCP private networks via the `config.virtual-private-cloud` stanza of a PAPI v2 backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project wi",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:15705,Deployability,pipeline,pipelines,15705,"(https://hub.docker.com/r/fusera/fusera/) image is; recommended.; * `ngc`: A base-64 encoded NGC file. This is provided through the NCBI; interface. Please see [the; documentation](https://www.ncbi.nlm.nih.gov/books/NBK63512/#Download.are_downloaded_files_encrypted); for more information on obtaining your NGC. The `ngc` value provided above; is the sample credential file. ### Virtual Private Network. Cromwell can arrange for jobs to run in specific GCP private networks via the `config.virtual-private-cloud` stanza of a PAPI v2 backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running the Pipelines API. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:16194,Deployability,configurat,configuration,16194,"e `config.virtual-private-cloud` stanza of a PAPI v2 backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running the Pipelines API. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to the Pipelines API. See the documentation for the; [Cloud Life Sciences API](https://cloud.google.com/life-sciences/docs/reference/rest/v2beta/projects.locations.pipelines/run#Network); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:16952,Deployability,pipeline,pipelines,16952,"; network respectively. The `subnetwork-name` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running the Pipelines API. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to the Pipelines API. See the documentation for the; [Cloud Life Sciences API](https://cloud.google.com/life-sciences/docs/reference/rest/v2beta/projects.locations.pipelines/run#Network); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""reference-to-auth-scheme""; }; ...; }; }; }; }; ```. The `network-label-key` and `subnetwork-label-key` should reference the keys in your project's labels whose value is the name of your private network; and subnetwork within that network respectively. `auth` should reference an auth scheme in the `google` stanza which will be used to get the project metadata from Google Cloud.; The `subnetwork-label-key` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get l",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:17197,Deployability,pipeline,pipelines,17197,"ation key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running the Pipelines API. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to the Pipelines API. See the documentation for the; [Cloud Life Sciences API](https://cloud.google.com/life-sciences/docs/reference/rest/v2beta/projects.locations.pipelines/run#Network); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""reference-to-auth-scheme""; }; ...; }; }; }; }; ```. The `network-label-key` and `subnetwork-label-key` should reference the keys in your project's labels whose value is the name of your private network; and subnetwork within that network respectively. `auth` should reference an auth scheme in the `google` stanza which will be used to get the project metadata from Google Cloud.; The `subnetwork-label-key` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:19133,Deployability,configurat,configuration,19133,"e value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container; Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads. Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `genomics.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:19372,Deployability,pipeline,pipelines,19372,"n fall back to running on the default network. ### Custom Google Cloud SDK container; Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads. Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `genomics.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:19692,Deployability,configurat,configuration,19692,"o used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads. Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `genomics.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature i",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:19904,Deployability,deploy,deployment,19904,"be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `genomics.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod in",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20677,Deployability,install,installed,20677,"akes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20826,Deployability,install,installation,20826," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20955,Deployability,install,installing,20955,"files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Geno",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21394,Deployability,install,installed,21394,"te_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.ba",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:22207,Deployability,configurat,configuration,22207," is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-url` value should be changed from `https://genomics.googleapis.com/` to ; `https://lifesciences.googleapis.com/`.; 3. Also you should add a new mandatory parameter `genomics.location` to your backend configuration. Currently Google Cloud ; Life Sciences API is available only in `us-central1` and `europe-west2` locations. ### Alpha support for WDL optional outputs on PAPI v2. Cromwell 53 adds alpha-quality support for WDL optional outputs on PAPI v2 backends. Constructs such as: . ```; struct MyStruct {; String name; File? file; }; .; .; .; output {; File? file_does_not_exist = ""does_not_exist""; Pair[String, File?] pair_file_does_not_exist = (""this"", ""does_not_exist""); Map[String, Fil",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:22299,Deployability,pipeline,pipelines,22299,"g.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-url` value should be changed from `https://genomics.googleapis.com/` to ; `https://lifesciences.googleapis.com/`.; 3. Also you should add a new mandatory parameter `genomics.location` to your backend configuration. Currently Google Cloud ; Life Sciences API is available only in `us-central1` and `europe-west2` locations. ### Alpha support for WDL optional outputs on PAPI v2. Cromwell 53 adds alpha-quality support for WDL optional outputs on PAPI v2 backends. Constructs such as: . ```; struct MyStruct {; String name; File? file; }; .; .; .; output {; File? file_does_not_exist = ""does_not_exist""; Pair[String, File?] pair_file_does_not_exist = (""this"", ""does_not_exist""); Map[String, File?] map_file_does_not_exist = { ""does_not_exist"": ""does_not_exist"" }; Array[File?] array_file_does_not_exist = [""does_not_exist""]; MyStruct struc",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:22381,Deployability,pipeline,pipelines,22381,"crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-url` value should be changed from `https://genomics.googleapis.com/` to ; `https://lifesciences.googleapis.com/`.; 3. Also you should add a new mandatory parameter `genomics.location` to your backend configuration. Currently Google Cloud ; Life Sciences API is available only in `us-central1` and `europe-west2` locations. ### Alpha support for WDL optional outputs on PAPI v2. Cromwell 53 adds alpha-quality support for WDL optional outputs on PAPI v2 backends. Constructs such as: . ```; struct MyStruct {; String name; File? file; }; .; .; .; output {; File? file_does_not_exist = ""does_not_exist""; Pair[String, File?] pair_file_does_not_exist = (""this"", ""does_not_exist""); Map[String, File?] map_file_does_not_exist = { ""does_not_exist"": ""does_not_exist"" }; Array[File?] array_file_does_not_exist = [""does_not_exist""]; MyStruct struct_file_does_not_exist = object { name: ""this"", file: ""does_not_exist"" } ; }; ```. ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:22667,Deployability,configurat,configuration,22667," call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-url` value should be changed from `https://genomics.googleapis.com/` to ; `https://lifesciences.googleapis.com/`.; 3. Also you should add a new mandatory parameter `genomics.location` to your backend configuration. Currently Google Cloud ; Life Sciences API is available only in `us-central1` and `europe-west2` locations. ### Alpha support for WDL optional outputs on PAPI v2. Cromwell 53 adds alpha-quality support for WDL optional outputs on PAPI v2 backends. Constructs such as: . ```; struct MyStruct {; String name; File? file; }; .; .; .; output {; File? file_does_not_exist = ""does_not_exist""; Pair[String, File?] pair_file_does_not_exist = (""this"", ""does_not_exist""); Map[String, File?] map_file_does_not_exist = { ""does_not_exist"": ""does_not_exist"" }; Array[File?] array_file_does_not_exist = [""does_not_exist""]; MyStruct struct_file_does_not_exist = object { name: ""this"", file: ""does_not_exist"" } ; }; ```. will not produce errors if the file `does_not_exist` does not exist. This support for optional files is considered alpha; quality for two reasons:. 1. As seen in the example above, support for optional files extends to complex WDL types but there is a restriction that; all `",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:24047,Deployability,release,release,24047,"String name; File? file; }; .; .; .; output {; File? file_does_not_exist = ""does_not_exist""; Pair[String, File?] pair_file_does_not_exist = (""this"", ""does_not_exist""); Map[String, File?] map_file_does_not_exist = { ""does_not_exist"": ""does_not_exist"" }; Array[File?] array_file_does_not_exist = [""does_not_exist""]; MyStruct struct_file_does_not_exist = object { name: ""this"", file: ""does_not_exist"" } ; }; ```. will not produce errors if the file `does_not_exist` does not exist. This support for optional files is considered alpha; quality for two reasons:. 1. As seen in the example above, support for optional files extends to complex WDL types but there is a restriction that; all `File` components of non-primitive types must be optional. e.g. Cromwell would not allow the assignment of a ; missing file to the right side of a pair of type `Pair[File, File?]` since the left member of the pair is a non-optional; file. This restriction exists solely due to technical limitations in how type evaluation works in Cromwell today and; may be removed in a future Cromwell release. 2. Call caching does not work for calls with empty optional outputs. Cromwell currently does not recognize; that it is okay for optional output files to be missing, will incorrectly claim that any cache hits with missing ; optional output files are unusable, and will proceed to search for more cache hits which if found will also be unusable,; before eventually giving up and running the job. This behavior may be corrected in a future Cromwell release. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on PAPI v2. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a PAPI v2 backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manif",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:24502,Deployability,release,release,24502,"l files is considered alpha; quality for two reasons:. 1. As seen in the example above, support for optional files extends to complex WDL types but there is a restriction that; all `File` components of non-primitive types must be optional. e.g. Cromwell would not allow the assignment of a ; missing file to the right side of a pair of type `Pair[File, File?]` since the left member of the pair is a non-optional; file. This restriction exists solely due to technical limitations in how type evaluation works in Cromwell today and; may be removed in a future Cromwell release. 2. Call caching does not work for calls with empty optional outputs. Cromwell currently does not recognize; that it is okay for optional output files to be missing, will incorrectly claim that any cache hits with missing ; optional output files are unusable, and will proceed to search for more cache hits which if found will also be unusable,; before eventually giving up and running the job. This behavior may be corrected in a future Cromwell release. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on PAPI v2. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a PAPI v2 backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" :",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:24713,Deployability,configurat,configuration,24713,"f a ; missing file to the right side of a pair of type `Pair[File, File?]` since the left member of the pair is a non-optional; file. This restriction exists solely due to technical limitations in how type evaluation works in Cromwell today and; may be removed in a future Cromwell release. 2. Call caching does not work for calls with empty optional outputs. Cromwell currently does not recognize; that it is okay for optional output files to be missing, will incorrectly claim that any cache hits with missing ; optional output files are unusable, and will proceed to search for more cache hits which if found will also be unusable,; before eventually giving up and running the job. This behavior may be corrected in a future Cromwell release. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on PAPI v2. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a PAPI v2 backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" : 407769621; }, {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.sa"",; ""crc32c"" : 1902048083; },; ...; },; ...; ]; ...; }; }; }; }; ```. Reference disk usage is an opt-in feature, so workflow submiss",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:25080,Deployability,pipeline,pipelines,25080," for calls with empty optional outputs. Cromwell currently does not recognize; that it is okay for optional output files to be missing, will incorrectly claim that any cache hits with missing ; optional output files are unusable, and will proceed to search for more cache hits which if found will also be unusable,; before eventually giving up and running the job. This behavior may be corrected in a future Cromwell release. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on PAPI v2. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a PAPI v2 backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" : 407769621; }, {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.sa"",; ""crc32c"" : 1902048083; },; ...; },; ...; ]; ...; }; }; }; }; ```. Reference disk usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_reference_disks"": true,; ...; }; ```. Using the first file in the manifest above as an example, assume a PAPI v2 backend is configured to use this manifest and the appropriate; `use_reference_disks` workflow option is set to `true` in the workflow submissi",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:27484,Deployability,pipeline,pipelines,27484," Cromwell git repository includes a Java-based tool to facilitate the creation of manifests called; [CromwellRefdiskManifestCreatorApp](https://github.com/broadinstitute/cromwell/tree/develop/CromwellRefdiskManifestCreator).; Please see the help command of that tool for more details. Alternatively for public data stored under `gs://gcp-public-data--broad-references` there exists a shell script to; extract reference data to a new disk and then convert that disk to a public image. For more information see; [create_images.sh](https://github.com/broadinstitute/cromwell/tree/develop/scripts/reference_disks/create_images.sh). ### Docker Image Cache Support. To optimize job execution time, Cromwell 55 and later support the use of Docker image caches on the PAPI v2 lifesciences beta backend. Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can turn off Docker image caching through the use of a runtime attribute:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: false; }; }; ```. If Cromwell is running a workflow on PAPI v2 beta with Docker image caching enabled and a task specifies a; Docker im",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9628,Energy Efficiency,monitor,monitor,9628,"_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9881,Energy Efficiency,monitor,monitoring,9881,"r private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9960,Energy Efficiency,monitor,monitoring,9960,"ame/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipelin",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11503,Energy Efficiency,monitor,monitoring,11503,"ed locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.providers.Papiv2.config {; genomics {; enable-fuse = true; }; }; ```. There is a list of limitations regarding the usage of FUSE filesystems:. + Any inputs brought in via a FUSE filesystem will",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:1101,Integrability,protocol,protocols,1101,"Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defi",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11325,Integrability,message,messages,11325,"e Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.provi",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:14848,Integrability,interface,interface,14848,"_options/Google) are also applied to Google resources by the Pipelines API. ## Using NCBI Sequence Read Archive (SRA) Data. The v2alpha1 and v2beta backends support accessing [NCBI; SRA](https://www.ncbi.nlm.nih.gov/sra) accessions natively. To configure this; support you'll need to enable it in your config file like so:. ```hocon; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. This filesystem has two required configuration options:; * `docker-image`: The [fusera](https://github.com/mitre/fusera) docker image to; use to provide access. This can be a custom image, but using the public; [fusera/fusera:alpine](https://hub.docker.com/r/fusera/fusera/) image is; recommended.; * `ngc`: A base-64 encoded NGC file. This is provided through the NCBI; interface. Please see [the; documentation](https://www.ncbi.nlm.nih.gov/books/NBK63512/#Download.are_downloaded_files_encrypted); for more information on obtaining your NGC. The `ngc` value provided above; is the sample credential file. ### Virtual Private Network. Cromwell can arrange for jobs to run in specific GCP private networks via the `config.virtual-private-cloud` stanza of a PAPI v2 backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetw",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20195,Integrability,message,message,20195," parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data co",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21325,Integrability,message,message,21325,"te_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.ba",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:220,Modifiability,config,configuration,220,"**Google Cloud Backend**. Google Genomics Pipelines API is a Docker-as-a-service from Google. It was formerly called JES (Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For exa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:279,Modifiability,config,configuration,279,"**Google Cloud Backend**. Google Genomics Pipelines API is a Docker-as-a-service from Google. It was formerly called JES (Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For exa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:859,Modifiability,config,configuration,859,"**Google Cloud Backend**. Google Genomics Pipelines API is a Docker-as-a-service from Google. It was formerly called JES (Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For exa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:1974,Modifiability,config,configuration,1974,"; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service acco",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:2075,Modifiability,config,configuration,2075,"identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:2638,Modifiability,config,config,2638,"scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:3921,Modifiability,config,configuration,3921," in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `JES.config.genomics.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Pipelines API and not GCS, it's important that this service account, and the service account specified in `JES.config.genomics.auth` can both read/write the location specified by `JES.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_se",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:4262,Modifiability,config,configuration,4262,"""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `JES.config.genomics.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Pipelines API and not GCS, it's important that this service account, and the service account specified in `JES.config.genomics.auth` can both read/write the location specified by `JES.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service accou",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:4288,Modifiability,config,config,4288,"-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `JES.config.genomics.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Pipelines API and not GCS, it's important that this service account, and the service account specified in `JES.config.genomics.auth` can both read/write the location specified by `JES.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:4665,Modifiability,config,config,4665,"1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `JES.config.genomics.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Pipelines API and not GCS, it's important that this service account, and the service account specified in `JES.config.genomics.auth` can both read/write the location specified by `JES.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images t",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:4738,Modifiability,config,config,4738,"etadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `JES.config.genomics.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Pipelines API and not GCS, it's important that this service account, and the service account specified in `JES.config.genomics.auth` can both read/write the location specified by `JES.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytas",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:5912,Modifiability,config,configuration,5912,"on` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; bac",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6017,Modifiability,config,configuration,6017,"ng this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6224,Modifiability,config,config,6224," the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6603,Modifiability,config,config,6603,"ave access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7041,Modifiability,config,config,7041,"l.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `aut",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7751,Modifiability,config,config,7751,"e/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_s",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7972,Modifiability,config,configuration,7972,"ult = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; """,MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:8135,Modifiability,config,config,8135,"ncrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:8237,Modifiability,config,configuration,8237,"tion""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are mi",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:8552,Modifiability,config,configuration,8552,"t and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:8884,Modifiability,config,configuration,8884,"oken` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"":",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9212,Modifiability,config,configuration,9212,"n, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens an",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9585,Modifiability,config,configuration,9585,"s not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root`",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10690,Modifiability,config,config,10690,"Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10710,Modifiability,config,configuration,10710,"flow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE c",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11063,Modifiability,config,config,11063,"ript in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so doe",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11424,Modifiability,config,configuration,11424,"; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.providers.Papiv2.config {; genomics {; enable-fuse = true; }; }; ```. There is a list of limitations regarding the usage ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11590,Modifiability,config,config,11590," the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.providers.Papiv2.config {; genomics {; enable-fuse = true; }; }; ```. There is a list of limitations regarding the usage of FUSE filesystems:. + Any inputs brought in via a FUSE filesystem will not be considered for call caching.; + Any outputs stored via a FUSE filesystem will not be recreated if a task is repla",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:12301,Modifiability,config,configuration,12301,"are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.providers.Papiv2.config {; genomics {; enable-fuse = true; }; }; ```. There is a list of limitations regarding the usage of FUSE filesystems:. + Any inputs brought in via a FUSE filesystem will not be considered for call caching.; + Any outputs stored via a FUSE filesystem will not be recreated if a task is replayed from a call-cache hit.; + If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens. #### Google Labels. Every call run on the Pipelines API backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |-----|-------|---------|-------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cro",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:12346,Modifiability,config,config,12346,"lable zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.providers.Papiv2.config {; genomics {; enable-fuse = true; }; }; ```. There is a list of limitations regarding the usage of FUSE filesystems:. + Any inputs brought in via a FUSE filesystem will not be considered for call caching.; + Any outputs stored via a FUSE filesystem will not be recreated if a task is replayed from a call-cache hit.; + If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens. #### Google Labels. Every call run on the Pipelines API backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |-----|-------|---------|-------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:14080,Modifiability,config,configure,14080,"alue | Example | Notes |; |-----|-------|---------|-------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | cromwell-sub-workflow-name | The name of this job's sub-workflow | my-sub-workflow | Only present if the task is called in a subworkflow. |; | wdl-task-name | The name of the WDL task | my-task | |; | wdl-call-alias | The alias of the WDL call that created this job | my-task-1 | Only present if the task was called with an alias. |. Any custom labels provided as '`google_labels`' in the [workflow options](../wf_options/Google) are also applied to Google resources by the Pipelines API. ## Using NCBI Sequence Read Archive (SRA) Data. The v2alpha1 and v2beta backends support accessing [NCBI; SRA](https://www.ncbi.nlm.nih.gov/sra) accessions natively. To configure this; support you'll need to enable it in your config file like so:. ```hocon; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. This filesystem has two required configuration options:; * `docker-image`: The [fusera](https://github.com/mitre/fusera) docker image to; use to provide access. This can be a custom image, but using the public; [fusera/fusera:alpine](https://hub.docker.com/r/fusera/fusera/) image is; recommended.; * `ngc`: A base-64 encoded NGC file. This is provided through the NCBI; interface. Please see [the; documentation](https://www.ncbi.nlm.nih.gov/books/NBK63512/#Download.are_downloaded_files_encrypted); for more information on obtaining your NGC. The `ngc` value provided above; is the sample credential file. ### Virtual Private Network. Cro",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:14137,Modifiability,config,config,14137,"alue | Example | Notes |; |-----|-------|---------|-------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | cromwell-sub-workflow-name | The name of this job's sub-workflow | my-sub-workflow | Only present if the task is called in a subworkflow. |; | wdl-task-name | The name of the WDL task | my-task | |; | wdl-call-alias | The alias of the WDL call that created this job | my-task-1 | Only present if the task was called with an alias. |. Any custom labels provided as '`google_labels`' in the [workflow options](../wf_options/Google) are also applied to Google resources by the Pipelines API. ## Using NCBI Sequence Read Archive (SRA) Data. The v2alpha1 and v2beta backends support accessing [NCBI; SRA](https://www.ncbi.nlm.nih.gov/sra) accessions natively. To configure this; support you'll need to enable it in your config file like so:. ```hocon; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. This filesystem has two required configuration options:; * `docker-image`: The [fusera](https://github.com/mitre/fusera) docker image to; use to provide access. This can be a custom image, but using the public; [fusera/fusera:alpine](https://hub.docker.com/r/fusera/fusera/) image is; recommended.; * `ngc`: A base-64 encoded NGC file. This is provided through the NCBI; interface. Please see [the; documentation](https://www.ncbi.nlm.nih.gov/books/NBK63512/#Download.are_downloaded_files_encrypted); for more information on obtaining your NGC. The `ngc` value provided above; is the sample credential file. ### Virtual Private Network. Cro",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:14510,Modifiability,config,configuration,14510,"ask is called in a subworkflow. |; | wdl-task-name | The name of the WDL task | my-task | |; | wdl-call-alias | The alias of the WDL call that created this job | my-task-1 | Only present if the task was called with an alias. |. Any custom labels provided as '`google_labels`' in the [workflow options](../wf_options/Google) are also applied to Google resources by the Pipelines API. ## Using NCBI Sequence Read Archive (SRA) Data. The v2alpha1 and v2beta backends support accessing [NCBI; SRA](https://www.ncbi.nlm.nih.gov/sra) accessions natively. To configure this; support you'll need to enable it in your config file like so:. ```hocon; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. This filesystem has two required configuration options:; * `docker-image`: The [fusera](https://github.com/mitre/fusera) docker image to; use to provide access. This can be a custom image, but using the public; [fusera/fusera:alpine](https://hub.docker.com/r/fusera/fusera/) image is; recommended.; * `ngc`: A base-64 encoded NGC file. This is provided through the NCBI; interface. Please see [the; documentation](https://www.ncbi.nlm.nih.gov/books/NBK63512/#Download.are_downloaded_files_encrypted); for more information on obtaining your NGC. The `ngc` value provided above; is the sample credential file. ### Virtual Private Network. Cromwell can arrange for jobs to run in specific GCP private networks via the `config.virtual-private-cloud` stanza of a PAPI v2 backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project wi",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:15193,Modifiability,config,config,15193,":. ```hocon; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. This filesystem has two required configuration options:; * `docker-image`: The [fusera](https://github.com/mitre/fusera) docker image to; use to provide access. This can be a custom image, but using the public; [fusera/fusera:alpine](https://hub.docker.com/r/fusera/fusera/) image is; recommended.; * `ngc`: A base-64 encoded NGC file. This is provided through the NCBI; interface. Please see [the; documentation](https://www.ncbi.nlm.nih.gov/books/NBK63512/#Download.are_downloaded_files_encrypted); for more information on obtaining your NGC. The `ngc` value provided above; is the sample credential file. ### Virtual Private Network. Cromwell can arrange for jobs to run in specific GCP private networks via the `config.virtual-private-cloud` stanza of a PAPI v2 backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, then",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:15758,Modifiability,config,config,15758,"sera/) image is; recommended.; * `ngc`: A base-64 encoded NGC file. This is provided through the NCBI; interface. Please see [the; documentation](https://www.ncbi.nlm.nih.gov/books/NBK63512/#Download.are_downloaded_files_encrypted); for more information on obtaining your NGC. The `ngc` value provided above; is the sample credential file. ### Virtual Private Network. Cromwell can arrange for jobs to run in specific GCP private networks via the `config.virtual-private-cloud` stanza of a PAPI v2 backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running the Pipelines API. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will th",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:16067,Modifiability,config,config,16067,"e; is the sample credential file. ### Virtual Private Network. Cromwell can arrange for jobs to run in specific GCP private networks via the `config.virtual-private-cloud` stanza of a PAPI v2 backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running the Pipelines API. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to the Pipelines API. See the documentation for the; [Cloud Life Sciences API](https://cloud.google.com/life-sciences/docs/reference/rest/v2beta/projects.locations.pipelines/run#Network); for more information on the various formats accepted for `network` and `subn",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:16120,Modifiability,config,config,16120,"e `config.virtual-private-cloud` stanza of a PAPI v2 backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running the Pipelines API. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to the Pipelines API. See the documentation for the; [Cloud Life Sciences API](https://cloud.google.com/life-sciences/docs/reference/rest/v2beta/projects.locations.pipelines/run#Network); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:16194,Modifiability,config,configuration,16194,"e `config.virtual-private-cloud` stanza of a PAPI v2 backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running the Pipelines API. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to the Pipelines API. See the documentation for the; [Cloud Life Sciences API](https://cloud.google.com/life-sciences/docs/reference/rest/v2beta/projects.locations.pipelines/run#Network); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:16350,Modifiability,config,config,16350,"twork-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-name = ""vpc-network""; subnetwork-name = ""vpc-subnetwork""; }; ...; }; }; }; }; ```. The `network-name` and `subnetwork-name` should reference the name of your private network and subnetwork within that; network respectively. The `subnetwork-name` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, then Cromwell will use the value of the; configuration key, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running the Pipelines API. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to the Pipelines API. See the documentation for the; [Cloud Life Sciences API](https://cloud.google.com/life-sciences/docs/reference/rest/v2beta/projects.locations.pipelines/run#Network); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:17250,Modifiability,config,config,17250,"ere, as the name of private network and run the jobs on this network.; If the network name is not present in the config Cromwell will fall back to trying to run jobs on the default network. If the `network-name` or `subnetwork-name` values contain the string `${projectId}` then that value will be replaced; by Cromwell with the name of the project running the Pipelines API. If the `network-name` does not contain a `/` then it will be prefixed with `projects/${projectId}/global/networks/`. Cromwell will then pass the network and subnetwork values to the Pipelines API. See the documentation for the; [Cloud Life Sciences API](https://cloud.google.com/life-sciences/docs/reference/rest/v2beta/projects.locations.pipelines/run#Network); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""reference-to-auth-scheme""; }; ...; }; }; }; }; ```. The `network-label-key` and `subnetwork-label-key` should reference the keys in your project's labels whose value is the name of your private network; and subnetwork within that network respectively. `auth` should reference an auth scheme in the `google` stanza which will be used to get the project metadata from Google Cloud.; The `subnetwork-label-key` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If th",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:17811,Modifiability,config,config,17811,"he Pipelines API. See the documentation for the; [Cloud Life Sciences API](https://cloud.google.com/life-sciences/docs/reference/rest/v2beta/projects.locations.pipelines/run#Network); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""reference-to-auth-scheme""; }; ...; }; }; }; }; ```. The `network-label-key` and `subnetwork-label-key` should reference the keys in your project's labels whose value is the name of your private network; and subnetwork within that network respectively. `auth` should reference an auth scheme in the `google` stanza which will be used to get the project metadata from Google Cloud.; The `subnetwork-label-key` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container; Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parall",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:17864,Modifiability,config,config,17864,"ud.google.com/life-sciences/docs/reference/rest/v2beta/projects.locations.pipelines/run#Network); for more information on the various formats accepted for `network` and `subnetwork`. #### Virtual Private Network via Labels. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; virtual-private-cloud {; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; auth = ""reference-to-auth-scheme""; }; ...; }; }; }; }; ```. The `network-label-key` and `subnetwork-label-key` should reference the keys in your project's labels whose value is the name of your private network; and subnetwork within that network respectively. `auth` should reference an auth scheme in the `google` stanza which will be used to get the project metadata from Google Cloud.; The `subnetwork-label-key` is an optional config. For example, if your `virtual-private-cloud` config looks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container; Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads. Cromwell can be configured to use GCS parallel composite upload",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:18831,Modifiability,config,configured,18831,"oks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container; Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads. Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `genomics.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel com",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:19133,Modifiability,config,configuration,19133,"e value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container; Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads. Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `genomics.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:19425,Modifiability,config,config,19425,"lt network. ### Custom Google Cloud SDK container; Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads. Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `genomics.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/com",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:19692,Modifiability,config,configuration,19692,"o used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads. Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `genomics.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature i",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:19943,Modifiability,config,config,19943,"be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `genomics.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod in",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21137,Modifiability,config,config,21137,"arallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:22207,Modifiability,config,configuration,22207," is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-url` value should be changed from `https://genomics.googleapis.com/` to ; `https://lifesciences.googleapis.com/`.; 3. Also you should add a new mandatory parameter `genomics.location` to your backend configuration. Currently Google Cloud ; Life Sciences API is available only in `us-central1` and `europe-west2` locations. ### Alpha support for WDL optional outputs on PAPI v2. Cromwell 53 adds alpha-quality support for WDL optional outputs on PAPI v2 backends. Constructs such as: . ```; struct MyStruct {; String name; File? file; }; .; .; .; output {; File? file_does_not_exist = ""does_not_exist""; Pair[String, File?] pair_file_does_not_exist = (""this"", ""does_not_exist""); Map[String, Fil",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:22667,Modifiability,config,configuration,22667," call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-url` value should be changed from `https://genomics.googleapis.com/` to ; `https://lifesciences.googleapis.com/`.; 3. Also you should add a new mandatory parameter `genomics.location` to your backend configuration. Currently Google Cloud ; Life Sciences API is available only in `us-central1` and `europe-west2` locations. ### Alpha support for WDL optional outputs on PAPI v2. Cromwell 53 adds alpha-quality support for WDL optional outputs on PAPI v2 backends. Constructs such as: . ```; struct MyStruct {; String name; File? file; }; .; .; .; output {; File? file_does_not_exist = ""does_not_exist""; Pair[String, File?] pair_file_does_not_exist = (""this"", ""does_not_exist""); Map[String, File?] map_file_does_not_exist = { ""does_not_exist"": ""does_not_exist"" }; Array[File?] array_file_does_not_exist = [""does_not_exist""]; MyStruct struct_file_does_not_exist = object { name: ""this"", file: ""does_not_exist"" } ; }; ```. will not produce errors if the file `does_not_exist` does not exist. This support for optional files is considered alpha; quality for two reasons:. 1. As seen in the example above, support for optional files extends to complex WDL types but there is a restriction that; all `",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:23594,Modifiability,extend,extends,23594,"ter `genomics.location` to your backend configuration. Currently Google Cloud ; Life Sciences API is available only in `us-central1` and `europe-west2` locations. ### Alpha support for WDL optional outputs on PAPI v2. Cromwell 53 adds alpha-quality support for WDL optional outputs on PAPI v2 backends. Constructs such as: . ```; struct MyStruct {; String name; File? file; }; .; .; .; output {; File? file_does_not_exist = ""does_not_exist""; Pair[String, File?] pair_file_does_not_exist = (""this"", ""does_not_exist""); Map[String, File?] map_file_does_not_exist = { ""does_not_exist"": ""does_not_exist"" }; Array[File?] array_file_does_not_exist = [""does_not_exist""]; MyStruct struct_file_does_not_exist = object { name: ""this"", file: ""does_not_exist"" } ; }; ```. will not produce errors if the file `does_not_exist` does not exist. This support for optional files is considered alpha; quality for two reasons:. 1. As seen in the example above, support for optional files extends to complex WDL types but there is a restriction that; all `File` components of non-primitive types must be optional. e.g. Cromwell would not allow the assignment of a ; missing file to the right side of a pair of type `Pair[File, File?]` since the left member of the pair is a non-optional; file. This restriction exists solely due to technical limitations in how type evaluation works in Cromwell today and; may be removed in a future Cromwell release. 2. Call caching does not work for calls with empty optional outputs. Cromwell currently does not recognize; that it is okay for optional output files to be missing, will incorrectly claim that any cache hits with missing ; optional output files are unusable, and will proceed to search for more cache hits which if found will also be unusable,; before eventually giving up and running the job. This behavior may be corrected in a future Cromwell release. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:24713,Modifiability,config,configuration,24713,"f a ; missing file to the right side of a pair of type `Pair[File, File?]` since the left member of the pair is a non-optional; file. This restriction exists solely due to technical limitations in how type evaluation works in Cromwell today and; may be removed in a future Cromwell release. 2. Call caching does not work for calls with empty optional outputs. Cromwell currently does not recognize; that it is okay for optional output files to be missing, will incorrectly claim that any cache hits with missing ; optional output files are unusable, and will proceed to search for more cache hits which if found will also be unusable,; before eventually giving up and running the job. This behavior may be corrected in a future Cromwell release. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on PAPI v2. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a PAPI v2 backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" : 407769621; }, {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.sa"",; ""crc32c"" : 1902048083; },; ...; },; ...; ]; ...; }; }; }; }; ```. Reference disk usage is an opt-in feature, so workflow submiss",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:24850,Modifiability,config,config,24850,"ction exists solely due to technical limitations in how type evaluation works in Cromwell today and; may be removed in a future Cromwell release. 2. Call caching does not work for calls with empty optional outputs. Cromwell currently does not recognize; that it is okay for optional output files to be missing, will incorrectly claim that any cache hits with missing ; optional output files are unusable, and will proceed to search for more cache hits which if found will also be unusable,; before eventually giving up and running the job. This behavior may be corrected in a future Cromwell release. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on PAPI v2. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a PAPI v2 backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" : 407769621; }, {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.sa"",; ""crc32c"" : 1902048083; },; ...; },; ...; ]; ...; }; }; }; }; ```. Reference disk usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_reference_disks"": true,; ...; }; ```. Using the first file in the manifest above a",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:25133,Modifiability,config,config,25133,"uts. Cromwell currently does not recognize; that it is okay for optional output files to be missing, will incorrectly claim that any cache hits with missing ; optional output files are unusable, and will proceed to search for more cache hits which if found will also be unusable,; before eventually giving up and running the job. This behavior may be corrected in a future Cromwell release. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on PAPI v2. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a PAPI v2 backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" : 407769621; }, {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.sa"",; ""crc32c"" : 1902048083; },; ...; },; ...; ]; ...; }; }; }; }; ```. Reference disk usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_reference_disks"": true,; ...; }; ```. Using the first file in the manifest above as an example, assume a PAPI v2 backend is configured to use this manifest and the appropriate; `use_reference_disks` workflow option is set to `true` in the workflow submission. If a call in that workflow ; spe",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:25953,Modifiability,config,configured,25953,"ests: . ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-image"",; ""diskSizeGb"" : 500,; ""files"" : [ {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.nhr"",; ""crc32c"" : 407769621; }, {; ""path"" : ""gcp-public-data--broad-references/Homo_sapiens_assembly19_1000genomes_decoy/Homo_sapiens_assembly19_1000genomes_decoy.fasta.sa"",; ""crc32c"" : 1902048083; },; ...; },; ...; ]; ...; }; }; }; }; ```. Reference disk usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_reference_disks"": true,; ...; }; ```. Using the first file in the manifest above as an example, assume a PAPI v2 backend is configured to use this manifest and the appropriate; `use_reference_disks` workflow option is set to `true` in the workflow submission. If a call in that workflow ; specifies the input `gs://my-references/enormous_reference.bam` and because that input matches the path of a file on the; reference image without the leading `gs://`, Cromwell would; arrange for a reference disk based on this image to be mounted and for the call's input to refer to the ; copy of the file on the reference disk, bypassing localization of the input. . The Cromwell git repository includes a Java-based tool to facilitate the creation of manifests called; [CromwellRefdiskManifestCreatorApp](https://github.com/broadinstitute/cromwell/tree/develop/CromwellRefdiskManifestCreator).; Please see the help command of that tool for more details. Alternatively for public data stored under `gs://gcp-public-data--broad-references` there exists a shell script to; extract reference data to a new disk and then convert that disk to a public image. For ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:27537,Modifiability,config,config,27537," Java-based tool to facilitate the creation of manifests called; [CromwellRefdiskManifestCreatorApp](https://github.com/broadinstitute/cromwell/tree/develop/CromwellRefdiskManifestCreator).; Please see the help command of that tool for more details. Alternatively for public data stored under `gs://gcp-public-data--broad-references` there exists a shell script to; extract reference data to a new disk and then convert that disk to a public image. For more information see; [create_images.sh](https://github.com/broadinstitute/cromwell/tree/develop/scripts/reference_disks/create_images.sh). ### Docker Image Cache Support. To optimize job execution time, Cromwell 55 and later support the use of Docker image caches on the PAPI v2 lifesciences beta backend. Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can turn off Docker image caching through the use of a runtime attribute:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: false; }; }; ```. If Cromwell is running a workflow on PAPI v2 beta with Docker image caching enabled and a task specifies a; Docker image which corresponds to a configure",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:28516,Modifiability,config,configured,28516," Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can turn off Docker image caching through the use of a runtime attribute:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: false; }; }; ```. If Cromwell is running a workflow on PAPI v2 beta with Docker image caching enabled and a task specifies a; Docker image which corresponds to a configured Docker image cache JSON, Cromwell will arrange for the; job's VM to mount a disk built from the corresponding disk image. In the event that multiple; manifests describe disk images containing the specified Docker image, Cromwell will choose the disk image with the; smallest `diskSizeGb` value. Conversely, Docker image caching can be turned off at the workflow level (either turned off explicitly or left at the; default setting of `false`) but turned on at the individual task level:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: true; }; }; ```. These settings could be useful for cost reasons: mounting Docker image caches adds nonzero cost; which might not be offset by eliminating Docker image pull times for long-running jobs.; ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9298,Performance,perform,perform,9298,"n, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens an",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11233,Performance,queue,queueing,11233,"e log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow optio",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11310,Performance,queue,queueing,11310,"e Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.provi",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:12659,Performance,cache,cache,12659,"mpts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.providers.Papiv2.config {; genomics {; enable-fuse = true; }; }; ```. There is a list of limitations regarding the usage of FUSE filesystems:. + Any inputs brought in via a FUSE filesystem will not be considered for call caching.; + Any outputs stored via a FUSE filesystem will not be recreated if a task is replayed from a call-cache hit.; + If the filesystem is writable, your job is potentially no longer idempotent - Cromwell may decide to retry your job for you, and you might get unforeseen file collisions or even incorrect results if that happens. #### Google Labels. Every call run on the Pipelines API backend is given certain labels by default, so that Google resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |-----|-------|---------|-------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | cromwell-sub-workflow-name | The name of this job's sub-workflow | my-sub-workflow | Only present if the task is called in a subworkflow. |; | wdl-task-name | The name of the WDL task | my-tas",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:18921,Performance,perform,performance,18921,"oks like the one above, and one of the labels in your project is. ```; ""my-private-network"" = ""vpc-network""; ```. Cromwell will get labels from the project's metadata and look for a label whose key is `my-private-network`.; Then it will use the value of the label, which is `vpc-network` here, as the name of private network and run the jobs on this network.; If the network key is not present in the project's metadata Cromwell will fall back to trying to run jobs using literal; network labels, and then fall back to running on the default network. ### Custom Google Cloud SDK container; Cromwell can't use Google's container registry if VPC Perimeter is used in project.; Own repository can be used by adding `cloud-sdk-image-url` reference to used container:. ```; google {; ...; cloud-sdk-image-url = ""eu.gcr.io/your-project-id/cloudsdktool/cloud-sdk:354.0.0-alpine""; cloud-sdk-image-size-gb = 1; }; ```. ### Parallel Composite Uploads. Cromwell can be configured to use GCS parallel composite uploads which can greatly improve delocalization performance. This feature; is turned off by default but can be enabled backend-wide by specifying a `gsutil`-compatible memory specification for the key; `genomics.parallel-composite-upload-threshold` in backend configuration. This memory value represents the minimum size an output file; must have to be a candidate for `gsutil` parallel composite uploading:. ```; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; genomics {; ...; parallel-composite-upload-threshold = 150M; ...; }; ...; }; }; }; }; ```. Alternatively this threshold can be specified in workflow options using the key `parallel-composite-upload-threshold`,; which takes precedence over a setting in configuration. The default setting for this threshold is `0` which turns off; parallel composite uploads; a value of `0` can also be used in workflow options to turn off parallel com",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20914,Performance,throttle,throttle,20914," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20932,Performance,perform,performance,20932," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21047,Performance,perform,performance,21047,"arallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21759,Performance,cache,cache,21759," ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-url` value should be changed from `https://genomics.googleapis.com/` to ; `https://lifesciences.googleapis.com/`.; 3. Also you should add a new mandatory parameter `genomics.location` to your backend configuration. Currently",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21847,Performance,cache,cached,21847,"so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-url` value should be changed from `https://genomics.googleapis.com/` to ; `https://lifesciences.googleapis.com/`.; 3. Also you should add a new mandatory parameter `genomics.location` to your backend configuration. Currently Google Cloud ; Life Sciences API is available only in `us-central1` and `europe-west2` locations. ### Alpha support for WDL optional outputs on PAPI v2. Cromwell 53 adds alpha-quality su",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:24253,Performance,cache,cache,24253,"""]; MyStruct struct_file_does_not_exist = object { name: ""this"", file: ""does_not_exist"" } ; }; ```. will not produce errors if the file `does_not_exist` does not exist. This support for optional files is considered alpha; quality for two reasons:. 1. As seen in the example above, support for optional files extends to complex WDL types but there is a restriction that; all `File` components of non-primitive types must be optional. e.g. Cromwell would not allow the assignment of a ; missing file to the right side of a pair of type `Pair[File, File?]` since the left member of the pair is a non-optional; file. This restriction exists solely due to technical limitations in how type evaluation works in Cromwell today and; may be removed in a future Cromwell release. 2. Call caching does not work for calls with empty optional outputs. Cromwell currently does not recognize; that it is okay for optional output files to be missing, will incorrectly claim that any cache hits with missing ; optional output files are unusable, and will proceed to search for more cache hits which if found will also be unusable,; before eventually giving up and running the job. This behavior may be corrected in a future Cromwell release. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on PAPI v2. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a PAPI v2 backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-ima",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:24351,Performance,cache,cache,24351,"""]; MyStruct struct_file_does_not_exist = object { name: ""this"", file: ""does_not_exist"" } ; }; ```. will not produce errors if the file `does_not_exist` does not exist. This support for optional files is considered alpha; quality for two reasons:. 1. As seen in the example above, support for optional files extends to complex WDL types but there is a restriction that; all `File` components of non-primitive types must be optional. e.g. Cromwell would not allow the assignment of a ; missing file to the right side of a pair of type `Pair[File, File?]` since the left member of the pair is a non-optional; file. This restriction exists solely due to technical limitations in how type evaluation works in Cromwell today and; may be removed in a future Cromwell release. 2. Call caching does not work for calls with empty optional outputs. Cromwell currently does not recognize; that it is okay for optional output files to be missing, will incorrectly claim that any cache hits with missing ; optional output files are unusable, and will proceed to search for more cache hits which if found will also be unusable,; before eventually giving up and running the job. This behavior may be corrected in a future Cromwell release. ### Reference Disk Support. Cromwell 55 and later support mounting reference disks from prebuilt GCP disk images as an alternative to localizing large; input reference files on PAPI v2. Please note the configuration of reference disk manifests has changed starting with; Cromwell 57 and now uses the format documented below. . Within the `config` stanza of a PAPI v2 backend the `reference-disk-localization-manifests`; key specifies an array of reference disk manifests: . ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/broad-dsde-cromwell-dev/global/images/broad-references-disk-ima",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:27152,Performance,optimiz,optimize,27152,"e path of a file on the; reference image without the leading `gs://`, Cromwell would; arrange for a reference disk based on this image to be mounted and for the call's input to refer to the ; copy of the file on the reference disk, bypassing localization of the input. . The Cromwell git repository includes a Java-based tool to facilitate the creation of manifests called; [CromwellRefdiskManifestCreatorApp](https://github.com/broadinstitute/cromwell/tree/develop/CromwellRefdiskManifestCreator).; Please see the help command of that tool for more details. Alternatively for public data stored under `gs://gcp-public-data--broad-references` there exists a shell script to; extract reference data to a new disk and then convert that disk to a public image. For more information see; [create_images.sh](https://github.com/broadinstitute/cromwell/tree/develop/scripts/reference_disks/create_images.sh). ### Docker Image Cache Support. To optimize job execution time, Cromwell 55 and later support the use of Docker image caches on the PAPI v2 lifesciences beta backend. Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:27235,Performance,cache,caches,27235,"e path of a file on the; reference image without the leading `gs://`, Cromwell would; arrange for a reference disk based on this image to be mounted and for the call's input to refer to the ; copy of the file on the reference disk, bypassing localization of the input. . The Cromwell git repository includes a Java-based tool to facilitate the creation of manifests called; [CromwellRefdiskManifestCreatorApp](https://github.com/broadinstitute/cromwell/tree/develop/CromwellRefdiskManifestCreator).; Please see the help command of that tool for more details. Alternatively for public data stored under `gs://gcp-public-data--broad-references` there exists a shell script to; extract reference data to a new disk and then convert that disk to a public image. For more information see; [create_images.sh](https://github.com/broadinstitute/cromwell/tree/develop/scripts/reference_disks/create_images.sh). ### Docker Image Cache Support. To optimize job execution time, Cromwell 55 and later support the use of Docker image caches on the PAPI v2 lifesciences beta backend. Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:27297,Performance,cache,caches,27297,"nce disk based on this image to be mounted and for the call's input to refer to the ; copy of the file on the reference disk, bypassing localization of the input. . The Cromwell git repository includes a Java-based tool to facilitate the creation of manifests called; [CromwellRefdiskManifestCreatorApp](https://github.com/broadinstitute/cromwell/tree/develop/CromwellRefdiskManifestCreator).; Please see the help command of that tool for more details. Alternatively for public data stored under `gs://gcp-public-data--broad-references` there exists a shell script to; extract reference data to a new disk and then convert that disk to a public image. For more information see; [create_images.sh](https://github.com/broadinstitute/cromwell/tree/develop/scripts/reference_disks/create_images.sh). ### Docker Image Cache Support. To optimize job execution time, Cromwell 55 and later support the use of Docker image caches on the PAPI v2 lifesciences beta backend. Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can turn off Docker image caching through the use of a runtime attribute:. ```wdl; task my_task {; ...; runti",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:27565,Performance,cache,cache-manifest-file,27565,"CromwellRefdiskManifestCreatorApp](https://github.com/broadinstitute/cromwell/tree/develop/CromwellRefdiskManifestCreator).; Please see the help command of that tool for more details. Alternatively for public data stored under `gs://gcp-public-data--broad-references` there exists a shell script to; extract reference data to a new disk and then convert that disk to a public image. For more information see; [create_images.sh](https://github.com/broadinstitute/cromwell/tree/develop/scripts/reference_disks/create_images.sh). ### Docker Image Cache Support. To optimize job execution time, Cromwell 55 and later support the use of Docker image caches on the PAPI v2 lifesciences beta backend. Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can turn off Docker image caching through the use of a runtime attribute:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: false; }; }; ```. If Cromwell is running a workflow on PAPI v2 beta with Docker image caching enabled and a task specifies a; Docker image which corresponds to a configured Docker image cache JSON, Cromwell will arrange for the; job's VM",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:27616,Performance,cache,cache,27616,"CromwellRefdiskManifestCreatorApp](https://github.com/broadinstitute/cromwell/tree/develop/CromwellRefdiskManifestCreator).; Please see the help command of that tool for more details. Alternatively for public data stored under `gs://gcp-public-data--broad-references` there exists a shell script to; extract reference data to a new disk and then convert that disk to a public image. For more information see; [create_images.sh](https://github.com/broadinstitute/cromwell/tree/develop/scripts/reference_disks/create_images.sh). ### Docker Image Cache Support. To optimize job execution time, Cromwell 55 and later support the use of Docker image caches on the PAPI v2 lifesciences beta backend. Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can turn off Docker image caching through the use of a runtime attribute:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: false; }; }; ```. If Cromwell is running a workflow on PAPI v2 beta with Docker image caching enabled and a task specifies a; Docker image which corresponds to a configured Docker image cache JSON, Cromwell will arrange for the; job's VM",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:27673,Performance,cache,cache,27673,"ellRefdiskManifestCreator).; Please see the help command of that tool for more details. Alternatively for public data stored under `gs://gcp-public-data--broad-references` there exists a shell script to; extract reference data to a new disk and then convert that disk to a public image. For more information see; [create_images.sh](https://github.com/broadinstitute/cromwell/tree/develop/scripts/reference_disks/create_images.sh). ### Docker Image Cache Support. To optimize job execution time, Cromwell 55 and later support the use of Docker image caches on the PAPI v2 lifesciences beta backend. Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can turn off Docker image caching through the use of a runtime attribute:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: false; }; }; ```. If Cromwell is running a workflow on PAPI v2 beta with Docker image caching enabled and a task specifies a; Docker image which corresponds to a configured Docker image cache JSON, Cromwell will arrange for the; job's VM to mount a disk built from the corresponding disk image. In the event that multiple; manifests ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:28021,Performance,cache,cache,28021,"/tree/develop/scripts/reference_disks/create_images.sh). ### Docker Image Cache Support. To optimize job execution time, Cromwell 55 and later support the use of Docker image caches on the PAPI v2 lifesciences beta backend. Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can turn off Docker image caching through the use of a runtime attribute:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: false; }; }; ```. If Cromwell is running a workflow on PAPI v2 beta with Docker image caching enabled and a task specifies a; Docker image which corresponds to a configured Docker image cache JSON, Cromwell will arrange for the; job's VM to mount a disk built from the corresponding disk image. In the event that multiple; manifests describe disk images containing the specified Docker image, Cromwell will choose the disk image with the; smallest `diskSizeGb` value. Conversely, Docker image caching can be turned off at the workflow level (either turned off explicitly or left at the; default setting of `false`) but turned on at the individual task level:. ```wdl; task my_task {; ...; runtime {; ...; us",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:28540,Performance,cache,cache,28540," Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can turn off Docker image caching through the use of a runtime attribute:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: false; }; }; ```. If Cromwell is running a workflow on PAPI v2 beta with Docker image caching enabled and a task specifies a; Docker image which corresponds to a configured Docker image cache JSON, Cromwell will arrange for the; job's VM to mount a disk built from the corresponding disk image. In the event that multiple; manifests describe disk images containing the specified Docker image, Cromwell will choose the disk image with the; smallest `diskSizeGb` value. Conversely, Docker image caching can be turned off at the workflow level (either turned off explicitly or left at the; default setting of `false`) but turned on at the individual task level:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: true; }; }; ```. These settings could be useful for cost reasons: mounting Docker image caches adds nonzero cost; which might not be offset by eliminating Docker image pull times for long-running jobs.; ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:29168,Performance,cache,caches,29168," Docker image caches are not available on the PAPI v2 genomics alpha backend.; Configuration looks like:. ```hocon; backend {; ...; providers {; ...; PapiV2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; ...; docker-image-cache-manifest-file = ""gs://path/to/a/docker/image/cache/manifest.json""; ...; }; }; }; }; ```. Docker image cache manifest JSONs have a format like:. ```json; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. Docker image cache usage is an opt-in feature, so workflow submissions must specify this workflow option:. ```json; {; ...; ""use_docker_image_cache"": true,; ...; }; ```. Individual tasks within a workflow can turn off Docker image caching through the use of a runtime attribute:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: false; }; }; ```. If Cromwell is running a workflow on PAPI v2 beta with Docker image caching enabled and a task specifies a; Docker image which corresponds to a configured Docker image cache JSON, Cromwell will arrange for the; job's VM to mount a disk built from the corresponding disk image. In the event that multiple; manifests describe disk images containing the specified Docker image, Cromwell will choose the disk image with the; smallest `diskSizeGb` value. Conversely, Docker image caching can be turned off at the workflow level (either turned off explicitly or left at the; default setting of `false`) but turned on at the individual task level:. ```wdl; task my_task {; ...; runtime {; ...; useDockerImageCache: true; }; }; ```. These settings could be useful for cost reasons: mounting Docker image caches adds nonzero cost; which might not be offset by eliminating Docker image pull times for long-running jobs.; ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10857,Safety,timeout,timeout,10857,"_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10899,Safety,timeout,timeout,10899,". The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens beca",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10948,Safety,abort,abort,10948,". The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens beca",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10973,Safety,timeout,timeout,10973," be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ;",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11082,Safety,timeout,timeout,11082,"ript in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so doe",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:11458,Safety,detect,detect,11458,"; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to quota exhaustion; by queueing jobs internally. However, there are cases when jobs fail instead of queueing, with messages such; as ""PAPI error code 9"", ""no available zones"", and/or ""quota too low"". The following configuration permits Cromwell to detect and retry these failures. Proactively monitoring; and raising quota is still recommended. ```hocon; backend.providers.PAPIv2.config {; # Counts attempts (total jobs) not just retries after to the first; quota-attempts: 20; }; ```. **Enabling FUSE capabilities**. *This is a community contribution and not officially supported by the Cromwell team.*; By default Cromwell task containers doesn't allow to mount any FUSE filesystems. It happens because containers are launched without specific linux capabilities being enabled. ; Google pipelines backend supports running containers with the enabled capabilities and so does Cromwell. . If you need to use fuses within task containers then you can set `enable_fuse` workflow option. . ```; {; ""enable_fuse"": true; }; ```. Differently you can enable support for fuses right in your backend configuration. ```; backend.providers.Papiv2.config {; genomics {; enable-fuse = true; }; }; ```. There is a list of limitations regarding the usage ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:366,Security,authenticat,authentication,366,"**Google Cloud Backend**. Google Genomics Pipelines API is a Docker-as-a-service from Google. It was formerly called JES (Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For exa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:545,Security,authenticat,authentication,545,"**Google Cloud Backend**. Google Genomics Pipelines API is a Docker-as-a-service from Google. It was formerly called JES (Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For exa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:621,Security,authenticat,authentication,621,"**Google Cloud Backend**. Google Genomics Pipelines API is a Docker-as-a-service from Google. It was formerly called JES (Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For exa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:893,Security,authenticat,authenticate,893,"**Google Cloud Backend**. Google Genomics Pipelines API is a Docker-as-a-service from Google. It was formerly called JES (Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For exa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:943,Security,authenticat,authentication,943,"**Google Cloud Backend**. Google Genomics Pipelines API is a Docker-as-a-service from Google. It was formerly called JES (Job Execution Service);; you may see outdated references to the older JES terminology in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For exa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:1243,Security,authenticat,authenticate,1243,"y in Cromwell configuration files and code. This section offers detailed configuration instructions for using Cromwell with the Pipelines API in all supported; authentication modes. Before reading futher in this section please see the; [Getting started on Google Pipelines API](../tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions wit",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:1488,Security,authenticat,authentication,1488,"tutorials/PipelinesApi101) for instructions common to all authentication modes; and detailed instructions for the application default authentication scheme in particular.; The instructions below assume you have created a Google Cloud Storage bucket and a Google project enabled for the appropriate APIs. **Configuring Authentication**. The `google` stanza in the Cromwell configuration file defines how to authenticate to Google. There are four different; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authentic",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:1896,Security,authenticat,authentication,1896,"; authentication schemes that might be used:. * `application_default` (default, recommended) - Use [application default](https://developers.google.com/identity/protocols/application-default-credentials) credentials.; * `service_account` - Use a specific service account and key file (in PEM format) to authenticate.; * `user_account` - Authenticate as a user.; * `user_service_account` - Authenticate each individual workflow using service account credentials supplied in the workflow options. The `auths` block in the `google` stanza defines the authentication schemes within a Cromwell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service acco",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:2479,Security,authenticat,authenticate,2479,"ell deployment:. ```hocon; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:4147,Security,authenticat,authentication,4147,"""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https://www.googleapis.com/oauth2/v1/certs"",; ""client_x509_cert_url"": ""https://www.googleapis.com/robot/v1/metadata/x509/my-account%40my-project.iam.gserviceaccount.com""; }; ```. Most importantly, the value of the `client_email` field should go into the `service-account-id` field in the configuration (see below). The; `private_key` portion needs to be pulled into its own file (e.g. `my-key.pem`). The `\n`s in the string need to be converted to newline characters. While technically not part of Service Account authentication mode, one can also override the default service account that the compute VM is started with via the configuration option `JES.config.genomics.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Pipelines API and not GCS, it's important that this service account, and the service account specified in `JES.config.genomics.auth` can both read/write the location specified by `JES.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service accou",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:5211,Security,access,access,5211,"compute VM is started with via the configuration option `JES.config.genomics.compute-service-account` or through the workflow options parameter `google_compute_service_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Pipelines API and not GCS, it's important that this service account, and the service account specified in `JES.config.genomics.auth` can both read/write the location specified by `JES.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; con",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:5368,Security,access,access,5368,"ervice_account`. The service account you provide must have been granted Service Account Actor role to Cromwell's primary service account. As this only affects Google Pipelines API and not GCS, it's important that this service account, and the service account specified in `JES.config.genomics.auth` can both read/write the location specified by `JES.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the a",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:5718,Security,access,access,5718,"genomics.auth` can both read/write the location specified by `JES.config.root`. **User Service Account**. A [JSON key file for the service account](../wf_options/Google.md) must be passed in via the `user_service_account_json` field in the [Workflow Options](../wf_options/Google.md) when submitting the job. Omitting this field will cause the workflow to fail. The JSON should be passed as a string and will need to have no newlines and all instances of `""` and `\n` escaped. . In the likely event that this service account does not have access to Cromwell's default google project the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:pa",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6291,Security,password,password,6291," the `google_project` workflow option must be set. In the similarly likely case that this service account can not access Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6370,Security,password,password,6370,"ccess Cromwell's default google bucket, the `jes_gcs_root` workflow option should be set appropriately. For information on the interaction of `user_service_account_json` with private Docker images please see the `Docker` section below. . **Docker**. It's possible to reference private Docker images to which only particular Docker Hub accounts have access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for e",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6670,Security,password,password,6670,"ave access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6722,Security,encrypt,encrypting,6722,"ave access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:6829,Security,encrypt,encryption,6829,"ave access:. ```; task mytask {; command {; ...; }; runtime {; docker: ""private_repo/image""; memory: ""8 GB""; cpu: ""1""; }; ...; }; ```. In order for a private image to be used the appropriate Docker configuration must be provided. If the Docker images being used; are public there is no need to add this configuration. For Pipelines API (PAPI) version 1:; ```; backend {; default = ""PAPIv1""; providers {; PAPIv1 {; actor-factory = ""cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7108,Security,password,password,7108,"l.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `aut",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7160,Security,encrypt,encrypting,7160,"l.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `aut",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7267,Security,encrypt,encryption,7267,"l.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; }; }; }; }; }; ```. `token` is the standard base64-encoded username:password for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `aut",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7369,Security,encrypt,encrypting,7369,"rd for the appropriate Docker Hub account. For PAPI version 2 alpha 1:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Acco",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7614,Security,access,access,7614,"elines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7693,Security,authoriz,authorization,7693,"e/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_s",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7783,Security,authoriz,authorization,7783,"e-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7801,Security,encrypt,encrypting,7801,"e-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:8969,Security,encrypt,encrypting,8969,"`docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9064,Security,password,password,9064,"`docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9550,Security,encrypt,encrypted-fields,9550,"s not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root`",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:14000,Security,access,accessing,14000,"ogle resources can be queried by these labels later. ; The current default label set automatically applied is:. | Key | Value | Example | Notes |; |-----|-------|---------|-------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | cromwell-sub-workflow-name | The name of this job's sub-workflow | my-sub-workflow | Only present if the task is called in a subworkflow. |; | wdl-task-name | The name of the WDL task | my-task | |; | wdl-call-alias | The alias of the WDL call that created this job | my-task-1 | Only present if the task was called with an alias. |. Any custom labels provided as '`google_labels`' in the [workflow options](../wf_options/Google) are also applied to Google resources by the Pipelines API. ## Using NCBI Sequence Read Archive (SRA) Data. The v2alpha1 and v2beta backends support accessing [NCBI; SRA](https://www.ncbi.nlm.nih.gov/sra) accessions natively. To configure this; support you'll need to enable it in your config file like so:. ```hocon; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. This filesystem has two required configuration options:; * `docker-image`: The [fusera](https://github.com/mitre/fusera) docker image to; use to provide access. This can be a custom image, but using the public; [fusera/fusera:alpine](https://hub.docker.com/r/fusera/fusera/) image is; recommended.; * `ngc`: A base-64 encoded NGC file. This is provided through the NCBI; interface. Please see [the; documentation](https://www.ncbi.nlm.nih.gov/books/NBK63512/#Download.are_downloaded_files_encrypted); for more informati",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:14056,Security,access,accessions,14056,"t default label set automatically applied is:. | Key | Value | Example | Notes |; |-----|-------|---------|-------|; | cromwell-workflow-id | The Cromwell ID given to the root workflow (i.e. the ID returned by Cromwell on submission) | cromwell-d4b412c5-bf3d-4169-91b0-1b635ce47a26 | To fit the required [format](#label-format), we prefix with 'cromwell-' |; | cromwell-sub-workflow-name | The name of this job's sub-workflow | my-sub-workflow | Only present if the task is called in a subworkflow. |; | wdl-task-name | The name of the WDL task | my-task | |; | wdl-call-alias | The alias of the WDL call that created this job | my-task-1 | Only present if the task was called with an alias. |. Any custom labels provided as '`google_labels`' in the [workflow options](../wf_options/Google) are also applied to Google resources by the Pipelines API. ## Using NCBI Sequence Read Archive (SRA) Data. The v2alpha1 and v2beta backends support accessing [NCBI; SRA](https://www.ncbi.nlm.nih.gov/sra) accessions natively. To configure this; support you'll need to enable it in your config file like so:. ```hocon; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. This filesystem has two required configuration options:; * `docker-image`: The [fusera](https://github.com/mitre/fusera) docker image to; use to provide access. This can be a custom image, but using the public; [fusera/fusera:alpine](https://hub.docker.com/r/fusera/fusera/) image is; recommended.; * `ngc`: A base-64 encoded NGC file. This is provided through the NCBI; interface. Please see [the; documentation](https://www.ncbi.nlm.nih.gov/books/NBK63512/#Download.are_downloaded_files_encrypted); for more information on obtaining your NGC. The `ngc` value provided above; is the s",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:14630,Security,access,access,14630,"my-task | |; | wdl-call-alias | The alias of the WDL call that created this job | my-task-1 | Only present if the task was called with an alias. |. Any custom labels provided as '`google_labels`' in the [workflow options](../wf_options/Google) are also applied to Google resources by the Pipelines API. ## Using NCBI Sequence Read Archive (SRA) Data. The v2alpha1 and v2beta backends support accessing [NCBI; SRA](https://www.ncbi.nlm.nih.gov/sra) accessions natively. To configure this; support you'll need to enable it in your config file like so:. ```hocon; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""bmNiaV9nYXAfiwgAAAAAAAADBcHBDYQgEADAv1XQAGYXcfErUe5x0diCiFESA0Y8/VD8zTzrlXwMDEsoII9usPT5znZSmTqUohaSg5Gay14TbxsluMGOSBuqDEKefvbwCzv3BAAKoexb5uIbjjg7dq/p9mH7A5VTImxjAAAA""; }; }; ```. This filesystem has two required configuration options:; * `docker-image`: The [fusera](https://github.com/mitre/fusera) docker image to; use to provide access. This can be a custom image, but using the public; [fusera/fusera:alpine](https://hub.docker.com/r/fusera/fusera/) image is; recommended.; * `ngc`: A base-64 encoded NGC file. This is provided through the NCBI; interface. Please see [the; documentation](https://www.ncbi.nlm.nih.gov/books/NBK63512/#Download.are_downloaded_files_encrypted); for more information on obtaining your NGC. The `ngc` value provided above; is the sample credential file. ### Virtual Private Network. Cromwell can arrange for jobs to run in specific GCP private networks via the `config.virtual-private-cloud` stanza of a PAPI v2 backend.; There are two ways of specifying private networks:. * [Literal network and subnetwork values](#virtual-private-network-via-literals) that will apply to all projects; * [Google project labels](#virtual-private-network-via-labels) whose values in a particular Google project will specify the network and subnetwork. #### Virtual Private Network via Literals.",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20777,Security,integrity,integrity,20777," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:20885,Security,hash,hash,20885," in workflow options to turn off parallel composite uploads; in a Cromwell deployment where they are turned on in config. #### Issues with composite files. Please see the [Google documentation](https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21075,Security,integrity,integrity,21075,"arallel-composite-uploads); describing the benefits and drawbacks of parallel composite uploads. The actual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21204,Security,integrity,integrity,21204,"ual error message observed when attempting to download a composite file on a system without a compiled `crcmod`; looks like the following:. ```; / # gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp gs://my-bucket/composite.bam .; Copying gs://my-bucket/composite.bam...; ==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21432,Security,integrity,integrity,21432,"==> NOTE: You are downloading one or more large file(s), which would; run significantly faster if you enabled sliced object downloads. This; feature is enabled by default but requires that compiled crcmod be; installed (see ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-ur",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:21648,Security,hash,hash,21648," ""gsutil help crcmod""). CommandException:; Downloading this composite object requires integrity checking with CRC32c,; but your crcmod installation isn't using the module's C extension, so the; hash computation will likely throttle download performance. For help; installing the extension, please see ""gsutil help crcmod"". To download regardless of crcmod performance or to skip slow integrity; checks, see the ""check_hashes"" option in your boto config file. NOTE: It is strongly recommended that you not disable integrity checks. Doing so; could allow data corruption to go undetected during uploading/downloading.; / #; ```. As the message states, the best option would be to have a compiled `crcmod` installed on the system.; Turning off integrity checks on downloads does get around this issue but really isn't a great idea. #### Parallel composite uploads and call caching. Because the parallel composite upload threshold is not considered part of the hash used for call caching purposes, calls; which would be expected to generate non-composite outputs may call cache to results that did generate composite; outputs. Calls which are executed and not cached will always honor the parallel composite upload setting at the time of; their execution. ### Migration from Google Cloud Genomics v2alpha1 to Google Cloud Life Sciences v2beta. 1. If you currently run your workflows using Cloud Genomics v2alpha1 and would like to switch to Google Cloud Life ; Sciences v2beta, you will need to do a few changes to your configuration file: `actor-factory` value should be changed ; from `cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory` to `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory`.; 2. Parameter `genomics.endpoint-url` value should be changed from `https://genomics.googleapis.com/` to ; `https://lifesciences.googleapis.com/`.; 3. Also you should add a new mandatory parameter `genomics.location` to your backend configuration. Currently",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:2622,Testability,log,login,2622,"scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""my-service-account""; pem-file = ""/path/to/file.pem""; },; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```. These authentication schemes can be referenced by name within other portions of the configuration file. For example, both; the `genomics` and `filesystems.gcs` sections within a Google configuration block must reference an auth defined in this block.; The auth for the `genomics` section governs the interactions with Google itself, while `filesystems.gcs` governs the localization; of data into and out of GCE VMs. **Application Default Credentials**. By default, application default credentials will be used. Only `name` and `scheme` are required for application default credentials. To authenticate, run the following commands from your command line (requires [gcloud](https://cloud.google.com/sdk/gcloud/)):. ```; $ gcloud auth login; $ gcloud config set project my-project; ```. **Service Account**. First create a new service account through the [API Credentials](https://console.developers.google.com/apis/credentials) page. Go to **Create credentials -> Service account key**. Then in the **Service account** dropdown select **New service account**. Fill in a name (e.g. `my-account`), and select key type of JSON. Creating the account will cause the JSON file to be downloaded. The structure of this file is roughly like this (account name is `my-account`):. ```; {; ""type"": ""service_account"",; ""project_id"": ""my-project"",; ""private_key_id"": ""OMITTED"",; ""private_key"": ""-----BEGIN PRIVATE KEY-----\nBASE64 ENCODED KEY WITH \n TO REPRESENT NEWLINES\n-----END PRIVATE KEY-----\n"",; ""client_email"": ""my-account@my-project.iam.gserviceaccount.com"",; ""client_id"": ""22377410244549202395"",; ""auth_uri"": ""https://accounts.google.com/o/oauth2/auth"",; ""token_uri"": ""https://accounts.google.com/o/oauth2/token"",; ""auth_provider_x509_cert_url"": ""https",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:7597,Testability,log,login,7597,"elines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. For PAPI version 2 beta:. ```; backend {; default = ""PAPIv2""; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; dockerhub {; token = ""base64-encoded-docker-hub-username:password""; key-name = ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token""; auth = ""reference-to-the-auth-cromwell-should-use-for-kms-encryption""; }; }; }; }; }; ```. `key-name` is the name of the Google KMS key Cromwell should use for encrypting the Docker `token` before including it; in the PAPI job execution request. This `key-name` will also be included in the PAPI job execution; request and will be used by PAPI to decrypt the Docker token used by `docker login` to enable access to the private Docker image.; ; `auth` is a reference to the name of an authorization in the `auths` block of Cromwell's `google` config.; Cromwell will use this authorization for encrypting the Google KMS key. The equivalents of `key-name`, `token` and `auth` can also be specified in workflow options which take; precedence over values specified in configuration. The corresponding workflow options are named `docker_credentials_key_name`,; `docker_credentials_token`, and `user_service_account_json`. While the config value `auth` refers to an auth defined in the ; `google.auths` stanza elsewhere in Cromwell's; configuration, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker ",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9316,Testability,log,login,9316,"n, `user_service_account_json` is expected to be a literal escaped Google service account auth JSON.; See the `User Service Account` section above for more information on using user service accounts.; If the key, token or auth value is provided in workflow options then the corresponding private Docker configuration value; is not required, and vice versa. Also note that for the `user_service_account_json` workflow option to work an auth of type `user_service_account`; must be defined in Cromwell's `google.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens an",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9827,Testability,log,log,9827,"oogle.auths` stanza; more details in the `User Service Account` section above. Example PAPI v2 workflow options for private Docker configuration:. ```; {; ""docker_credentials_key_name"": ""name/of/the/kms/key/used/for/encrypting/and/decrypting/the/docker/hub/token"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URI",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:9971,Testability,log,log,9971,"oken"",; ""docker_credentials_token"": ""base64_username:password"",; ""user_service_account_json"": ""<properly escaped user service account JSON file>""; }; ```. Important. If any of the three private Docker configuration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a max",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md:10193,Testability,log,log,10193,"nfiguration values of key name, auth, or Docker token are missing, PAPI v2 will not perform a `docker login`.; If the Docker image to be pulled is not public the `docker pull` will fail which will cause the overall job to fail. If using any of these private Docker workflow options it is advisable to add; them to the `workflow-options.encrypted-fields` list in Cromwell configuration. **Monitoring**. In order to monitor metrics (CPU, Memory, Disk usage...) about the VM during Call Runtime, a workflow option can be used to specify the path to a script that will run in the background and write its output to a log file. ```; {; ""monitoring_script"": ""gs://cromwell/monitoring/script.sh""; }; ```. The output of this script will be written to a `monitoring.log` file that will be available in the call gcs bucket when the call completes. This feature is meant to run a script in the background during long-running processes. It's possible that if the task is very short that the log file does not flush before de-localization happens and you will end up with a zero byte file. **Google Cloud Storage Filesystem**. On the Google Pipelines backend the GCS (Google Cloud Storage) filesystem is used for the root of the workflow execution.; On the Local, SGE, and associated backends any GCS URI will be downloaded locally. For the Google backend the `jes_gcs_root` [Workflow Option](../wf_options/Google) will take; precedence over the `root` specified at `backend.providers.JES.config.root` in the configuration file. Google Cloud Storage URIs are the only acceptable values for `File` inputs for; workflows using the Google backend. **Pipeline timeout**. Google sets a default pipeline timeout of 7 days, after which the pipeline will abort. Setting `pipeline-timeout` overrides this limit to a maximum of 30 days. ```hocon; backend.providers.PAPIv2.config {; pipeline-timeout: 14 days; }; ```. #### Quota retry. Typically, Life Sciences API is designed to accept all jobs sent to it and respond to qu",MatchSource.DOCS,docs/backends/Google.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Google.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:4666,Availability,avail,available,4666,"CKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engi",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5924,Availability,alive,aliveness,5924,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5961,Availability,alive,alive,5961,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:6046,Availability,alive,alive,6046,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:6281,Availability,alive,alive,6281,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:6349,Availability,alive,alive,6349,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:6460,Availability,alive,alive,6460,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:341,Deployability,configurat,configurations,341,"Cromwell provides a generic way to configure a backend relying on most High Performance Computing (HPC) frameworks, and with access to a shared filesystem. The two main features that are needed for this backend to be used are a way to submit a job to the compute cluster and to get its status through the command line.; You can find example configurations for a variety of those backends here:. * [SGE](SGE); * [LSF](LSF); * [SLURM](SLURM); * [HTCondor](HTcondor). ## FileSystems. ### Shared FileSystem; HPC backends rely on being able to access and use a shared filesystem to store workflow results. Cromwell is configured with a root execution directory which is set in the configuration file under `backend.providers.<backend_name>.config.root`. This is called the `cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:676,Deployability,configurat,configuration,676,"Cromwell provides a generic way to configure a backend relying on most High Performance Computing (HPC) frameworks, and with access to a shared filesystem. The two main features that are needed for this backend to be used are a way to submit a job to the compute cluster and to get its status through the command line.; You can find example configurations for a variety of those backends here:. * [SGE](SGE); * [LSF](LSF); * [SLURM](SLURM); * [HTCondor](HTcondor). ## FileSystems. ### Shared FileSystem; HPC backends rely on being able to access and use a shared filesystem to store workflow results. Cromwell is configured with a root execution directory which is set in the configuration file under `backend.providers.<backend_name>.config.root`. This is called the `cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:4187,Deployability,configurat,configuration,4187,"t paths such that soft links resolve outside and; inside containers. ```hocon; backend {; default = ""SlurmDocker""; providers {; SlurmDocker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:4398,Deployability,configurat,configuration,4398,"ifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.;",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5491,Deployability,configurat,configuration,5491,"low-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-a",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:4011,Integrability,wrap,wrap,4011,"ing `cache-copy`, `soft-link` is disabled by default on docker and other container; environments, and hard-links do not work across different physical disks. However, it is possible to manually configure Cromwell to mount input paths such that soft links resolve outside and; inside containers. ```hocon; backend {; default = ""SlurmDocker""; providers {; SlurmDocker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:35,Modifiability,config,configure,35,"Cromwell provides a generic way to configure a backend relying on most High Performance Computing (HPC) frameworks, and with access to a shared filesystem. The two main features that are needed for this backend to be used are a way to submit a job to the compute cluster and to get its status through the command line.; You can find example configurations for a variety of those backends here:. * [SGE](SGE); * [LSF](LSF); * [SLURM](SLURM); * [HTCondor](HTcondor). ## FileSystems. ### Shared FileSystem; HPC backends rely on being able to access and use a shared filesystem to store workflow results. Cromwell is configured with a root execution directory which is set in the configuration file under `backend.providers.<backend_name>.config.root`. This is called the `cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:341,Modifiability,config,configurations,341,"Cromwell provides a generic way to configure a backend relying on most High Performance Computing (HPC) frameworks, and with access to a shared filesystem. The two main features that are needed for this backend to be used are a way to submit a job to the compute cluster and to get its status through the command line.; You can find example configurations for a variety of those backends here:. * [SGE](SGE); * [LSF](LSF); * [SLURM](SLURM); * [HTCondor](HTcondor). ## FileSystems. ### Shared FileSystem; HPC backends rely on being able to access and use a shared filesystem to store workflow results. Cromwell is configured with a root execution directory which is set in the configuration file under `backend.providers.<backend_name>.config.root`. This is called the `cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:613,Modifiability,config,configured,613,"Cromwell provides a generic way to configure a backend relying on most High Performance Computing (HPC) frameworks, and with access to a shared filesystem. The two main features that are needed for this backend to be used are a way to submit a job to the compute cluster and to get its status through the command line.; You can find example configurations for a variety of those backends here:. * [SGE](SGE); * [LSF](LSF); * [SLURM](SLURM); * [HTCondor](HTcondor). ## FileSystems. ### Shared FileSystem; HPC backends rely on being able to access and use a shared filesystem to store workflow results. Cromwell is configured with a root execution directory which is set in the configuration file under `backend.providers.<backend_name>.config.root`. This is called the `cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:676,Modifiability,config,configuration,676,"Cromwell provides a generic way to configure a backend relying on most High Performance Computing (HPC) frameworks, and with access to a shared filesystem. The two main features that are needed for this backend to be used are a way to submit a job to the compute cluster and to get its status through the command line.; You can find example configurations for a variety of those backends here:. * [SGE](SGE); * [LSF](LSF); * [SLURM](SLURM); * [HTCondor](HTcondor). ## FileSystems. ### Shared FileSystem; HPC backends rely on being able to access and use a shared filesystem to store workflow results. Cromwell is configured with a root execution directory which is set in the configuration file under `backend.providers.<backend_name>.config.root`. This is called the `cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:735,Modifiability,config,config,735,"Cromwell provides a generic way to configure a backend relying on most High Performance Computing (HPC) frameworks, and with access to a shared filesystem. The two main features that are needed for this backend to be used are a way to submit a job to the compute cluster and to get its status through the command line.; You can find example configurations for a variety of those backends here:. * [SGE](SGE); * [LSF](LSF); * [SLURM](SLURM); * [HTCondor](HTcondor). ## FileSystems. ### Shared FileSystem; HPC backends rely on being able to access and use a shared filesystem to store workflow results. Cromwell is configured with a root execution directory which is set in the configuration file under `backend.providers.<backend_name>.config.root`. This is called the `cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:2457,Modifiability,config,config,2457," a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different physical disks and soft-links don't work with docker by default. Copying uses a lot of; space if a multitude of tasks use the same input. `cached-copy` copies the file only once to the physical disk containing; the `<workflow_root>` and then uses hard links for every task that needs the input file. This can save a lot of space. The default order in `reference.conf` is `hard-link`, `soft-link`, `copy`. Shared filesystem localization is defined in the `config` section of each backend. The default stanza for the Local and HPC backends looks like this:. ```; filesystems {; local {; localization: [; 	 ""hard-link"", ""soft-link"", ""copy""; ]; }; }; ```. ### Optional docker soft links. By default when Cromwell runs a local container it only mounts the workflow's execution directory. Thus any symbolic or; soft links pointing to files outside of the execution directory will resolve to paths that are not accessible within the; container. As discussed above regarding `cache-copy`, `soft-link` is disabled by default on docker and other container; environments, and hard-links do not work across different physical disks. However, it is possible to manually configure Cromwell to mount input paths such that soft links resolve outside and; inside containers. ```hocon; backend {; default = ""SlurmDocker""; providers {; SlurmDocker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attribute",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:3159,Modifiability,config,configure,3159,"sical disk containing; the `<workflow_root>` and then uses hard links for every task that needs the input file. This can save a lot of space. The default order in `reference.conf` is `hard-link`, `soft-link`, `copy`. Shared filesystem localization is defined in the `config` section of each backend. The default stanza for the Local and HPC backends looks like this:. ```; filesystems {; local {; localization: [; 	 ""hard-link"", ""soft-link"", ""copy""; ]; }; }; ```. ### Optional docker soft links. By default when Cromwell runs a local container it only mounts the workflow's execution directory. Thus any symbolic or; soft links pointing to files outside of the execution directory will resolve to paths that are not accessible within the; container. As discussed above regarding `cache-copy`, `soft-link` is disabled by default on docker and other container; environments, and hard-links do not work across different physical disks. However, it is possible to manually configure Cromwell to mount input paths such that soft links resolve outside and; inside containers. ```hocon; backend {; default = ""SlurmDocker""; providers {; SlurmDocker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other conf",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:3377,Modifiability,config,config,3377,"ink`, `soft-link`, `copy`. Shared filesystem localization is defined in the `config` section of each backend. The default stanza for the Local and HPC backends looks like this:. ```; filesystems {; local {; localization: [; 	 ""hard-link"", ""soft-link"", ""copy""; ]; }; }; ```. ### Optional docker soft links. By default when Cromwell runs a local container it only mounts the workflow's execution directory. Thus any symbolic or; soft links pointing to files outside of the execution directory will resolve to paths that are not accessible within the; container. As discussed above regarding `cache-copy`, `soft-link` is disabled by default on docker and other container; environments, and hard-links do not work across different physical disks. However, it is possible to manually configure Cromwell to mount input paths such that soft links resolve outside and; inside containers. ```hocon; backend {; default = ""SlurmDocker""; providers {; SlurmDocker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important part",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:3421,Modifiability,config,config,3421,"s defined in the `config` section of each backend. The default stanza for the Local and HPC backends looks like this:. ```; filesystems {; local {; localization: [; 	 ""hard-link"", ""soft-link"", ""copy""; ]; }; }; ```. ### Optional docker soft links. By default when Cromwell runs a local container it only mounts the workflow's execution directory. Thus any symbolic or; soft links pointing to files outside of the execution directory will resolve to paths that are not accessible within the; container. As discussed above regarding `cache-copy`, `soft-link` is disabled by default on docker and other container; environments, and hard-links do not work across different physical disks. However, it is possible to manually configure Cromwell to mount input paths such that soft links resolve outside and; inside containers. ```hocon; backend {; default = ""SlurmDocker""; providers {; SlurmDocker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.files",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:4187,Modifiability,config,configuration,4187,"t paths such that soft links resolve outside and; inside containers. ```hocon; backend {; default = ""SlurmDocker""; providers {; SlurmDocker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:4398,Modifiability,config,configuration,4398,"ifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.;",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:4427,Modifiability,config,config,4427,"ifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.;",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:4495,Modifiability,config,config,4495,"https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanz",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5097,Modifiability,config,configured,5097,"ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC fil",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5371,Modifiability,config,configure,5371,"}; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `c",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5437,Modifiability,config,configured,5437,"low-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-a",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5491,Modifiability,config,configuration,5491,"low-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-a",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:6559,Modifiability,config,config,6559,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:6610,Modifiability,config,config,6610,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:1664,Performance,cache,cached-copy,1664,"s set in the configuration file under `backend.providers.<backend_name>.config.root`. This is called the `cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different physical disks and soft-links don't work with docker by default. Copying uses a lot of; space if a multitude of tasks use the same input. `cached-copy` copies the file only once to the physical disk containing; the `<workflow_root>` and then uses hard links for every task that needs the input file. This can save a lot of space. The default order in `reference.conf` is `hard-link`, `soft-link`, `copy`. Shared filesystem localization is defined in the `config` section of each backend. The default stanza for the Local and HPC backends looks like this:. ```; filesystems {; local {; localization: [; 	 ""hard-link"", ""soft-link"", ""copy""; ]; }; }; ```. ### Option",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:1730,Performance,cache,cache,1730,"`cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different physical disks and soft-links don't work with docker by default. Copying uses a lot of; space if a multitude of tasks use the same input. `cached-copy` copies the file only once to the physical disk containing; the `<workflow_root>` and then uses hard links for every task that needs the input file. This can save a lot of space. The default order in `reference.conf` is `hard-link`, `soft-link`, `copy`. Shared filesystem localization is defined in the `config` section of each backend. The default stanza for the Local and HPC backends looks like this:. ```; filesystems {; local {; localization: [; 	 ""hard-link"", ""soft-link"", ""copy""; ]; }; }; ```. ### Optional docker soft links. By default when Cromwell runs a local container it only mounts the workflow's exec",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:1758,Performance,cache,cached-inputs,1758,"`cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different physical disks and soft-links don't work with docker by default. Copying uses a lot of; space if a multitude of tasks use the same input. `cached-copy` copies the file only once to the physical disk containing; the `<workflow_root>` and then uses hard links for every task that needs the input file. This can save a lot of space. The default order in `reference.conf` is `hard-link`, `soft-link`, `copy`. Shared filesystem localization is defined in the `config` section of each backend. The default stanza for the Local and HPC backends looks like this:. ```; filesystems {; local {; localization: [; 	 ""hard-link"", ""soft-link"", ""copy""; ]; }; }; ```. ### Optional docker soft links. By default when Cromwell runs a local container it only mounts the workflow's exec",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:1839,Performance,cache,cached-copy,1839,"king directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different physical disks and soft-links don't work with docker by default. Copying uses a lot of; space if a multitude of tasks use the same input. `cached-copy` copies the file only once to the physical disk containing; the `<workflow_root>` and then uses hard links for every task that needs the input file. This can save a lot of space. The default order in `reference.conf` is `hard-link`, `soft-link`, `copy`. Shared filesystem localization is defined in the `config` section of each backend. The default stanza for the Local and HPC backends looks like this:. ```; filesystems {; local {; localization: [; 	 ""hard-link"", ""soft-link"", ""copy""; ]; }; }; ```. ### Optional docker soft links. By default when Cromwell runs a local container it only mounts the workflow's execution directory. Thus any symbolic or; soft links pointing to files outside of the execution directory will resolve to paths that ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:2141,Performance,cache,cached-copy,2141," is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different physical disks and soft-links don't work with docker by default. Copying uses a lot of; space if a multitude of tasks use the same input. `cached-copy` copies the file only once to the physical disk containing; the `<workflow_root>` and then uses hard links for every task that needs the input file. This can save a lot of space. The default order in `reference.conf` is `hard-link`, `soft-link`, `copy`. Shared filesystem localization is defined in the `config` section of each backend. The default stanza for the Local and HPC backends looks like this:. ```; filesystems {; local {; localization: [; 	 ""hard-link"", ""soft-link"", ""copy""; ]; }; }; ```. ### Optional docker soft links. By default when Cromwell runs a local container it only mounts the workflow's execution directory. Thus any symbolic or; soft links pointing to files outside of the execution directory will resolve to paths that are not accessible within the; container. As discussed above regarding `cache-copy`, `soft-link` is disabled by default on docker and other container; environments, and hard-links do not work across different physical disks. However, it is possible to manually configure Cromwell to mount input paths such that soft links ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:2970,Performance,cache,cache-copy,2970,"s don't work with docker by default. Copying uses a lot of; space if a multitude of tasks use the same input. `cached-copy` copies the file only once to the physical disk containing; the `<workflow_root>` and then uses hard links for every task that needs the input file. This can save a lot of space. The default order in `reference.conf` is `hard-link`, `soft-link`, `copy`. Shared filesystem localization is defined in the `config` section of each backend. The default stanza for the Local and HPC backends looks like this:. ```; filesystems {; local {; localization: [; 	 ""hard-link"", ""soft-link"", ""copy""; ]; }; }; ```. ### Optional docker soft links. By default when Cromwell runs a local container it only mounts the workflow's execution directory. Thus any symbolic or; soft links pointing to files outside of the execution directory will resolve to paths that are not accessible within the; container. As discussed above regarding `cache-copy`, `soft-link` is disabled by default on docker and other container; environments, and hard-links do not work across different physical disks. However, it is possible to manually configure Cromwell to mount input paths such that soft links resolve outside and; inside containers. ```hocon; backend {; default = ""SlurmDocker""; providers {; SlurmDocker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity ex",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:3588,Performance,cache,cache,3588,"y""; ]; }; }; ```. ### Optional docker soft links. By default when Cromwell runs a local container it only mounts the workflow's execution directory. Thus any symbolic or; soft links pointing to files outside of the execution directory will resolve to paths that are not accessible within the; container. As discussed above regarding `cache-copy`, `soft-link` is disabled by default on docker and other container; environments, and hard-links do not work across different physical disks. However, it is possible to manually configure Cromwell to mount input paths such that soft links resolve outside and; inside containers. ```hocon; backend {; default = ""SlurmDocker""; providers {; SlurmDocker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:6318,Performance,load,load,6318,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:3790,Safety,timeout,timeout,3790,"ing `cache-copy`, `soft-link` is disabled by default on docker and other container; environments, and hard-links do not work across different physical disks. However, it is possible to manually configure Cromwell to mount input paths such that soft links resolve outside and; inside containers. ```hocon; backend {; default = ""SlurmDocker""; providers {; SlurmDocker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $IMAGE docker://${docker}; fi; ) 9>$LOCK_FILE; sbatch \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} --bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro $IMAGE ${job_shell} ${docker_script}""; """"""; # ... other configuration ...; filesystems {; local {; caching.duplication-strategy = [""copy""]; localization = [""soft-link"", ""copy""]; docker.allow-soft-links: true; }; }; }; }; }; }; ```. The important parts of the example configuration above are:; * `config.filesystems.local.docker.allow-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5737,Safety,timeout,timeout,5737,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5869,Safety,timeout,timeout-seconds,5869,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5993,Safety,timeout,timeout-seconds,5993,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:6075,Safety,timeout,timeout-seconds,6075,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:6390,Safety,timeout,timeout-seconds,6390,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:6579,Safety,timeout,timeout-seconds,6579,"/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-alive` option after a restart of a cromwell server. ```; backend {; providers {; <backend name> {; config {; exit-code-timeout-seconds = 120; # other config options; }; }; }; }; ```; ",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:125,Security,access,access,125,"Cromwell provides a generic way to configure a backend relying on most High Performance Computing (HPC) frameworks, and with access to a shared filesystem. The two main features that are needed for this backend to be used are a way to submit a job to the compute cluster and to get its status through the command line.; You can find example configurations for a variety of those backends here:. * [SGE](SGE); * [LSF](LSF); * [SLURM](SLURM); * [HTCondor](HTcondor). ## FileSystems. ### Shared FileSystem; HPC backends rely on being able to access and use a shared filesystem to store workflow results. Cromwell is configured with a root execution directory which is set in the configuration file under `backend.providers.<backend_name>.config.root`. This is called the `cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:539,Security,access,access,539,"Cromwell provides a generic way to configure a backend relying on most High Performance Computing (HPC) frameworks, and with access to a shared filesystem. The two main features that are needed for this backend to be used are a way to submit a job to the compute cluster and to get its status through the command line.; You can find example configurations for a variety of those backends here:. * [SGE](SGE); * [LSF](LSF); * [SLURM](SLURM); * [HTCondor](HTcondor). ## FileSystems. ### Shared FileSystem; HPC backends rely on being able to access and use a shared filesystem to store workflow results. Cromwell is configured with a root execution directory which is set in the configuration file under `backend.providers.<backend_name>.config.root`. This is called the `cromwell_root` and it is set to `./cromwell-executions` by default. Relative paths are interpreted as relative to the current working directory of the Cromwell process. When Cromwell runs a workflow, it first creates a directory `<cromwell_root>/<workflow_uuid>`. This is called the `workflow_root` and it is the root directory for all activity in this workflow. Each `call` has its own subdirectory located at `<workflow_root>/call-<call_name>`. This is the `<call_dir>`.; Any input files to a call need to be localized into the `<call_dir>/inputs` directory. There are different localization strategies that Cromwell will try until one works:. * `hard-link` - This will create a hard link to the file; * `soft-link` - Create a symbolic link to the file. This strategy is not enabled by default for tasks which specify a ; Docker image and will be ignored.; * `copy` - Make a copy the file; * `cached-copy` An experimental feature. This copies files to a file cache in ; `<workflow_root>/cached-inputs` and then hard links them in the `<call_dir>/inputs` directory. . `cached-copy` is intended for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:2906,Security,access,accessible,2906,"ed for a shared filesystem that runs on multiple physical disks, where docker containers are used. ; Hard-links don't work between different physical disks and soft-links don't work with docker by default. Copying uses a lot of; space if a multitude of tasks use the same input. `cached-copy` copies the file only once to the physical disk containing; the `<workflow_root>` and then uses hard links for every task that needs the input file. This can save a lot of space. The default order in `reference.conf` is `hard-link`, `soft-link`, `copy`. Shared filesystem localization is defined in the `config` section of each backend. The default stanza for the Local and HPC backends looks like this:. ```; filesystems {; local {; localization: [; 	 ""hard-link"", ""soft-link"", ""copy""; ]; }; }; ```. ### Optional docker soft links. By default when Cromwell runs a local container it only mounts the workflow's execution directory. Thus any symbolic or; soft links pointing to files outside of the execution directory will resolve to paths that are not accessible within the; container. As discussed above regarding `cache-copy`, `soft-link` is disabled by default on docker and other container; environments, and hard-links do not work across different physical disks. However, it is possible to manually configure Cromwell to mount input paths such that soft links resolve outside and; inside containers. ```hocon; backend {; default = ""SlurmDocker""; providers {; SlurmDocker {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String? docker; """"""; # https://slurm.schedmd.com/sbatch.html; submit-docker = """"""; set -euo pipefail; CACHE_DIR=$HOME/.singularity/cache; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=$CACHE_DIR/$DOCKER_NAME.sif; (; flock --verbose --exclusive --timeout 900 9 || exit 1; if [ ! -e ""$IMAGE"" ]; then; singularity build $",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5422,Security,authenticat,authentication,5422,"low-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-a",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md:5457,Usability,simpl,simply,5457,"low-soft-links` set to `true`; * `config.submit-docker` containing `--bind /mnt/one:/mnt/one:ro --bind /mnt/two:/mnt/two:ro`. In this example the two directories `/mnt/one` and and `/mnt/two` will also be available within containers at their; original paths outside the container. So soft links pointing to paths under those directories will resolve during the; job execution. Note that if a user runs a workflow using an input file `/mnt/three/path/to/file` the job will fail; during execution as `/mnt/three` was not present inside the running container. ### Additional FileSystems. HPC backends (as well as the Local backend) can be configured to be able to interact with other type of filesystems, where the input files can be located for example.; Currently the only other filesystem supported is Google Cloud Storage (GCS). See the [Google section](Google) of the documentation for information on how to configure GCS in Cromwell.; Once you have a google authentication configured, you can simply add a `gcs` stanza in your configuration file to enable GCS:. ```; backend.providers.MyHPCBackend {; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; ```. ### Exit code timeout. If the cluster forcefully kills a job, it is unable to write its exit code anymore.; To address this the option `exit-code-timeout-seconds` can be used.; Cromwell will check the aliveness of the job with the `check-alive` script, every `exit-code-timeout-seconds` (polling).; When a job is no longer alive and another `exit-code-timeout-seconds` seconds have passed without an RC file being made, Cromwell can mark the job as failed.; If retries are enabled the job is submitted again.; This option will enable polling with the `check-alive` option, this could cause high load on whatever system `check-alive` calls. When the option `exit-code-timeout-seconds` is **not** set cromwell will only execute the `check-a",MatchSource.DOCS,docs/backends/HPC.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HPC.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:881,Availability,echo,echo,881,"**HTCondor Backend**. Allows to execute jobs using HTCondor which is a specialized workload management system for compute-intensive jobs created by the Center for High Throughput Computing in the Department of Computer Sciences at the University of Wisconsin-Madison (UW-Madison). The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; HtCondor {; config {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; *",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:1158,Availability,error,error,1158,"te-intensive jobs created by the Center for High Throughput Computing in the Department of Computer Sciences at the University of Wisconsin-Madison (UW-Madison). The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; HtCondor {; config {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This bac",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:1454,Availability,echo,echo,1454,"on; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:1574,Availability,failure,failure,1574,"on; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3085,Availability,error,error,3085,"t should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native S",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:517,Deployability,configurat,configuration,517,"**HTCondor Backend**. Allows to execute jobs using HTCondor which is a specialized workload management system for compute-intensive jobs created by the Center for High Throughput Computing in the Department of Computer Sciences at the University of Wisconsin-Madison (UW-Madison). The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; HtCondor {; config {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; *",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:1676,Deployability,configurat,configuration,1676,"nd the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - Th",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3398,Deployability,configurat,configuration,3398,". * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute n",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3588,Deployability,configurat,configurations,3588,"ocker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3794,Deployability,configurat,configuration,3794,"ocker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:4087,Deployability,configurat,configuration,4087,"ocker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:4193,Deployability,configurat,configuration,4193,"ocker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:408,Modifiability,config,config,408,"**HTCondor Backend**. Allows to execute jobs using HTCondor which is a specialized workload management system for compute-intensive jobs created by the Center for High Throughput Computing in the Department of Computer Sciences at the University of Wisconsin-Madison (UW-Madison). The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; HtCondor {; config {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; *",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:461,Modifiability,config,config,461,"**HTCondor Backend**. Allows to execute jobs using HTCondor which is a specialized workload management system for compute-intensive jobs created by the Center for High Throughput Computing in the Department of Computer Sciences at the University of Wisconsin-Madison (UW-Madison). The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; HtCondor {; config {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; *",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:517,Modifiability,config,configuration,517,"**HTCondor Backend**. Allows to execute jobs using HTCondor which is a specialized workload management system for compute-intensive jobs created by the Center for High Throughput Computing in the Department of Computer Sciences at the University of Wisconsin-Madison (UW-Madison). The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; HtCondor {; config {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; *",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:927,Modifiability,config,configurable,927,"**HTCondor Backend**. Allows to execute jobs using HTCondor which is a specialized workload management system for compute-intensive jobs created by the Center for High Throughput Computing in the Department of Computer Sciences at the University of Wisconsin-Madison (UW-Madison). The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; HtCondor {; config {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; *",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:1676,Modifiability,config,configuration,1676,"nd the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - Th",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:1722,Modifiability,config,config,1722,"e CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:1808,Modifiability,variab,variables,1808,"ain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:2213,Modifiability,config,config,2213,"cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/docker",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:2362,Modifiability,variab,variables,2362,"ution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; `",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3398,Modifiability,config,configuration,3398,". * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute n",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3444,Modifiability,config,config,3444,"` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code ti",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3588,Modifiability,config,configurations,3588,"ocker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3772,Modifiability,config,config,3772,"ocker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3794,Modifiability,config,configuration,3794,"ocker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:4087,Modifiability,config,configuration,4087,"ocker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:4193,Modifiability,config,configuration,4193,"ocker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:1280,Performance,queue,queue,1280,"ia the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; HtCondor {; config {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attrib",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3261,Performance,queue,queue,3261,"cker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs:",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:4446,Safety,timeout,timeout,4446,"ocker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:4469,Safety,timeout,timeout,4469,"ocker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs: ""TARGET.Arch == \""INTEL\"" && TARGET.Memory >= 64""; }; ```. `nativeSpecs` attribute needs to be specified as String. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:693,Security,access,access,693,"**HTCondor Backend**. Allows to execute jobs using HTCondor which is a specialized workload management system for compute-intensive jobs created by the Center for High Throughput Computing in the Department of Computer Sciences at the University of Wisconsin-Madison (UW-Madison). The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; HtCondor {; config {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; *",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:1244,Testability,log,log,1244,"te-intensive jobs created by the Center for High Throughput Computing in the Department of Computer Sciences at the University of Wisconsin-Madison (UW-Madison). The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; HtCondor {; config {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This bac",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:1275,Testability,log,log,1275,"ia the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; HtCondor {; config {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable script command such as:. ```; chmod 755 ${script}; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${err}; output=${out}; log_xml=true; request_cpus=${cpu}; executable=${script}; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. The HtCondor backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.HtCondor.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attrib",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3225,Testability,log,log,3225,"t should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. This backend also supports docker as optional feature. Configuration key `backend.providers.HtCondor.config.submit-docker` is specified for this end. When the WDL contains a docker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native S",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md:3256,Testability,log,log,3256,"cker runtime attribute, this command will be provided with two additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_script` - The path of the `script` within the docker container.; * `docker_out` - The path of the `out` within the docker container.; * `docker_err` - The path of the `err` within the docker container. ```; chmod 755 ${script}; cat > ${cwd}/execution/dockerScript <<EOF; #!/bin/bash; docker run --rm -i -v ${cwd}:${docker_cwd} ${docker} /bin/bash ${docker_script}; EOF; chmod 755 ${cwd}/execution/dockerScript; cat > ${cwd}/execution/submitFile <<EOF; Iwd=${cwd}/execution; requirements=${nativeSpecs}; leave_in_queue=true; request_memory=${memory_mb}; request_disk=${disk_kb}; error=${cwd}/execution/stderr; output=${cwd}/execution/stdout; log_xml=true; request_cpus=${cpu}; executable=${cwd}/execution/dockerScript; log=${cwd}/execution/execution.log; queue; EOF; condor_submit ${cwd}/execution/submitFile; ```. This backend support additional runtime attributes that are specified in the configuration key `backend.providers.HtCondor.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are five special runtime attribute configurations, `cpu`, `memory_mb`, `disk_kb`, `nativeSpecs`, `docker`.; Optional values are defined with the prefix `?` attached to the type. ```; backend {; providers {; HtCondor {; config {; # ... other configuration; 	 runtime-attributes = """"""; 	 Int cpu = 1; 	 Float memory_mb = 512.0; 	 Float disk_kb = 256000.0; 	 String? nativeSpecs; 	 String? docker; 	 """"""; }; }; }; }; ```. **Native Specifications**. The use of runtime attribute 'nativeSpecs' allows to the user to attach custom HtCondor configuration to tasks.; An example of this is when there is a need to work with 'requirements' or 'rank' configuration. ```; ""runtimeAttributes"": {; cpu = 2; memory = ""1GB""; disk = ""1GB""; nativeSpecs:",MatchSource.DOCS,docs/backends/HTcondor.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/HTcondor.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md:258,Deployability,configurat,configuration,258,"**Local Backend**. The local backend will simply launch a subprocess for each job invocation and wait for it to produce a Return Code file (rc file) which will contain the exit code of the job's command.; It is pre-enabled by default and there is no further configuration needed to start using it. It uses the local filesystem on which Cromwell is running to store the workflow directory structure. You can find the complete set of configurable settings with explanations in the ; [Cromwell Example Configuration File][cromwell-examples-conf],; along with backend provider examples in the [Example Providers Folder][cromwell-examples-folder]. The Local backend makes use of the same generic configuration as HPC backends. The same [filesystem considerations](HPC#filesystems) apply. **Note to OSX users**: Docker on Mac restricts the directories that can be mounted. Only some directories are allowed by default.; If you try to mount a volume from a disallowed directory, jobs can fail in an odd manner. Before mounting a directory make sure it is in the list; of allowed directories. See the [Docker documentation](https://docs.docker.com/docker-for-mac/osxfs/#namespaces) for how to configure those directories. [cromwell-examples-conf]: https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/backends/Local.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md:691,Deployability,configurat,configuration,691,"**Local Backend**. The local backend will simply launch a subprocess for each job invocation and wait for it to produce a Return Code file (rc file) which will contain the exit code of the job's command.; It is pre-enabled by default and there is no further configuration needed to start using it. It uses the local filesystem on which Cromwell is running to store the workflow directory structure. You can find the complete set of configurable settings with explanations in the ; [Cromwell Example Configuration File][cromwell-examples-conf],; along with backend provider examples in the [Example Providers Folder][cromwell-examples-folder]. The Local backend makes use of the same generic configuration as HPC backends. The same [filesystem considerations](HPC#filesystems) apply. **Note to OSX users**: Docker on Mac restricts the directories that can be mounted. Only some directories are allowed by default.; If you try to mount a volume from a disallowed directory, jobs can fail in an odd manner. Before mounting a directory make sure it is in the list; of allowed directories. See the [Docker documentation](https://docs.docker.com/docker-for-mac/osxfs/#namespaces) for how to configure those directories. [cromwell-examples-conf]: https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/backends/Local.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md:258,Modifiability,config,configuration,258,"**Local Backend**. The local backend will simply launch a subprocess for each job invocation and wait for it to produce a Return Code file (rc file) which will contain the exit code of the job's command.; It is pre-enabled by default and there is no further configuration needed to start using it. It uses the local filesystem on which Cromwell is running to store the workflow directory structure. You can find the complete set of configurable settings with explanations in the ; [Cromwell Example Configuration File][cromwell-examples-conf],; along with backend provider examples in the [Example Providers Folder][cromwell-examples-folder]. The Local backend makes use of the same generic configuration as HPC backends. The same [filesystem considerations](HPC#filesystems) apply. **Note to OSX users**: Docker on Mac restricts the directories that can be mounted. Only some directories are allowed by default.; If you try to mount a volume from a disallowed directory, jobs can fail in an odd manner. Before mounting a directory make sure it is in the list; of allowed directories. See the [Docker documentation](https://docs.docker.com/docker-for-mac/osxfs/#namespaces) for how to configure those directories. [cromwell-examples-conf]: https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/backends/Local.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md:432,Modifiability,config,configurable,432,"**Local Backend**. The local backend will simply launch a subprocess for each job invocation and wait for it to produce a Return Code file (rc file) which will contain the exit code of the job's command.; It is pre-enabled by default and there is no further configuration needed to start using it. It uses the local filesystem on which Cromwell is running to store the workflow directory structure. You can find the complete set of configurable settings with explanations in the ; [Cromwell Example Configuration File][cromwell-examples-conf],; along with backend provider examples in the [Example Providers Folder][cromwell-examples-folder]. The Local backend makes use of the same generic configuration as HPC backends. The same [filesystem considerations](HPC#filesystems) apply. **Note to OSX users**: Docker on Mac restricts the directories that can be mounted. Only some directories are allowed by default.; If you try to mount a volume from a disallowed directory, jobs can fail in an odd manner. Before mounting a directory make sure it is in the list; of allowed directories. See the [Docker documentation](https://docs.docker.com/docker-for-mac/osxfs/#namespaces) for how to configure those directories. [cromwell-examples-conf]: https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/backends/Local.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md:691,Modifiability,config,configuration,691,"**Local Backend**. The local backend will simply launch a subprocess for each job invocation and wait for it to produce a Return Code file (rc file) which will contain the exit code of the job's command.; It is pre-enabled by default and there is no further configuration needed to start using it. It uses the local filesystem on which Cromwell is running to store the workflow directory structure. You can find the complete set of configurable settings with explanations in the ; [Cromwell Example Configuration File][cromwell-examples-conf],; along with backend provider examples in the [Example Providers Folder][cromwell-examples-folder]. The Local backend makes use of the same generic configuration as HPC backends. The same [filesystem considerations](HPC#filesystems) apply. **Note to OSX users**: Docker on Mac restricts the directories that can be mounted. Only some directories are allowed by default.; If you try to mount a volume from a disallowed directory, jobs can fail in an odd manner. Before mounting a directory make sure it is in the list; of allowed directories. See the [Docker documentation](https://docs.docker.com/docker-for-mac/osxfs/#namespaces) for how to configure those directories. [cromwell-examples-conf]: https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/backends/Local.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md:1185,Modifiability,config,configure,1185,"**Local Backend**. The local backend will simply launch a subprocess for each job invocation and wait for it to produce a Return Code file (rc file) which will contain the exit code of the job's command.; It is pre-enabled by default and there is no further configuration needed to start using it. It uses the local filesystem on which Cromwell is running to store the workflow directory structure. You can find the complete set of configurable settings with explanations in the ; [Cromwell Example Configuration File][cromwell-examples-conf],; along with backend provider examples in the [Example Providers Folder][cromwell-examples-folder]. The Local backend makes use of the same generic configuration as HPC backends. The same [filesystem considerations](HPC#filesystems) apply. **Note to OSX users**: Docker on Mac restricts the directories that can be mounted. Only some directories are allowed by default.; If you try to mount a volume from a disallowed directory, jobs can fail in an odd manner. Before mounting a directory make sure it is in the list; of allowed directories. See the [Docker documentation](https://docs.docker.com/docker-for-mac/osxfs/#namespaces) for how to configure those directories. [cromwell-examples-conf]: https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/backends/Local.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md:42,Usability,simpl,simply,42,"**Local Backend**. The local backend will simply launch a subprocess for each job invocation and wait for it to produce a Return Code file (rc file) which will contain the exit code of the job's command.; It is pre-enabled by default and there is no further configuration needed to start using it. It uses the local filesystem on which Cromwell is running to store the workflow directory structure. You can find the complete set of configurable settings with explanations in the ; [Cromwell Example Configuration File][cromwell-examples-conf],; along with backend provider examples in the [Example Providers Folder][cromwell-examples-folder]. The Local backend makes use of the same generic configuration as HPC backends. The same [filesystem considerations](HPC#filesystems) apply. **Note to OSX users**: Docker on Mac restricts the directories that can be mounted. Only some directories are allowed by default.; If you try to mount a volume from a disallowed directory, jobs can fail in an odd manner. Before mounting a directory make sure it is in the list; of allowed directories. See the [Docker documentation](https://docs.docker.com/docker-for-mac/osxfs/#namespaces) for how to configure those directories. [cromwell-examples-conf]: https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf; [cromwell-examples-folder]: https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends; ",MatchSource.DOCS,docs/backends/Local.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Local.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md:410,Availability,alive,alive,410,"The following configuration can be used as a base to allow Cromwell to interact with an [LSF](https://en.wikipedia.org/wiki/Platform_LSF) cluster and dispatch jobs to it:. ```hocon; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/LSF.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md:14,Deployability,configurat,configuration,14,"The following configuration can be used as a base to allow Cromwell to interact with an [LSF](https://en.wikipedia.org/wiki/Platform_LSF) cluster and dispatch jobs to it:. ```hocon; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/LSF.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md:14,Modifiability,config,configuration,14,"The following configuration can be used as a base to allow Cromwell to interact with an [LSF](https://en.wikipedia.org/wiki/Platform_LSF) cluster and dispatch jobs to it:. ```hocon; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/LSF.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md:232,Modifiability,config,config,232,"The following configuration can be used as a base to allow Cromwell to interact with an [LSF](https://en.wikipedia.org/wiki/Platform_LSF) cluster and dispatch jobs to it:. ```hocon; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/LSF.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md:276,Modifiability,config,config,276,"The following configuration can be used as a base to allow Cromwell to interact with an [LSF](https://en.wikipedia.org/wiki/Platform_LSF) cluster and dispatch jobs to it:. ```hocon; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/LSF.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md:515,Modifiability,config,configure,515,"The following configuration can be used as a base to allow Cromwell to interact with an [LSF](https://en.wikipedia.org/wiki/Platform_LSF) cluster and dispatch jobs to it:. ```hocon; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/LSF.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md:647,Safety,timeout,timeout,647,"The following configuration can be used as a base to allow Cromwell to interact with an [LSF](https://en.wikipedia.org/wiki/Platform_LSF) cluster and dispatch jobs to it:. ```hocon; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/LSF.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md:670,Safety,timeout,timeout,670,"The following configuration can be used as a base to allow Cromwell to interact with an [LSF](https://en.wikipedia.org/wiki/Platform_LSF) cluster and dispatch jobs to it:. ```hocon; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/LSF.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/LSF.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:768,Availability,echo,echo,768,"**Sun GridEngine Backend**. The GridEngine and similar backends use programs such as `qsub` to launch a job and will poll the filesystem to determine if a job is completed. The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:1189,Availability,echo,echo,1189," backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL c",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:1309,Availability,failure,failure,1309," backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL c",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4064,Availability,echo,echo,4064,"dv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; r",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4735,Availability,error,error,4735,"n WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` fi",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5664,Availability,alive,alive,5664,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:6061,Availability,alive,alive,6061,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:6213,Availability,alive,alive,6213,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:6382,Availability,alive,alive,6382,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:6428,Availability,alive,alive,6428,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:404,Deployability,configurat,configuration,404,"**Sun GridEngine Backend**. The GridEngine and similar backends use programs such as `qsub` to launch a job and will poll the filesystem to determine if a job is completed. The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:1411,Deployability,configurat,configuration,1411,".. other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.;",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:1900,Deployability,configurat,configuration,1900,"ub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:2107,Deployability,configurat,configuration,2107,"ts the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:2898,Deployability,configurat,configuration,2898," \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attribute",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3227,Deployability,configurat,configuration,3227,"roviders.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variabl",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3417,Deployability,configurat,configurations,3417,"ath where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attribut",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3488,Deployability,configurat,configuration,3488," which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3599,Deployability,configurat,configuration,3599,"th of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may speci",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3781,Deployability,configurat,configuration,3781," `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3918,Deployability,configurat,configuration,3918,"; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4314,Deployability,configurat,configuration,4314,"onfig.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuratio",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4387,Deployability,configurat,configuration,4387,"`cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backe",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4552,Deployability,configurat,configuration,4552,"it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4632,Deployability,configurat,configuration,4632,"nit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the uni",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4817,Deployability,configurat,configuration,4817,"ttributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how rea",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5057,Deployability,configurat,configuration,5057,"""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; prov",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5198,Deployability,configurat,configuration,5198,"mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5263,Deployability,configurat,configuration,5263," runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should con",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5452,Deployability,configurat,configuration,5452,"roject; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:6001,Deployability,configurat,configuration,6001,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:6142,Deployability,configurat,configuration,6142,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:338,Modifiability,config,config,338,"**Sun GridEngine Backend**. The GridEngine and similar backends use programs such as `qsub` to launch a job and will poll the filesystem to determine if a job is completed. The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:382,Modifiability,config,config,382,"**Sun GridEngine Backend**. The GridEngine and similar backends use programs such as `qsub` to launch a job and will poll the filesystem to determine if a job is completed. The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:404,Modifiability,config,configuration,404,"**Sun GridEngine Backend**. The GridEngine and similar backends use programs such as `qsub` to launch a job and will poll the filesystem to determine if a job is completed. The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:814,Modifiability,config,configurable,814,"**Sun GridEngine Backend**. The GridEngine and similar backends use programs such as `qsub` to launch a job and will poll the filesystem to determine if a job is completed. The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:1411,Modifiability,config,configuration,1411,".. other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.;",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:1452,Modifiability,config,config,1452,"the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted with",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:1538,Modifiability,variab,variables,1538,"al backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:1878,Modifiability,config,config,1878,"se \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:1900,Modifiability,config,configuration,1900,"ub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:2107,Modifiability,config,configuration,2107,"ts the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:2155,Modifiability,config,config,2155,"nce the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:2211,Modifiability,config,config,2211,"d will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be speci",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:2347,Modifiability,variab,variables,2347,"ss or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. I",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:2876,Modifiability,config,config,2876,"ers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```;",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:2898,Modifiability,config,configuration,2898," \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${script}; """"""; }; }; }; }; ```. If the backend supports docker, the optional configuration keys `backend.providers.<backend>.config.submit-docker`; and `backend.providers.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attribute",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3227,Modifiability,config,configuration,3227,"roviders.<backend>.config.kill-docker` may be specified. When the WDL contains a docker runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variabl",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3274,Modifiability,config,config,3274,"er runtime; attribute, this command will be provided three additional variables:. * `docker` - The docker image name.; * `docker_cwd` - The path where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying t",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3417,Modifiability,config,configurations,3417,"ath where `cwd` should be mounted within the docker container.; * `docker_cid` - The host path to which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attribut",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3488,Modifiability,config,configuration,3488," which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3599,Modifiability,config,configuration,3599,"th of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may speci",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3781,Modifiability,config,configuration,3781," `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3828,Modifiability,config,config,3828,"ntainer. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3896,Modifiability,config,config,3896,"; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; Str",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3918,Modifiability,config,configuration,3918,"; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4185,Modifiability,variab,variable,4185,"ditional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value w",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4314,Modifiability,config,configuration,4314,"onfig.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuratio",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4365,Modifiability,config,config,4365,"butes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4387,Modifiability,config,configuration,4387,"`cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backe",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4493,Modifiability,variab,variables,4493,"it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4552,Modifiability,config,configuration,4552,"it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4632,Modifiability,config,configuration,4632,"nit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the uni",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4795,Modifiability,config,config,4795,"nfiguration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-backgrou",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:4817,Modifiability,config,configuration,4817,"ttributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how rea",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5035,Modifiability,config,config,5035,":. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values a",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5057,Modifiability,config,configuration,5057,"""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; prov",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5198,Modifiability,config,configuration,5198,"mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5263,Modifiability,config,configuration,5263," runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should con",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5385,Modifiability,config,config,5385,"onfiguration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5452,Modifiability,config,configuration,5452,"roject; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5761,Modifiability,config,config,5761,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:6001,Modifiability,config,configuration,6001,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:6120,Modifiability,config,config,6120,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:6142,Modifiability,config,configuration,6142,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5333,Safety,abort,aborted,5333," runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should con",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:5508,Safety,abort,abort,5508,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:6476,Safety,timeout,timeout,6476,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:6499,Safety,timeout,timeout,6499,"hen be passed from the WDL into the submit configuration. If one would like to have a default value, just like in WDL, the configuration may specify that the value have a default. The default must match the defined type or an error will be produced. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb = 2.0; String sge_project = ""default""; """"""; }; }; }; }; ```. Optional values may also be used by appending `?` to the type:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float? memory_mb; String? sge_project; """"""; }; }; }; }; ```. The value will be passed to the submit configuration if provided, and omitted otherwise. There are also configuration values related to how jobs are rechecked on startup and aborted. The option is `backend.providers.<backend>.config.run-in-background`. When `true` the backend runs the submit configuration and records the unix process id (PID). To abort the job, the PID is stopped with the unix command `kill`. Upon a cromwell restart, the PID is checked via the unix command `ps` to see if it is still alive, before cromwell goes back to polling for the `rc` file. When `backend.providers.<backend>.config.run-in-background` is `false`, the default, the backend must specify how read the job identifier from the stdout of the submit, how to kill the job, and how to check if the job is still running during a cromwell restart. These three configuration values are `job-id-regex`, `kill`, and `check-alive`, respectively:. ```; backend {; providers {; SGE {; config {; # ... other configuration; job-id-regex = ""(\\d+)""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; """"""; }; }; }; }; ```. The `job-id-regex` should contain one capture group while matching against the whole line or stdout file. The `check-alive` should return zero if the job is still alive. ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:580,Security,access,access,580,"**Sun GridEngine Backend**. The GridEngine and similar backends use programs such as `qsub` to launch a job and will poll the filesystem to determine if a job is completed. The backend is specified via the actor factory `ConfigBackendLifecycleActorFactory`:. ```; backend {; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # ... other configuration; }; }; }; }; ```. This backend makes the same assumption about the filesystem that the local backend does: the Cromwell process and the jobs both have read/write access to the CWD of the job. The CWD will contain a `script.sh` file which will contain the same contents as the Local backend:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. The job is launched with a configurable command such as:. ```bash; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -pe smp ${cpu} \; ${""-l m_mem_free="" + memory_gb + ""gb""} \; ${""-q "" + sge_queue} \; ${""-P "" + sge_project} \; ${script}; ```. The SGE backend gets the job ID from parsing the `submit.stdout` text file. Since the `script.sh` ends with `echo $? > rc`, the backend will wait for the existence of this file, parse out the return code and determine success or failure and then subsequently post-process. The command used to submit the job is specified under the configuration key `backend.providers.SGE.config.submit`. It uses the same syntax as a command in WDL, and will be provided the variables:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `cwd` - The path where the script should be run.; * `out` - The path to the stdout.; * `err` - The path to the stderr.; * `job_name` - A unique name for the job. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md:3539,Security,validat,validated,3539," which the [container ID file](https://docs.docker.com/engine/reference/run/#pid-equivalent) should be written.; * `docker_script` - The path of the `script` inside the docker container.; * `docker_out` - The path of the `out` inside the docker container.; * `docker_err` - The path of the `err` inside the docker container. ```; backend {; providers {; SGE {; config {; # ... other configuration; submit-docker = """"""; qsub \; -terse \; -V \; -b n \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; -l docker,docker_images=""${docker}""; -xdv ${cwd}:${docker_cwd}; ${script}; """"""; }; }; }; }; ```. If the backend would like to support additional runtime attributes they may be specified in the configuration key `backend.providers.<backend>.config.runtime-attributes`. It uses the same syntax as specifying runtime attributes in a task in WDL. There are two special runtime attribute configurations, `cpu`, and `memory_<unit>`. When the runtime attribute configuration `Int cpu` is specified, it is always validated as a positive integer. When the runtime attribute configuration `Int memory_<unit>` or `Float memory_<unit>` is specified, it is provided to submit by the runtime attribute in WDL `memory`. For example, if the backend specifies the configuration for `backend.providers.<backend>.config.runtime-attributes` as:. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = ""Float memory_mb""; }; }; }; }; ```. And the WDL specifies a task with:. ```; task hello_gigabyte {; command { echo ""hello world"" }; runtime { memory: ""1 GB"" }; }; ```. Then for this call, the backend will be provided an additional variable `memory_mb` set to `1000.0`. Other runtime attributes may be defined by specifying them in under the runtime attributes configuration. ```; backend {; providers {; SGE {; config {; # ... other configuration; runtime-attributes = """"""; Float memory_mb; String sge_project; """"""; }; }; }; }; ```. These variables will then be ",MatchSource.DOCS,docs/backends/SGE.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SGE.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md:650,Availability,alive,alive,650,"The following configuration can be used as a base to allow Cromwell to interact with a [SLURM](https://slurm.schedmd.com/) cluster and dispatch jobs to it:. ```hocon; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro.md). ### Exit code. See also [HPC - Exit code timeout](HPC.md#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SLURM.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md:14,Deployability,configurat,configuration,14,"The following configuration can be used as a base to allow Cromwell to interact with a [SLURM](https://slurm.schedmd.com/) cluster and dispatch jobs to it:. ```hocon; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro.md). ### Exit code. See also [HPC - Exit code timeout](HPC.md#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SLURM.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md:583,Integrability,wrap,wrap,583,"The following configuration can be used as a base to allow Cromwell to interact with a [SLURM](https://slurm.schedmd.com/) cluster and dispatch jobs to it:. ```hocon; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro.md). ### Exit code. See also [HPC - Exit code timeout](HPC.md#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SLURM.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md:14,Modifiability,config,configuration,14,"The following configuration can be used as a base to allow Cromwell to interact with a [SLURM](https://slurm.schedmd.com/) cluster and dispatch jobs to it:. ```hocon; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro.md). ### Exit code. See also [HPC - Exit code timeout](HPC.md#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SLURM.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md:219,Modifiability,config,config,219,"The following configuration can be used as a base to allow Cromwell to interact with a [SLURM](https://slurm.schedmd.com/) cluster and dispatch jobs to it:. ```hocon; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro.md). ### Exit code. See also [HPC - Exit code timeout](HPC.md#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SLURM.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md:263,Modifiability,config,config,263,"The following configuration can be used as a base to allow Cromwell to interact with a [SLURM](https://slurm.schedmd.com/) cluster and dispatch jobs to it:. ```hocon; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro.md). ### Exit code. See also [HPC - Exit code timeout](HPC.md#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SLURM.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md:773,Modifiability,config,configure,773,"The following configuration can be used as a base to allow Cromwell to interact with a [SLURM](https://slurm.schedmd.com/) cluster and dispatch jobs to it:. ```hocon; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro.md). ### Exit code. See also [HPC - Exit code timeout](HPC.md#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SLURM.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md:388,Performance,queue,queue,388,"The following configuration can be used as a base to allow Cromwell to interact with a [SLURM](https://slurm.schedmd.com/) cluster and dispatch jobs to it:. ```hocon; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro.md). ### Exit code. See also [HPC - Exit code timeout](HPC.md#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SLURM.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md:503,Performance,queue,queue,503,"The following configuration can be used as a base to allow Cromwell to interact with a [SLURM](https://slurm.schedmd.com/) cluster and dispatch jobs to it:. ```hocon; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro.md). ### Exit code. See also [HPC - Exit code timeout](HPC.md#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SLURM.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md:908,Safety,timeout,timeout,908,"The following configuration can be used as a base to allow Cromwell to interact with a [SLURM](https://slurm.schedmd.com/) cluster and dispatch jobs to it:. ```hocon; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro.md). ### Exit code. See also [HPC - Exit code timeout](HPC.md#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SLURM.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md:934,Safety,timeout,timeout,934,"The following configuration can be used as a base to allow Cromwell to interact with a [SLURM](https://slurm.schedmd.com/) cluster and dispatch jobs to it:. ```hocon; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro.md). ### Exit code. See also [HPC - Exit code timeout](HPC.md#Exit-code-timeout); ",MatchSource.DOCS,docs/backends/SLURM.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/SLURM.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:429,Availability,error,error,429,"## TES Backend. The TES backend submits jobs to a server that complies with the protocol described by the [GA4GH schema](https://github.com/ga4gh/task-execution-schemas). This backend creates three files in the `<call_dir>`:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `stdout` - The standard output of the process; * `stderr` - The standard error of the process. The `script` file contains:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. `<container_call_root>` would be equal to the runtime attribute `dockerWorkingDir` or `/cromwell-executions/<workflow_uuid>/call-<call_name>/execution` if this attribute is not supplied. ### Configuring. Configuring the TES backend is straightforward; one must only provide the TES API endpoint for the service. ```hocon; backend {; default = ""TES""; providers {; TES {; actor-factory = ""cromwell.backend.impl.tes.TesBackendLifecycleActorFactory""; config {; endpoint = ""https://<some-url>/v1/tasks""; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; concurrent-job-limit = 1000; }; }; }; }; ```. Cromwell uses an exponential backoff strategy to control the rate of requests sent to the TES server. ; Users can override the default settings for this behavior in config, see `cromwell.example.backends/TES.conf` ; for full expected format. For example, to force a constant polling rate rather than exponential backoff:; ```hocon; backend.providers.TES.config.poll-backoff {; min: ""1 minute""; max: ""1 minute""; multiplier: 1; randomization-factor: 0; }; ```. ### Supported File Systems. Currently this backend only works with files on a Local or Shared File System. ### Docker. This backend supports the following optional [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/) for working with Docker:. * `docker`: Docker image to use such as ""Ubuntu"".; * `dockerWorkingDir`: defines the working",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:538,Availability,echo,echo,538,"## TES Backend. The TES backend submits jobs to a server that complies with the protocol described by the [GA4GH schema](https://github.com/ga4gh/task-execution-schemas). This backend creates three files in the `<call_dir>`:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `stdout` - The standard output of the process; * `stderr` - The standard error of the process. The `script` file contains:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. `<container_call_root>` would be equal to the runtime attribute `dockerWorkingDir` or `/cromwell-executions/<workflow_uuid>/call-<call_name>/execution` if this attribute is not supplied. ### Configuring. Configuring the TES backend is straightforward; one must only provide the TES API endpoint for the service. ```hocon; backend {; default = ""TES""; providers {; TES {; actor-factory = ""cromwell.backend.impl.tes.TesBackendLifecycleActorFactory""; config {; endpoint = ""https://<some-url>/v1/tasks""; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; concurrent-job-limit = 1000; }; }; }; }; ```. Cromwell uses an exponential backoff strategy to control the rate of requests sent to the TES server. ; Users can override the default settings for this behavior in config, see `cromwell.example.backends/TES.conf` ; for full expected format. For example, to force a constant polling rate rather than exponential backoff:; ```hocon; backend.providers.TES.config.poll-backoff {; min: ""1 minute""; max: ""1 minute""; multiplier: 1; randomization-factor: 0; }; ```. ### Supported File Systems. Currently this backend only works with files on a Local or Shared File System. ### Docker. This backend supports the following optional [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/) for working with Docker:. * `docker`: Docker image to use such as ""Ubuntu"".; * `dockerWorkingDir`: defines the working",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:2104,Deployability,configurat,configuration,2104,"utions""; concurrent-job-limit = 1000; }; }; }; }; ```. Cromwell uses an exponential backoff strategy to control the rate of requests sent to the TES server. ; Users can override the default settings for this behavior in config, see `cromwell.example.backends/TES.conf` ; for full expected format. For example, to force a constant polling rate rather than exponential backoff:; ```hocon; backend.providers.TES.config.poll-backoff {; min: ""1 minute""; max: ""1 minute""; multiplier: 1; randomization-factor: 0; }; ```. ### Supported File Systems. Currently this backend only works with files on a Local or Shared File System. ### Docker. This backend supports the following optional [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/) for working with Docker:. * `docker`: Docker image to use such as ""Ubuntu"".; * `dockerWorkingDir`: defines the working directory in the container. ### CPU, Memory and Disk. This backend supports CPU, memory and disk size configuration through the use of the following [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/): . * `cpu` defines the amount of CPU to use.; * Type: Integer (ex: 4); * `memory` defines the amount of memory to use.; * Type: String (ex: ""4 GB"" or ""4096 MB""); * `disk` defines the amount of disk to use.; * Type: String (ex: ""1 GB"" or ""1024 MB""); * `disks` accepts a GCP-style disk declaration and attempts to translate it for use on TES; * See table below for supported translations ; * `preemptible` defines whether or not to use preemptible VMs.; * Type: Boolean (ex: ""true"" or ""false""); * Integers are accepted and will be converted to boolean (true if > 0). If they are not set, the TES backend may use default values. #### GCP `disks` to TES `disk` compatibility. | GCP `disks` value | Supported | TES translation | Remark |; |---------------------------------------|-----------|-----------------|-----------------------------------|; | `local-disk 25 HDD` | ✅ | 2",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:80,Integrability,protocol,protocol,80,"## TES Backend. The TES backend submits jobs to a server that complies with the protocol described by the [GA4GH schema](https://github.com/ga4gh/task-execution-schemas). This backend creates three files in the `<call_dir>`:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `stdout` - The standard output of the process; * `stderr` - The standard error of the process. The `script` file contains:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. `<container_call_root>` would be equal to the runtime attribute `dockerWorkingDir` or `/cromwell-executions/<workflow_uuid>/call-<call_name>/execution` if this attribute is not supplied. ### Configuring. Configuring the TES backend is straightforward; one must only provide the TES API endpoint for the service. ```hocon; backend {; default = ""TES""; providers {; TES {; actor-factory = ""cromwell.backend.impl.tes.TesBackendLifecycleActorFactory""; config {; endpoint = ""https://<some-url>/v1/tasks""; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; concurrent-job-limit = 1000; }; }; }; }; ```. Cromwell uses an exponential backoff strategy to control the rate of requests sent to the TES server. ; Users can override the default settings for this behavior in config, see `cromwell.example.backends/TES.conf` ; for full expected format. For example, to force a constant polling rate rather than exponential backoff:; ```hocon; backend.providers.TES.config.poll-backoff {; min: ""1 minute""; max: ""1 minute""; multiplier: 1; randomization-factor: 0; }; ```. ### Supported File Systems. Currently this backend only works with files on a Local or Shared File System. ### Docker. This backend supports the following optional [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/) for working with Docker:. * `docker`: Docker image to use such as ""Ubuntu"".; * `dockerWorkingDir`: defines the working",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:3588,Integrability,interface,interface,3588,"imeAttributes) and [Workflow Options](../wf_options/Overview/): . * `cpu` defines the amount of CPU to use.; * Type: Integer (ex: 4); * `memory` defines the amount of memory to use.; * Type: String (ex: ""4 GB"" or ""4096 MB""); * `disk` defines the amount of disk to use.; * Type: String (ex: ""1 GB"" or ""1024 MB""); * `disks` accepts a GCP-style disk declaration and attempts to translate it for use on TES; * See table below for supported translations ; * `preemptible` defines whether or not to use preemptible VMs.; * Type: Boolean (ex: ""true"" or ""false""); * Integers are accepted and will be converted to boolean (true if > 0). If they are not set, the TES backend may use default values. #### GCP `disks` to TES `disk` compatibility. | GCP `disks` value | Supported | TES translation | Remark |; |---------------------------------------|-----------|-----------------|-----------------------------------|; | `local-disk 25 HDD` | ✅ | 25 GB disk | |; | `local-disk 25 SSD` | ✅ | 25 GB disk | Disk type info is dropped |; | `/some/mnt 25 SSD` | ❌ | | Custom mount points not supported | ; | `local-disk 25 HDD, /some/mnt 50 SSD` | ❌ | | Multiple disks are not supported | ; ; > Note: if both `disk` and `disks` attributes are specified, the TES backend will automatically use the value in `disk` and not attempt to translate `disks`. ### Azure; [Azure](Azure) is an implementation of Cromwell that uses the TES interface for orchestrating the tasks on Azure. ### TESK. [TESK](https://github.com/EMBL-EBI-TSI/TESK) is an implementation of the TES interface that uses Kubernetes and FTP.; When running Cromwell with a TESK backend, you will want to customize the way Cromwell process globs, as kubernetes will not work well with hard links in a lot of cases which is the default behavior in Cromwell.; By adding this to the `config` section of the TES backend in Cromwell, Cromwell will use symlinks instead. . `glob-link-command = ""ls -L GLOB_PATTERN 2> /dev/null | xargs -I ? ln -s ? GLOB_DIRECTORY""`; ",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:3723,Integrability,interface,interface,3723,"imeAttributes) and [Workflow Options](../wf_options/Overview/): . * `cpu` defines the amount of CPU to use.; * Type: Integer (ex: 4); * `memory` defines the amount of memory to use.; * Type: String (ex: ""4 GB"" or ""4096 MB""); * `disk` defines the amount of disk to use.; * Type: String (ex: ""1 GB"" or ""1024 MB""); * `disks` accepts a GCP-style disk declaration and attempts to translate it for use on TES; * See table below for supported translations ; * `preemptible` defines whether or not to use preemptible VMs.; * Type: Boolean (ex: ""true"" or ""false""); * Integers are accepted and will be converted to boolean (true if > 0). If they are not set, the TES backend may use default values. #### GCP `disks` to TES `disk` compatibility. | GCP `disks` value | Supported | TES translation | Remark |; |---------------------------------------|-----------|-----------------|-----------------------------------|; | `local-disk 25 HDD` | ✅ | 25 GB disk | |; | `local-disk 25 SSD` | ✅ | 25 GB disk | Disk type info is dropped |; | `/some/mnt 25 SSD` | ❌ | | Custom mount points not supported | ; | `local-disk 25 HDD, /some/mnt 50 SSD` | ❌ | | Multiple disks are not supported | ; ; > Note: if both `disk` and `disks` attributes are specified, the TES backend will automatically use the value in `disk` and not attempt to translate `disks`. ### Azure; [Azure](Azure) is an implementation of Cromwell that uses the TES interface for orchestrating the tasks on Azure. ### TESK. [TESK](https://github.com/EMBL-EBI-TSI/TESK) is an implementation of the TES interface that uses Kubernetes and FTP.; When running Cromwell with a TESK backend, you will want to customize the way Cromwell process globs, as kubernetes will not work well with hard links in a lot of cases which is the default behavior in Cromwell.; By adding this to the `config` section of the TES backend in Cromwell, Cromwell will use symlinks instead. . `glob-link-command = ""ls -L GLOB_PATTERN 2> /dev/null | xargs -I ? ln -s ? GLOB_DIRECTORY""`; ",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:1004,Modifiability,config,config,1004,"s with the protocol described by the [GA4GH schema](https://github.com/ga4gh/task-execution-schemas). This backend creates three files in the `<call_dir>`:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `stdout` - The standard output of the process; * `stderr` - The standard error of the process. The `script` file contains:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. `<container_call_root>` would be equal to the runtime attribute `dockerWorkingDir` or `/cromwell-executions/<workflow_uuid>/call-<call_name>/execution` if this attribute is not supplied. ### Configuring. Configuring the TES backend is straightforward; one must only provide the TES API endpoint for the service. ```hocon; backend {; default = ""TES""; providers {; TES {; actor-factory = ""cromwell.backend.impl.tes.TesBackendLifecycleActorFactory""; config {; endpoint = ""https://<some-url>/v1/tasks""; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; concurrent-job-limit = 1000; }; }; }; }; ```. Cromwell uses an exponential backoff strategy to control the rate of requests sent to the TES server. ; Users can override the default settings for this behavior in config, see `cromwell.example.backends/TES.conf` ; for full expected format. For example, to force a constant polling rate rather than exponential backoff:; ```hocon; backend.providers.TES.config.poll-backoff {; min: ""1 minute""; max: ""1 minute""; multiplier: 1; randomization-factor: 0; }; ```. ### Supported File Systems. Currently this backend only works with files on a Local or Shared File System. ### Docker. This backend supports the following optional [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/) for working with Docker:. * `docker`: Docker image to use such as ""Ubuntu"".; * `dockerWorkingDir`: defines the working directory in the container. ### CPU, Memory and Disk. This backend ",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:1334,Modifiability,config,config,1334," the `command` section of the WDL code.; * `stdout` - The standard output of the process; * `stderr` - The standard error of the process. The `script` file contains:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. `<container_call_root>` would be equal to the runtime attribute `dockerWorkingDir` or `/cromwell-executions/<workflow_uuid>/call-<call_name>/execution` if this attribute is not supplied. ### Configuring. Configuring the TES backend is straightforward; one must only provide the TES API endpoint for the service. ```hocon; backend {; default = ""TES""; providers {; TES {; actor-factory = ""cromwell.backend.impl.tes.TesBackendLifecycleActorFactory""; config {; endpoint = ""https://<some-url>/v1/tasks""; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; concurrent-job-limit = 1000; }; }; }; }; ```. Cromwell uses an exponential backoff strategy to control the rate of requests sent to the TES server. ; Users can override the default settings for this behavior in config, see `cromwell.example.backends/TES.conf` ; for full expected format. For example, to force a constant polling rate rather than exponential backoff:; ```hocon; backend.providers.TES.config.poll-backoff {; min: ""1 minute""; max: ""1 minute""; multiplier: 1; randomization-factor: 0; }; ```. ### Supported File Systems. Currently this backend only works with files on a Local or Shared File System. ### Docker. This backend supports the following optional [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/) for working with Docker:. * `docker`: Docker image to use such as ""Ubuntu"".; * `dockerWorkingDir`: defines the working directory in the container. ### CPU, Memory and Disk. This backend supports CPU, memory and disk size configuration through the use of the following [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/): . * `cpu` defines the amount of CPU to use.; * Type: Integer (ex: 4); ",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:1523,Modifiability,config,config,1523,"r_command>; echo $? > rc; ```. `<container_call_root>` would be equal to the runtime attribute `dockerWorkingDir` or `/cromwell-executions/<workflow_uuid>/call-<call_name>/execution` if this attribute is not supplied. ### Configuring. Configuring the TES backend is straightforward; one must only provide the TES API endpoint for the service. ```hocon; backend {; default = ""TES""; providers {; TES {; actor-factory = ""cromwell.backend.impl.tes.TesBackendLifecycleActorFactory""; config {; endpoint = ""https://<some-url>/v1/tasks""; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; concurrent-job-limit = 1000; }; }; }; }; ```. Cromwell uses an exponential backoff strategy to control the rate of requests sent to the TES server. ; Users can override the default settings for this behavior in config, see `cromwell.example.backends/TES.conf` ; for full expected format. For example, to force a constant polling rate rather than exponential backoff:; ```hocon; backend.providers.TES.config.poll-backoff {; min: ""1 minute""; max: ""1 minute""; multiplier: 1; randomization-factor: 0; }; ```. ### Supported File Systems. Currently this backend only works with files on a Local or Shared File System. ### Docker. This backend supports the following optional [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/) for working with Docker:. * `docker`: Docker image to use such as ""Ubuntu"".; * `dockerWorkingDir`: defines the working directory in the container. ### CPU, Memory and Disk. This backend supports CPU, memory and disk size configuration through the use of the following [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/): . * `cpu` defines the amount of CPU to use.; * Type: Integer (ex: 4); * `memory` defines the amount of memory to use.; * Type: String (ex: ""4 GB"" or ""4096 MB""); * `disk` defines the amount of disk to use.; * Type: String (ex: ""1 GB"" or ""1024 MB""); * `disks` accepts a GCP-style disk d",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:2104,Modifiability,config,configuration,2104,"utions""; concurrent-job-limit = 1000; }; }; }; }; ```. Cromwell uses an exponential backoff strategy to control the rate of requests sent to the TES server. ; Users can override the default settings for this behavior in config, see `cromwell.example.backends/TES.conf` ; for full expected format. For example, to force a constant polling rate rather than exponential backoff:; ```hocon; backend.providers.TES.config.poll-backoff {; min: ""1 minute""; max: ""1 minute""; multiplier: 1; randomization-factor: 0; }; ```. ### Supported File Systems. Currently this backend only works with files on a Local or Shared File System. ### Docker. This backend supports the following optional [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/) for working with Docker:. * `docker`: Docker image to use such as ""Ubuntu"".; * `dockerWorkingDir`: defines the working directory in the container. ### CPU, Memory and Disk. This backend supports CPU, memory and disk size configuration through the use of the following [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/): . * `cpu` defines the amount of CPU to use.; * Type: Integer (ex: 4); * `memory` defines the amount of memory to use.; * Type: String (ex: ""4 GB"" or ""4096 MB""); * `disk` defines the amount of disk to use.; * Type: String (ex: ""1 GB"" or ""1024 MB""); * `disks` accepts a GCP-style disk declaration and attempts to translate it for use on TES; * See table below for supported translations ; * `preemptible` defines whether or not to use preemptible VMs.; * Type: Boolean (ex: ""true"" or ""false""); * Integers are accepted and will be converted to boolean (true if > 0). If they are not set, the TES backend may use default values. #### GCP `disks` to TES `disk` compatibility. | GCP `disks` value | Supported | TES translation | Remark |; |---------------------------------------|-----------|-----------------|-----------------------------------|; | `local-disk 25 HDD` | ✅ | 2",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:4000,Modifiability,config,config,4000,"imeAttributes) and [Workflow Options](../wf_options/Overview/): . * `cpu` defines the amount of CPU to use.; * Type: Integer (ex: 4); * `memory` defines the amount of memory to use.; * Type: String (ex: ""4 GB"" or ""4096 MB""); * `disk` defines the amount of disk to use.; * Type: String (ex: ""1 GB"" or ""1024 MB""); * `disks` accepts a GCP-style disk declaration and attempts to translate it for use on TES; * See table below for supported translations ; * `preemptible` defines whether or not to use preemptible VMs.; * Type: Boolean (ex: ""true"" or ""false""); * Integers are accepted and will be converted to boolean (true if > 0). If they are not set, the TES backend may use default values. #### GCP `disks` to TES `disk` compatibility. | GCP `disks` value | Supported | TES translation | Remark |; |---------------------------------------|-----------|-----------------|-----------------------------------|; | `local-disk 25 HDD` | ✅ | 25 GB disk | |; | `local-disk 25 SSD` | ✅ | 25 GB disk | Disk type info is dropped |; | `/some/mnt 25 SSD` | ❌ | | Custom mount points not supported | ; | `local-disk 25 HDD, /some/mnt 50 SSD` | ❌ | | Multiple disks are not supported | ; ; > Note: if both `disk` and `disks` attributes are specified, the TES backend will automatically use the value in `disk` and not attempt to translate `disks`. ### Azure; [Azure](Azure) is an implementation of Cromwell that uses the TES interface for orchestrating the tasks on Azure. ### TESK. [TESK](https://github.com/EMBL-EBI-TSI/TESK) is an implementation of the TES interface that uses Kubernetes and FTP.; When running Cromwell with a TESK backend, you will want to customize the way Cromwell process globs, as kubernetes will not work well with hard links in a lot of cases which is the default behavior in Cromwell.; By adding this to the `config` section of the TES backend in Cromwell, Cromwell will use symlinks instead. . `glob-link-command = ""ls -L GLOB_PATTERN 2> /dev/null | xargs -I ? ln -s ? GLOB_DIRECTORY""`; ",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md:1123,Performance,concurren,concurrent-job-limit,1123,"s with the protocol described by the [GA4GH schema](https://github.com/ga4gh/task-execution-schemas). This backend creates three files in the `<call_dir>`:. * `script` - A shell script of the job to be run. This contains the user's command from the `command` section of the WDL code.; * `stdout` - The standard output of the process; * `stderr` - The standard error of the process. The `script` file contains:. ```; #!/bin/sh; cd <container_call_root>; <user_command>; echo $? > rc; ```. `<container_call_root>` would be equal to the runtime attribute `dockerWorkingDir` or `/cromwell-executions/<workflow_uuid>/call-<call_name>/execution` if this attribute is not supplied. ### Configuring. Configuring the TES backend is straightforward; one must only provide the TES API endpoint for the service. ```hocon; backend {; default = ""TES""; providers {; TES {; actor-factory = ""cromwell.backend.impl.tes.TesBackendLifecycleActorFactory""; config {; endpoint = ""https://<some-url>/v1/tasks""; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions""; concurrent-job-limit = 1000; }; }; }; }; ```. Cromwell uses an exponential backoff strategy to control the rate of requests sent to the TES server. ; Users can override the default settings for this behavior in config, see `cromwell.example.backends/TES.conf` ; for full expected format. For example, to force a constant polling rate rather than exponential backoff:; ```hocon; backend.providers.TES.config.poll-backoff {; min: ""1 minute""; max: ""1 minute""; multiplier: 1; randomization-factor: 0; }; ```. ### Supported File Systems. Currently this backend only works with files on a Local or Shared File System. ### Docker. This backend supports the following optional [Runtime Attributes](../RuntimeAttributes) and [Workflow Options](../wf_options/Overview/) for working with Docker:. * `docker`: Docker image to use such as ""Ubuntu"".; * `dockerWorkingDir`: defines the working directory in the container. ### CPU, Memory and Disk. This backend ",MatchSource.DOCS,docs/backends/TES.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/TES.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md:619,Availability,alive,alive,619,"*This sample configuration is a community contribution and therefore not officially supported.*. The following configuration can be used as a base to allow Cromwell to interact with a [volcano](https://www.github.com/volcano-sh/volcano) cluster and dispatch jobs to it:. ```hocon; Volcano {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; vcctl job run -f ${script}; """"""; kill = ""vcctl job delete -N ${job_id}""; check-alive = ""vcctl job view -N ${job_id}""; job-id-regex = ""(\\d+)""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout). If you have any questions about Volcano, please open an issue at; https://www.github.com/volcano-sh/volcano/issues; or contact us at; Slack Channel : https://volcano-sh.slack.com; Mailing List : https://groups.google.com/forum/#!forum/volcano-sh; otherwise, feel free to mail to : klaus1982.cn@gmail.com; ",MatchSource.DOCS,docs/backends/Volcano.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md:13,Deployability,configurat,configuration,13,"*This sample configuration is a community contribution and therefore not officially supported.*. The following configuration can be used as a base to allow Cromwell to interact with a [volcano](https://www.github.com/volcano-sh/volcano) cluster and dispatch jobs to it:. ```hocon; Volcano {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; vcctl job run -f ${script}; """"""; kill = ""vcctl job delete -N ${job_id}""; check-alive = ""vcctl job view -N ${job_id}""; job-id-regex = ""(\\d+)""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout). If you have any questions about Volcano, please open an issue at; https://www.github.com/volcano-sh/volcano/issues; or contact us at; Slack Channel : https://volcano-sh.slack.com; Mailing List : https://groups.google.com/forum/#!forum/volcano-sh; otherwise, feel free to mail to : klaus1982.cn@gmail.com; ",MatchSource.DOCS,docs/backends/Volcano.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md:111,Deployability,configurat,configuration,111,"*This sample configuration is a community contribution and therefore not officially supported.*. The following configuration can be used as a base to allow Cromwell to interact with a [volcano](https://www.github.com/volcano-sh/volcano) cluster and dispatch jobs to it:. ```hocon; Volcano {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; vcctl job run -f ${script}; """"""; kill = ""vcctl job delete -N ${job_id}""; check-alive = ""vcctl job view -N ${job_id}""; job-id-regex = ""(\\d+)""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout). If you have any questions about Volcano, please open an issue at; https://www.github.com/volcano-sh/volcano/issues; or contact us at; Slack Channel : https://volcano-sh.slack.com; Mailing List : https://groups.google.com/forum/#!forum/volcano-sh; otherwise, feel free to mail to : klaus1982.cn@gmail.com; ",MatchSource.DOCS,docs/backends/Volcano.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md:13,Modifiability,config,configuration,13,"*This sample configuration is a community contribution and therefore not officially supported.*. The following configuration can be used as a base to allow Cromwell to interact with a [volcano](https://www.github.com/volcano-sh/volcano) cluster and dispatch jobs to it:. ```hocon; Volcano {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; vcctl job run -f ${script}; """"""; kill = ""vcctl job delete -N ${job_id}""; check-alive = ""vcctl job view -N ${job_id}""; job-id-regex = ""(\\d+)""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout). If you have any questions about Volcano, please open an issue at; https://www.github.com/volcano-sh/volcano/issues; or contact us at; Slack Channel : https://volcano-sh.slack.com; Mailing List : https://groups.google.com/forum/#!forum/volcano-sh; otherwise, feel free to mail to : klaus1982.cn@gmail.com; ",MatchSource.DOCS,docs/backends/Volcano.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md:111,Modifiability,config,configuration,111,"*This sample configuration is a community contribution and therefore not officially supported.*. The following configuration can be used as a base to allow Cromwell to interact with a [volcano](https://www.github.com/volcano-sh/volcano) cluster and dispatch jobs to it:. ```hocon; Volcano {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; vcctl job run -f ${script}; """"""; kill = ""vcctl job delete -N ${job_id}""; check-alive = ""vcctl job view -N ${job_id}""; job-id-regex = ""(\\d+)""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout). If you have any questions about Volcano, please open an issue at; https://www.github.com/volcano-sh/volcano/issues; or contact us at; Slack Channel : https://volcano-sh.slack.com; Mailing List : https://groups.google.com/forum/#!forum/volcano-sh; otherwise, feel free to mail to : klaus1982.cn@gmail.com; ",MatchSource.DOCS,docs/backends/Volcano.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md:335,Modifiability,config,config,335,"*This sample configuration is a community contribution and therefore not officially supported.*. The following configuration can be used as a base to allow Cromwell to interact with a [volcano](https://www.github.com/volcano-sh/volcano) cluster and dispatch jobs to it:. ```hocon; Volcano {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; vcctl job run -f ${script}; """"""; kill = ""vcctl job delete -N ${job_id}""; check-alive = ""vcctl job view -N ${job_id}""; job-id-regex = ""(\\d+)""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout). If you have any questions about Volcano, please open an issue at; https://www.github.com/volcano-sh/volcano/issues; or contact us at; Slack Channel : https://volcano-sh.slack.com; Mailing List : https://groups.google.com/forum/#!forum/volcano-sh; otherwise, feel free to mail to : klaus1982.cn@gmail.com; ",MatchSource.DOCS,docs/backends/Volcano.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md:379,Modifiability,config,config,379,"*This sample configuration is a community contribution and therefore not officially supported.*. The following configuration can be used as a base to allow Cromwell to interact with a [volcano](https://www.github.com/volcano-sh/volcano) cluster and dispatch jobs to it:. ```hocon; Volcano {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; vcctl job run -f ${script}; """"""; kill = ""vcctl job delete -N ${job_id}""; check-alive = ""vcctl job view -N ${job_id}""; job-id-regex = ""(\\d+)""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout). If you have any questions about Volcano, please open an issue at; https://www.github.com/volcano-sh/volcano/issues; or contact us at; Slack Channel : https://volcano-sh.slack.com; Mailing List : https://groups.google.com/forum/#!forum/volcano-sh; otherwise, feel free to mail to : klaus1982.cn@gmail.com; ",MatchSource.DOCS,docs/backends/Volcano.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md:728,Modifiability,config,configure,728,"*This sample configuration is a community contribution and therefore not officially supported.*. The following configuration can be used as a base to allow Cromwell to interact with a [volcano](https://www.github.com/volcano-sh/volcano) cluster and dispatch jobs to it:. ```hocon; Volcano {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; vcctl job run -f ${script}; """"""; kill = ""vcctl job delete -N ${job_id}""; check-alive = ""vcctl job view -N ${job_id}""; job-id-regex = ""(\\d+)""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout). If you have any questions about Volcano, please open an issue at; https://www.github.com/volcano-sh/volcano/issues; or contact us at; Slack Channel : https://volcano-sh.slack.com; Mailing List : https://groups.google.com/forum/#!forum/volcano-sh; otherwise, feel free to mail to : klaus1982.cn@gmail.com; ",MatchSource.DOCS,docs/backends/Volcano.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md:504,Performance,queue,queue,504,"*This sample configuration is a community contribution and therefore not officially supported.*. The following configuration can be used as a base to allow Cromwell to interact with a [volcano](https://www.github.com/volcano-sh/volcano) cluster and dispatch jobs to it:. ```hocon; Volcano {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; vcctl job run -f ${script}; """"""; kill = ""vcctl job delete -N ${job_id}""; check-alive = ""vcctl job view -N ${job_id}""; job-id-regex = ""(\\d+)""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout). If you have any questions about Volcano, please open an issue at; https://www.github.com/volcano-sh/volcano/issues; or contact us at; Slack Channel : https://volcano-sh.slack.com; Mailing List : https://groups.google.com/forum/#!forum/volcano-sh; otherwise, feel free to mail to : klaus1982.cn@gmail.com; ",MatchSource.DOCS,docs/backends/Volcano.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md:860,Safety,timeout,timeout,860,"*This sample configuration is a community contribution and therefore not officially supported.*. The following configuration can be used as a base to allow Cromwell to interact with a [volcano](https://www.github.com/volcano-sh/volcano) cluster and dispatch jobs to it:. ```hocon; Volcano {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; vcctl job run -f ${script}; """"""; kill = ""vcctl job delete -N ${job_id}""; check-alive = ""vcctl job view -N ${job_id}""; job-id-regex = ""(\\d+)""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout). If you have any questions about Volcano, please open an issue at; https://www.github.com/volcano-sh/volcano/issues; or contact us at; Slack Channel : https://volcano-sh.slack.com; Mailing List : https://groups.google.com/forum/#!forum/volcano-sh; otherwise, feel free to mail to : klaus1982.cn@gmail.com; ",MatchSource.DOCS,docs/backends/Volcano.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md
https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md:883,Safety,timeout,timeout,883,"*This sample configuration is a community contribution and therefore not officially supported.*. The following configuration can be used as a base to allow Cromwell to interact with a [volcano](https://www.github.com/volcano-sh/volcano) cluster and dispatch jobs to it:. ```hocon; Volcano {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; vcctl job run -f ${script}; """"""; kill = ""vcctl job delete -N ${job_id}""; check-alive = ""vcctl job view -N ${job_id}""; job-id-regex = ""(\\d+)""; }; }; ```. For information on how to further configure it, take a look at the [Getting Started on HPC Clusters](../tutorials/HPCIntro). ### Exit code. See also [HPC - Exit code timeout](HPC#Exit-code-timeout). If you have any questions about Volcano, please open an issue at; https://www.github.com/volcano-sh/volcano/issues; or contact us at; Slack Channel : https://volcano-sh.slack.com; Mailing List : https://groups.google.com/forum/#!forum/volcano-sh; otherwise, feel free to mail to : klaus1982.cn@gmail.com; ",MatchSource.DOCS,docs/backends/Volcano.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/backends/Volcano.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2510,Availability,failure,failures,2510,"cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurr",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:4991,Availability,failure,failure,4991," Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. *",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:5256,Availability,error,errors,5256," specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; C",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:5574,Availability,failure,failures,5574,"hare a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7834,Availability,failure,failure,7834,"ice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7997,Availability,failure,failure,7997," a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when Call Caching is turned on in Cromwell, and could lead to unpredictable behaviors if a tag is updated in the middle of a workflow or even a scatter for exampl",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2361,Deployability,configurat,configuration,2361," was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; wo",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2546,Deployability,configurat,configuration,2546,"b's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3210,Deployability,configurat,configuration,3210,"es of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:4315,Deployability,configurat,configuration,4315,"roup.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:8930,Deployability,update,updated,8930,"revious result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when Call Caching is turned on in Cromwell, and could lead to unpredictable behaviors if a tag is updated in the middle of a workflow or even a scatter for example. In order to ensure perfect reproducibility, Docker provides another way of identifying an image version by using the specific digest of the image, which is an immutable identifier. The digest is guaranteed to be different if 2 images have different byte content. For more information see [Docker's api specs](https://docs.docker.com/registry/spec/api/#/content-digests).; A docker image can be referenced using the digest (e.g. `ubuntu@sha256:71cd81252a3563a03ad8daee81047b62ab5d892ebbfbf71cf53415f29c130950`).; This image refers to a specific image of ubuntu that does not depend on a floating tag.; A workflow containing this Docker image run now and a year from now will run in the exact same container. However, in order to remove unpredictable behaviors, when Cromwell finds a job ready to be run, it will first look at its docker runtime attribute, and apply the following logic:. * If the job d",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:11604,Deployability,install,installed,11604,"even if Cromwell is restarted when the workflow is running.; * If Cromwell fails to lookup the digest (for instance an unsupported docker registry, wrong credentials, ...) it will run the job with the user provided floating tag.; * The actual docker image (floating tag or digest) used for the job will be reported in the `dockerImageUsed` attribute of the call metadata. **Docker Lookup**. Cromwell provides two methods to lookup a Docker hash from a Docker tag:. * _Local_ ; In this mode, Cromwell will first attempt to find the image on the local machine where it's running using the `docker` CLI. If the image is locally present, then its digest will be used.; If the image is not present locally, Cromwell will execute a `docker pull` to try and retrieve it. If this succeeds, the newly retrieved digest will be used. Otherwise the lookup will be considered failed.; Note that Cromwell runs the `docker` CLI the same way a human would. This means two things:; * The machine Cromwell is running on needs to have Docker installed and a Docker daemon running.; * The current `docker` CLI credentials on that machine will be used to pull the image.; ; * _Remote_ ; In this mode, Cromwell will attempt to retrieve the hash by contacting the remote docker registry where the image is stored. This currently supports Docker Hub and GCR.; ; Docker registry and access levels supported by Cromwell for docker digest lookup in ""remote"" mode:; ; <!-- Pasted into then regenerated at https://www.tablesgenerator.com/markdown_tables -->. | | DockerHub | DockerHub | GCR | GCR | ECR | ECR | ACR | ACR |; |:-------------:|:---------:|:---------:|:------:|:-------:|:------:|:-------:|:------:|:-------:|; | | Public | Private | Public | Private | Public | Private | Public | Private |; | Pipelines API | X | X | X | X | | | | |; | AWS Batch | X | | X | | | | | |; | Other | X | | X | | | | | |. <!-- Pasted then regenerated at https://www.tablesgenerator.com/markdown_tables -->. **Runtime Attributes**. As well",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7942,Energy Efficiency,reduce,reduce,7942," a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when Call Caching is turned on in Cromwell, and could lead to unpredictable behaviors if a tag is updated in the middle of a workflow or even a scatter for exampl",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:1617,Integrability,depend,depending,1617," MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configur",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:9571,Integrability,depend,depend,9571,"nt in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when Call Caching is turned on in Cromwell, and could lead to unpredictable behaviors if a tag is updated in the middle of a workflow or even a scatter for example. In order to ensure perfect reproducibility, Docker provides another way of identifying an image version by using the specific digest of the image, which is an immutable identifier. The digest is guaranteed to be different if 2 images have different byte content. For more information see [Docker's api specs](https://docs.docker.com/registry/spec/api/#/content-digests).; A docker image can be referenced using the digest (e.g. `ubuntu@sha256:71cd81252a3563a03ad8daee81047b62ab5d892ebbfbf71cf53415f29c130950`).; This image refers to a specific image of ubuntu that does not depend on a floating tag.; A workflow containing this Docker image run now and a year from now will run in the exact same container. However, in order to remove unpredictable behaviors, when Cromwell finds a job ready to be run, it will first look at its docker runtime attribute, and apply the following logic:. * If the job doesn't specify a docker image it will be dispatched and all call caching settings (read/write) will apply normally.; * If the job does specify a docker runtime attribute, then:; * if the docker image uses a hash, all call caching settings apply normally; * if the docker image uses a floating tag:; * Cromwell will attempt to look up the immutable digest of the image with this floating tag. Upon success it will pass both the floating tag and this digest value to the backend.; * All backends currently included with Cromwell will utilize this digest value to run the job.; * Within a single workflow, all floating tags within a given workflow will reso",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:520,Modifiability,config,configure,520,"Call Caching allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. Cromwell searches the cache of previously run jobs for one that has the exact same command and exact same inputs. If a previously run job is found in the cache, Cromwell will use the results of the previous job instead of re-running it. Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs,; it is best to configure Cromwell to [point to a MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additiona",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2361,Modifiability,config,configuration,2361," was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; wo",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2546,Modifiability,config,configuration,2546,"b's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3210,Modifiability,config,configuration,3210,"es of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:4008,Modifiability,config,configured,4008,"or permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:4315,Modifiability,config,configuration,4315,"roup.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:172,Performance,cache,cache,172,"Call Caching allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. Cromwell searches the cache of previously run jobs for one that has the exact same command and exact same inputs. If a previously run job is found in the cache, Cromwell will use the results of the previous job instead of re-running it. Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs,; it is best to configure Cromwell to [point to a MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additiona",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:304,Performance,cache,cache,304,"Call Caching allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. Cromwell searches the cache of previously run jobs for one that has the exact same command and exact same inputs. If a previously run job is found in the cache, Cromwell will use the results of the previous job instead of re-running it. Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs,; it is best to configure Cromwell to [point to a MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additiona",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:403,Performance,cache,cache,403,"Call Caching allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. Cromwell searches the cache of previously run jobs for one that has the exact same command and exact same inputs. If a previously run job is found in the cache, Cromwell will use the results of the previous job instead of re-running it. Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs,; it is best to configure Cromwell to [point to a MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additiona",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:1287,Performance,cache,cache,1287,"un job is found in the cache, Cromwell will use the results of the previous job instead of re-running it. Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs,; it is best to configure Cromwell to [point to a MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files mu",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:1350,Performance,cache,cache,1350,"ead of re-running it. Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs,; it is best to configure Cromwell to [point to a MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the conf",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:1440,Performance,cache,cache,1440," in its database. In order for call caching to be used on any previously run jobs,; it is best to configure Cromwell to [point to a MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call ca",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:1465,Performance,cache,cache,1465," MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configur",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:1522,Performance,cache,cached,1522," MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configur",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:1604,Performance,cache,cached,1604," MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configur",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:1853,Performance,cache,cache,1853," [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2162,Performance,cache,cache,2162," following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in con",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2400,Performance,cache,cache,2400,"nce finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 100",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2421,Performance,cache,cache,2421,"cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurr",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2482,Performance,cache,cache,2482,"cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurr",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2527,Performance,cache,cache,2527,"b's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2747,Performance,cache,cache-results,2747,"ful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2780,Performance,cache,cache,2780,"or any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum n",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2818,Performance,cache,cache,2818,"or any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum n",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2848,Performance,cache,cache,2848,"does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's bla",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2875,Performance,cache,cache,2875,"does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's bla",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2895,Performance,cache,cache,2895,"does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's bla",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2926,Performance,cache,cache,2926,"does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's bla",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3110,Performance,cache,caches,3110," Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3362,Performance,cache,cache-blacklist-group,3362,"on `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.work",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3385,Performance,concurren,concurrency,3385,"on `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.work",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3453,Performance,cache,cache,3453," offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. `",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3459,Performance,concurren,concurrency,3459," offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. `",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3473,Performance,concurren,concurrency,3473," Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3519,Performance,cache,cache,3519," Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3622,Performance,cache,cache,3622,"d = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""proj",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3661,Performance,cache,cache,3661,"so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3667,Performance,concurren,concurrency,3667,"so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3681,Performance,concurren,concurrency,3681," ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performa",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3727,Performance,cache,cache,3727," ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performa",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3830,Performance,cache,cache,3830,"blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3880,Performance,cache,cache,3880,"its based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3933,Performance,cache,caches,3933,"or permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:4041,Performance,cache,cache,4041,"or permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:4281,Performance,cache,cache,4281,"iguration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (G",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:4532,Performance,cache,cache,4532,"of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache resu",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:4637,Performance,cache,caches,4637,"ency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix,",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:4722,Performance,perform,performance,4722,"ency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix,",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:4925,Performance,cache,cache,4925," Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. *",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:5016,Performance,cache,cache,5016," Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. *",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:5124,Performance,perform,performance,5124,"listing.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:5310,Performance,cache,cache,5310,"isting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAP",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:5454,Performance,cache,cache,5454,"hare a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:5542,Performance,cache,cache,5542,"hare a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:5638,Performance,cache,cache,5638,"klisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of strings representing ; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming A",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:5994,Performance,cache,cache,5994,"ing call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of strings representing ; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming Alice and Bob have; made their data accessible to each other, Alice could submit a workflow with these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" b",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:6143,Performance,cache,cache,6143,"ing call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of strings representing ; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming Alice and Bob have; made their data accessible to each other, Alice could submit a workflow with these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" b",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:6212,Performance,cache,cache,6212,"ing call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of strings representing ; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming Alice and Bob have; made their data accessible to each other, Alice could submit a workflow with these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" b",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:6558,Performance,cache,cache,6558," cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of strings representing ; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming Alice and Bob have; made their data accessible to each other, Alice could submit a workflow with these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:6624,Performance,cache,cache,6624," cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of strings representing ; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming Alice and Bob have; made their data accessible to each other, Alice could submit a workflow with these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of t",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:6935,Performance,cache,cache,6935,"is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of strings representing ; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming Alice and Bob have; made their data accessible to each other, Alice could submit a workflow with these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be log",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7005,Performance,optimiz,optimization,7005,"be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of strings representing ; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming Alice and Bob have; made their data accessible to each other, Alice could submit a workflow with these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7322,Performance,cache,cache,7322,"ache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of strings representing ; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming Alice and Bob have; made their data accessible to each other, Alice could submit a workflow with these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching perfor",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7520,Performance,cache,cache,7520,"; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming Alice and Bob have; made their data accessible to each other, Alice could submit a workflow with these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7789,Performance,cache,cache,7789," these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7828,Performance,cache,cache,7828,"ice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7877,Performance,cache,cache,7877,"ow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when Call Caching is turned on in Cromwell, and could lead to un",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:8256,Performance,perform,performance,8256,"on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when Call Caching is turned on in Cromwell, and could lead to unpredictable behaviors if a tag is updated in the middle of a workflow or even a scatter for example. In order to ensure perfect reproducibility, Docker provides another way of identifying an image version by using the specific digest of the image, which is an immutable identifier. The digest is guaranteed to be different if 2 images have diff",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:12755,Performance,cache,cache,12755," succeeds, the newly retrieved digest will be used. Otherwise the lookup will be considered failed.; Note that Cromwell runs the `docker` CLI the same way a human would. This means two things:; * The machine Cromwell is running on needs to have Docker installed and a Docker daemon running.; * The current `docker` CLI credentials on that machine will be used to pull the image.; ; * _Remote_ ; In this mode, Cromwell will attempt to retrieve the hash by contacting the remote docker registry where the image is stored. This currently supports Docker Hub and GCR.; ; Docker registry and access levels supported by Cromwell for docker digest lookup in ""remote"" mode:; ; <!-- Pasted into then regenerated at https://www.tablesgenerator.com/markdown_tables -->. | | DockerHub | DockerHub | GCR | GCR | ECR | ECR | ACR | ACR |; |:-------------:|:---------:|:---------:|:------:|:-------:|:------:|:-------:|:------:|:-------:|; | | Public | Private | Public | Private | Public | Private | Public | Private |; | Pipelines API | X | X | X | X | | | | |; | AWS Batch | X | | X | | | | | |; | Other | X | | X | | | | | |. <!-- Pasted then regenerated at https://www.tablesgenerator.com/markdown_tables -->. **Runtime Attributes**. As well as call inputs and the command to run, call caching considers the following [runtime; attributes](../RuntimeAttributes.md) of a given task when determining whether to call cache:. * [`ContinueOnReturnCode`](../RuntimeAttributes.md#continueonreturncode); * [`Docker`](../RuntimeAttributes.md#docker); * [`FailOnStderr`](../RuntimeAttributes.md#failonstderr). If any of these attributes have changed from a previous instance of the same task, that instance will not be call-cached; from. Other runtime attributes, including [`memory`](../RuntimeAttributes.md#memory),; [`cpu`](../RuntimeAttributes.md#cpu), and [`disks`](../RuntimeAttributes.md#disks), are not considered by call caching; and therefore may be changed without preventing a cached result from being used.; ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:13055,Performance,cache,cached,13055," succeeds, the newly retrieved digest will be used. Otherwise the lookup will be considered failed.; Note that Cromwell runs the `docker` CLI the same way a human would. This means two things:; * The machine Cromwell is running on needs to have Docker installed and a Docker daemon running.; * The current `docker` CLI credentials on that machine will be used to pull the image.; ; * _Remote_ ; In this mode, Cromwell will attempt to retrieve the hash by contacting the remote docker registry where the image is stored. This currently supports Docker Hub and GCR.; ; Docker registry and access levels supported by Cromwell for docker digest lookup in ""remote"" mode:; ; <!-- Pasted into then regenerated at https://www.tablesgenerator.com/markdown_tables -->. | | DockerHub | DockerHub | GCR | GCR | ECR | ECR | ACR | ACR |; |:-------------:|:---------:|:---------:|:------:|:-------:|:------:|:-------:|:------:|:-------:|; | | Public | Private | Public | Private | Public | Private | Public | Private |; | Pipelines API | X | X | X | X | | | | |; | AWS Batch | X | | X | | | | | |; | Other | X | | X | | | | | |. <!-- Pasted then regenerated at https://www.tablesgenerator.com/markdown_tables -->. **Runtime Attributes**. As well as call inputs and the command to run, call caching considers the following [runtime; attributes](../RuntimeAttributes.md) of a given task when determining whether to call cache:. * [`ContinueOnReturnCode`](../RuntimeAttributes.md#continueonreturncode); * [`Docker`](../RuntimeAttributes.md#docker); * [`FailOnStderr`](../RuntimeAttributes.md#failonstderr). If any of these attributes have changed from a previous instance of the same task, that instance will not be call-cached; from. Other runtime attributes, including [`memory`](../RuntimeAttributes.md#memory),; [`cpu`](../RuntimeAttributes.md#cpu), and [`disks`](../RuntimeAttributes.md#disks), are not considered by call caching; and therefore may be changed without preventing a cached result from being used.; ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:13320,Performance,cache,cached,13320," succeeds, the newly retrieved digest will be used. Otherwise the lookup will be considered failed.; Note that Cromwell runs the `docker` CLI the same way a human would. This means two things:; * The machine Cromwell is running on needs to have Docker installed and a Docker daemon running.; * The current `docker` CLI credentials on that machine will be used to pull the image.; ; * _Remote_ ; In this mode, Cromwell will attempt to retrieve the hash by contacting the remote docker registry where the image is stored. This currently supports Docker Hub and GCR.; ; Docker registry and access levels supported by Cromwell for docker digest lookup in ""remote"" mode:; ; <!-- Pasted into then regenerated at https://www.tablesgenerator.com/markdown_tables -->. | | DockerHub | DockerHub | GCR | GCR | ECR | ECR | ACR | ACR |; |:-------------:|:---------:|:---------:|:------:|:-------:|:------:|:-------:|:------:|:-------:|; | | Public | Private | Public | Private | Public | Private | Public | Private |; | Pipelines API | X | X | X | X | | | | |; | AWS Batch | X | | X | | | | | |; | Other | X | | X | | | | | |. <!-- Pasted then regenerated at https://www.tablesgenerator.com/markdown_tables -->. **Runtime Attributes**. As well as call inputs and the command to run, call caching considers the following [runtime; attributes](../RuntimeAttributes.md) of a given task when determining whether to call cache:. * [`ContinueOnReturnCode`](../RuntimeAttributes.md#continueonreturncode); * [`Docker`](../RuntimeAttributes.md#docker); * [`FailOnStderr`](../RuntimeAttributes.md#failonstderr). If any of these attributes have changed from a previous instance of the same task, that instance will not be call-cached; from. Other runtime attributes, including [`memory`](../RuntimeAttributes.md#memory),; [`cpu`](../RuntimeAttributes.md#cpu), and [`disks`](../RuntimeAttributes.md#disks), are not considered by call caching; and therefore may be changed without preventing a cached result from being used.; ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:32,Safety,detect,detect,32,"Call Caching allows Cromwell to detect when a job has been run in the past so that it doesn't have to re-compute results, saving both time and money. Cromwell searches the cache of previously run jobs for one that has the exact same command and exact same inputs. If a previously run job is found in the cache, Cromwell will use the results of the previous job instead of re-running it. Cromwell's call cache is maintained in its database. In order for call caching to be used on any previously run jobs,; it is best to configure Cromwell to [point to a MySQL database](../Configuring.md#database) instead of the default; in-memory database. This way any invocation of Cromwell (either with `run` or `server` subcommands) will be able to; utilize results from all calls that are in that database. **Configuring Call Caching**. *Call Caching is disabled by default.* Call Caching can be enabled in your Cromwell; [Configuration](../Configuring.md#call-caching) and the behavior can be modified via; [Workflow Options](../wf_options/Overview.md). If you are adding Workflow options, do not set; [`read_from_cache` or `write_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additiona",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2115,Security,hash,hash,2115,"ite_to_cache`](../wf_options/Overview.md#call-caching-options) = false, as it will impact the; following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches sh",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2173,Security,hash,hashes,2173," following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in con",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2253,Security,hash,hashes,2253," following process. Once enabled, Cromwell by default will search the call cache for every `call` statement invocation. * If there was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in con",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2300,Security,hash,hash,2300," was no cache hit, the `call` will be executed as normal. Once finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; wo",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2395,Security,hash,hash-cache,2395,"nce finished it will add itself to the cache.; * If there was a cache hit, outputs are either **copied from the original cached job to the new job's output directory**; or **referenced from the original cached job** depending on the Cromwell; [Configuration](../Configuring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 100",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:2712,Security,authoriz,authorized,2712,"uring.md#call-caching) settings. > **Note:** If call caching is enabled, be careful not to change the contents of the output directory for any previously run job. Doing so might cause cache hits in Cromwell to copy over modified data and Cromwell currently does not check that the contents of the output directory changed. Additionally, if any files from a previous job directory are removed, call caching will fail due to missing files. ***File hash caching***. Cromwell offers the option to cache file hashes within the scope of a root workflow to prevent repeatedly requesting the hashes of the; same files multiple times. File hash caching is off by default and can be turned on with the configuration option `system.file-hash-cache=true`. ***Call cache blacklisting***; Cromwell offers the ability to filter cache hits based on copying failures. . Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache co",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3565,Security,access,access,3565," Call cache blacklisting configuration looks like:. ```; call-caching {. enabled = true. # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:3773,Security,access,access,3773," ; invalidate-bad-cache-results = false. blacklist-cache {; # The call caching blacklist cache is off by default. This cache is used to blacklist cache hits based on cache; # hit ids or buckets of cache hit paths that Cromwell has previously failed to copy for permissions reasons.; enabled: true. # All blacklisting values below are optional. In order to use groupings (blacklist caches shared among root; # workflows) a value must be specified for `groupings.workflow-option` in configuration and the workflows to; # be grouped must be submitted with workflow options specifying the same group.; groupings {; workflow-option: call-cache-blacklist-group; concurrency: 10000; ttl: 2 hours; size: 1000; }. buckets {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 1000; }. hits {; # Guava cache concurrency.; concurrency: 10000; # How long entries in the cache should live from the time of their last access.; ttl: 1 hour; # Maximum number of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performa",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:4783,Security,authoriz,authorization,4783,"of entries in the cache.; size: 20000; }; }; }; ```. **** Blacklist cache grouping ****. By default Cromwell's blacklist caches work at the granularity of root workflows, but Cromwell can also be configured to; share a blacklist cache among a group of workflows. ; If a value is specified for `call-caching.blacklisting.groupings.workflow-option` and a workflow option is specified; having a matching key, all workflows specifying the same value will share a blacklist cache. . For example, if Cromwell configuration contains `call-caching.blacklisting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:5392,Security,authoriz,authorized,5392,"isting.groupings.workflow-option = ""project""` and; a workflow is submitted with the options. ```json; {; ""project"": ""Mary""; }; ```. then this workflow will share a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAP",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:5560,Security,authoriz,authorization,5560,"hare a blacklist cache with any other workflows whose workflow options contain `""project"": ""Mary""`. Grouping of blacklist caches can significantly improve blacklisting effectiveness and overall call caching performance.; Workflows should be grouped by their effective authorization to ensure the same filesystem/object store permissions; exist for every workflow in the group. **** Hit blacklisting ****. If a cache hit fails copying for any reason, Cromwell will record that failure in the blacklist cache and will not use; the hit again. Hit blacklisting is particularly effective at improving call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:6053,Security,access,access,6053,"ing call caching performance in conjunction with the ; grouping feature described above. **** Path prefix (GCS bucket) blacklisting on 403 Forbidden errors ****. In a multi-user environment user A might cache hit to one of user B's results; but that doesn't necessarily mean user A is authorized to read user B's outputs from the filesystem. Call cache blacklisting; allows Cromwell to record which file path prefixes were involved in cache result copy authorization failures.; If Cromwell sees that the file paths for a candidate cache hit have a blacklisted prefix, Cromwell will quickly ; fail the copy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of strings representing ; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming Alice and Bob have; made their data accessible to each other, Alice could submit a workflow with these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" b",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:6709,Security,access,accessible,6709,"opy attempt without doing any potentially expensive I/O. Path prefix blacklisting could be supported by any backend type though it is currently implemented only for Google; (PAPI) backends. For Google backends the GCS bucket is considered the prefix for blacklisting purposes. ***Call cache whitelisting***; ; In a multi-user environment where access to job outputs may be restricted among different users, it can be useful to limit; cache hits to those that are more likely to actually be readable for cache hit copies.; Cromwell now supports a `call_cache_hit_path_prefixes` workflow option for this purpose. This is particularly useful in the PAPI backend where the workflow; root can be specified in workflow options via `jes_gcs_root`. The value of `call_cache_hit_path_prefixes` should be an array of strings representing ; prefixes that call cache hit output files should have in order to be considered as a cache hit. Using PAPI as an example and assuming Alice and Bob have; made their data accessible to each other, Alice could submit a workflow with these options:. ```; {; ""call_cache_hit_path_prefixes"": [ ""gs://alice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_p",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:10105,Security,hash,hash,10105,"s an immutable identifier. The digest is guaranteed to be different if 2 images have different byte content. For more information see [Docker's api specs](https://docs.docker.com/registry/spec/api/#/content-digests).; A docker image can be referenced using the digest (e.g. `ubuntu@sha256:71cd81252a3563a03ad8daee81047b62ab5d892ebbfbf71cf53415f29c130950`).; This image refers to a specific image of ubuntu that does not depend on a floating tag.; A workflow containing this Docker image run now and a year from now will run in the exact same container. However, in order to remove unpredictable behaviors, when Cromwell finds a job ready to be run, it will first look at its docker runtime attribute, and apply the following logic:. * If the job doesn't specify a docker image it will be dispatched and all call caching settings (read/write) will apply normally.; * If the job does specify a docker runtime attribute, then:; * if the docker image uses a hash, all call caching settings apply normally; * if the docker image uses a floating tag:; * Cromwell will attempt to look up the immutable digest of the image with this floating tag. Upon success it will pass both the floating tag and this digest value to the backend.; * All backends currently included with Cromwell will utilize this digest value to run the job.; * Within a single workflow, all floating tags within a given workflow will resolve to the same digest value even if Cromwell is restarted when the workflow is running.; * If Cromwell fails to lookup the digest (for instance an unsupported docker registry, wrong credentials, ...) it will run the job with the user provided floating tag.; * The actual docker image (floating tag or digest) used for the job will be reported in the `dockerImageUsed` attribute of the call metadata. **Docker Lookup**. Cromwell provides two methods to lookup a Docker hash from a Docker tag:. * _Local_ ; In this mode, Cromwell will first attempt to find the image on the local machine where it's r",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:11021,Security,hash,hash,11021,"ally.; * If the job does specify a docker runtime attribute, then:; * if the docker image uses a hash, all call caching settings apply normally; * if the docker image uses a floating tag:; * Cromwell will attempt to look up the immutable digest of the image with this floating tag. Upon success it will pass both the floating tag and this digest value to the backend.; * All backends currently included with Cromwell will utilize this digest value to run the job.; * Within a single workflow, all floating tags within a given workflow will resolve to the same digest value even if Cromwell is restarted when the workflow is running.; * If Cromwell fails to lookup the digest (for instance an unsupported docker registry, wrong credentials, ...) it will run the job with the user provided floating tag.; * The actual docker image (floating tag or digest) used for the job will be reported in the `dockerImageUsed` attribute of the call metadata. **Docker Lookup**. Cromwell provides two methods to lookup a Docker hash from a Docker tag:. * _Local_ ; In this mode, Cromwell will first attempt to find the image on the local machine where it's running using the `docker` CLI. If the image is locally present, then its digest will be used.; If the image is not present locally, Cromwell will execute a `docker pull` to try and retrieve it. If this succeeds, the newly retrieved digest will be used. Otherwise the lookup will be considered failed.; Note that Cromwell runs the `docker` CLI the same way a human would. This means two things:; * The machine Cromwell is running on needs to have Docker installed and a Docker daemon running.; * The current `docker` CLI credentials on that machine will be used to pull the image.; ; * _Remote_ ; In this mode, Cromwell will attempt to retrieve the hash by contacting the remote docker registry where the image is stored. This currently supports Docker Hub and GCR.; ; Docker registry and access levels supported by Cromwell for docker digest lookup in ""remo",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:11799,Security,hash,hash,11799,"ting tag.; * The actual docker image (floating tag or digest) used for the job will be reported in the `dockerImageUsed` attribute of the call metadata. **Docker Lookup**. Cromwell provides two methods to lookup a Docker hash from a Docker tag:. * _Local_ ; In this mode, Cromwell will first attempt to find the image on the local machine where it's running using the `docker` CLI. If the image is locally present, then its digest will be used.; If the image is not present locally, Cromwell will execute a `docker pull` to try and retrieve it. If this succeeds, the newly retrieved digest will be used. Otherwise the lookup will be considered failed.; Note that Cromwell runs the `docker` CLI the same way a human would. This means two things:; * The machine Cromwell is running on needs to have Docker installed and a Docker daemon running.; * The current `docker` CLI credentials on that machine will be used to pull the image.; ; * _Remote_ ; In this mode, Cromwell will attempt to retrieve the hash by contacting the remote docker registry where the image is stored. This currently supports Docker Hub and GCR.; ; Docker registry and access levels supported by Cromwell for docker digest lookup in ""remote"" mode:; ; <!-- Pasted into then regenerated at https://www.tablesgenerator.com/markdown_tables -->. | | DockerHub | DockerHub | GCR | GCR | ECR | ECR | ACR | ACR |; |:-------------:|:---------:|:---------:|:------:|:-------:|:------:|:-------:|:------:|:-------:|; | | Public | Private | Public | Private | Public | Private | Public | Private |; | Pipelines API | X | X | X | X | | | | |; | AWS Batch | X | | X | | | | | |; | Other | X | | X | | | | | |. <!-- Pasted then regenerated at https://www.tablesgenerator.com/markdown_tables -->. **Runtime Attributes**. As well as call inputs and the command to run, call caching considers the following [runtime; attributes](../RuntimeAttributes.md) of a given task when determining whether to call cache:. * [`ContinueOnReturnCode`](../RuntimeA",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:11939,Security,access,access,11939,"o methods to lookup a Docker hash from a Docker tag:. * _Local_ ; In this mode, Cromwell will first attempt to find the image on the local machine where it's running using the `docker` CLI. If the image is locally present, then its digest will be used.; If the image is not present locally, Cromwell will execute a `docker pull` to try and retrieve it. If this succeeds, the newly retrieved digest will be used. Otherwise the lookup will be considered failed.; Note that Cromwell runs the `docker` CLI the same way a human would. This means two things:; * The machine Cromwell is running on needs to have Docker installed and a Docker daemon running.; * The current `docker` CLI credentials on that machine will be used to pull the image.; ; * _Remote_ ; In this mode, Cromwell will attempt to retrieve the hash by contacting the remote docker registry where the image is stored. This currently supports Docker Hub and GCR.; ; Docker registry and access levels supported by Cromwell for docker digest lookup in ""remote"" mode:; ; <!-- Pasted into then regenerated at https://www.tablesgenerator.com/markdown_tables -->. | | DockerHub | DockerHub | GCR | GCR | ECR | ECR | ACR | ACR |; |:-------------:|:---------:|:---------:|:------:|:-------:|:------:|:-------:|:------:|:-------:|; | | Public | Private | Public | Private | Public | Private | Public | Private |; | Pipelines API | X | X | X | X | | | | |; | AWS Batch | X | | X | | | | | |; | Other | X | | X | | | | | |. <!-- Pasted then regenerated at https://www.tablesgenerator.com/markdown_tables -->. **Runtime Attributes**. As well as call inputs and the command to run, call caching considers the following [runtime; attributes](../RuntimeAttributes.md) of a given task when determining whether to call cache:. * [`ContinueOnReturnCode`](../RuntimeAttributes.md#continueonreturncode); * [`Docker`](../RuntimeAttributes.md#docker); * [`FailOnStderr`](../RuntimeAttributes.md#failonstderr). If any of these attributes have changed from a previ",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7842,Testability,log,logging,7842,"ice_bucket"", ""gs://bob_bucket"" ]; }; ```. With these workflow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7931,Testability,log,logged,7931,"ow options Cromwell would only look for cache hits for Alice's jobs in Alice's or Bob's buckets. As a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when Call Caching is turned on in Cromwell, and could lead to un",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:7970,Testability,log,logs,7970," a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when Call Caching is turned on in Cromwell, and could lead to unpredictable behaviors if a tag is updated in the middle of a workflow or even a scatter for exampl",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:8021,Testability,log,logged,8021," a further optimization the PAPI backend has the concept of ""this"" bucket on a per-workflow basis, where ""this"" bucket is; the bucket that contains the current workflow root.; If `call_cache_hit_path_prefixes` is specified in ; workflow options on the PAPI backend, Cromwell will automatically prepend ""this"" bucket to the call cache hit path prefixes to search.; For example, if Charles specified the same workflow options as in the example above and his workflow root was under `gs://charles_bucket`,; Cromwell would search cache hits in all of the `gs://alice_bucket`, `gs://bob_bucket` and `gs://charles_bucket` buckets without having to specify; `gs://charles_bucket` bucket explicitly in `call_cache_hit_path_prefixes`. If no `call_cache_hit_path_prefixes` are specified then all matching cache hits will be considered. ***Call cache failure logging***. When Cromwell fails to cache a job from a previous result the reason will be logged. To reduce the verbosity of the logs; only the first three failure reasons will be logged per shard of each job. Cromwell will continue to try copying; previous results for the call, and when no candidates are left Cromwell will run the job on the backend. **Docker Tags**. Certain Docker tags can impact call caching performance. ; Docker tags are a convenient way to point to a version of an image (`ubuntu:14.04`), or even the latest version (`ubuntu:latest`).; For that purpose, tags are mutable, meaning that the image they point to can change, while the tag name stays the same.; While this is very convenient in some cases, using mutable, or ""floating"" tags in tasks affects the reproducibility of a workflow. ; If you were to run the same workflow using `ubuntu:latest` now, and again in a year (or even in a month) may run with different docker images.; This has an even bigger impact when Call Caching is turned on in Cromwell, and could lead to unpredictable behaviors if a tag is updated in the middle of a workflow or even a scatter for exampl",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md:9876,Testability,log,logic,9876,"mages.; This has an even bigger impact when Call Caching is turned on in Cromwell, and could lead to unpredictable behaviors if a tag is updated in the middle of a workflow or even a scatter for example. In order to ensure perfect reproducibility, Docker provides another way of identifying an image version by using the specific digest of the image, which is an immutable identifier. The digest is guaranteed to be different if 2 images have different byte content. For more information see [Docker's api specs](https://docs.docker.com/registry/spec/api/#/content-digests).; A docker image can be referenced using the digest (e.g. `ubuntu@sha256:71cd81252a3563a03ad8daee81047b62ab5d892ebbfbf71cf53415f29c130950`).; This image refers to a specific image of ubuntu that does not depend on a floating tag.; A workflow containing this Docker image run now and a year from now will run in the exact same container. However, in order to remove unpredictable behaviors, when Cromwell finds a job ready to be run, it will first look at its docker runtime attribute, and apply the following logic:. * If the job doesn't specify a docker image it will be dispatched and all call caching settings (read/write) will apply normally.; * If the job does specify a docker runtime attribute, then:; * if the docker image uses a hash, all call caching settings apply normally; * if the docker image uses a floating tag:; * Cromwell will attempt to look up the immutable digest of the image with this floating tag. Upon success it will pass both the floating tag and this digest value to the backend.; * All backends currently included with Cromwell will utilize this digest value to run the job.; * Within a single workflow, all floating tags within a given workflow will resolve to the same digest value even if Cromwell is restarted when the workflow is running.; * If Cromwell fails to lookup the digest (for instance an unsupported docker registry, wrong credentials, ...) it will run the job with the user provid",MatchSource.DOCS,docs/cromwell_features/CallCaching.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/CallCaching.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:1602,Availability,avail,available,1602,"ne workflow or group of workflows from locking out everyone else by ; introducing hog factors, hog groups and hog limits. This page describes what they are and how they work. ## Concepts. ### Hog Group . The Hog Group is a way of grouping workflow from different submissions together to restrain their overall resource ; usage as a whole. - Every top-level workflow is assigned to a hog group when Cromwell receives it. ; + Exactly how this happens is [configurable](#configuration).; - Every sub-workflow or call started by a workflow is associated with the same hog group as its parent workflow.; - Multiple top-level workflows can be assigned to the same hog group to allow them to be grouped together. Thus:. - Every workflow is assigned to a hog group when submitted.; - Every hog group may have many workflows assigned to it. ### Hog Factor. The hog factor is an integer greater than or equal to 1. It represents a trade-off between: . - Fully utilizing all resources available to Cromwell to complete jobs, for as long as there are jobs to be processed.; - Reserving resources for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration value",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:1096,Deployability,configurat,configuration,1096,"at its disposal. WDL workflows allow scattered tasks to be; run a huge number of times with very simple syntax.; This makes it easy for a very small number of workflows to hog all of the resources of Cromwell, forcing all other ; workflows (even a simple 'hello_world') to wait in line behind them. ; Sometimes that's exactly what you want, but often you would like Cromwell to remain responsive to new users' small ; workflows even while continuing to process production workflows from established users. Cromwell 35 provides new ways of stopping one workflow or group of workflows from locking out everyone else by ; introducing hog factors, hog groups and hog limits. This page describes what they are and how they work. ## Concepts. ### Hog Group . The Hog Group is a way of grouping workflow from different submissions together to restrain their overall resource ; usage as a whole. - Every top-level workflow is assigned to a hog group when Cromwell receives it. ; + Exactly how this happens is [configurable](#configuration).; - Every sub-workflow or call started by a workflow is associated with the same hog group as its parent workflow.; - Multiple top-level workflows can be assigned to the same hog group to allow them to be grouped together. Thus:. - Every workflow is assigned to a hog group when submitted.; - Every hog group may have many workflows assigned to it. ### Hog Factor. The hog factor is an integer greater than or equal to 1. It represents a trade-off between: . - Fully utilizing all resources available to Cromwell to complete jobs, for as long as there are jobs to be processed.; - Reserving resources for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th ",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:2610,Deployability,configurat,configuration,2610,"rces for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up wit",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:2693,Deployability,configurat,configuration,2693,"rces for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up wit",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:2953,Deployability,configurat,configuration,2953," be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflo",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:3308,Deployability,configurat,configuration,3308,"ed by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at thei",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4490,Deployability,configurat,configuration,4490,"s file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA1000000` was added before `jobD1",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:7385,Energy Efficiency,allocate,allocated,7385,"that Cromwell is currently only using 1/25th of its overall limit because its hog factor is 25. ##### Another hog group appears. - Now hog group B submits 1,000 workflows and between them they generate 200,000 jobs for PAPIv2.; - Even though 16,000 of group A's jobs are still queued, Cromwell starts 4,000 of group B's jobs immediately,; - The remaining 196,000 of group B's jobs are queued up waiting for group B's existing jobs to complete. ##### Where do we stand?. - Cromwell knows about 220,000 jobs that could be started; - Cromwell has an overall limit of 100,000; - Cromwell is running 8,000 jobs in two hog groups. *In other words, not so great - perhaps we should have set the hog factor lower...?*. #####But wait, more workflows appear... - Now another 23 hog groups (""C"" through ""Y"") submit workflows of a similar scale to hog group A.; - One by one, the workflows of each hog group fill up their share of the overall concurrent job limit.; - So Cromwell is now running 100,000 jobs and each hog group has been allocated 4,000 of those. ##### What about poor hog group ""Z""?. - A final group submits workflows under hog group ""Z"".; - Alas, even though hog group ""Z"" is not running anything yet, we cannot start their workflows because we're ; at the global maximum of 100,000. *In other words, perhaps we should have set the hog factor higher...?*. ##### So what now?. - As jobs in other hog group complete, we will begin to see hog group ""Z"" jobs started alongside new jobs from the ; other hog groups.; - Going forward Cromwell will start jobs from all groups at the same rate, even though hog group Z's jobs arrived; later than those from hog group A. Thus, over time, each group will approach approximately 1/26th of the total pool.; ; ## FAQs. #### Can I opt out of using hog groups?. Yes, to various degrees:. - No matter what, your workflows will be assigned to a hog group. ; - To opt out of reserving Cromwell's resources for new hog groups, leave the hog factor set to 1.; - To o",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:1081,Modifiability,config,configurable,1081,"at its disposal. WDL workflows allow scattered tasks to be; run a huge number of times with very simple syntax.; This makes it easy for a very small number of workflows to hog all of the resources of Cromwell, forcing all other ; workflows (even a simple 'hello_world') to wait in line behind them. ; Sometimes that's exactly what you want, but often you would like Cromwell to remain responsive to new users' small ; workflows even while continuing to process production workflows from established users. Cromwell 35 provides new ways of stopping one workflow or group of workflows from locking out everyone else by ; introducing hog factors, hog groups and hog limits. This page describes what they are and how they work. ## Concepts. ### Hog Group . The Hog Group is a way of grouping workflow from different submissions together to restrain their overall resource ; usage as a whole. - Every top-level workflow is assigned to a hog group when Cromwell receives it. ; + Exactly how this happens is [configurable](#configuration).; - Every sub-workflow or call started by a workflow is associated with the same hog group as its parent workflow.; - Multiple top-level workflows can be assigned to the same hog group to allow them to be grouped together. Thus:. - Every workflow is assigned to a hog group when submitted.; - Every hog group may have many workflows assigned to it. ### Hog Factor. The hog factor is an integer greater than or equal to 1. It represents a trade-off between: . - Fully utilizing all resources available to Cromwell to complete jobs, for as long as there are jobs to be processed.; - Reserving resources for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th ",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:1096,Modifiability,config,configuration,1096,"at its disposal. WDL workflows allow scattered tasks to be; run a huge number of times with very simple syntax.; This makes it easy for a very small number of workflows to hog all of the resources of Cromwell, forcing all other ; workflows (even a simple 'hello_world') to wait in line behind them. ; Sometimes that's exactly what you want, but often you would like Cromwell to remain responsive to new users' small ; workflows even while continuing to process production workflows from established users. Cromwell 35 provides new ways of stopping one workflow or group of workflows from locking out everyone else by ; introducing hog factors, hog groups and hog limits. This page describes what they are and how they work. ## Concepts. ### Hog Group . The Hog Group is a way of grouping workflow from different submissions together to restrain their overall resource ; usage as a whole. - Every top-level workflow is assigned to a hog group when Cromwell receives it. ; + Exactly how this happens is [configurable](#configuration).; - Every sub-workflow or call started by a workflow is associated with the same hog group as its parent workflow.; - Multiple top-level workflows can be assigned to the same hog group to allow them to be grouped together. Thus:. - Every workflow is assigned to a hog group when submitted.; - Every hog group may have many workflows assigned to it. ### Hog Factor. The hog factor is an integer greater than or equal to 1. It represents a trade-off between: . - Fully utilizing all resources available to Cromwell to complete jobs, for as long as there are jobs to be processed.; - Reserving resources for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th ",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:2610,Modifiability,config,configuration,2610,"rces for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up wit",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:2693,Modifiability,config,configuration,2693,"rces for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up wit",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:2953,Modifiability,config,configuration,2953," be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflo",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:3022,Modifiability,config,config,3022," be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflo",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:3308,Modifiability,config,configuration,3308,"ed by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at thei",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4490,Modifiability,config,configuration,4490,"s file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA1000000` was added before `jobD1",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:2470,Performance,queue,queued,2470," hog factor is an integer greater than or equal to 1. It represents a trade-off between: . - Fully utilizing all resources available to Cromwell to complete jobs, for as long as there are jobs to be processed.; - Reserving resources for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow ",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:2534,Performance,concurren,concurrent,2534," hog factor is an integer greater than or equal to 1. It represents a trade-off between: . - Fully utilizing all resources available to Cromwell to complete jobs, for as long as there are jobs to be processed.; - Reserving resources for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow ",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4123,Performance,queue,queues,4123," hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4225,Performance,queue,queue,4225,"efault value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *with",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4684,Performance,concurren,concurrent,4684," `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA1000000` was added before `jobD1`:; ```; jobA1, jobB1, jobC1, jobD1, jobA2, jobB2, jobD2, jobA3, ..., jobA1000000; ```. #### Example: How job execution is affected by hog factors. ##### An administrator sets up a Cro",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4832,Performance,concurren,concurrent,4832,"g used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA1000000` was added before `jobD1`:; ```; jobA1, jobB1, jobC1, jobD1, jobA2, jobB2, jobD2, jobA3, ..., jobA1000000; ```. #### Example: How job execution is affected by hog factors. ##### An administrator sets up a Cromwell server. - A Cromwell administrator sets the overall maximum concurrent job limit to 100,000 PAPIv2 jobs.; - The administrator also sets the ho",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4976,Performance,queue,queued,4976," options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA1000000` was added before `jobD1`:; ```; jobA1, jobB1, jobC1, jobD1, jobA2, jobB2, jobD2, jobA3, ..., jobA1000000; ```. #### Example: How job execution is affected by hog factors. ##### An administrator sets up a Cromwell server. - A Cromwell administrator sets the overall maximum concurrent job limit to 100,000 PAPIv2 jobs.; - The administrator also sets the hog factor to be 25.; - Cromwell will therefore calculate a per-hog-group concurrent job limit of 4,000 PAPIv2 jo",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:5270,Performance,queue,queued,5270," you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA1000000` was added before `jobD1`:; ```; jobA1, jobB1, jobC1, jobD1, jobA2, jobB2, jobD2, jobA3, ..., jobA1000000; ```. #### Example: How job execution is affected by hog factors. ##### An administrator sets up a Cromwell server. - A Cromwell administrator sets the overall maximum concurrent job limit to 100,000 PAPIv2 jobs.; - The administrator also sets the hog factor to be 25.; - Cromwell will therefore calculate a per-hog-group concurrent job limit of 4,000 PAPIv2 jobs. ##### Our first hog group hits its limit. - 100 workflows are running in hog group ""A"" and between them have generated 20,000 jobs for PAPIv2. ; + Cromwell initially starts 4,000 jobs.; + Cromwell then starts the remaining 16,000 new jobs as existing jobs from this group finish.; + New workflows in this group will not be able ",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:5736,Performance,concurren,concurrent,5736,"/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA1000000` was added before `jobD1`:; ```; jobA1, jobB1, jobC1, jobD1, jobA2, jobB2, jobD2, jobA3, ..., jobA1000000; ```. #### Example: How job execution is affected by hog factors. ##### An administrator sets up a Cromwell server. - A Cromwell administrator sets the overall maximum concurrent job limit to 100,000 PAPIv2 jobs.; - The administrator also sets the hog factor to be 25.; - Cromwell will therefore calculate a per-hog-group concurrent job limit of 4,000 PAPIv2 jobs. ##### Our first hog group hits its limit. - 100 workflows are running in hog group ""A"" and between them have generated 20,000 jobs for PAPIv2. ; + Cromwell initially starts 4,000 jobs.; + Cromwell then starts the remaining 16,000 new jobs as existing jobs from this group finish.; + New workflows in this group will not be able to start jobs either; + Their jobs are queued behind the existing jobs from this hog group.; + Note that Cromwell is currently only using 1/25th of its overall limit because its hog factor is 25. ##### Another hog group appears. - Now hog group B submits 1,000 workflows and between them they generate 200,000 jobs for PAPIv2.; - Even though 16,000 of group A's jobs are still queued, Cromwell starts 4,000 of group B's jobs immediately,; - The remaining 196,000 of group",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:5890,Performance,concurren,concurrent,5890,"what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA1000000` was added before `jobD1`:; ```; jobA1, jobB1, jobC1, jobD1, jobA2, jobB2, jobD2, jobA3, ..., jobA1000000; ```. #### Example: How job execution is affected by hog factors. ##### An administrator sets up a Cromwell server. - A Cromwell administrator sets the overall maximum concurrent job limit to 100,000 PAPIv2 jobs.; - The administrator also sets the hog factor to be 25.; - Cromwell will therefore calculate a per-hog-group concurrent job limit of 4,000 PAPIv2 jobs. ##### Our first hog group hits its limit. - 100 workflows are running in hog group ""A"" and between them have generated 20,000 jobs for PAPIv2. ; + Cromwell initially starts 4,000 jobs.; + Cromwell then starts the remaining 16,000 new jobs as existing jobs from this group finish.; + New workflows in this group will not be able to start jobs either; + Their jobs are queued behind the existing jobs from this hog group.; + Note that Cromwell is currently only using 1/25th of its overall limit because its hog factor is 25. ##### Another hog group appears. - Now hog group B submits 1,000 workflows and between them they generate 200,000 jobs for PAPIv2.; - Even though 16,000 of group A's jobs are still queued, Cromwell starts 4,000 of group B's jobs immediately,; - The remaining 196,000 of group B's jobs are queued up waiting for group B's existing jobs to complete. ##### Where do we stand?. - Cromwell knows about 220,000 jobs that could be sta",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:6300,Performance,queue,queued,6300,"```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA1000000` was added before `jobD1`:; ```; jobA1, jobB1, jobC1, jobD1, jobA2, jobB2, jobD2, jobA3, ..., jobA1000000; ```. #### Example: How job execution is affected by hog factors. ##### An administrator sets up a Cromwell server. - A Cromwell administrator sets the overall maximum concurrent job limit to 100,000 PAPIv2 jobs.; - The administrator also sets the hog factor to be 25.; - Cromwell will therefore calculate a per-hog-group concurrent job limit of 4,000 PAPIv2 jobs. ##### Our first hog group hits its limit. - 100 workflows are running in hog group ""A"" and between them have generated 20,000 jobs for PAPIv2. ; + Cromwell initially starts 4,000 jobs.; + Cromwell then starts the remaining 16,000 new jobs as existing jobs from this group finish.; + New workflows in this group will not be able to start jobs either; + Their jobs are queued behind the existing jobs from this hog group.; + Note that Cromwell is currently only using 1/25th of its overall limit because its hog factor is 25. ##### Another hog group appears. - Now hog group B submits 1,000 workflows and between them they generate 200,000 jobs for PAPIv2.; - Even though 16,000 of group A's jobs are still queued, Cromwell starts 4,000 of group B's jobs immediately,; - The remaining 196,000 of group B's jobs are queued up waiting for group B's existing jobs to complete. ##### Where do we stand?. - Cromwell knows about 220,000 jobs that could be started; - Cromwell has an overall limit of 100,000; - Cromwell is running 8,000 jobs in two hog groups. *In other words, not so great - perhaps we should have set the hog factor lower...?*. #####But wait, more workflows appear... - Now another 23 hog groups (""C"" through ""Y"") submit workflows of a similar scale to hog group A.; - One by one, the workflows of each hog group fill up their share of th",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:6638,Performance,queue,queued,6638,"l administrator sets the overall maximum concurrent job limit to 100,000 PAPIv2 jobs.; - The administrator also sets the hog factor to be 25.; - Cromwell will therefore calculate a per-hog-group concurrent job limit of 4,000 PAPIv2 jobs. ##### Our first hog group hits its limit. - 100 workflows are running in hog group ""A"" and between them have generated 20,000 jobs for PAPIv2. ; + Cromwell initially starts 4,000 jobs.; + Cromwell then starts the remaining 16,000 new jobs as existing jobs from this group finish.; + New workflows in this group will not be able to start jobs either; + Their jobs are queued behind the existing jobs from this hog group.; + Note that Cromwell is currently only using 1/25th of its overall limit because its hog factor is 25. ##### Another hog group appears. - Now hog group B submits 1,000 workflows and between them they generate 200,000 jobs for PAPIv2.; - Even though 16,000 of group A's jobs are still queued, Cromwell starts 4,000 of group B's jobs immediately,; - The remaining 196,000 of group B's jobs are queued up waiting for group B's existing jobs to complete. ##### Where do we stand?. - Cromwell knows about 220,000 jobs that could be started; - Cromwell has an overall limit of 100,000; - Cromwell is running 8,000 jobs in two hog groups. *In other words, not so great - perhaps we should have set the hog factor lower...?*. #####But wait, more workflows appear... - Now another 23 hog groups (""C"" through ""Y"") submit workflows of a similar scale to hog group A.; - One by one, the workflows of each hog group fill up their share of the overall concurrent job limit.; - So Cromwell is now running 100,000 jobs and each hog group has been allocated 4,000 of those. ##### What about poor hog group ""Z""?. - A final group submits workflows under hog group ""Z"".; - Alas, even though hog group ""Z"" is not running anything yet, we cannot start their workflows because we're ; at the global maximum of 100,000. *In other words, perhaps we should have set th",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:6746,Performance,queue,queued,6746,"l administrator sets the overall maximum concurrent job limit to 100,000 PAPIv2 jobs.; - The administrator also sets the hog factor to be 25.; - Cromwell will therefore calculate a per-hog-group concurrent job limit of 4,000 PAPIv2 jobs. ##### Our first hog group hits its limit. - 100 workflows are running in hog group ""A"" and between them have generated 20,000 jobs for PAPIv2. ; + Cromwell initially starts 4,000 jobs.; + Cromwell then starts the remaining 16,000 new jobs as existing jobs from this group finish.; + New workflows in this group will not be able to start jobs either; + Their jobs are queued behind the existing jobs from this hog group.; + Note that Cromwell is currently only using 1/25th of its overall limit because its hog factor is 25. ##### Another hog group appears. - Now hog group B submits 1,000 workflows and between them they generate 200,000 jobs for PAPIv2.; - Even though 16,000 of group A's jobs are still queued, Cromwell starts 4,000 of group B's jobs immediately,; - The remaining 196,000 of group B's jobs are queued up waiting for group B's existing jobs to complete. ##### Where do we stand?. - Cromwell knows about 220,000 jobs that could be started; - Cromwell has an overall limit of 100,000; - Cromwell is running 8,000 jobs in two hog groups. *In other words, not so great - perhaps we should have set the hog factor lower...?*. #####But wait, more workflows appear... - Now another 23 hog groups (""C"" through ""Y"") submit workflows of a similar scale to hog group A.; - One by one, the workflows of each hog group fill up their share of the overall concurrent job limit.; - So Cromwell is now running 100,000 jobs and each hog group has been allocated 4,000 of those. ##### What about poor hog group ""Z""?. - A final group submits workflows under hog group ""Z"".; - Alas, even though hog group ""Z"" is not running anything yet, we cannot start their workflows because we're ; at the global maximum of 100,000. *In other words, perhaps we should have set th",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:7292,Performance,concurren,concurrent,7292,"to start jobs either; + Their jobs are queued behind the existing jobs from this hog group.; + Note that Cromwell is currently only using 1/25th of its overall limit because its hog factor is 25. ##### Another hog group appears. - Now hog group B submits 1,000 workflows and between them they generate 200,000 jobs for PAPIv2.; - Even though 16,000 of group A's jobs are still queued, Cromwell starts 4,000 of group B's jobs immediately,; - The remaining 196,000 of group B's jobs are queued up waiting for group B's existing jobs to complete. ##### Where do we stand?. - Cromwell knows about 220,000 jobs that could be started; - Cromwell has an overall limit of 100,000; - Cromwell is running 8,000 jobs in two hog groups. *In other words, not so great - perhaps we should have set the hog factor lower...?*. #####But wait, more workflows appear... - Now another 23 hog groups (""C"" through ""Y"") submit workflows of a similar scale to hog group A.; - One by one, the workflows of each hog group fill up their share of the overall concurrent job limit.; - So Cromwell is now running 100,000 jobs and each hog group has been allocated 4,000 of those. ##### What about poor hog group ""Z""?. - A final group submits workflows under hog group ""Z"".; - Alas, even though hog group ""Z"" is not running anything yet, we cannot start their workflows because we're ; at the global maximum of 100,000. *In other words, perhaps we should have set the hog factor higher...?*. ##### So what now?. - As jobs in other hog group complete, we will begin to see hog group ""Z"" jobs started alongside new jobs from the ; other hog groups.; - Going forward Cromwell will start jobs from all groups at the same rate, even though hog group Z's jobs arrived; later than those from hog group A. Thus, over time, each group will approach approximately 1/26th of the total pool.; ; ## FAQs. #### Can I opt out of using hog groups?. Yes, to various degrees:. - No matter what, your workflows will be assigned to a hog group. ; - To",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:2659,Safety,safe,safety,2659,"rces for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up wit",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:2739,Safety,safe,safety,2739,"rces for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up wit",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4455,Safety,safe,safety,4455," the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA100",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:2801,Testability,log,log-interval-seconds,2801,"rces for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ; - A hog factor of 100 means ""any 1 group is only ever allowed to use 1/100th of the resources of the total Cromwell server"". ### Hog Limit. A Hog Limit is how much of a given resource a hog group is allowed use. Hog limits are not set directly; they are ; values that Cromwell calculates internally.; For example, a single hog group may be limited by Cromwell to a hog limit of 200 jobs per group. Therefore no matter how ; many workflows, sub-workflows, and jobs are queued in a greedy hog group, the whole group is limited to 200 concurrent ; running jobs. ## Configuration. Cromwell accepts the following configuration values for hog factors in the `hog-safety` stanza of `system` in the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up wit",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4174,Testability,log,logged,4174,"efault value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *with",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4231,Testability,log,logging,4231,"efault value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *with",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4385,Testability,log,logging,4385,"t is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the j",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4468,Testability,log,log-interval-seconds,4468,"s file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA1000000` was added before `jobD1",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4546,Testability,log,logging,4546,"orkflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served basis, Cromwell now assigns in a round-robin; fashion between hog groups and then on a first-come-first-served *within* a hog group. In other words if the hog groups had the following entries queued up:; ```; A: jobA1, jobA2, jobA3, ..., jobA1000000; B: jobB1, jobB2; C: jobC1; D: jobD1, jobD2; ```. Then Cromwell would start the jobs in the following order, even though `jobA1000000` was added before `jobD1`:; ```; jobA1, jobB1, jobC1, jobD1, jobA2, jobB2, j",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:176,Usability,simpl,simple,176,"# Hog Factors. ## Introduction. Cromwell has only a finite amount of resources at its disposal. WDL workflows allow scattered tasks to be; run a huge number of times with very simple syntax.; This makes it easy for a very small number of workflows to hog all of the resources of Cromwell, forcing all other ; workflows (even a simple 'hello_world') to wait in line behind them. ; Sometimes that's exactly what you want, but often you would like Cromwell to remain responsive to new users' small ; workflows even while continuing to process production workflows from established users. Cromwell 35 provides new ways of stopping one workflow or group of workflows from locking out everyone else by ; introducing hog factors, hog groups and hog limits. This page describes what they are and how they work. ## Concepts. ### Hog Group . The Hog Group is a way of grouping workflow from different submissions together to restrain their overall resource ; usage as a whole. - Every top-level workflow is assigned to a hog group when Cromwell receives it. ; + Exactly how this happens is [configurable](#configuration).; - Every sub-workflow or call started by a workflow is associated with the same hog group as its parent workflow.; - Multiple top-level workflows can be assigned to the same hog group to allow them to be grouped together. Thus:. - Every workflow is assigned to a hog group when submitted.; - Every hog group may have many workflows assigned to it. ### Hog Factor. The hog factor is an integer greater than or equal to 1. It represents a trade-off between: . - Fully utilizing all resources available to Cromwell to complete jobs, for as long as there are jobs to be processed.; - Reserving resources for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ;",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:327,Usability,simpl,simple,327,"# Hog Factors. ## Introduction. Cromwell has only a finite amount of resources at its disposal. WDL workflows allow scattered tasks to be; run a huge number of times with very simple syntax.; This makes it easy for a very small number of workflows to hog all of the resources of Cromwell, forcing all other ; workflows (even a simple 'hello_world') to wait in line behind them. ; Sometimes that's exactly what you want, but often you would like Cromwell to remain responsive to new users' small ; workflows even while continuing to process production workflows from established users. Cromwell 35 provides new ways of stopping one workflow or group of workflows from locking out everyone else by ; introducing hog factors, hog groups and hog limits. This page describes what they are and how they work. ## Concepts. ### Hog Group . The Hog Group is a way of grouping workflow from different submissions together to restrain their overall resource ; usage as a whole. - Every top-level workflow is assigned to a hog group when Cromwell receives it. ; + Exactly how this happens is [configurable](#configuration).; - Every sub-workflow or call started by a workflow is associated with the same hog group as its parent workflow.; - Multiple top-level workflows can be assigned to the same hog group to allow them to be grouped together. Thus:. - Every workflow is assigned to a hog group when submitted.; - Every hog group may have many workflows assigned to it. ### Hog Factor. The hog factor is an integer greater than or equal to 1. It represents a trade-off between: . - Fully utilizing all resources available to Cromwell to complete jobs, for as long as there are jobs to be processed.; - Reserving resources for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ;",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:464,Usability,responsiv,responsive,464,"# Hog Factors. ## Introduction. Cromwell has only a finite amount of resources at its disposal. WDL workflows allow scattered tasks to be; run a huge number of times with very simple syntax.; This makes it easy for a very small number of workflows to hog all of the resources of Cromwell, forcing all other ; workflows (even a simple 'hello_world') to wait in line behind them. ; Sometimes that's exactly what you want, but often you would like Cromwell to remain responsive to new users' small ; workflows even while continuing to process production workflows from established users. Cromwell 35 provides new ways of stopping one workflow or group of workflows from locking out everyone else by ; introducing hog factors, hog groups and hog limits. This page describes what they are and how they work. ## Concepts. ### Hog Group . The Hog Group is a way of grouping workflow from different submissions together to restrain their overall resource ; usage as a whole. - Every top-level workflow is assigned to a hog group when Cromwell receives it. ; + Exactly how this happens is [configurable](#configuration).; - Every sub-workflow or call started by a workflow is associated with the same hog group as its parent workflow.; - Multiple top-level workflows can be assigned to the same hog group to allow them to be grouped together. Thus:. - Every workflow is assigned to a hog group when submitted.; - Every hog group may have many workflows assigned to it. ### Hog Factor. The hog factor is an integer greater than or equal to 1. It represents a trade-off between: . - Fully utilizing all resources available to Cromwell to complete jobs, for as long as there are jobs to be processed.; - Reserving resources for requests from other hog groups - even if we have jobs waiting to run that could be using them. Here are a few mental models which might be helpful to thinking about the hog factor:. - A hog factor of 2 means that ""2 greedy users would be able to hog the entire resources of Cromwell"" ;",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:3663,Usability,simpl,simple,3663,"n the configuration ; file:; ```conf; system {; hog-safety {; hog-factor = 1; workflow-option = ""hogGroup""; token-log-interval-seconds = 0; }; }; ```. Additionally, you can override system-level hog-factor on a backend level, by setting it in the particular backend configuration like this:; ```conf; backend {; providers {; PAPIv2 {; config {; hog-factor: 2; }; }; }; }; ```. ### Setting a hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum con",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md:4037,Usability,simpl,simple,4037," hog-factor. The hog factor option sets the integer described in the [Hog Factor](#hog-factor) section above.; The default value is `1` (which is equivalent to not limiting by hog group). . ### Assignment of hog groups. Within the configuration file, you can specify the workflow option that will determine the hog group. The default is; `hogGroup`. So if a workflow arrives with the following workflow options file, Cromwell will assign `hogGroupA` as the ; workflow's hog group:. ```json; {; ""hogGroup"": ""hogGroupA""; }; ```. - Any workflow option value can be used so long as it is a simple `String` value:; + You can come up with a new field and set it specifically for assigning hog groups.; + You can choose a field that is already being used for other reasons; - If a workflow is submitted without a value for the designated field in its workflow options, the workflow ID is used ; as the hog group identifier. . ### Logging. Because the system is not a simple first-in-first-out, it can be valuable to see the status of all ; the existing queues inside Cromwell. . To have this information logged on a regular basis, you can enable periodic queue logging. This will; also alert you at the same frequency when events are happening, such as hog groups being at their individual limits. * You can enable logging for the Job Execution Token Dispenser, using; the `system.hog-safety.token-log-interval-seconds` configuration value.; * The default, `0`, means that no logging will occur. ## Effects. ### Job Execution. #### Reserving Space. - Cromwell allows administrators to designate an overall maximum concurrent job limit per; [backend](../backends/Backends.md#backend-job-limits). ; - Within that limit, a hog factor allows us to limit the maximum concurrent jobs started *per hog group*.; + This is what allows new jobs to run immediately even if many jobs from bigger workflows are already queued up. #### Round robin allocation. Rather than starting jobs on a strict first-come first served",MatchSource.DOCS,docs/cromwell_features/HogFactors.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/HogFactors.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/Labels.md:879,Deployability,patch,patch,879,"Labels in Cromwell are a way to group together workflows that are related or associated to each other.; For example, if you ran workflows to analyze data from a diabetes study, you can assign the label `project:diabetes-cohort`. . **Custom Labels JSON**. In order to assign labels to a workflow, the first step is to create a JSON file with key-value pairs that define a label. For the example above, the labels JSON should look like:. ```; {; ""project"":""diabetes-cohort""; }; ```. When choosing key-value pairs, it's important to make sure you're adhering to Cromwell supported label syntax below. . There are two ways to add labels to a workflow: ; 1. Upon workflow submission set the `labels` parameter of the Submit endpoint, or ; 2. Setting the `-l` argument when running in [Command Line](/CommandLine) mode. Labels can be added to existing workflows by using the `/labels` patch endpoint. After adding labels to your workflows, you can take advantage of the `/query` endpoint to filter tagged workflows. The Google backend supports labelling cloud resources and you can learn more about that [here](backends/Google#google-labels). #### Label Format. When labels are supplied to Cromwell, it will fail any request containing invalid label strings. Below are the requirements for a valid label key/value pair in Cromwell:. * Label keys may not be empty but label values may be empty.; * Label key and values have a max char limit of 255. For [default labels](backends/Google#google-labels) applied by the Google backend, Cromwell will modify workflow/task/call names to fit the schema, according to the following rules:. * Any capital letters are converted to lowercase.; * Any character which is not one of `[a-z]`, `[0-9]` or `-` will be replaced with `-`.; * If the start character does not match `[a-z]` then prefix with `x--`; * If the final character does not match `[a-z0-9]` then suffix with `--x`; * If the string is too long, only take the first 30 and last 30 characters and add `---` b",MatchSource.DOCS,docs/cromwell_features/Labels.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/Labels.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/Labels.md:1076,Usability,learn,learn,1076,"ell are a way to group together workflows that are related or associated to each other.; For example, if you ran workflows to analyze data from a diabetes study, you can assign the label `project:diabetes-cohort`. . **Custom Labels JSON**. In order to assign labels to a workflow, the first step is to create a JSON file with key-value pairs that define a label. For the example above, the labels JSON should look like:. ```; {; ""project"":""diabetes-cohort""; }; ```. When choosing key-value pairs, it's important to make sure you're adhering to Cromwell supported label syntax below. . There are two ways to add labels to a workflow: ; 1. Upon workflow submission set the `labels` parameter of the Submit endpoint, or ; 2. Setting the `-l` argument when running in [Command Line](/CommandLine) mode. Labels can be added to existing workflows by using the `/labels` patch endpoint. After adding labels to your workflows, you can take advantage of the `/query` endpoint to filter tagged workflows. The Google backend supports labelling cloud resources and you can learn more about that [here](backends/Google#google-labels). #### Label Format. When labels are supplied to Cromwell, it will fail any request containing invalid label strings. Below are the requirements for a valid label key/value pair in Cromwell:. * Label keys may not be empty but label values may be empty.; * Label key and values have a max char limit of 255. For [default labels](backends/Google#google-labels) applied by the Google backend, Cromwell will modify workflow/task/call names to fit the schema, according to the following rules:. * Any capital letters are converted to lowercase.; * Any character which is not one of `[a-z]`, `[0-9]` or `-` will be replaced with `-`.; * If the start character does not match `[a-z]` then prefix with `x--`; * If the final character does not match `[a-z0-9]` then suffix with `--x`; * If the string is too long, only take the first 30 and last 30 characters and add `---` between them.; ",MatchSource.DOCS,docs/cromwell_features/Labels.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/Labels.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md:359,Availability,error,error-keys,359,"# Retry with More Memory. With this feature one can specify an array of strings which when encountered in the `stderr` file by Cromwell, ; allows the task to be retried with more memory. The retry will be counted against the `maxRetries` count mentioned in ; the `runtimeAtrributes` in the task. There are 2 settings for this feature:. * `system.memory-retry-error-keys` : the error keys that need to be set in Cromwell config; * `memory_retry_multiplier` : [optional] the factor by which the memory should be multiplied while retrying. This needs ; to be passed in through workflow options and should be in the range `1.0 ≤ multiplier ≤ 99.0` (note: if set to `1.0` the task; will retry with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not ; change the memory amount. For example, if the error keys set in Cromwell config are as below, and the multiplier passed through workflow options is ; `""memory_retry_multiplier"": 1.1` ; ```hocon; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed""]; }; ``` ; this tells Cromwell to retry the task with 1.1x memory when it sees either `OutOfMemoryError` or `Killed` in the `stderr` ; file. . If the task has runtime attributes as below ; ```hocon; runtimeAtrributes {; memory: ""1 GB""; maxRetries: 1; }; ``` ; the task will be retried 1 more time if it runs out of memory, and this time with ""1.1 GB"". If the task return code is 0, the task will not be retried with more memory, even if the `stderr` file contains a; string present in `system.memory-retry-error-keys`. Similarly, if the runtime attribute `continueOnReturnCode` is; specified as a true, or the return code of the task matches a value specified by `continueOnReturnCode`, the task; will be considered successful and will not be retried with more memory. Please note that this feature currently only works in Google Cloud backend. Also, Pipelines API might adjust the ; memory value based on their standards for memory for a VM. So i",MatchSource.DOCS,docs/cromwell_features/RetryWithMoreMemory.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md:377,Availability,error,error,377,"# Retry with More Memory. With this feature one can specify an array of strings which when encountered in the `stderr` file by Cromwell, ; allows the task to be retried with more memory. The retry will be counted against the `maxRetries` count mentioned in ; the `runtimeAtrributes` in the task. There are 2 settings for this feature:. * `system.memory-retry-error-keys` : the error keys that need to be set in Cromwell config; * `memory_retry_multiplier` : [optional] the factor by which the memory should be multiplied while retrying. This needs ; to be passed in through workflow options and should be in the range `1.0 ≤ multiplier ≤ 99.0` (note: if set to `1.0` the task; will retry with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not ; change the memory amount. For example, if the error keys set in Cromwell config are as below, and the multiplier passed through workflow options is ; `""memory_retry_multiplier"": 1.1` ; ```hocon; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed""]; }; ``` ; this tells Cromwell to retry the task with 1.1x memory when it sees either `OutOfMemoryError` or `Killed` in the `stderr` ; file. . If the task has runtime attributes as below ; ```hocon; runtimeAtrributes {; memory: ""1 GB""; maxRetries: 1; }; ``` ; the task will be retried 1 more time if it runs out of memory, and this time with ""1.1 GB"". If the task return code is 0, the task will not be retried with more memory, even if the `stderr` file contains a; string present in `system.memory-retry-error-keys`. Similarly, if the runtime attribute `continueOnReturnCode` is; specified as a true, or the return code of the task matches a value specified by `continueOnReturnCode`, the task; will be considered successful and will not be retried with more memory. Please note that this feature currently only works in Google Cloud backend. Also, Pipelines API might adjust the ; memory value based on their standards for memory for a VM. So i",MatchSource.DOCS,docs/cromwell_features/RetryWithMoreMemory.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md:844,Availability,error,error,844,"# Retry with More Memory. With this feature one can specify an array of strings which when encountered in the `stderr` file by Cromwell, ; allows the task to be retried with more memory. The retry will be counted against the `maxRetries` count mentioned in ; the `runtimeAtrributes` in the task. There are 2 settings for this feature:. * `system.memory-retry-error-keys` : the error keys that need to be set in Cromwell config; * `memory_retry_multiplier` : [optional] the factor by which the memory should be multiplied while retrying. This needs ; to be passed in through workflow options and should be in the range `1.0 ≤ multiplier ≤ 99.0` (note: if set to `1.0` the task; will retry with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not ; change the memory amount. For example, if the error keys set in Cromwell config are as below, and the multiplier passed through workflow options is ; `""memory_retry_multiplier"": 1.1` ; ```hocon; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed""]; }; ``` ; this tells Cromwell to retry the task with 1.1x memory when it sees either `OutOfMemoryError` or `Killed` in the `stderr` ; file. . If the task has runtime attributes as below ; ```hocon; runtimeAtrributes {; memory: ""1 GB""; maxRetries: 1; }; ``` ; the task will be retried 1 more time if it runs out of memory, and this time with ""1.1 GB"". If the task return code is 0, the task will not be retried with more memory, even if the `stderr` file contains a; string present in `system.memory-retry-error-keys`. Similarly, if the runtime attribute `continueOnReturnCode` is; specified as a true, or the return code of the task matches a value specified by `continueOnReturnCode`, the task; will be considered successful and will not be retried with more memory. Please note that this feature currently only works in Google Cloud backend. Also, Pipelines API might adjust the ; memory value based on their standards for memory for a VM. So i",MatchSource.DOCS,docs/cromwell_features/RetryWithMoreMemory.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md:1016,Availability,error,error-keys,1016,"one can specify an array of strings which when encountered in the `stderr` file by Cromwell, ; allows the task to be retried with more memory. The retry will be counted against the `maxRetries` count mentioned in ; the `runtimeAtrributes` in the task. There are 2 settings for this feature:. * `system.memory-retry-error-keys` : the error keys that need to be set in Cromwell config; * `memory_retry_multiplier` : [optional] the factor by which the memory should be multiplied while retrying. This needs ; to be passed in through workflow options and should be in the range `1.0 ≤ multiplier ≤ 99.0` (note: if set to `1.0` the task; will retry with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not ; change the memory amount. For example, if the error keys set in Cromwell config are as below, and the multiplier passed through workflow options is ; `""memory_retry_multiplier"": 1.1` ; ```hocon; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed""]; }; ``` ; this tells Cromwell to retry the task with 1.1x memory when it sees either `OutOfMemoryError` or `Killed` in the `stderr` ; file. . If the task has runtime attributes as below ; ```hocon; runtimeAtrributes {; memory: ""1 GB""; maxRetries: 1; }; ``` ; the task will be retried 1 more time if it runs out of memory, and this time with ""1.1 GB"". If the task return code is 0, the task will not be retried with more memory, even if the `stderr` file contains a; string present in `system.memory-retry-error-keys`. Similarly, if the runtime attribute `continueOnReturnCode` is; specified as a true, or the return code of the task matches a value specified by `continueOnReturnCode`, the task; will be considered successful and will not be retried with more memory. Please note that this feature currently only works in Google Cloud backend. Also, Pipelines API might adjust the ; memory value based on their standards for memory for a VM. So it's possible that even though the request s",MatchSource.DOCS,docs/cromwell_features/RetryWithMoreMemory.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md:1560,Availability,error,error-keys,1560," 2 settings for this feature:. * `system.memory-retry-error-keys` : the error keys that need to be set in Cromwell config; * `memory_retry_multiplier` : [optional] the factor by which the memory should be multiplied while retrying. This needs ; to be passed in through workflow options and should be in the range `1.0 ≤ multiplier ≤ 99.0` (note: if set to `1.0` the task; will retry with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not ; change the memory amount. For example, if the error keys set in Cromwell config are as below, and the multiplier passed through workflow options is ; `""memory_retry_multiplier"": 1.1` ; ```hocon; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed""]; }; ``` ; this tells Cromwell to retry the task with 1.1x memory when it sees either `OutOfMemoryError` or `Killed` in the `stderr` ; file. . If the task has runtime attributes as below ; ```hocon; runtimeAtrributes {; memory: ""1 GB""; maxRetries: 1; }; ``` ; the task will be retried 1 more time if it runs out of memory, and this time with ""1.1 GB"". If the task return code is 0, the task will not be retried with more memory, even if the `stderr` file contains a; string present in `system.memory-retry-error-keys`. Similarly, if the runtime attribute `continueOnReturnCode` is; specified as a true, or the return code of the task matches a value specified by `continueOnReturnCode`, the task; will be considered successful and will not be retried with more memory. Please note that this feature currently only works in Google Cloud backend. Also, Pipelines API might adjust the ; memory value based on their standards for memory for a VM. So it's possible that even though the request says 1.1 GB ; memory, it actually allocated a bit more memory to the VM. Two environment variables called `${MEM_UNIT}` and `${MEM_SIZE}` are also available inside the command block of a task,; making it easy to retrieve the new value of memory on the machine.; ",MatchSource.DOCS,docs/cromwell_features/RetryWithMoreMemory.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md:2190,Availability,avail,available,2190," 2 settings for this feature:. * `system.memory-retry-error-keys` : the error keys that need to be set in Cromwell config; * `memory_retry_multiplier` : [optional] the factor by which the memory should be multiplied while retrying. This needs ; to be passed in through workflow options and should be in the range `1.0 ≤ multiplier ≤ 99.0` (note: if set to `1.0` the task; will retry with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not ; change the memory amount. For example, if the error keys set in Cromwell config are as below, and the multiplier passed through workflow options is ; `""memory_retry_multiplier"": 1.1` ; ```hocon; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed""]; }; ``` ; this tells Cromwell to retry the task with 1.1x memory when it sees either `OutOfMemoryError` or `Killed` in the `stderr` ; file. . If the task has runtime attributes as below ; ```hocon; runtimeAtrributes {; memory: ""1 GB""; maxRetries: 1; }; ``` ; the task will be retried 1 more time if it runs out of memory, and this time with ""1.1 GB"". If the task return code is 0, the task will not be retried with more memory, even if the `stderr` file contains a; string present in `system.memory-retry-error-keys`. Similarly, if the runtime attribute `continueOnReturnCode` is; specified as a true, or the return code of the task matches a value specified by `continueOnReturnCode`, the task; will be considered successful and will not be retried with more memory. Please note that this feature currently only works in Google Cloud backend. Also, Pipelines API might adjust the ; memory value based on their standards for memory for a VM. So it's possible that even though the request says 1.1 GB ; memory, it actually allocated a bit more memory to the VM. Two environment variables called `${MEM_UNIT}` and `${MEM_SIZE}` are also available inside the command block of a task,; making it easy to retrieve the new value of memory on the machine.; ",MatchSource.DOCS,docs/cromwell_features/RetryWithMoreMemory.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md:2077,Energy Efficiency,allocate,allocated,2077," 2 settings for this feature:. * `system.memory-retry-error-keys` : the error keys that need to be set in Cromwell config; * `memory_retry_multiplier` : [optional] the factor by which the memory should be multiplied while retrying. This needs ; to be passed in through workflow options and should be in the range `1.0 ≤ multiplier ≤ 99.0` (note: if set to `1.0` the task; will retry with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not ; change the memory amount. For example, if the error keys set in Cromwell config are as below, and the multiplier passed through workflow options is ; `""memory_retry_multiplier"": 1.1` ; ```hocon; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed""]; }; ``` ; this tells Cromwell to retry the task with 1.1x memory when it sees either `OutOfMemoryError` or `Killed` in the `stderr` ; file. . If the task has runtime attributes as below ; ```hocon; runtimeAtrributes {; memory: ""1 GB""; maxRetries: 1; }; ``` ; the task will be retried 1 more time if it runs out of memory, and this time with ""1.1 GB"". If the task return code is 0, the task will not be retried with more memory, even if the `stderr` file contains a; string present in `system.memory-retry-error-keys`. Similarly, if the runtime attribute `continueOnReturnCode` is; specified as a true, or the return code of the task matches a value specified by `continueOnReturnCode`, the task; will be considered successful and will not be retried with more memory. Please note that this feature currently only works in Google Cloud backend. Also, Pipelines API might adjust the ; memory value based on their standards for memory for a VM. So it's possible that even though the request says 1.1 GB ; memory, it actually allocated a bit more memory to the VM. Two environment variables called `${MEM_UNIT}` and `${MEM_SIZE}` are also available inside the command block of a task,; making it easy to retrieve the new value of memory on the machine.; ",MatchSource.DOCS,docs/cromwell_features/RetryWithMoreMemory.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md:420,Modifiability,config,config,420,"# Retry with More Memory. With this feature one can specify an array of strings which when encountered in the `stderr` file by Cromwell, ; allows the task to be retried with more memory. The retry will be counted against the `maxRetries` count mentioned in ; the `runtimeAtrributes` in the task. There are 2 settings for this feature:. * `system.memory-retry-error-keys` : the error keys that need to be set in Cromwell config; * `memory_retry_multiplier` : [optional] the factor by which the memory should be multiplied while retrying. This needs ; to be passed in through workflow options and should be in the range `1.0 ≤ multiplier ≤ 99.0` (note: if set to `1.0` the task; will retry with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not ; change the memory amount. For example, if the error keys set in Cromwell config are as below, and the multiplier passed through workflow options is ; `""memory_retry_multiplier"": 1.1` ; ```hocon; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed""]; }; ``` ; this tells Cromwell to retry the task with 1.1x memory when it sees either `OutOfMemoryError` or `Killed` in the `stderr` ; file. . If the task has runtime attributes as below ; ```hocon; runtimeAtrributes {; memory: ""1 GB""; maxRetries: 1; }; ``` ; the task will be retried 1 more time if it runs out of memory, and this time with ""1.1 GB"". If the task return code is 0, the task will not be retried with more memory, even if the `stderr` file contains a; string present in `system.memory-retry-error-keys`. Similarly, if the runtime attribute `continueOnReturnCode` is; specified as a true, or the return code of the task matches a value specified by `continueOnReturnCode`, the task; will be considered successful and will not be retried with more memory. Please note that this feature currently only works in Google Cloud backend. Also, Pipelines API might adjust the ; memory value based on their standards for memory for a VM. So i",MatchSource.DOCS,docs/cromwell_features/RetryWithMoreMemory.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md:871,Modifiability,config,config,871,"# Retry with More Memory. With this feature one can specify an array of strings which when encountered in the `stderr` file by Cromwell, ; allows the task to be retried with more memory. The retry will be counted against the `maxRetries` count mentioned in ; the `runtimeAtrributes` in the task. There are 2 settings for this feature:. * `system.memory-retry-error-keys` : the error keys that need to be set in Cromwell config; * `memory_retry_multiplier` : [optional] the factor by which the memory should be multiplied while retrying. This needs ; to be passed in through workflow options and should be in the range `1.0 ≤ multiplier ≤ 99.0` (note: if set to `1.0` the task; will retry with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not ; change the memory amount. For example, if the error keys set in Cromwell config are as below, and the multiplier passed through workflow options is ; `""memory_retry_multiplier"": 1.1` ; ```hocon; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed""]; }; ``` ; this tells Cromwell to retry the task with 1.1x memory when it sees either `OutOfMemoryError` or `Killed` in the `stderr` ; file. . If the task has runtime attributes as below ; ```hocon; runtimeAtrributes {; memory: ""1 GB""; maxRetries: 1; }; ``` ; the task will be retried 1 more time if it runs out of memory, and this time with ""1.1 GB"". If the task return code is 0, the task will not be retried with more memory, even if the `stderr` file contains a; string present in `system.memory-retry-error-keys`. Similarly, if the runtime attribute `continueOnReturnCode` is; specified as a true, or the return code of the task matches a value specified by `continueOnReturnCode`, the task; will be considered successful and will not be retried with more memory. Please note that this feature currently only works in Google Cloud backend. Also, Pipelines API might adjust the ; memory value based on their standards for memory for a VM. So i",MatchSource.DOCS,docs/cromwell_features/RetryWithMoreMemory.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md:2132,Modifiability,variab,variables,2132," 2 settings for this feature:. * `system.memory-retry-error-keys` : the error keys that need to be set in Cromwell config; * `memory_retry_multiplier` : [optional] the factor by which the memory should be multiplied while retrying. This needs ; to be passed in through workflow options and should be in the range `1.0 ≤ multiplier ≤ 99.0` (note: if set to `1.0` the task; will retry with same amount of memory). If this is not specified, Cromwell will retry the task (if applicable) but not ; change the memory amount. For example, if the error keys set in Cromwell config are as below, and the multiplier passed through workflow options is ; `""memory_retry_multiplier"": 1.1` ; ```hocon; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed""]; }; ``` ; this tells Cromwell to retry the task with 1.1x memory when it sees either `OutOfMemoryError` or `Killed` in the `stderr` ; file. . If the task has runtime attributes as below ; ```hocon; runtimeAtrributes {; memory: ""1 GB""; maxRetries: 1; }; ``` ; the task will be retried 1 more time if it runs out of memory, and this time with ""1.1 GB"". If the task return code is 0, the task will not be retried with more memory, even if the `stderr` file contains a; string present in `system.memory-retry-error-keys`. Similarly, if the runtime attribute `continueOnReturnCode` is; specified as a true, or the return code of the task matches a value specified by `continueOnReturnCode`, the task; will be considered successful and will not be retried with more memory. Please note that this feature currently only works in Google Cloud backend. Also, Pipelines API might adjust the ; memory value based on their standards for memory for a VM. So it's possible that even though the request says 1.1 GB ; memory, it actually allocated a bit more memory to the VM. Two environment variables called `${MEM_UNIT}` and `${MEM_SIZE}` are also available inside the command block of a task,; making it easy to retrieve the new value of memory on the machine.; ",MatchSource.DOCS,docs/cromwell_features/RetryWithMoreMemory.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/RetryWithMoreMemory.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:342,Availability,failure,failure,342,"The workflow callback is a simple way to integrate Cromwell with an external system. When each workflow reaches a terminal; state, Cromwell will attempt to POST a message to a provided URL (see below for schema of this message). ; Messages are sent for root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would b",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:2133,Availability,failure,failures,2133,"or root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would be returned from the `api/workflows/{version}/{id}/outputs` endpoint. Expected to be empty when the workflow is not successful..; * `failures`: A list of strings describing the workflow's failures. Expected to be empty if the workflow did not fail.; ",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:2188,Availability,failure,failures,2188,"or root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would be returned from the `api/workflows/{version}/{id}/outputs` endpoint. Expected to be empty when the workflow is not successful..; * `failures`: A list of strings describing the workflow's failures. Expected to be empty if the workflow did not fail.; ",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:41,Deployability,integrat,integrate,41,"The workflow callback is a simple way to integrate Cromwell with an external system. When each workflow reaches a terminal; state, Cromwell will attempt to POST a message to a provided URL (see below for schema of this message). ; Messages are sent for root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would b",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:903,Energy Efficiency,allocate,allocate,903,"The workflow callback is a simple way to integrate Cromwell with an external system. When each workflow reaches a terminal; state, Cromwell will attempt to POST a message to a provided URL (see below for schema of this message). ; Messages are sent for root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would b",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:41,Integrability,integrat,integrate,41,"The workflow callback is a simple way to integrate Cromwell with an external system. When each workflow reaches a terminal; state, Cromwell will attempt to POST a message to a provided URL (see below for schema of this message). ; Messages are sent for root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would b",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:163,Integrability,message,message,163,"The workflow callback is a simple way to integrate Cromwell with an external system. When each workflow reaches a terminal; state, Cromwell will attempt to POST a message to a provided URL (see below for schema of this message). ; Messages are sent for root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would b",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:219,Integrability,message,message,219,"The workflow callback is a simple way to integrate Cromwell with an external system. When each workflow reaches a terminal; state, Cromwell will attempt to POST a message to a provided URL (see below for schema of this message). ; Messages are sent for root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would b",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:989,Integrability,message,message,989,"The workflow callback is a simple way to integrate Cromwell with an external system. When each workflow reaches a terminal; state, Cromwell will attempt to POST a message to a provided URL (see below for schema of this message). ; Messages are sent for root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would b",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:497,Modifiability,config,config,497,"The workflow callback is a simple way to integrate Cromwell with an external system. When each workflow reaches a terminal; state, Cromwell will attempt to POST a message to a provided URL (see below for schema of this message). ; Messages are sent for root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would b",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:509,Modifiability,config,config,509,"The workflow callback is a simple way to integrate Cromwell with an external system. When each workflow reaches a terminal; state, Cromwell will attempt to POST a message to a provided URL (see below for schema of this message). ; Messages are sent for root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would b",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:1452,Modifiability,config,config,1452,"or root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would be returned from the `api/workflows/{version}/{id}/outputs` endpoint. Expected to be empty when the workflow is not successful..; * `failures`: A list of strings describing the workflow's failures. Expected to be empty if the workflow did not fail.; ",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:916,Performance,perform,performing,916,"The workflow callback is a simple way to integrate Cromwell with an external system. When each workflow reaches a terminal; state, Cromwell will attempt to POST a message to a provided URL (see below for schema of this message). ; Messages are sent for root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would b",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md:27,Usability,simpl,simple,27,"The workflow callback is a simple way to integrate Cromwell with an external system. When each workflow reaches a terminal; state, Cromwell will attempt to POST a message to a provided URL (see below for schema of this message). ; Messages are sent for root workflows only, not subworkflows. Callback status information, including success or failure, ; will be recorded in workflow metadata with keys containing `workflowCallback`. ### Configuration. This feature will only be used if enabled via config. All config items except `enabled` are optional. ```; workflow-state-callback {; enabled: true; num-threads: 5; endpoint: ""http://example.com""; auth.azure: true; request-backoff {; min: ""3 seconds"",; max: ""5 minutes"",; multiplier: 1.1; }; max-retries = 10; }; ```. * `enabled`: This boolean controls whether a callback will be attempted or not.; * `num-threads`: The number of threads Cromwell will allocate for performing callbacks.; * `endpoint`: This is the default URL to send the message to. If this is unset, and no URL is set in workflow options, no callback will be sent.; * `auth.azure`: If true, and if Cromwell is running in an Azure environment, Cromwell will include an auth header with bearer token generated from local Azure credentials.; * `request-backoff` and `max-retries`: Include these to override the default retry behavior (default behavior shown here). ### Workflow Options. You may choose to override the `endpoint` set in config by including this workflow option:; ```; {; ""workflow_callback_uri"": ""http://mywebsite.com""; }; ```. ### Callback schema. Below is an example of a callback request body. ```; {; ""workflowId"": ""00001111-2222-3333-4444-555566667777"",; ""state"": ""Succeeded"",; ""outputs"": {; ""task1.out"": 5,; ""task2.out"": ""/some/file.txt""; }; }; ```. * `workflowId`: The UUID of the workflow; * `state`: The terminal state of the workflow. The list of possible values is: `Succeeded`, `Failed`, `Aborted`; * `outputs`: The final outputs of the workflow, as would b",MatchSource.DOCS,docs/cromwell_features/WorkflowCallback.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/cromwell_features/WorkflowCallback.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Arch.md:2995,Security,hash,hash,2995,RI | NULL | auto_increment |; | HASH_KEY | varchar(255) | NO | | NULL | |; | HASH_VALUE | varchar(255) | NO | | NULL | |; | CALL_CACHING_ENTRY_ID | int(11) | YES | MUL | NULL | |. ## Call Caching Simpleton Entry. | Field | Type | Null | Key | Default | Extra |; |---------------------------------|--------------|------|-----|---------|----------------|; | CALL_CACHING_SIMPLETON_ENTRY_ID | int(11) | NO | PRI | NULL | auto_increment |; | HASH_KEY | varchar(255) | NO | | NULL | |; | HASH_VALUE | varchar(255) | NO | | NULL | |; | CALL_CACHING_ENTRY_ID | int(11) | YES | MUL | NULL | |. ## Custom Labels. | Field | Type | Null | Key | Default | Extra |; |-------------------------|--------------|------|-----|---------|----------------|; | CUSTOM_LABEL_ENTRY_ID | bigint(20) | NO | PRI | NULL | auto_increment |; | CUSTOM_LABEL_KEY | varchar(255) | YES | MUL | NULL | |; | CUSTOM_LABEL_VALUE | varchar(255) | YES | | NULL | |; | WORKFLOW_EXECUTION_UUID | varchar(100) | NO | MUL | NULL | |. ## Docker hash store. | Field | Type | Null | Key | Default | Extra |; |----------------------------|--------------|------|-----|---------|----------------|; | DOCKER_HASH_STORE_ENTRY_ID | int(11) | NO | PRI | NULL | auto_increment |; | WORKFLOW_EXECUTION_UUID | varchar(255) | NO | MUL | NULL | |; | DOCKER_TAG | varchar(255) | NO | | NULL | |; | DOCKER_HASH | varchar(255) | NO | | NULL | |. ## Job Key Value. | Field | Type | Null | Key | Default | Extra |; |---------------------------|--------------|------|-----|---------|----------------|; | JOB_KEY_VALUE_ENTRY_ID | int(11) | NO | PRI | NULL | auto_increment |; | WORKFLOW_EXECUTION_UUID | varchar(255) | NO | MUL | NULL | |; | CALL_FULLY_QUALIFIED_NAME | varchar(255) | YES | | NULL | |; | JOB_INDEX | int(11) | YES | | NULL | |; | JOB_ATTEMPT | int(11) | YES | | NULL | |; | STORE_KEY | varchar(255) | NO | | NULL | |; | STORE_VALUE | varchar(255) | NO | | NULL | |. ## Sub Workflow Store. | Field | Type | Null | Key | Default | Extra |; |----------,MatchSource.DOCS,docs/developers/Arch.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Arch.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:3913,Availability,recover,recover,3913," to run. To further the function analogy this would be an; invocation of that function.; * Job: The physical manifestation of a call by the backend. Examples of this would be an SGE job, a unix process, or a; Google Life Sciences operation ID. ### Backend Lifecycle. Within a running workflow, the backend is used in the following manner:. * Initialization: Initialization routine called the first time a workflow uses a backend; * Execute: The workflow requests that the backend run a job; * Recover: Attempt to reconnect to a previously started job. An example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finalization: When a workflow is complete, allows for any workflow level cleanup to take place. The initialization and finalization steps are optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:3987,Availability,recover,recover,3987," of a call by the backend. Examples of this would be an SGE job, a unix process, or a; Google Life Sciences operation ID. ### Backend Lifecycle. Within a running workflow, the backend is used in the following manner:. * Initialization: Initialization routine called the first time a workflow uses a backend; * Execute: The workflow requests that the backend run a job; * Recover: Attempt to reconnect to a previously started job. An example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finalization: When a workflow is complete, allows for any workflow level cleanup to take place. The initialization and finalization steps are optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f00976ee258142185917083460d724f7fe3d/cromwell.example.backends/cromwell.examples.conf#L370); * [`BackendWorkflowInitializat",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:4154,Availability,robust,robust,4154,"ifecycle. Within a running workflow, the backend is used in the following manner:. * Initialization: Initialization routine called the first time a workflow uses a backend; * Execute: The workflow requests that the backend run a job; * Recover: Attempt to reconnect to a previously started job. An example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finalization: When a workflow is complete, allows for any workflow level cleanup to take place. The initialization and finalization steps are optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f00976ee258142185917083460d724f7fe3d/cromwell.example.backends/cromwell.examples.conf#L370); * [`BackendWorkflowInitializationActor`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/bac",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:4804,Deployability,configurat,configuration,4804,re optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f00976ee258142185917083460d724f7fe3d/cromwell.example.backends/cromwell.examples.conf#L370); * [`BackendWorkflowInitializationActor`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L27); Handles the initialization phase; * Usually a derivative; of [`StandardAsyncExecutionActor`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L75); to run an individual job within the workflow; * [`BackendWorkflowFinalizationActor`](https://github.com/broadinstitute/cromwell/blob/a40de672c565c4bbd40f57ff96d4ee520dc2b4fc/backend/src/main/scala/cromwell/backend/BackendWorkflowFinalizationActor.scala#L10); Handle,MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:4835,Deployability,configurat,configuration,4835,backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f00976ee258142185917083460d724f7fe3d/cromwell.example.backends/cromwell.examples.conf#L370); * [`BackendWorkflowInitializationActor`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L27); Handles the initialization phase; * Usually a derivative; of [`StandardAsyncExecutionActor`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L75); to run an individual job within the workflow; * [`BackendWorkflowFinalizationActor`](https://github.com/broadinstitute/cromwell/blob/a40de672c565c4bbd40f57ff96d4ee520dc2b4fc/backend/src/main/scala/cromwell/backend/BackendWorkflowFinalizationActor.scala#L10); Handles the finalization phase. These three actors are all represented by a trait which a b,MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:8405,Deployability,pipeline,pipelines,8405,"acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L173); is the actual initialization functionality. If a backend requires any work to be done prior to handling a call, that; code must be called from here. #### StandardAsyncExecutionActor. Nearly all production backend implementations in Cromwell extend; the [StandardAsyncExecutionActor](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L75); trait. The minimum overrides for implementations of this trait are enumerated below, but overrides of other methods will; also be required. There are several backend implementations in the Cromwell codebase that can serve as references, for; example:. * [Google Life Sciences / Pipelines API](https://github.com/broadinstitute/cromwell/blob/0aff35336b4e2ba19b18530a68e622df1462d9b7/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L95); common layer; * [Life Sciences API (beta)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [Pipelines API (alpha)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [GA4GH Task Execution Service (TES)](https://github.com/broadinstitute/cromwell/blob/6bf7af3c12a411db26786ac34646238fc053ec97/supportedBackends/tes/src/main/scala/cromwell/backend/impl/tes/TesAsyncBackendJobExecutionActor.scala#L55); * [AWS Batch](https://github.com/broadinstitute/cromwell/blob/470d482e8ba2a9e2bc544896a4e6",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:8461,Deployability,pipeline,pipelines,8461,"acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L173); is the actual initialization functionality. If a backend requires any work to be done prior to handling a call, that; code must be called from here. #### StandardAsyncExecutionActor. Nearly all production backend implementations in Cromwell extend; the [StandardAsyncExecutionActor](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L75); trait. The minimum overrides for implementations of this trait are enumerated below, but overrides of other methods will; also be required. There are several backend implementations in the Cromwell codebase that can serve as references, for; example:. * [Google Life Sciences / Pipelines API](https://github.com/broadinstitute/cromwell/blob/0aff35336b4e2ba19b18530a68e622df1462d9b7/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L95); common layer; * [Life Sciences API (beta)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [Pipelines API (alpha)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [GA4GH Task Execution Service (TES)](https://github.com/broadinstitute/cromwell/blob/6bf7af3c12a411db26786ac34646238fc053ec97/supportedBackends/tes/src/main/scala/cromwell/backend/impl/tes/TesAsyncBackendJobExecutionActor.scala#L55); * [AWS Batch](https://github.com/broadinstitute/cromwell/blob/470d482e8ba2a9e2bc544896a4e6",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:8689,Deployability,pipeline,pipelines,8689,"rdAsyncExecutionActor. Nearly all production backend implementations in Cromwell extend; the [StandardAsyncExecutionActor](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L75); trait. The minimum overrides for implementations of this trait are enumerated below, but overrides of other methods will; also be required. There are several backend implementations in the Cromwell codebase that can serve as references, for; example:. * [Google Life Sciences / Pipelines API](https://github.com/broadinstitute/cromwell/blob/0aff35336b4e2ba19b18530a68e622df1462d9b7/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L95); common layer; * [Life Sciences API (beta)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [Pipelines API (alpha)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [GA4GH Task Execution Service (TES)](https://github.com/broadinstitute/cromwell/blob/6bf7af3c12a411db26786ac34646238fc053ec97/supportedBackends/tes/src/main/scala/cromwell/backend/impl/tes/TesAsyncBackendJobExecutionActor.scala#L55); * [AWS Batch](https://github.com/broadinstitute/cromwell/blob/470d482e8ba2a9e2bc544896a4e6ceea57d55bb2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchAsyncBackendJobExecutionActor.scala#L83). Overrides required for compilation and basic execution of `StandardAsyncExecutionActor` implementations:. * [`type StandardAsyncRunInfo`](https://github.com/b",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:8745,Deployability,pipeline,pipelines,8745,"rdAsyncExecutionActor. Nearly all production backend implementations in Cromwell extend; the [StandardAsyncExecutionActor](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L75); trait. The minimum overrides for implementations of this trait are enumerated below, but overrides of other methods will; also be required. There are several backend implementations in the Cromwell codebase that can serve as references, for; example:. * [Google Life Sciences / Pipelines API](https://github.com/broadinstitute/cromwell/blob/0aff35336b4e2ba19b18530a68e622df1462d9b7/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L95); common layer; * [Life Sciences API (beta)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [Pipelines API (alpha)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [GA4GH Task Execution Service (TES)](https://github.com/broadinstitute/cromwell/blob/6bf7af3c12a411db26786ac34646238fc053ec97/supportedBackends/tes/src/main/scala/cromwell/backend/impl/tes/TesAsyncBackendJobExecutionActor.scala#L55); * [AWS Batch](https://github.com/broadinstitute/cromwell/blob/470d482e8ba2a9e2bc544896a4e6ceea57d55bb2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchAsyncBackendJobExecutionActor.scala#L83). Overrides required for compilation and basic execution of `StandardAsyncExecutionActor` implementations:. * [`type StandardAsyncRunInfo`](https://github.com/b",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:8956,Deployability,pipeline,pipelines,8956,"AsyncExecutionActor.scala#L75); trait. The minimum overrides for implementations of this trait are enumerated below, but overrides of other methods will; also be required. There are several backend implementations in the Cromwell codebase that can serve as references, for; example:. * [Google Life Sciences / Pipelines API](https://github.com/broadinstitute/cromwell/blob/0aff35336b4e2ba19b18530a68e622df1462d9b7/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L95); common layer; * [Life Sciences API (beta)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [Pipelines API (alpha)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [GA4GH Task Execution Service (TES)](https://github.com/broadinstitute/cromwell/blob/6bf7af3c12a411db26786ac34646238fc053ec97/supportedBackends/tes/src/main/scala/cromwell/backend/impl/tes/TesAsyncBackendJobExecutionActor.scala#L55); * [AWS Batch](https://github.com/broadinstitute/cromwell/blob/470d482e8ba2a9e2bc544896a4e6ceea57d55bb2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchAsyncBackendJobExecutionActor.scala#L83). Overrides required for compilation and basic execution of `StandardAsyncExecutionActor` implementations:. * [`type StandardAsyncRunInfo`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L89); encapsulates the type of the run info when a job is started.; * [`type StandardAsyncRunState`](https://github.c",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:9014,Deployability,pipeline,pipelines,9014,"AsyncExecutionActor.scala#L75); trait. The minimum overrides for implementations of this trait are enumerated below, but overrides of other methods will; also be required. There are several backend implementations in the Cromwell codebase that can serve as references, for; example:. * [Google Life Sciences / Pipelines API](https://github.com/broadinstitute/cromwell/blob/0aff35336b4e2ba19b18530a68e622df1462d9b7/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L95); common layer; * [Life Sciences API (beta)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [Pipelines API (alpha)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/PipelinesApiAsyncBackendJobExecutionActor.scala#L27); * [GA4GH Task Execution Service (TES)](https://github.com/broadinstitute/cromwell/blob/6bf7af3c12a411db26786ac34646238fc053ec97/supportedBackends/tes/src/main/scala/cromwell/backend/impl/tes/TesAsyncBackendJobExecutionActor.scala#L55); * [AWS Batch](https://github.com/broadinstitute/cromwell/blob/470d482e8ba2a9e2bc544896a4e6ceea57d55bb2/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchAsyncBackendJobExecutionActor.scala#L83). Overrides required for compilation and basic execution of `StandardAsyncExecutionActor` implementations:. * [`type StandardAsyncRunInfo`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L89); encapsulates the type of the run info when a job is started.; * [`type StandardAsyncRunState`](https://github.c",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:174,Integrability,depend,dependencies,174,"## Cromwell Backend Development. ### What’s a Backend?. Cromwell is a workflow execution service, which means it takes a representation of work to do (tasks) as well as the; dependencies and code flow between them. When it comes to actually executing the specific task, Cromwell delegates that; work to a “backend”. A backend is therefore responsible for the actual execution of a single task on some underlying; computing platform, such as Google Cloud Platform, AWS, TES, Local, etc. Backends are implemented as a software layer; between the Cromwell engine and that underlying platform. The underlying platform services requests for a job to run while the backend shim provides the interface layer between; Cromwell and the platform. In general, the more sophisticated the platform the thinner the shim needs to be and vice; versa. A job from Cromwell’s perspective is a collection of the following information:. * A unix command line to run; * A mapping of where its input files currently live to where the command line expects them to be; * A mapping of where the command line will write its outputs to where they should eventually wind up; * An optional Docker image which if supplied will be the environment in which the command will be run; * A collection of arbitrary key/value pairs which can be used by the execution platform to tune the request, e.g. amount; of memory or the number of CPUs. The Docker image is not required for a backend but is highly recommended. The bioinformatics workflow field is rapidly; moving towards a Docker model so one will find better support for their backend if it is built around using Docker; containers. The input/output file mappings will depend on the needs of the platform. In the simplest example these values would be; identical, living on a shared filesystem. However an example of where this would be useful would be a model where an; input file lives in a cloud bucket and is directly copied onto the machine running the command line followed by",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:685,Integrability,interface,interface,685,"## Cromwell Backend Development. ### What’s a Backend?. Cromwell is a workflow execution service, which means it takes a representation of work to do (tasks) as well as the; dependencies and code flow between them. When it comes to actually executing the specific task, Cromwell delegates that; work to a “backend”. A backend is therefore responsible for the actual execution of a single task on some underlying; computing platform, such as Google Cloud Platform, AWS, TES, Local, etc. Backends are implemented as a software layer; between the Cromwell engine and that underlying platform. The underlying platform services requests for a job to run while the backend shim provides the interface layer between; Cromwell and the platform. In general, the more sophisticated the platform the thinner the shim needs to be and vice; versa. A job from Cromwell’s perspective is a collection of the following information:. * A unix command line to run; * A mapping of where its input files currently live to where the command line expects them to be; * A mapping of where the command line will write its outputs to where they should eventually wind up; * An optional Docker image which if supplied will be the environment in which the command will be run; * A collection of arbitrary key/value pairs which can be used by the execution platform to tune the request, e.g. amount; of memory or the number of CPUs. The Docker image is not required for a backend but is highly recommended. The bioinformatics workflow field is rapidly; moving towards a Docker model so one will find better support for their backend if it is built around using Docker; containers. The input/output file mappings will depend on the needs of the platform. In the simplest example these values would be; identical, living on a shared filesystem. However an example of where this would be useful would be a model where an; input file lives in a cloud bucket and is directly copied onto the machine running the command line followed by",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:1688,Integrability,depend,depend,1688,"terface layer between; Cromwell and the platform. In general, the more sophisticated the platform the thinner the shim needs to be and vice; versa. A job from Cromwell’s perspective is a collection of the following information:. * A unix command line to run; * A mapping of where its input files currently live to where the command line expects them to be; * A mapping of where the command line will write its outputs to where they should eventually wind up; * An optional Docker image which if supplied will be the environment in which the command will be run; * A collection of arbitrary key/value pairs which can be used by the execution platform to tune the request, e.g. amount; of memory or the number of CPUs. The Docker image is not required for a backend but is highly recommended. The bioinformatics workflow field is rapidly; moving towards a Docker model so one will find better support for their backend if it is built around using Docker; containers. The input/output file mappings will depend on the needs of the platform. In the simplest example these values would be; identical, living on a shared filesystem. However an example of where this would be useful would be a model where an; input file lives in a cloud bucket and is directly copied onto the machine running the command line followed by copying; the program’s outputs back to a cloud bucket. The underlying platform can be extremely simple or have arbitrary levels of complexity as long as one can map from the; above concepts to running a command. That mapping could happen completely in the platform, the Cromwell backend, or a; mixture of the two. The implementer of a backend will need to strike the balance which works best for both their needs; and the particulars of the platform itself. Throughout the rest of this document the following terms are used, the difference between them may be subtle but; important:. * Workflow: a container of one or more calls, possibly expressing execution dependencies on each other",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:2662,Integrability,depend,dependencies,2662,"r; containers. The input/output file mappings will depend on the needs of the platform. In the simplest example these values would be; identical, living on a shared filesystem. However an example of where this would be useful would be a model where an; input file lives in a cloud bucket and is directly copied onto the machine running the command line followed by copying; the program’s outputs back to a cloud bucket. The underlying platform can be extremely simple or have arbitrary levels of complexity as long as one can map from the; above concepts to running a command. That mapping could happen completely in the platform, the Cromwell backend, or a; mixture of the two. The implementer of a backend will need to strike the balance which works best for both their needs; and the particulars of the platform itself. Throughout the rest of this document the following terms are used, the difference between them may be subtle but; important:. * Workflow: a container of one or more calls, possibly expressing execution dependencies on each other. May contain; scatters, conditional logic, or subworkflow invocations.; * Task: The abstract definition of a thing to run. Think of this like a function in a programming language.; * Call: An instantiated request in Cromwell of a thing to run. To further the function analogy this would be an; invocation of that function.; * Job: The physical manifestation of a call by the backend. Examples of this would be an SGE job, a unix process, or a; Google Life Sciences operation ID. ### Backend Lifecycle. Within a running workflow, the backend is used in the following manner:. * Initialization: Initialization routine called the first time a workflow uses a backend; * Execute: The workflow requests that the backend run a job; * Recover: Attempt to reconnect to a previously started job. An example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finali",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:3297,Integrability,rout,routine,3297,"balance which works best for both their needs; and the particulars of the platform itself. Throughout the rest of this document the following terms are used, the difference between them may be subtle but; important:. * Workflow: a container of one or more calls, possibly expressing execution dependencies on each other. May contain; scatters, conditional logic, or subworkflow invocations.; * Task: The abstract definition of a thing to run. Think of this like a function in a programming language.; * Call: An instantiated request in Cromwell of a thing to run. To further the function analogy this would be an; invocation of that function.; * Job: The physical manifestation of a call by the backend. Examples of this would be an SGE job, a unix process, or a; Google Life Sciences operation ID. ### Backend Lifecycle. Within a running workflow, the backend is used in the following manner:. * Initialization: Initialization routine called the first time a workflow uses a backend; * Execute: The workflow requests that the backend run a job; * Recover: Attempt to reconnect to a previously started job. An example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finalization: When a workflow is complete, allows for any workflow level cleanup to take place. The initialization and finalization steps are optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:6096,Integrability,wrap,wraps,6096,"b/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L27); Handles the initialization phase; * Usually a derivative; of [`StandardAsyncExecutionActor`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L75); to run an individual job within the workflow; * [`BackendWorkflowFinalizationActor`](https://github.com/broadinstitute/cromwell/blob/a40de672c565c4bbd40f57ff96d4ee520dc2b4fc/backend/src/main/scala/cromwell/backend/BackendWorkflowFinalizationActor.scala#L10); Handles the finalization phase. These three actors are all represented by a trait which a backend developer would need to implement. Both the; initialization and finalization actors are optional. The following explanations of the traits mention multiple types defined in the Cromwell codebase. In particular; `BackendJobDescriptor` wraps all the information necessary for a backend to instantiate a job and `JobKey` provides the; information to uniquely identify a job. It is recommended that one look in the Cromwell codebase for more information on; these and other types. #### BackendWorkflowInitializationActor. If a backend developer wishes to take advantage of the initialization phase of the backend lifecycle they must implement; this trait. There are three functions which must be implemented:. * [`abortInitialization: Unit`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L168); specifies what to do, if anything, when a workflow is requested to abort while a backend initialization is in; progress.; * [`validate: Future[Unit]`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L178); is provided so that the backend can ensu",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:4476,Modifiability,extend,extending,4476,"n example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finalization: When a workflow is complete, allows for any workflow level cleanup to take place. The initialization and finalization steps are optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f00976ee258142185917083460d724f7fe3d/cromwell.example.backends/cromwell.examples.conf#L370); * [`BackendWorkflowInitializationActor`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L27); Handles the initialization phase; * Usually a derivative; of [`StandardAsyncExecutionActor`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsync",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:4804,Modifiability,config,configuration,4804,re optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f00976ee258142185917083460d724f7fe3d/cromwell.example.backends/cromwell.examples.conf#L370); * [`BackendWorkflowInitializationActor`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L27); Handles the initialization phase; * Usually a derivative; of [`StandardAsyncExecutionActor`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L75); to run an individual job within the workflow; * [`BackendWorkflowFinalizationActor`](https://github.com/broadinstitute/cromwell/blob/a40de672c565c4bbd40f57ff96d4ee520dc2b4fc/backend/src/main/scala/cromwell/backend/BackendWorkflowFinalizationActor.scala#L10); Handle,MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:4835,Modifiability,config,configuration,4835,backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f00976ee258142185917083460d724f7fe3d/cromwell.example.backends/cromwell.examples.conf#L370); * [`BackendWorkflowInitializationActor`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L27); Handles the initialization phase; * Usually a derivative; of [`StandardAsyncExecutionActor`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L75); to run an individual job within the workflow; * [`BackendWorkflowFinalizationActor`](https://github.com/broadinstitute/cromwell/blob/a40de672c565c4bbd40f57ff96d4ee520dc2b4fc/backend/src/main/scala/cromwell/backend/BackendWorkflowFinalizationActor.scala#L10); Handles the finalization phase. These three actors are all represented by a trait which a b,MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:7778,Modifiability,extend,extend,7778,"specifies what to do, if anything, when a workflow is requested to abort while a backend initialization is in; progress.; * [`validate: Future[Unit]`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L178); is provided so that the backend can ensure that all of the calls it will handle conform to the rules of that backend.; For instance, if a backend requires particular runtime attributes to exist.; * [`beforeAll: Future[Option[BackendInitializationData]]`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L173); is the actual initialization functionality. If a backend requires any work to be done prior to handling a call, that; code must be called from here. #### StandardAsyncExecutionActor. Nearly all production backend implementations in Cromwell extend; the [StandardAsyncExecutionActor](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L75); trait. The minimum overrides for implementations of this trait are enumerated below, but overrides of other methods will; also be required. There are several backend implementations in the Cromwell codebase that can serve as references, for; example:. * [Google Life Sciences / Pipelines API](https://github.com/broadinstitute/cromwell/blob/0aff35336b4e2ba19b18530a68e622df1462d9b7/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala#L95); common layer; * [Life Sciences API (beta)](https://github.com/broadinstitute/cromwell/blob/a49e1fc65703ccfda2840d1d9266fad2bdbb7339/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/PipelinesApiAsy",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:1340,Performance,tune,tune,1340,"f work to do (tasks) as well as the; dependencies and code flow between them. When it comes to actually executing the specific task, Cromwell delegates that; work to a “backend”. A backend is therefore responsible for the actual execution of a single task on some underlying; computing platform, such as Google Cloud Platform, AWS, TES, Local, etc. Backends are implemented as a software layer; between the Cromwell engine and that underlying platform. The underlying platform services requests for a job to run while the backend shim provides the interface layer between; Cromwell and the platform. In general, the more sophisticated the platform the thinner the shim needs to be and vice; versa. A job from Cromwell’s perspective is a collection of the following information:. * A unix command line to run; * A mapping of where its input files currently live to where the command line expects them to be; * A mapping of where the command line will write its outputs to where they should eventually wind up; * An optional Docker image which if supplied will be the environment in which the command will be run; * A collection of arbitrary key/value pairs which can be used by the execution platform to tune the request, e.g. amount; of memory or the number of CPUs. The Docker image is not required for a backend but is highly recommended. The bioinformatics workflow field is rapidly; moving towards a Docker model so one will find better support for their backend if it is built around using Docker; containers. The input/output file mappings will depend on the needs of the platform. In the simplest example these values would be; identical, living on a shared filesystem. However an example of where this would be useful would be a model where an; input file lives in a cloud bucket and is directly copied onto the machine running the command line followed by copying; the program’s outputs back to a cloud bucket. The underlying platform can be extremely simple or have arbitrary levels of compl",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:4038,Performance,perform,perform,4038," of a call by the backend. Examples of this would be an SGE job, a unix process, or a; Google Life Sciences operation ID. ### Backend Lifecycle. Within a running workflow, the backend is used in the following manner:. * Initialization: Initialization routine called the first time a workflow uses a backend; * Execute: The workflow requests that the backend run a job; * Recover: Attempt to reconnect to a previously started job. An example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finalization: When a workflow is complete, allows for any workflow level cleanup to take place. The initialization and finalization steps are optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f00976ee258142185917083460d724f7fe3d/cromwell.example.backends/cromwell.examples.conf#L370); * [`BackendWorkflowInitializat",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:12831,Performance,perform,perform,12831,"`def isTerminal(runStatus: StandardAsyncRunState): Boolean`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L829); Returns true when a job is complete, either successfully or unsuccessfully.; * At least one of:; * [`def execute(): ExecutionHandle`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L738) executes the job specified in the params; * [`def executeAsync(): Future[ExecutionHandle]`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L748) asynchronously executes the job specified in the params; * At least one of:; * [`def pollStatus(handle: StandardAsyncPendingExecutionHandle): StandardAsyncRunState`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L798) returns the run status for the job; * [`def pollStatusAsync(handle: StandardAsyncPendingExecutionHandle): Future[StandardAsyncRunState]`](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L808) asynchronously returns the run status for the job. #### BackendWorkflowFinalizationActor. There is only one function to override for this trait if the backend developer chooses to use the finalization; functionality:. * [`afterAll: Future[Unit]`](https://github.com/broadinstitute/cromwell/blob/a40de672c565c4bbd40f57ff96d4ee520dc2b4fc/backend/src/main/scala/cromwell/backend/BackendWorkflowFinalizationActor.scala#L37); is the hook to perform any desired functionality, and is called when the workflow is completed.; ",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:3913,Safety,recover,recover,3913," to run. To further the function analogy this would be an; invocation of that function.; * Job: The physical manifestation of a call by the backend. Examples of this would be an SGE job, a unix process, or a; Google Life Sciences operation ID. ### Backend Lifecycle. Within a running workflow, the backend is used in the following manner:. * Initialization: Initialization routine called the first time a workflow uses a backend; * Execute: The workflow requests that the backend run a job; * Recover: Attempt to reconnect to a previously started job. An example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finalization: When a workflow is complete, allows for any workflow level cleanup to take place. The initialization and finalization steps are optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:3925,Safety,abort,abort,3925," to run. To further the function analogy this would be an; invocation of that function.; * Job: The physical manifestation of a call by the backend. Examples of this would be an SGE job, a unix process, or a; Google Life Sciences operation ID. ### Backend Lifecycle. Within a running workflow, the backend is used in the following manner:. * Initialization: Initialization routine called the first time a workflow uses a backend; * Execute: The workflow requests that the backend run a job; * Recover: Attempt to reconnect to a previously started job. An example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finalization: When a workflow is complete, allows for any workflow level cleanup to take place. The initialization and finalization steps are optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:3987,Safety,recover,recover,3987," of a call by the backend. Examples of this would be an SGE job, a unix process, or a; Google Life Sciences operation ID. ### Backend Lifecycle. Within a running workflow, the backend is used in the following manner:. * Initialization: Initialization routine called the first time a workflow uses a backend; * Execute: The workflow requests that the backend run a job; * Recover: Attempt to reconnect to a previously started job. An example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finalization: When a workflow is complete, allows for any workflow level cleanup to take place. The initialization and finalization steps are optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f00976ee258142185917083460d724f7fe3d/cromwell.example.backends/cromwell.examples.conf#L370); * [`BackendWorkflowInitializat",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:4070,Safety,abort,abort,4070," of a call by the backend. Examples of this would be an SGE job, a unix process, or a; Google Life Sciences operation ID. ### Backend Lifecycle. Within a running workflow, the backend is used in the following manner:. * Initialization: Initialization routine called the first time a workflow uses a backend; * Execute: The workflow requests that the backend run a job; * Recover: Attempt to reconnect to a previously started job. An example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finalization: When a workflow is complete, allows for any workflow level cleanup to take place. The initialization and finalization steps are optional and are provided for cases where a backend needs that behavior;; not all backends will need these. The implementations of both the recover and abort steps are up to the backend developer. For instance the recover; function could be implemented to actually perform an execution and/or the abort function could be written to do nothing; at all. Implementing these in a more robust manner is recommended for most cases but neither are universally; appropriate. ### How do I create a Backend?. This section is assuming that you both know the underlying execution platform you wish to use and that you know how to; programmatically submit work to it. The Cromwell engine uses backend-specific types extending Akka `Actor` while processing a workflow:. * [`BackendLifecycleActorFactory`](https://github.com/broadinstitute/cromwell/blob/9bf1622ca8988365477b77b9f26ce388b54fc58c/backend/src/main/scala/cromwell/backend/BackendLifecycleActorFactory.scala#L17); The entry point into the backend implementation specified in Cromwell configuration. e.g.; a [sample configuration for the Local backend](https://github.com/broadinstitute/cromwell/blob/2b19f00976ee258142185917083460d724f7fe3d/cromwell.example.backends/cromwell.examples.conf#L370); * [`BackendWorkflowInitializat",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:6572,Safety,abort,abortInitialization,6572,"ttps://github.com/broadinstitute/cromwell/blob/a40de672c565c4bbd40f57ff96d4ee520dc2b4fc/backend/src/main/scala/cromwell/backend/BackendWorkflowFinalizationActor.scala#L10); Handles the finalization phase. These three actors are all represented by a trait which a backend developer would need to implement. Both the; initialization and finalization actors are optional. The following explanations of the traits mention multiple types defined in the Cromwell codebase. In particular; `BackendJobDescriptor` wraps all the information necessary for a backend to instantiate a job and `JobKey` provides the; information to uniquely identify a job. It is recommended that one look in the Cromwell codebase for more information on; these and other types. #### BackendWorkflowInitializationActor. If a backend developer wishes to take advantage of the initialization phase of the backend lifecycle they must implement; this trait. There are three functions which must be implemented:. * [`abortInitialization: Unit`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L168); specifies what to do, if anything, when a workflow is requested to abort while a backend initialization is in; progress.; * [`validate: Future[Unit]`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L178); is provided so that the backend can ensure that all of the calls it will handle conform to the rules of that backend.; For instance, if a backend requires particular runtime attributes to exist.; * [`beforeAll: Future[Option[BackendInitializationData]]`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L173); is the actual initialization functionality. If a backe",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:6844,Safety,abort,abort,6844,"nted by a trait which a backend developer would need to implement. Both the; initialization and finalization actors are optional. The following explanations of the traits mention multiple types defined in the Cromwell codebase. In particular; `BackendJobDescriptor` wraps all the information necessary for a backend to instantiate a job and `JobKey` provides the; information to uniquely identify a job. It is recommended that one look in the Cromwell codebase for more information on; these and other types. #### BackendWorkflowInitializationActor. If a backend developer wishes to take advantage of the initialization phase of the backend lifecycle they must implement; this trait. There are three functions which must be implemented:. * [`abortInitialization: Unit`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L168); specifies what to do, if anything, when a workflow is requested to abort while a backend initialization is in; progress.; * [`validate: Future[Unit]`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L178); is provided so that the backend can ensure that all of the calls it will handle conform to the rules of that backend.; For instance, if a backend requires particular runtime attributes to exist.; * [`beforeAll: Future[Option[BackendInitializationData]]`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L173); is the actual initialization functionality. If a backend requires any work to be done prior to handling a call, that; code must be called from here. #### StandardAsyncExecutionActor. Nearly all production backend implementations in Cromwell extend; the [StandardAsyncExecutionActor](https://git",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:6903,Security,validat,validate,6903,"n and finalization actors are optional. The following explanations of the traits mention multiple types defined in the Cromwell codebase. In particular; `BackendJobDescriptor` wraps all the information necessary for a backend to instantiate a job and `JobKey` provides the; information to uniquely identify a job. It is recommended that one look in the Cromwell codebase for more information on; these and other types. #### BackendWorkflowInitializationActor. If a backend developer wishes to take advantage of the initialization phase of the backend lifecycle they must implement; this trait. There are three functions which must be implemented:. * [`abortInitialization: Unit`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L168); specifies what to do, if anything, when a workflow is requested to abort while a backend initialization is in; progress.; * [`validate: Future[Unit]`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L178); is provided so that the backend can ensure that all of the calls it will handle conform to the rules of that backend.; For instance, if a backend requires particular runtime attributes to exist.; * [`beforeAll: Future[Option[BackendInitializationData]]`](https://github.com/broadinstitute/cromwell/blob/93392acf2881921dcf22ef4dbda12af42339b3ab/backend/src/main/scala/cromwell/backend/BackendWorkflowInitializationActor.scala#L173); is the actual initialization functionality. If a backend requires any work to be done prior to handling a call, that; code must be called from here. #### StandardAsyncExecutionActor. Nearly all production backend implementations in Cromwell extend; the [StandardAsyncExecutionActor](https://github.com/broadinstitute/cromwell/blob/9181235d364712b78dbea1f35042c3c6e431af87/backend/src",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:2725,Testability,log,logic,2725," In the simplest example these values would be; identical, living on a shared filesystem. However an example of where this would be useful would be a model where an; input file lives in a cloud bucket and is directly copied onto the machine running the command line followed by copying; the program’s outputs back to a cloud bucket. The underlying platform can be extremely simple or have arbitrary levels of complexity as long as one can map from the; above concepts to running a command. That mapping could happen completely in the platform, the Cromwell backend, or a; mixture of the two. The implementer of a backend will need to strike the balance which works best for both their needs; and the particulars of the platform itself. Throughout the rest of this document the following terms are used, the difference between them may be subtle but; important:. * Workflow: a container of one or more calls, possibly expressing execution dependencies on each other. May contain; scatters, conditional logic, or subworkflow invocations.; * Task: The abstract definition of a thing to run. Think of this like a function in a programming language.; * Call: An instantiated request in Cromwell of a thing to run. To further the function analogy this would be an; invocation of that function.; * Job: The physical manifestation of a call by the backend. Examples of this would be an SGE job, a unix process, or a; Google Life Sciences operation ID. ### Backend Lifecycle. Within a running workflow, the backend is used in the following manner:. * Initialization: Initialization routine called the first time a workflow uses a backend; * Execute: The workflow requests that the backend run a job; * Recover: Attempt to reconnect to a previously started job. An example of this would be if Cromwell was restarted and; wanted to reattach to currently running jobs.; * Abort: Request that a running job be halted; * Finalization: When a workflow is complete, allows for any workflow level cleanup to take plac",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:1732,Usability,simpl,simplest,1732,"ted the platform the thinner the shim needs to be and vice; versa. A job from Cromwell’s perspective is a collection of the following information:. * A unix command line to run; * A mapping of where its input files currently live to where the command line expects them to be; * A mapping of where the command line will write its outputs to where they should eventually wind up; * An optional Docker image which if supplied will be the environment in which the command will be run; * A collection of arbitrary key/value pairs which can be used by the execution platform to tune the request, e.g. amount; of memory or the number of CPUs. The Docker image is not required for a backend but is highly recommended. The bioinformatics workflow field is rapidly; moving towards a Docker model so one will find better support for their backend if it is built around using Docker; containers. The input/output file mappings will depend on the needs of the platform. In the simplest example these values would be; identical, living on a shared filesystem. However an example of where this would be useful would be a model where an; input file lives in a cloud bucket and is directly copied onto the machine running the command line followed by copying; the program’s outputs back to a cloud bucket. The underlying platform can be extremely simple or have arbitrary levels of complexity as long as one can map from the; above concepts to running a command. That mapping could happen completely in the platform, the Cromwell backend, or a; mixture of the two. The implementer of a backend will need to strike the balance which works best for both their needs; and the particulars of the platform itself. Throughout the rest of this document the following terms are used, the difference between them may be subtle but; important:. * Workflow: a container of one or more calls, possibly expressing execution dependencies on each other. May contain; scatters, conditional logic, or subworkflow invocations.; * Task: ",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md:2098,Usability,simpl,simple,2098,"ly wind up; * An optional Docker image which if supplied will be the environment in which the command will be run; * A collection of arbitrary key/value pairs which can be used by the execution platform to tune the request, e.g. amount; of memory or the number of CPUs. The Docker image is not required for a backend but is highly recommended. The bioinformatics workflow field is rapidly; moving towards a Docker model so one will find better support for their backend if it is built around using Docker; containers. The input/output file mappings will depend on the needs of the platform. In the simplest example these values would be; identical, living on a shared filesystem. However an example of where this would be useful would be a model where an; input file lives in a cloud bucket and is directly copied onto the machine running the command line followed by copying; the program’s outputs back to a cloud bucket. The underlying platform can be extremely simple or have arbitrary levels of complexity as long as one can map from the; above concepts to running a command. That mapping could happen completely in the platform, the Cromwell backend, or a; mixture of the two. The implementer of a backend will need to strike the balance which works best for both their needs; and the particulars of the platform itself. Throughout the rest of this document the following terms are used, the difference between them may be subtle but; important:. * Workflow: a container of one or more calls, possibly expressing execution dependencies on each other. May contain; scatters, conditional logic, or subworkflow invocations.; * Task: The abstract definition of a thing to run. Think of this like a function in a programming language.; * Call: An instantiated request in Cromwell of a thing to run. To further the function analogy this would be an; invocation of that function.; * Job: The physical manifestation of a call by the backend. Examples of this would be an SGE job, a unix process, or a; Go",MatchSource.DOCS,docs/developers/Backend.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Backend.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Building.md:77,Deployability,release,releases,77,"Most users should not need to build Cromwell and can use pre-built Cromwell [releases](Getting). If for some reason you require a non-release version of Cromwell or are developing new Cromwell; features or fixes, the following are required to build Cromwell from source:. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). First start by cloning the Cromwell repository from GitHub:. ```bash; $ git clone git@github.com:broadinstitute/cromwell.git; ```. Next change into the `cromwell` directory:. ```bash; $ cd cromwell; ```. If you require a specific version of Cromwell as a starting point, do the appropriate `git checkout` now. . Finally build the Cromwell jar:. ```bash; $ sbt assembly; ```. `sbt assembly` will build the runnable Cromwell JAR in `server/target/scala-2.13/` with a name like `cromwell-<VERSION>.jar`. It will also build a runnable Womtool JAR in `womtool/target/scala-2.13/` with a name like `womtool-<VERSION>.jar`. ## Docker. The following Docker build configurations are supported. Most users will want Snapshot, resulting in an image like `broadinstitute/cromwell:<VERSION>-SNAP`. | Command | Build Type | Debug Tools | Description |; |------------------------------------------------|------------|-------------|--------------------------------------|; | `sbt server/docker` | Snapshot | No | Most common local build |; | `sbt -Dproject.isDebug=true server/docker` | Debug | Yes | Local build with debugging/profiling |; | `sbt -Dproject.isSnapshot=false server/docker` | Standard | No | Reserved for CI: commit on `develop` |; | `sbt -Dproject.isRelease=true server/docker` | Release | No | Reserved for CI: numbered release |; ",MatchSource.DOCS,docs/developers/Building.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Building.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Building.md:134,Deployability,release,release,134,"Most users should not need to build Cromwell and can use pre-built Cromwell [releases](Getting). If for some reason you require a non-release version of Cromwell or are developing new Cromwell; features or fixes, the following are required to build Cromwell from source:. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). First start by cloning the Cromwell repository from GitHub:. ```bash; $ git clone git@github.com:broadinstitute/cromwell.git; ```. Next change into the `cromwell` directory:. ```bash; $ cd cromwell; ```. If you require a specific version of Cromwell as a starting point, do the appropriate `git checkout` now. . Finally build the Cromwell jar:. ```bash; $ sbt assembly; ```. `sbt assembly` will build the runnable Cromwell JAR in `server/target/scala-2.13/` with a name like `cromwell-<VERSION>.jar`. It will also build a runnable Womtool JAR in `womtool/target/scala-2.13/` with a name like `womtool-<VERSION>.jar`. ## Docker. The following Docker build configurations are supported. Most users will want Snapshot, resulting in an image like `broadinstitute/cromwell:<VERSION>-SNAP`. | Command | Build Type | Debug Tools | Description |; |------------------------------------------------|------------|-------------|--------------------------------------|; | `sbt server/docker` | Snapshot | No | Most common local build |; | `sbt -Dproject.isDebug=true server/docker` | Debug | Yes | Local build with debugging/profiling |; | `sbt -Dproject.isSnapshot=false server/docker` | Standard | No | Reserved for CI: commit on `develop` |; | `sbt -Dproject.isRelease=true server/docker` | Release | No | Reserved for CI: numbered release |; ",MatchSource.DOCS,docs/developers/Building.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Building.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Building.md:1099,Deployability,configurat,configurations,1099,"Most users should not need to build Cromwell and can use pre-built Cromwell [releases](Getting). If for some reason you require a non-release version of Cromwell or are developing new Cromwell; features or fixes, the following are required to build Cromwell from source:. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). First start by cloning the Cromwell repository from GitHub:. ```bash; $ git clone git@github.com:broadinstitute/cromwell.git; ```. Next change into the `cromwell` directory:. ```bash; $ cd cromwell; ```. If you require a specific version of Cromwell as a starting point, do the appropriate `git checkout` now. . Finally build the Cromwell jar:. ```bash; $ sbt assembly; ```. `sbt assembly` will build the runnable Cromwell JAR in `server/target/scala-2.13/` with a name like `cromwell-<VERSION>.jar`. It will also build a runnable Womtool JAR in `womtool/target/scala-2.13/` with a name like `womtool-<VERSION>.jar`. ## Docker. The following Docker build configurations are supported. Most users will want Snapshot, resulting in an image like `broadinstitute/cromwell:<VERSION>-SNAP`. | Command | Build Type | Debug Tools | Description |; |------------------------------------------------|------------|-------------|--------------------------------------|; | `sbt server/docker` | Snapshot | No | Most common local build |; | `sbt -Dproject.isDebug=true server/docker` | Debug | Yes | Local build with debugging/profiling |; | `sbt -Dproject.isSnapshot=false server/docker` | Standard | No | Reserved for CI: commit on `develop` |; | `sbt -Dproject.isRelease=true server/docker` | Release | No | Reserved for CI: numbered release |; ",MatchSource.DOCS,docs/developers/Building.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Building.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Building.md:1766,Deployability,release,release,1766,"Most users should not need to build Cromwell and can use pre-built Cromwell [releases](Getting). If for some reason you require a non-release version of Cromwell or are developing new Cromwell; features or fixes, the following are required to build Cromwell from source:. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). First start by cloning the Cromwell repository from GitHub:. ```bash; $ git clone git@github.com:broadinstitute/cromwell.git; ```. Next change into the `cromwell` directory:. ```bash; $ cd cromwell; ```. If you require a specific version of Cromwell as a starting point, do the appropriate `git checkout` now. . Finally build the Cromwell jar:. ```bash; $ sbt assembly; ```. `sbt assembly` will build the runnable Cromwell JAR in `server/target/scala-2.13/` with a name like `cromwell-<VERSION>.jar`. It will also build a runnable Womtool JAR in `womtool/target/scala-2.13/` with a name like `womtool-<VERSION>.jar`. ## Docker. The following Docker build configurations are supported. Most users will want Snapshot, resulting in an image like `broadinstitute/cromwell:<VERSION>-SNAP`. | Command | Build Type | Debug Tools | Description |; |------------------------------------------------|------------|-------------|--------------------------------------|; | `sbt server/docker` | Snapshot | No | Most common local build |; | `sbt -Dproject.isDebug=true server/docker` | Debug | Yes | Local build with debugging/profiling |; | `sbt -Dproject.isSnapshot=false server/docker` | Standard | No | Reserved for CI: commit on `develop` |; | `sbt -Dproject.isRelease=true server/docker` | Release | No | Reserved for CI: numbered release |; ",MatchSource.DOCS,docs/developers/Building.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Building.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Building.md:1099,Modifiability,config,configurations,1099,"Most users should not need to build Cromwell and can use pre-built Cromwell [releases](Getting). If for some reason you require a non-release version of Cromwell or are developing new Cromwell; features or fixes, the following are required to build Cromwell from source:. * [Scala 2.13](http://www.scala-lang.org/); * [SBT 1.x](https://www.scala-sbt.org/); * [AdoptOpenJDK 11 HotSpot](https://adoptopenjdk.net/); * [Git](https://git-scm.com/). First start by cloning the Cromwell repository from GitHub:. ```bash; $ git clone git@github.com:broadinstitute/cromwell.git; ```. Next change into the `cromwell` directory:. ```bash; $ cd cromwell; ```. If you require a specific version of Cromwell as a starting point, do the appropriate `git checkout` now. . Finally build the Cromwell jar:. ```bash; $ sbt assembly; ```. `sbt assembly` will build the runnable Cromwell JAR in `server/target/scala-2.13/` with a name like `cromwell-<VERSION>.jar`. It will also build a runnable Womtool JAR in `womtool/target/scala-2.13/` with a name like `womtool-<VERSION>.jar`. ## Docker. The following Docker build configurations are supported. Most users will want Snapshot, resulting in an image like `broadinstitute/cromwell:<VERSION>-SNAP`. | Command | Build Type | Debug Tools | Description |; |------------------------------------------------|------------|-------------|--------------------------------------|; | `sbt server/docker` | Snapshot | No | Most common local build |; | `sbt -Dproject.isDebug=true server/docker` | Debug | Yes | Local build with debugging/profiling |; | `sbt -Dproject.isSnapshot=false server/docker` | Standard | No | Reserved for CI: commit on `develop` |; | `sbt -Dproject.isRelease=true server/docker` | Release | No | Reserved for CI: numbered release |; ",MatchSource.DOCS,docs/developers/Building.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Building.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:3429,Availability,failure,failures,3429,"AT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: ""local"", ""gcs"". Used in conjunction with outputExpectations to define files we expect to exist after running this workflow.; outputExpectations: {; ""/path/to/my/output/file1"": 1; ""/path/to/file/that/should/not/exist"": 0; }; ```. The tags are optional. If supplied they will allow people to turn on or off this test case by including or excluding tags when running (see above). The `basePath` field is optional, but if supplied all paths will be resolved from that directory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workf",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:3482,Availability,error,error,3482,"AT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: ""local"", ""gcs"". Used in conjunction with outputExpectations to define files we expect to exist after running this workflow.; outputExpectations: {; ""/path/to/my/output/file1"": 1; ""/path/to/file/that/should/not/exist"": 0; }; ```. The tags are optional. If supplied they will allow people to turn on or off this test case by including or excluding tags when running (see above). The `basePath` field is optional, but if supplied all paths will be resolved from that directory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workf",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:3521,Availability,error,errors,3521,"ching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: ""local"", ""gcs"". Used in conjunction with outputExpectations to define files we expect to exist after running this workflow.; outputExpectations: {; ""/path/to/my/output/file1"": 1; ""/path/to/file/that/should/not/exist"": 0; }; ```. The tags are optional. If supplied they will allow people to turn on or off this test case by including or excluding tags when running (see above). The `basePath` field is optional, but if supplied all paths will be resolved from that directory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workflowsuccess`: The workflow being supplied is expected to successfully complete; * `workf",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:3623,Availability,failure,failures,3623,"pplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: ""local"", ""gcs"". Used in conjunction with outputExpectations to define files we expect to exist after running this workflow.; outputExpectations: {; ""/path/to/my/output/file1"": 1; ""/path/to/file/that/should/not/exist"": 0; }; ```. The tags are optional. If supplied they will allow people to turn on or off this test case by including or excluding tags when running (see above). The `basePath` field is optional, but if supplied all paths will be resolved from that directory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workflowsuccess`: The workflow being supplied is expected to successfully complete; * `workflowfailure`: The workflow being supplied is expected to fail. The `metadata` is option",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:3698,Availability,failure,failures,3698,"he Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: ""local"", ""gcs"". Used in conjunction with outputExpectations to define files we expect to exist after running this workflow.; outputExpectations: {; ""/path/to/my/output/file1"": 1; ""/path/to/file/that/should/not/exist"": 0; }; ```. The tags are optional. If supplied they will allow people to turn on or off this test case by including or excluding tags when running (see above). The `basePath` field is optional, but if supplied all paths will be resolved from that directory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workflowsuccess`: The workflow being supplied is expected to successfully complete; * `workflowfailure`: The workflow being supplied is expected to fail. The `metadata` is optional. If supplied, Centaur will retrieve the metadata from the successfully c",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:14,Deployability,integrat,integration,14,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:179,Deployability,deploy,deployment,179,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:806,Deployability,integrat,integration,806,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1123,Deployability,continuous,continuous,1123,"ose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be ac",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1134,Deployability,integrat,integration,1134,"ose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be ac",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1146,Deployability,pipeline,pipeline,1146,"ose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be ac",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1155,Deployability,configurat,configuration,1155,"ose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be ac",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1222,Deployability,configurat,configurations,1222,"g properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; c",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5729,Deployability,configurat,configuration,5729,"retrieved to those supplied. At the moment the only fields supported are strings, numbers and booleans. You can find which metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |; | PAPI Upgrade | `papi_v1_v2alpha1_upgrade_application.conf`** | `centaur_application.conf`* |; | Papi Upgrade<br>New Workflows | `(backend)_application.conf` | `centau",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5884,Deployability,configurat,configuration,5884,"metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |; | PAPI Upgrade | `papi_v1_v2alpha1_upgrade_application.conf`** | `centaur_application.conf`* |; | Papi Upgrade<br>New Workflows | `(backend)_application.conf` | `centaur_application.conf`* |; | Azure Blob | `centaur_blob_test.conf`** | `centaur_application.conf`* |; | WDL Upgrade | `(backen",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5949,Deployability,configurat,configuration,5949,"my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |; | PAPI Upgrade | `papi_v1_v2alpha1_upgrade_application.conf`** | `centaur_application.conf`* |; | Papi Upgrade<br>New Workflows | `(backend)_application.conf` | `centaur_application.conf`* |; | Azure Blob | `centaur_blob_test.conf`** | `centaur_application.conf`* |; | WDL Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | (other) | `(backend)_application.co",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:9631,Deployability,upgrade,upgrade,9631,"pt when overridden with `papi_v2alpha1_centaur_application.conf`; or `papi_v2beta_centaur_application.conf`; ([48 preview link](https://github.com/broadinstitute/cromwell/blob/a7d0601/src/ci/bin/test.inc.sh#L455-L457)) ; \*\* Cromwell Config overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L213-L221)) ; \*\*\* Test Directory overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L440-L449)); </small>. - Engine Upgrade: Retrieves the [Cromwell Version](https://github.com/broadinstitute/cromwell/blob/47/project/Version.scala#L8) then retrieves the previous jar/docker-image from DockerHub. Centaur starts with the prior version, then restarts with the compiled source code.; - Horicromtal: Runs a [docker-compose](https://github.com/broadinstitute/cromwell/blob/47/src/ci/docker-compose/docker-compose-horicromtal.yml) with:; 1. db-mstr: started first; 2. sum-back: runs summarizer; 3. front-back: exposes HTTP; - Horicromtal Engine Upgrade: Combination of Horicromtal and Engine Upgrade; - PAPI Upgrade: Tests run with an older version of Papi and upon restart use a newer version of Papi; - PAPI Upgrade New Workflows: Test definition [does not run any tests](https://travis-ci.org/broadinstitute/cromwell/jobs/475378412); - WDL Upgrade: Upgrades WDL from draft-2 to 1.0 before testing; - (other): Runs `*.test` files listing the configured backend names. ## RDBMS. | Backend | MySQL | PostgreSQL | MariaDB |; |---------|:------:|:-----------:|:--------:|; | AWS | ✅ | | |; | Local | ✅ | ✅ | |; | PAPI V2 | ✅ | | ⭕ |; | SLURM | ✅ | | |; | TES | ✅ | | |. <small>; ⭕ Tests Horicromtal Engine Upgrade versus standard Centaur suite; </small>. All backends run against MySQL. The Local backend also test PostgreSQL, allowing contributors ensure WDLs work with PostgreSQL. MariaDB is tested on a specialized upgrade, where the MySQL connector client is used first, and the MariaDB client is used after restart.; ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:14,Integrability,integrat,integration,14,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:806,Integrability,integrat,integration,806,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1134,Integrability,integrat,integration,1134,"ose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be ac",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:3588,Integrability,message,message,3588,"tional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: ""local"", ""gcs"". Used in conjunction with outputExpectations to define files we expect to exist after running this workflow.; outputExpectations: {; ""/path/to/my/output/file1"": 1; ""/path/to/file/that/should/not/exist"": 0; }; ```. The tags are optional. If supplied they will allow people to turn on or off this test case by including or excluding tags when running (see above). The `basePath` field is optional, but if supplied all paths will be resolved from that directory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workflowsuccess`: The workflow being supplied is expected to successfully complete; * `workflowfailure`: The workflow being supplied is expected to ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:3634,Integrability,message,message,3634,"hese backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: ""local"", ""gcs"". Used in conjunction with outputExpectations to define files we expect to exist after running this workflow.; outputExpectations: {; ""/path/to/my/output/file1"": 1; ""/path/to/file/that/should/not/exist"": 0; }; ```. The tags are optional. If supplied they will allow people to turn on or off this test case by including or excluding tags when running (see above). The `basePath` field is optional, but if supplied all paths will be resolved from that directory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workflowsuccess`: The workflow being supplied is expected to successfully complete; * `workflowfailure`: The workflow being supplied is expected to fail. The `metadata` is optional. If supplied, Centaur will retrieve",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5865,Integrability,depend,depending,5865,"metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |; | PAPI Upgrade | `papi_v1_v2alpha1_upgrade_application.conf`** | `centaur_application.conf`* |; | Papi Upgrade<br>New Workflows | `(backend)_application.conf` | `centaur_application.conf`* |; | Azure Blob | `centaur_blob_test.conf`** | `centaur_application.conf`* |; | WDL Upgrade | `(backen",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:331,Modifiability,config,configured,331,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:408,Modifiability,config,configured,408,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1155,Modifiability,config,configuration,1155,"ose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be ac",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1222,Modifiability,config,configurations,1222,"g properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; c",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5729,Modifiability,config,configuration,5729,"retrieved to those supplied. At the moment the only fields supported are strings, numbers and booleans. You can find which metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |; | PAPI Upgrade | `papi_v1_v2alpha1_upgrade_application.conf`** | `centaur_application.conf`* |; | Papi Upgrade<br>New Workflows | `(backend)_application.conf` | `centau",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5884,Modifiability,config,configuration,5884,"metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |; | PAPI Upgrade | `papi_v1_v2alpha1_upgrade_application.conf`** | `centaur_application.conf`* |; | Papi Upgrade<br>New Workflows | `(backend)_application.conf` | `centaur_application.conf`* |; | Azure Blob | `centaur_blob_test.conf`** | `centaur_application.conf`* |; | WDL Upgrade | `(backen",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5949,Modifiability,config,configuration,5949,"my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |; | PAPI Upgrade | `papi_v1_v2alpha1_upgrade_application.conf`** | `centaur_application.conf`* |; | Papi Upgrade<br>New Workflows | `(backend)_application.conf` | `centaur_application.conf`* |; | Azure Blob | `centaur_blob_test.conf`** | `centaur_application.conf`* |; | WDL Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | (other) | `(backend)_application.co",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:9159,Modifiability,config,configured,9159,"pt when overridden with `papi_v2alpha1_centaur_application.conf`; or `papi_v2beta_centaur_application.conf`; ([48 preview link](https://github.com/broadinstitute/cromwell/blob/a7d0601/src/ci/bin/test.inc.sh#L455-L457)) ; \*\* Cromwell Config overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L213-L221)) ; \*\*\* Test Directory overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L440-L449)); </small>. - Engine Upgrade: Retrieves the [Cromwell Version](https://github.com/broadinstitute/cromwell/blob/47/project/Version.scala#L8) then retrieves the previous jar/docker-image from DockerHub. Centaur starts with the prior version, then restarts with the compiled source code.; - Horicromtal: Runs a [docker-compose](https://github.com/broadinstitute/cromwell/blob/47/src/ci/docker-compose/docker-compose-horicromtal.yml) with:; 1. db-mstr: started first; 2. sum-back: runs summarizer; 3. front-back: exposes HTTP; - Horicromtal Engine Upgrade: Combination of Horicromtal and Engine Upgrade; - PAPI Upgrade: Tests run with an older version of Papi and upon restart use a newer version of Papi; - PAPI Upgrade New Workflows: Test definition [does not run any tests](https://travis-ci.org/broadinstitute/cromwell/jobs/475378412); - WDL Upgrade: Upgrades WDL from draft-2 to 1.0 before testing; - (other): Runs `*.test` files listing the configured backend names. ## RDBMS. | Backend | MySQL | PostgreSQL | MariaDB |; |---------|:------:|:-----------:|:--------:|; | AWS | ✅ | | |; | Local | ✅ | ✅ | |; | PAPI V2 | ✅ | | ⭕ |; | SLURM | ✅ | | |; | TES | ✅ | | |. <small>; ⭕ Tests Horicromtal Engine Upgrade versus standard Centaur suite; </small>. All backends run against MySQL. The Local backend also test PostgreSQL, allowing contributors ensure WDLs work with PostgreSQL. MariaDB is tested on a specialized upgrade, where the MySQL connector client is used first, and the MariaDB client is used after restart.; ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:3685,Security,validat,validate,3685,"hese backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: ""local"", ""gcs"". Used in conjunction with outputExpectations to define files we expect to exist after running this workflow.; outputExpectations: {; ""/path/to/my/output/file1"": 1; ""/path/to/file/that/should/not/exist"": 0; }; ```. The tags are optional. If supplied they will allow people to turn on or off this test case by including or excluding tags when running (see above). The `basePath` field is optional, but if supplied all paths will be resolved from that directory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workflowsuccess`: The workflow being supplied is expected to successfully complete; * `workflowfailure`: The workflow being supplied is expected to fail. The `metadata` is optional. If supplied, Centaur will retrieve",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:8725,Security,expose,exposes,8725,"pt when overridden with `papi_v2alpha1_centaur_application.conf`; or `papi_v2beta_centaur_application.conf`; ([48 preview link](https://github.com/broadinstitute/cromwell/blob/a7d0601/src/ci/bin/test.inc.sh#L455-L457)) ; \*\* Cromwell Config overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L213-L221)) ; \*\*\* Test Directory overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L440-L449)); </small>. - Engine Upgrade: Retrieves the [Cromwell Version](https://github.com/broadinstitute/cromwell/blob/47/project/Version.scala#L8) then retrieves the previous jar/docker-image from DockerHub. Centaur starts with the prior version, then restarts with the compiled source code.; - Horicromtal: Runs a [docker-compose](https://github.com/broadinstitute/cromwell/blob/47/src/ci/docker-compose/docker-compose-horicromtal.yml) with:; 1. db-mstr: started first; 2. sum-back: runs summarizer; 3. front-back: exposes HTTP; - Horicromtal Engine Upgrade: Combination of Horicromtal and Engine Upgrade; - PAPI Upgrade: Tests run with an older version of Papi and upon restart use a newer version of Papi; - PAPI Upgrade New Workflows: Test definition [does not run any tests](https://travis-ci.org/broadinstitute/cromwell/jobs/475378412); - WDL Upgrade: Upgrades WDL from draft-2 to 1.0 before testing; - (other): Runs `*.test` files listing the configured backend names. ## RDBMS. | Backend | MySQL | PostgreSQL | MariaDB |; |---------|:------:|:-----------:|:--------:|; | AWS | ✅ | | |; | Local | ✅ | ✅ | |; | PAPI V2 | ✅ | | ⭕ |; | SLURM | ✅ | | |; | TES | ✅ | | |. <small>; ⭕ Tests Horicromtal Engine Upgrade versus standard Centaur suite; </small>. All backends run against MySQL. The Local backend also test PostgreSQL, allowing contributors ensure WDLs work with PostgreSQL. MariaDB is tested on a specialized upgrade, where the MySQL connector client is used first, and the MariaDB client is used after restart.; ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:26,Testability,test,testing,26,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:732,Testability,test,tests,732,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:818,Testability,test,tests,818,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:862,Testability,test,test,862,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:901,Testability,test,tests,901,"Centaur is an integration testing suite for the [Cromwell](http://github.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests te",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1070,Testability,test,testCentaurLocal,1070,"b.com/broadinstitute/cromwell) execution engine. Its purpose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1107,Testability,test,tests,1107,"ose is to exercise the functionality of a specific deployment of Cromwell, to ensure that it is functioning properly 'in the wild'. . ## Prerequisites. Centaur expects to find a Cromwell server properly configured and running in server mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be ac",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1314,Testability,test,tests,1314,"er mode, listening on port 8000. ; This can be configured by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with th",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1413,Testability,test,test,1413," by modifying the `cromwellUrl` parameter in `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1460,Testability,test,test,1460,"n `application.conf`. You can get a build of your current Cromwell code with [these instructions](Building.md).; The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Requ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1518,Testability,test,tests,1518," The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, B",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1617,Testability,test,testOnly,1617," The server can be run with `java -jar <Cromwell JAR> server`, checkout [this page](../CommandLine.md) ; for more detailed instructions. ; You can now run the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, B",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1674,Testability,test,tests,1674,"un the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; baseP",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1773,Testability,test,testOnly,1773,"un the tests from another terminal. ## Running. There are two ways to invoke the integration tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; baseP",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1821,Testability,test,tests,1821,"on tests:. * `sbt ""centaur / IntegrationTest / test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relativ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1849,Testability,test,tests,1849,"/ test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Opt",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1862,Testability,test,test,1862,"/ test""` - compiles Centaur and runs all tests via sbt directly. Tests are expected to be in the `centaur/src/main/standardTestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Opt",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1993,Testability,test,tests,1993,"TestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when ru",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:1999,Testability,test,test,1999,"TestCases` directory. This can be changed by modifying `reference.conf`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when ru",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:2083,Testability,test,test,2083,"f`. * `src/ci/bin/testCentaurLocal.sh` - runs the same tests using the continuous integration pipeline configuration. * Tests that require different Cromwell and Centaur configurations can be invoked by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file t",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:2278,Testability,test,tests,2278,"ed by calling the various scripts in `src/ci/bin`. ### Tags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:2306,Testability,test,test,2306,"ags. All tests are tagged with their name and their TESTFORMAT, and also any custom tags specified in the `.test` file. Tag names are all lower case, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:2323,Testability,test,test,2323,"e, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:2426,Testability,test,test,2426,"e, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:2432,Testability,test,testFormat,2432,"e, so a test named ""tagFoo"" has a tag ""tagfoo"". To run only those tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -n tagfoo""; ```. Or to instead exclude all tests which have been tagged with a specified tag `tagFoo`:; ```; sbt ""centaur / IntegrationTest / testOnly * -- -l tagfoo""; ```. ## Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:2639,Testability,test,test,2639,"# Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: """,MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:2906,Testability,test,test,2906,"# Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: """,MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:2972,Testability,test,test,2972,"# Adding custom tests. You can add your own tests to the test suite by adding `-Dcentaur.optionalTestPath=DIR` on your sbt invocation, ; e.g. `sbt -Dcentaur.optionalTestPath=/some/path/to/tests test`. The value of `DIR` is expected to be a directory; which contains one or more test case files.; ; The same result can be achieved more permanently by adding the custom directory into the `application.conf` file directly: ; ```; centaur {; optionalTestPath = ""/some/path/to/tests""; }; ```. ## Defining test cases. Each test case file is a HOCON file with the following structure:; ```; name: NAME // Required: Name of the test; testFormat: TESTFORMAT // Required: One of WorkflowSuccessTest, WorkflowFailureTest, runtwiceexpectingcallcaching; backends: [BACKENDNAME1, BACKENDNAME2, ...] // Optional list of backends. If supplied, this test will be ignored if these backends are not supported by the Cromwell server; basePath: /an/optional/field // Optional, location for the files {} entries to be found relative to; tags: [ ""any"", ""custom"", ""tags"" ] // Optional, a set of custom tags to apply to this test; ignore: false // Optional, whether centaur will ignore this test when running. files {; wdl: path/to/wdl // Required: path to the WDL file to submit; inputs: optional/path/to/inputs // Optional, a path to an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: """,MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:4114,Testability,test,test,4114,"an inputs JSON to include in the submission; options: optional/path/to/options // Optional, a path to an options JSON to include in the submission; }. // Optional, some metadata to verify on workflow completion:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: ""local"", ""gcs"". Used in conjunction with outputExpectations to define files we expect to exist after running this workflow.; outputExpectations: {; ""/path/to/my/output/file1"": 1; ""/path/to/file/that/should/not/exist"": 0; }; ```. The tags are optional. If supplied they will allow people to turn on or off this test case by including or excluding tags when running (see above). The `basePath` field is optional, but if supplied all paths will be resolved from that directory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workflowsuccess`: The workflow being supplied is expected to successfully complete; * `workflowfailure`: The workflow being supplied is expected to fail. The `metadata` is optional. If supplied, Centaur will retrieve the metadata from the successfully completed workflow and compare the values retrieved to those supplied. At the moment the only fields supported are strings, numbers and booleans. You can find which metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), u",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:4352,Testability,test,test,4352,"on:; metadata {; fully.qualified.key.name1: VALUE1; fully.qualified.key.name2: VALUE2; // Examples:; // failures is a list, the first entry (0) might be the error you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: ""local"", ""gcs"". Used in conjunction with outputExpectations to define files we expect to exist after running this workflow.; outputExpectations: {; ""/path/to/my/output/file1"": 1; ""/path/to/file/that/should/not/exist"": 0; }; ```. The tags are optional. If supplied they will allow people to turn on or off this test case by including or excluding tags when running (see above). The `basePath` field is optional, but if supplied all paths will be resolved from that directory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workflowsuccess`: The workflow being supplied is expected to successfully complete; * `workflowfailure`: The workflow being supplied is expected to fail. The `metadata` is optional. If supplied, Centaur will retrieve the metadata from the successfully completed workflow and compare the values retrieved to those supplied. At the moment the only fields supported are strings, numbers and booleans. You can find which metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:4379,Testability,test,testFormat,4379," you are looking for. If multiple errors are expected the entire list can be checked. ; // It has a ""message"" and a ""causedBy"" field.; ""failures.0.message"": ""Cromwell senses you did not use WomTool validate.""; ""failures.0.causedBy"": ""BetweenKeyboardAndChairException""; }. filesystemcheck: ""local"" // possible values: ""local"", ""gcs"". Used in conjunction with outputExpectations to define files we expect to exist after running this workflow.; outputExpectations: {; ""/path/to/my/output/file1"": 1; ""/path/to/file/that/should/not/exist"": 0; }; ```. The tags are optional. If supplied they will allow people to turn on or off this test case by including or excluding tags when running (see above). The `basePath` field is optional, but if supplied all paths will be resolved from that directory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workflowsuccess`: The workflow being supplied is expected to successfully complete; * `workflowfailure`: The workflow being supplied is expected to fail. The `metadata` is optional. If supplied, Centaur will retrieve the metadata from the successfully completed workflow and compare the values retrieved to those supplied. At the moment the only fields supported are strings, numbers and booleans. You can find which metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<W",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5268,Testability,log,log,5268,"ectory. If it is not supplied, all paths will be resolved from the directory the test case file is in. The `testFormat` field can be one of the following, case insensitive:; * `workflowsuccess`: The workflow being supplied is expected to successfully complete; * `workflowfailure`: The workflow being supplied is expected to fail. The `metadata` is optional. If supplied, Centaur will retrieve the metadata from the successfully completed workflow and compare the values retrieved to those supplied. At the moment the only fields supported are strings, numbers and booleans. You can find which metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5544,Testability,test,testing,5544,"upplied is expected to fail. The `metadata` is optional. If supplied, Centaur will retrieve the metadata from the successfully completed workflow and compare the values retrieved to those supplied. At the moment the only fields supported are strings, numbers and booleans. You can find which metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |;",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5619,Testability,test,testFormat,5619,", Centaur will retrieve the metadata from the successfully completed workflow and compare the values retrieved to those supplied. At the moment the only fields supported are strings, numbers and booleans. You can find which metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |; | PAPI Upgrade | `papi_v1_v2alpha1_upgrade_application.conf`** | `c",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5781,Testability,test,test,5781,"retrieved to those supplied. At the moment the only fields supported are strings, numbers and booleans. You can find which metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |; | PAPI Upgrade | `papi_v1_v2alpha1_upgrade_application.conf`** | `centaur_application.conf`* |; | Papi Upgrade<br>New Workflows | `(backend)_application.conf` | `centau",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5847,Testability,test,tests,5847,"metadata is recorded by running a workflow ```java -jar <Cromwell JAR> run -m metadata.json my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |; | PAPI Upgrade | `papi_v1_v2alpha1_upgrade_application.conf`** | `centaur_application.conf`* |; | Papi Upgrade<br>New Workflows | `(backend)_application.conf` | `centaur_application.conf`* |; | Azure Blob | `centaur_blob_test.conf`** | `centaur_application.conf`* |; | WDL Upgrade | `(backen",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:5973,Testability,test,test,5973,"my_workflow.wdl```.; This will save the metadata in `metadata.json`. For any metadata values or outputExpectations which require workflow ID (i.e, file paths), use `<<UUID>>` as a placeholder instead. For example:; * `""calls.hello.hello.stdout"": ""gs://google-project/jes/root/wdl/<<UUID>>/call-task/task-stdout.log""`. In case the absolute path the cromwell root is used (for example: `/home/my_user/projects/cromwell/cromwell-executions`); you can use `<<WORKFLOW_ROOT>>` as a replacement. ; * `""calls.hello.hello.exit_code"": ""<<WORKFLOW_ROOT>>/call-hello/execution/exit_code""`. In case testing of the caching is required `<<CACHE_HIT_UUID>>` can be used. ; The testFormat should be `runtwiceexpectingcallcaching`. ## Centaur Test Types; Both Cromwell and Centaur require configuration files in order to correctly build and test various parts of the system. Because of this, we divide; our tests into groups depending on which configuration files they require. Below is the current matrix of configuration files and test source directories. ## Upgrade / Horicromtal / etc. | CI Test Type | Cromwell Config | Centaur Config |; |-------------------------------|------------------------------------------------------------------|--------------------------------------------------------|; | Engine Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | Horicromtal | `papi_[v2beta or v2alpha1]_horicromtal_application.conf`** | `centaur_application_`<br>`horicromtal.conf` |; | Horicromtal<br>Engine Upgrade | `papi_v2beta_application.conf`** | `centaur_application_`<br>`horicromtal_no_assert.conf` |; | PAPI Upgrade | `papi_v1_v2alpha1_upgrade_application.conf`** | `centaur_application.conf`* |; | Papi Upgrade<br>New Workflows | `(backend)_application.conf` | `centaur_application.conf`* |; | Azure Blob | `centaur_blob_test.conf`** | `centaur_application.conf`* |; | WDL Upgrade | `(backend)_application.conf` | `centaur_application.conf`* |; | (other) | `(backend)_application.co",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:7930,Testability,test,test,7930,"ion.conf`* |; | (other) | `(backend)_application.conf` | `centaur_application.conf`* |. | CI Test Type | ScalaTest Spec | Test Directory |; |-------------------------------|-----------------------------|-------------------------------------|; | Engine Upgrade | `EngineUpgradeTestCaseSpec` | `engineUpgradeTestCases` |; | Horicromtal | `CentaurTestSuite` | `standardTestCases`*** |; | Horicromtal<br>Engine Upgrade | `EngineUpgradeTestCaseSpec` | `engineUpgradeTestCases`*** |; | PAPI Upgrade | `PapiUpgradeTestCaseSpec` | `papiUpgradeTestCases` |; | PAPI Upgrade<br>New Workflows | `CentaurTestSuite` | `papiUpgradeNewWorkflowsTestCases` |; | Azure Blob | `CentaurTestSuite ` | `azureBlobTestCases` |; | (other) | `CentaurTestSuite` | `standardTestCases` |. <small>; \* Centaur Config always uses `centaur_application.conf` except when overridden with `papi_v2alpha1_centaur_application.conf`; or `papi_v2beta_centaur_application.conf`; ([48 preview link](https://github.com/broadinstitute/cromwell/blob/a7d0601/src/ci/bin/test.inc.sh#L455-L457)) ; \*\* Cromwell Config overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L213-L221)) ; \*\*\* Test Directory overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L440-L449)); </small>. - Engine Upgrade: Retrieves the [Cromwell Version](https://github.com/broadinstitute/cromwell/blob/47/project/Version.scala#L8) then retrieves the previous jar/docker-image from DockerHub. Centaur starts with the prior version, then restarts with the compiled source code.; - Horicromtal: Runs a [docker-compose](https://github.com/broadinstitute/cromwell/blob/47/src/ci/docker-compose/docker-compose-horicromtal.yml) with:; 1. db-mstr: started first; 2. sum-back: runs summarizer; 3. front-back: exposes HTTP; - Horicromtal Engine Upgrade: Combination of Horicromtal and Engine Upgrade; - PAPI Upgrade: Tests run with an older version of Papi and upon restart use a newer versio",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:8061,Testability,test,test,8061,"ry |; |-------------------------------|-----------------------------|-------------------------------------|; | Engine Upgrade | `EngineUpgradeTestCaseSpec` | `engineUpgradeTestCases` |; | Horicromtal | `CentaurTestSuite` | `standardTestCases`*** |; | Horicromtal<br>Engine Upgrade | `EngineUpgradeTestCaseSpec` | `engineUpgradeTestCases`*** |; | PAPI Upgrade | `PapiUpgradeTestCaseSpec` | `papiUpgradeTestCases` |; | PAPI Upgrade<br>New Workflows | `CentaurTestSuite` | `papiUpgradeNewWorkflowsTestCases` |; | Azure Blob | `CentaurTestSuite ` | `azureBlobTestCases` |; | (other) | `CentaurTestSuite` | `standardTestCases` |. <small>; \* Centaur Config always uses `centaur_application.conf` except when overridden with `papi_v2alpha1_centaur_application.conf`; or `papi_v2beta_centaur_application.conf`; ([48 preview link](https://github.com/broadinstitute/cromwell/blob/a7d0601/src/ci/bin/test.inc.sh#L455-L457)) ; \*\* Cromwell Config overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L213-L221)) ; \*\*\* Test Directory overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L440-L449)); </small>. - Engine Upgrade: Retrieves the [Cromwell Version](https://github.com/broadinstitute/cromwell/blob/47/project/Version.scala#L8) then retrieves the previous jar/docker-image from DockerHub. Centaur starts with the prior version, then restarts with the compiled source code.; - Horicromtal: Runs a [docker-compose](https://github.com/broadinstitute/cromwell/blob/47/src/ci/docker-compose/docker-compose-horicromtal.yml) with:; 1. db-mstr: started first; 2. sum-back: runs summarizer; 3. front-back: exposes HTTP; - Horicromtal Engine Upgrade: Combination of Horicromtal and Engine Upgrade; - PAPI Upgrade: Tests run with an older version of Papi and upon restart use a newer version of Papi; - PAPI Upgrade New Workflows: Test definition [does not run any tests](https://travis-ci.org/broadinstitute/cromwell/jobs/",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:8193,Testability,test,test,8193,"ineUpgradeTestCaseSpec` | `engineUpgradeTestCases` |; | Horicromtal | `CentaurTestSuite` | `standardTestCases`*** |; | Horicromtal<br>Engine Upgrade | `EngineUpgradeTestCaseSpec` | `engineUpgradeTestCases`*** |; | PAPI Upgrade | `PapiUpgradeTestCaseSpec` | `papiUpgradeTestCases` |; | PAPI Upgrade<br>New Workflows | `CentaurTestSuite` | `papiUpgradeNewWorkflowsTestCases` |; | Azure Blob | `CentaurTestSuite ` | `azureBlobTestCases` |; | (other) | `CentaurTestSuite` | `standardTestCases` |. <small>; \* Centaur Config always uses `centaur_application.conf` except when overridden with `papi_v2alpha1_centaur_application.conf`; or `papi_v2beta_centaur_application.conf`; ([48 preview link](https://github.com/broadinstitute/cromwell/blob/a7d0601/src/ci/bin/test.inc.sh#L455-L457)) ; \*\* Cromwell Config overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L213-L221)) ; \*\*\* Test Directory overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L440-L449)); </small>. - Engine Upgrade: Retrieves the [Cromwell Version](https://github.com/broadinstitute/cromwell/blob/47/project/Version.scala#L8) then retrieves the previous jar/docker-image from DockerHub. Centaur starts with the prior version, then restarts with the compiled source code.; - Horicromtal: Runs a [docker-compose](https://github.com/broadinstitute/cromwell/blob/47/src/ci/docker-compose/docker-compose-horicromtal.yml) with:; 1. db-mstr: started first; 2. sum-back: runs summarizer; 3. front-back: exposes HTTP; - Horicromtal Engine Upgrade: Combination of Horicromtal and Engine Upgrade; - PAPI Upgrade: Tests run with an older version of Papi and upon restart use a newer version of Papi; - PAPI Upgrade New Workflows: Test definition [does not run any tests](https://travis-ci.org/broadinstitute/cromwell/jobs/475378412); - WDL Upgrade: Upgrades WDL from draft-2 to 1.0 before testing; - (other): Runs `*.test` files listing the configured ba",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:8982,Testability,test,tests,8982,"pt when overridden with `papi_v2alpha1_centaur_application.conf`; or `papi_v2beta_centaur_application.conf`; ([48 preview link](https://github.com/broadinstitute/cromwell/blob/a7d0601/src/ci/bin/test.inc.sh#L455-L457)) ; \*\* Cromwell Config overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L213-L221)) ; \*\*\* Test Directory overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L440-L449)); </small>. - Engine Upgrade: Retrieves the [Cromwell Version](https://github.com/broadinstitute/cromwell/blob/47/project/Version.scala#L8) then retrieves the previous jar/docker-image from DockerHub. Centaur starts with the prior version, then restarts with the compiled source code.; - Horicromtal: Runs a [docker-compose](https://github.com/broadinstitute/cromwell/blob/47/src/ci/docker-compose/docker-compose-horicromtal.yml) with:; 1. db-mstr: started first; 2. sum-back: runs summarizer; 3. front-back: exposes HTTP; - Horicromtal Engine Upgrade: Combination of Horicromtal and Engine Upgrade; - PAPI Upgrade: Tests run with an older version of Papi and upon restart use a newer version of Papi; - PAPI Upgrade New Workflows: Test definition [does not run any tests](https://travis-ci.org/broadinstitute/cromwell/jobs/475378412); - WDL Upgrade: Upgrades WDL from draft-2 to 1.0 before testing; - (other): Runs `*.test` files listing the configured backend names. ## RDBMS. | Backend | MySQL | PostgreSQL | MariaDB |; |---------|:------:|:-----------:|:--------:|; | AWS | ✅ | | |; | Local | ✅ | ✅ | |; | PAPI V2 | ✅ | | ⭕ |; | SLURM | ✅ | | |; | TES | ✅ | | |. <small>; ⭕ Tests Horicromtal Engine Upgrade versus standard Centaur suite; </small>. All backends run against MySQL. The Local backend also test PostgreSQL, allowing contributors ensure WDLs work with PostgreSQL. MariaDB is tested on a specialized upgrade, where the MySQL connector client is used first, and the MariaDB client is used after restart.; ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:9107,Testability,test,testing,9107,"pt when overridden with `papi_v2alpha1_centaur_application.conf`; or `papi_v2beta_centaur_application.conf`; ([48 preview link](https://github.com/broadinstitute/cromwell/blob/a7d0601/src/ci/bin/test.inc.sh#L455-L457)) ; \*\* Cromwell Config overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L213-L221)) ; \*\*\* Test Directory overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L440-L449)); </small>. - Engine Upgrade: Retrieves the [Cromwell Version](https://github.com/broadinstitute/cromwell/blob/47/project/Version.scala#L8) then retrieves the previous jar/docker-image from DockerHub. Centaur starts with the prior version, then restarts with the compiled source code.; - Horicromtal: Runs a [docker-compose](https://github.com/broadinstitute/cromwell/blob/47/src/ci/docker-compose/docker-compose-horicromtal.yml) with:; 1. db-mstr: started first; 2. sum-back: runs summarizer; 3. front-back: exposes HTTP; - Horicromtal Engine Upgrade: Combination of Horicromtal and Engine Upgrade; - PAPI Upgrade: Tests run with an older version of Papi and upon restart use a newer version of Papi; - PAPI Upgrade New Workflows: Test definition [does not run any tests](https://travis-ci.org/broadinstitute/cromwell/jobs/475378412); - WDL Upgrade: Upgrades WDL from draft-2 to 1.0 before testing; - (other): Runs `*.test` files listing the configured backend names. ## RDBMS. | Backend | MySQL | PostgreSQL | MariaDB |; |---------|:------:|:-----------:|:--------:|; | AWS | ✅ | | |; | Local | ✅ | ✅ | |; | PAPI V2 | ✅ | | ⭕ |; | SLURM | ✅ | | |; | TES | ✅ | | |. <small>; ⭕ Tests Horicromtal Engine Upgrade versus standard Centaur suite; </small>. All backends run against MySQL. The Local backend also test PostgreSQL, allowing contributors ensure WDLs work with PostgreSQL. MariaDB is tested on a specialized upgrade, where the MySQL connector client is used first, and the MariaDB client is used after restart.; ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:9135,Testability,test,test,9135,"pt when overridden with `papi_v2alpha1_centaur_application.conf`; or `papi_v2beta_centaur_application.conf`; ([48 preview link](https://github.com/broadinstitute/cromwell/blob/a7d0601/src/ci/bin/test.inc.sh#L455-L457)) ; \*\* Cromwell Config overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L213-L221)) ; \*\*\* Test Directory overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L440-L449)); </small>. - Engine Upgrade: Retrieves the [Cromwell Version](https://github.com/broadinstitute/cromwell/blob/47/project/Version.scala#L8) then retrieves the previous jar/docker-image from DockerHub. Centaur starts with the prior version, then restarts with the compiled source code.; - Horicromtal: Runs a [docker-compose](https://github.com/broadinstitute/cromwell/blob/47/src/ci/docker-compose/docker-compose-horicromtal.yml) with:; 1. db-mstr: started first; 2. sum-back: runs summarizer; 3. front-back: exposes HTTP; - Horicromtal Engine Upgrade: Combination of Horicromtal and Engine Upgrade; - PAPI Upgrade: Tests run with an older version of Papi and upon restart use a newer version of Papi; - PAPI Upgrade New Workflows: Test definition [does not run any tests](https://travis-ci.org/broadinstitute/cromwell/jobs/475378412); - WDL Upgrade: Upgrades WDL from draft-2 to 1.0 before testing; - (other): Runs `*.test` files listing the configured backend names. ## RDBMS. | Backend | MySQL | PostgreSQL | MariaDB |; |---------|:------:|:-----------:|:--------:|; | AWS | ✅ | | |; | Local | ✅ | ✅ | |; | PAPI V2 | ✅ | | ⭕ |; | SLURM | ✅ | | |; | TES | ✅ | | |. <small>; ⭕ Tests Horicromtal Engine Upgrade versus standard Centaur suite; </small>. All backends run against MySQL. The Local backend also test PostgreSQL, allowing contributors ensure WDLs work with PostgreSQL. MariaDB is tested on a specialized upgrade, where the MySQL connector client is used first, and the MariaDB client is used after restart.; ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:9523,Testability,test,test,9523,"pt when overridden with `papi_v2alpha1_centaur_application.conf`; or `papi_v2beta_centaur_application.conf`; ([48 preview link](https://github.com/broadinstitute/cromwell/blob/a7d0601/src/ci/bin/test.inc.sh#L455-L457)) ; \*\* Cromwell Config overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L213-L221)) ; \*\*\* Test Directory overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L440-L449)); </small>. - Engine Upgrade: Retrieves the [Cromwell Version](https://github.com/broadinstitute/cromwell/blob/47/project/Version.scala#L8) then retrieves the previous jar/docker-image from DockerHub. Centaur starts with the prior version, then restarts with the compiled source code.; - Horicromtal: Runs a [docker-compose](https://github.com/broadinstitute/cromwell/blob/47/src/ci/docker-compose/docker-compose-horicromtal.yml) with:; 1. db-mstr: started first; 2. sum-back: runs summarizer; 3. front-back: exposes HTTP; - Horicromtal Engine Upgrade: Combination of Horicromtal and Engine Upgrade; - PAPI Upgrade: Tests run with an older version of Papi and upon restart use a newer version of Papi; - PAPI Upgrade New Workflows: Test definition [does not run any tests](https://travis-ci.org/broadinstitute/cromwell/jobs/475378412); - WDL Upgrade: Upgrades WDL from draft-2 to 1.0 before testing; - (other): Runs `*.test` files listing the configured backend names. ## RDBMS. | Backend | MySQL | PostgreSQL | MariaDB |; |---------|:------:|:-----------:|:--------:|; | AWS | ✅ | | |; | Local | ✅ | ✅ | |; | PAPI V2 | ✅ | | ⭕ |; | SLURM | ✅ | | |; | TES | ✅ | | |. <small>; ⭕ Tests Horicromtal Engine Upgrade versus standard Centaur suite; </small>. All backends run against MySQL. The Local backend also test PostgreSQL, allowing contributors ensure WDLs work with PostgreSQL. MariaDB is tested on a specialized upgrade, where the MySQL connector client is used first, and the MariaDB client is used after restart.; ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md:9607,Testability,test,tested,9607,"pt when overridden with `papi_v2alpha1_centaur_application.conf`; or `papi_v2beta_centaur_application.conf`; ([48 preview link](https://github.com/broadinstitute/cromwell/blob/a7d0601/src/ci/bin/test.inc.sh#L455-L457)) ; \*\* Cromwell Config overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L213-L221)) ; \*\*\* Test Directory overrides; ([47 link](https://github.com/broadinstitute/cromwell/blob/47/src/ci/bin/test.inc.sh#L440-L449)); </small>. - Engine Upgrade: Retrieves the [Cromwell Version](https://github.com/broadinstitute/cromwell/blob/47/project/Version.scala#L8) then retrieves the previous jar/docker-image from DockerHub. Centaur starts with the prior version, then restarts with the compiled source code.; - Horicromtal: Runs a [docker-compose](https://github.com/broadinstitute/cromwell/blob/47/src/ci/docker-compose/docker-compose-horicromtal.yml) with:; 1. db-mstr: started first; 2. sum-back: runs summarizer; 3. front-back: exposes HTTP; - Horicromtal Engine Upgrade: Combination of Horicromtal and Engine Upgrade; - PAPI Upgrade: Tests run with an older version of Papi and upon restart use a newer version of Papi; - PAPI Upgrade New Workflows: Test definition [does not run any tests](https://travis-ci.org/broadinstitute/cromwell/jobs/475378412); - WDL Upgrade: Upgrades WDL from draft-2 to 1.0 before testing; - (other): Runs `*.test` files listing the configured backend names. ## RDBMS. | Backend | MySQL | PostgreSQL | MariaDB |; |---------|:------:|:-----------:|:--------:|; | AWS | ✅ | | |; | Local | ✅ | ✅ | |; | PAPI V2 | ✅ | | ⭕ |; | SLURM | ✅ | | |; | TES | ✅ | | |. <small>; ⭕ Tests Horicromtal Engine Upgrade versus standard Centaur suite; </small>. All backends run against MySQL. The Local backend also test PostgreSQL, allowing contributors ensure WDLs work with PostgreSQL. MariaDB is tested on a specialized upgrade, where the MySQL connector client is used first, and the MariaDB client is used after restart.; ",MatchSource.DOCS,docs/developers/Centaur.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Centaur.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md:730,Deployability,install,installation,730,"**Contribution Guidelines**. Cromwell is an open-source project and we heartily welcome community contributions to both our code and our documentation. Here are some guidelines for adding documentation and recommendations on where we could use help the most. . First off, here are useful links:. * Cromwell documentation: [cromwell.readthedocs.io](http://cromwell.readthedocs.io); * Source on Github: [github.com/broadinstitute/cromwell](https://github.com/broadinstitute/cromwell/tree/develop/docs); * Builds on ReadTheDocs: [readthedocs.org/projects/cromwell](https://readthedocs.org/projects/cromwell/builds/); * How to build and view the documentation locally: [mkdocs.readthedocs.io](https://mkdocs.readthedocs.io/en/stable/#installation). ### Writing Tips. 1. Keep it clear, accurate, and concise.; 2. Put the most important information first.; 3. Use the second person, use “you” instead of “the user”.; 4. No passive verbs (everything is done by something).; 5. Link to the original source, don't repeat documentation. . ### Formatting. The documentation is written in Markdown. Click here for a [Github Guide on Markdown](https://guides.github.com/features/mastering-markdown/), and click here for more [tips from MkDocs](http://www.mkdocs.org/user-guide/writing-your-docs/). ### Styling. **Links:**. * Absolute: `[link text](www.destinationURL.com)`; _Example:_ `[Broad Institute](www.broadinstitute.org)` _produces this link_ [Broad Institute](https://www.broadinstitute.org).; * Relative: `[link text](Destination_Page)`, where `Destination_Page` is the file name without the `.md` extension ; _Example:_ `[How to use the Cromwell CLI](CommandLine)` _produces this link_ [How to use the Cromwell CLI](CommandLine).; * Anchor link: `[anchor text](../Path/To/Page#Anchor)` ; _Example:_ `[HPC filesystems](backends/HPC#filesystems)` _produces this link_ [HPC filesystems](backends/HPC#filesystems). **Code:**. * To style a word of code, use a backtick (\`) before and after the word. ; _Examp",MatchSource.DOCS,docs/developers/Contribute.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md:2868,Testability,log,logo,2868,"LI](CommandLine)` _produces this link_ [How to use the Cromwell CLI](CommandLine).; * Anchor link: `[anchor text](../Path/To/Page#Anchor)` ; _Example:_ `[HPC filesystems](backends/HPC#filesystems)` _produces this link_ [HPC filesystems](backends/HPC#filesystems). **Code:**. * To style a word of code, use a backtick (\`) before and after the word. ; _Example:_ \`file.json\` _produces_ `file.json`. ; * To style a block of code, use three backticks (\`\`\`) before and after the block of code. ; _Example:_ ; 	\`\`\` ; 		workflow myWorkflow { ; 		call myTask ; 		} ; 	\`\`\` ; _produces this block_ ; ``` ; 	workflow myWorkflow { ; 	call myTask ; 	} ; ``` ; * To use syntax highlighting, include the language after the first three backticks (\`\`\`). ; _Example:_ ; 	\`\`\`json ; 		\{ ; 		 ""MyWorkflow.MyTask.VariableTwo"": ""Variable2"" ; 		\} ; 	\`\`\`	; _produces this block_ ; ```json ; 	{; 	 ""MyWorkflow.MyTask.VariableTwo"": ""Variable2""; 	}; ```	. **Images**. * Relative: `![](ImgName.png)`; 	* _Example:_ `![](../jamie_the_cromwell_pig.png)` _produces this image_ ; 	![](../jamie_the_cromwell_pig.png) ; * Absolute: `![](URLofImg.png)`; 	* _Example:_ `![](https://www.broadinstitute.org/sites/all/themes/custom/at_broad/logo.png)` _produces this image_ ; 	![](https://www.broadinstitute.org/sites/all/themes/custom/at_broad/logo.png) . **Left-side menu:**. To add or remove items from the menu, edit [mkdocs.yml](https://github.com/broadinstitute/cromwell/blob/develop/mkdocs.yml) in Cromwell. ### FAQs. **_Why isn't my documentation showing up?_** . * **Is your PR merged?** ; If not, [kindly ask the team to merge it](https://github.com/broadinstitute/cromwell/pulls). Once your PR is merged to develop, it will trigger an automatic build. . * **Has the build finished?** ; [Check build status here](https://readthedocs.org/projects/cromwell/builds/). * **Did you add the file(s) to the YAML file?** ; If not, [add it here](https://github.com/broadinstitute/cromwell/blob/develop/mkdocs.yml).; ",MatchSource.DOCS,docs/developers/Contribute.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md:2972,Testability,log,logo,2972,"LI](CommandLine)` _produces this link_ [How to use the Cromwell CLI](CommandLine).; * Anchor link: `[anchor text](../Path/To/Page#Anchor)` ; _Example:_ `[HPC filesystems](backends/HPC#filesystems)` _produces this link_ [HPC filesystems](backends/HPC#filesystems). **Code:**. * To style a word of code, use a backtick (\`) before and after the word. ; _Example:_ \`file.json\` _produces_ `file.json`. ; * To style a block of code, use three backticks (\`\`\`) before and after the block of code. ; _Example:_ ; 	\`\`\` ; 		workflow myWorkflow { ; 		call myTask ; 		} ; 	\`\`\` ; _produces this block_ ; ``` ; 	workflow myWorkflow { ; 	call myTask ; 	} ; ``` ; * To use syntax highlighting, include the language after the first three backticks (\`\`\`). ; _Example:_ ; 	\`\`\`json ; 		\{ ; 		 ""MyWorkflow.MyTask.VariableTwo"": ""Variable2"" ; 		\} ; 	\`\`\`	; _produces this block_ ; ```json ; 	{; 	 ""MyWorkflow.MyTask.VariableTwo"": ""Variable2""; 	}; ```	. **Images**. * Relative: `![](ImgName.png)`; 	* _Example:_ `![](../jamie_the_cromwell_pig.png)` _produces this image_ ; 	![](../jamie_the_cromwell_pig.png) ; * Absolute: `![](URLofImg.png)`; 	* _Example:_ `![](https://www.broadinstitute.org/sites/all/themes/custom/at_broad/logo.png)` _produces this image_ ; 	![](https://www.broadinstitute.org/sites/all/themes/custom/at_broad/logo.png) . **Left-side menu:**. To add or remove items from the menu, edit [mkdocs.yml](https://github.com/broadinstitute/cromwell/blob/develop/mkdocs.yml) in Cromwell. ### FAQs. **_Why isn't my documentation showing up?_** . * **Is your PR merged?** ; If not, [kindly ask the team to merge it](https://github.com/broadinstitute/cromwell/pulls). Once your PR is merged to develop, it will trigger an automatic build. . * **Has the build finished?** ; [Check build status here](https://readthedocs.org/projects/cromwell/builds/). * **Did you add the file(s) to the YAML file?** ; If not, [add it here](https://github.com/broadinstitute/cromwell/blob/develop/mkdocs.yml).; ",MatchSource.DOCS,docs/developers/Contribute.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md:166,Usability,guid,guidelines,166,"**Contribution Guidelines**. Cromwell is an open-source project and we heartily welcome community contributions to both our code and our documentation. Here are some guidelines for adding documentation and recommendations on where we could use help the most. . First off, here are useful links:. * Cromwell documentation: [cromwell.readthedocs.io](http://cromwell.readthedocs.io); * Source on Github: [github.com/broadinstitute/cromwell](https://github.com/broadinstitute/cromwell/tree/develop/docs); * Builds on ReadTheDocs: [readthedocs.org/projects/cromwell](https://readthedocs.org/projects/cromwell/builds/); * How to build and view the documentation locally: [mkdocs.readthedocs.io](https://mkdocs.readthedocs.io/en/stable/#installation). ### Writing Tips. 1. Keep it clear, accurate, and concise.; 2. Put the most important information first.; 3. Use the second person, use “you” instead of “the user”.; 4. No passive verbs (everything is done by something).; 5. Link to the original source, don't repeat documentation. . ### Formatting. The documentation is written in Markdown. Click here for a [Github Guide on Markdown](https://guides.github.com/features/mastering-markdown/), and click here for more [tips from MkDocs](http://www.mkdocs.org/user-guide/writing-your-docs/). ### Styling. **Links:**. * Absolute: `[link text](www.destinationURL.com)`; _Example:_ `[Broad Institute](www.broadinstitute.org)` _produces this link_ [Broad Institute](https://www.broadinstitute.org).; * Relative: `[link text](Destination_Page)`, where `Destination_Page` is the file name without the `.md` extension ; _Example:_ `[How to use the Cromwell CLI](CommandLine)` _produces this link_ [How to use the Cromwell CLI](CommandLine).; * Anchor link: `[anchor text](../Path/To/Page#Anchor)` ; _Example:_ `[HPC filesystems](backends/HPC#filesystems)` _produces this link_ [HPC filesystems](backends/HPC#filesystems). **Code:**. * To style a word of code, use a backtick (\`) before and after the word. ; _Examp",MatchSource.DOCS,docs/developers/Contribute.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md:774,Usability,clear,clear,774,"**Contribution Guidelines**. Cromwell is an open-source project and we heartily welcome community contributions to both our code and our documentation. Here are some guidelines for adding documentation and recommendations on where we could use help the most. . First off, here are useful links:. * Cromwell documentation: [cromwell.readthedocs.io](http://cromwell.readthedocs.io); * Source on Github: [github.com/broadinstitute/cromwell](https://github.com/broadinstitute/cromwell/tree/develop/docs); * Builds on ReadTheDocs: [readthedocs.org/projects/cromwell](https://readthedocs.org/projects/cromwell/builds/); * How to build and view the documentation locally: [mkdocs.readthedocs.io](https://mkdocs.readthedocs.io/en/stable/#installation). ### Writing Tips. 1. Keep it clear, accurate, and concise.; 2. Put the most important information first.; 3. Use the second person, use “you” instead of “the user”.; 4. No passive verbs (everything is done by something).; 5. Link to the original source, don't repeat documentation. . ### Formatting. The documentation is written in Markdown. Click here for a [Github Guide on Markdown](https://guides.github.com/features/mastering-markdown/), and click here for more [tips from MkDocs](http://www.mkdocs.org/user-guide/writing-your-docs/). ### Styling. **Links:**. * Absolute: `[link text](www.destinationURL.com)`; _Example:_ `[Broad Institute](www.broadinstitute.org)` _produces this link_ [Broad Institute](https://www.broadinstitute.org).; * Relative: `[link text](Destination_Page)`, where `Destination_Page` is the file name without the `.md` extension ; _Example:_ `[How to use the Cromwell CLI](CommandLine)` _produces this link_ [How to use the Cromwell CLI](CommandLine).; * Anchor link: `[anchor text](../Path/To/Page#Anchor)` ; _Example:_ `[HPC filesystems](backends/HPC#filesystems)` _produces this link_ [HPC filesystems](backends/HPC#filesystems). **Code:**. * To style a word of code, use a backtick (\`) before and after the word. ; _Examp",MatchSource.DOCS,docs/developers/Contribute.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md:1139,Usability,guid,guides,1139,"oth our code and our documentation. Here are some guidelines for adding documentation and recommendations on where we could use help the most. . First off, here are useful links:. * Cromwell documentation: [cromwell.readthedocs.io](http://cromwell.readthedocs.io); * Source on Github: [github.com/broadinstitute/cromwell](https://github.com/broadinstitute/cromwell/tree/develop/docs); * Builds on ReadTheDocs: [readthedocs.org/projects/cromwell](https://readthedocs.org/projects/cromwell/builds/); * How to build and view the documentation locally: [mkdocs.readthedocs.io](https://mkdocs.readthedocs.io/en/stable/#installation). ### Writing Tips. 1. Keep it clear, accurate, and concise.; 2. Put the most important information first.; 3. Use the second person, use “you” instead of “the user”.; 4. No passive verbs (everything is done by something).; 5. Link to the original source, don't repeat documentation. . ### Formatting. The documentation is written in Markdown. Click here for a [Github Guide on Markdown](https://guides.github.com/features/mastering-markdown/), and click here for more [tips from MkDocs](http://www.mkdocs.org/user-guide/writing-your-docs/). ### Styling. **Links:**. * Absolute: `[link text](www.destinationURL.com)`; _Example:_ `[Broad Institute](www.broadinstitute.org)` _produces this link_ [Broad Institute](https://www.broadinstitute.org).; * Relative: `[link text](Destination_Page)`, where `Destination_Page` is the file name without the `.md` extension ; _Example:_ `[How to use the Cromwell CLI](CommandLine)` _produces this link_ [How to use the Cromwell CLI](CommandLine).; * Anchor link: `[anchor text](../Path/To/Page#Anchor)` ; _Example:_ `[HPC filesystems](backends/HPC#filesystems)` _produces this link_ [HPC filesystems](backends/HPC#filesystems). **Code:**. * To style a word of code, use a backtick (\`) before and after the word. ; _Example:_ \`file.json\` _produces_ `file.json`. ; * To style a block of code, use three backticks (\`\`\`) before and af",MatchSource.DOCS,docs/developers/Contribute.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md:1258,Usability,guid,guide,1258," off, here are useful links:. * Cromwell documentation: [cromwell.readthedocs.io](http://cromwell.readthedocs.io); * Source on Github: [github.com/broadinstitute/cromwell](https://github.com/broadinstitute/cromwell/tree/develop/docs); * Builds on ReadTheDocs: [readthedocs.org/projects/cromwell](https://readthedocs.org/projects/cromwell/builds/); * How to build and view the documentation locally: [mkdocs.readthedocs.io](https://mkdocs.readthedocs.io/en/stable/#installation). ### Writing Tips. 1. Keep it clear, accurate, and concise.; 2. Put the most important information first.; 3. Use the second person, use “you” instead of “the user”.; 4. No passive verbs (everything is done by something).; 5. Link to the original source, don't repeat documentation. . ### Formatting. The documentation is written in Markdown. Click here for a [Github Guide on Markdown](https://guides.github.com/features/mastering-markdown/), and click here for more [tips from MkDocs](http://www.mkdocs.org/user-guide/writing-your-docs/). ### Styling. **Links:**. * Absolute: `[link text](www.destinationURL.com)`; _Example:_ `[Broad Institute](www.broadinstitute.org)` _produces this link_ [Broad Institute](https://www.broadinstitute.org).; * Relative: `[link text](Destination_Page)`, where `Destination_Page` is the file name without the `.md` extension ; _Example:_ `[How to use the Cromwell CLI](CommandLine)` _produces this link_ [How to use the Cromwell CLI](CommandLine).; * Anchor link: `[anchor text](../Path/To/Page#Anchor)` ; _Example:_ `[HPC filesystems](backends/HPC#filesystems)` _produces this link_ [HPC filesystems](backends/HPC#filesystems). **Code:**. * To style a word of code, use a backtick (\`) before and after the word. ; _Example:_ \`file.json\` _produces_ `file.json`. ; * To style a block of code, use three backticks (\`\`\`) before and after the block of code. ; _Example:_ ; 	\`\`\` ; 		workflow myWorkflow { ; 		call myTask ; 		} ; 	\`\`\` ; _produces this block_ ; ``` ; 	workflow myWor",MatchSource.DOCS,docs/developers/Contribute.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Contribute.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:1113,Deployability,configurat,configuration,1113,"; production environments. The default implementation of this ignores these metrics, but Cromwell includes alternate implementations that can forward metrics to a; specific server. ### StatsD. While this instrumentation support can be used in smaller environments it will still require setting up a; [StatsD](https://github.com/etsy/statsd) server outside of Cromwell and it's possible not enough data would be produced to be useful. ; Cromwell collects metrics while running and sends them to an internal service. . Make sure to configure your StatsD service:. ```hocon; services.Instrumentation {; 	class = ""cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActor"". 	config {; 	 hostname = ""localhost"" # Replace with your host; 	 port = 8125 # Replace with your port; 	 # prefix = ""my_prefix"" # All metrics will be prefixed by this value if present.; 	 flush-rate = 1 second # Rate at which metrics are sent to the StatsD server; 	}; }; ```. There is also an additional configuration value that can be set: . ```hocon; # Rate at which Cromwell updates its gauge values (number of workflows running, queued, etc...); system.instrumentation-rate = 5 seconds; ```. If you have multiple Cromwell instances, and would like to separate the instrumentation path for each instance, set the `system.cromwell_id` with the unique identifier for each Cromwell instance. For example,; ```hocon; system.cromwell_id = ""cromwell-instance-1""; ```; will prepend all the metrics with path `cromwell.cromwell-instance-1...` for that instance. ##### Metrics. The current StatsD implementation uses metrics-statsd to report instrumentation values.; metrics-statsd reports all metrics with a gauge type.; This means all metrics will be under the gauge section. We might add or remove metrics in the future depending on need and usage.; These are the current high level categories:. * `backend`; * `rest-api`; * `job`; * `workflow`; * `io`. ### Stackdriver. Cromwell now supports sending metrics to ",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:1187,Deployability,update,updates,1187,"but Cromwell includes alternate implementations that can forward metrics to a; specific server. ### StatsD. While this instrumentation support can be used in smaller environments it will still require setting up a; [StatsD](https://github.com/etsy/statsd) server outside of Cromwell and it's possible not enough data would be produced to be useful. ; Cromwell collects metrics while running and sends them to an internal service. . Make sure to configure your StatsD service:. ```hocon; services.Instrumentation {; 	class = ""cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActor"". 	config {; 	 hostname = ""localhost"" # Replace with your host; 	 port = 8125 # Replace with your port; 	 # prefix = ""my_prefix"" # All metrics will be prefixed by this value if present.; 	 flush-rate = 1 second # Rate at which metrics are sent to the StatsD server; 	}; }; ```. There is also an additional configuration value that can be set: . ```hocon; # Rate at which Cromwell updates its gauge values (number of workflows running, queued, etc...); system.instrumentation-rate = 5 seconds; ```. If you have multiple Cromwell instances, and would like to separate the instrumentation path for each instance, set the `system.cromwell_id` with the unique identifier for each Cromwell instance. For example,; ```hocon; system.cromwell_id = ""cromwell-instance-1""; ```; will prepend all the metrics with path `cromwell.cromwell-instance-1...` for that instance. ##### Metrics. The current StatsD implementation uses metrics-statsd to report instrumentation values.; metrics-statsd reports all metrics with a gauge type.; This means all metrics will be under the gauge section. We might add or remove metrics in the future depending on need and usage.; These are the current high level categories:. * `backend`; * `rest-api`; * `job`; * `workflow`; * `io`. ### Stackdriver. Cromwell now supports sending metrics to [Google's Stackdriver API](https://cloud.google.com/monitoring/api/v3/). To use the S",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:2170,Energy Efficiency,monitor,monitoring,2170,"Cromwell updates its gauge values (number of workflows running, queued, etc...); system.instrumentation-rate = 5 seconds; ```. If you have multiple Cromwell instances, and would like to separate the instrumentation path for each instance, set the `system.cromwell_id` with the unique identifier for each Cromwell instance. For example,; ```hocon; system.cromwell_id = ""cromwell-instance-1""; ```; will prepend all the metrics with path `cromwell.cromwell-instance-1...` for that instance. ##### Metrics. The current StatsD implementation uses metrics-statsd to report instrumentation values.; metrics-statsd reports all metrics with a gauge type.; This means all metrics will be under the gauge section. We might add or remove metrics in the future depending on need and usage.; These are the current high level categories:. * `backend`; * `rest-api`; * `job`; * `workflow`; * `io`. ### Stackdriver. Cromwell now supports sending metrics to [Google's Stackdriver API](https://cloud.google.com/monitoring/api/v3/). To use the Stackdriver instrumentation; specify this in your config:; ```hocon; services.Instrumentation {; class = ""cromwell.services.instrumentation.impl.stackdriver.StackdriverInstrumentationServiceActor"". config {; # auth scheme can be `application_default` or `service_account`; auth = ""service-account""; google-project = ""my-project""; # rate at which aggregated metrics will be sent to Stackdriver. It needs to be equal or greater than 1 minute.; # Google's Stackdriver API needs each metric to be sent not more than once per minute.; flush-rate = 1 minute; # below 3 keys are attached as labels to each metric. `cromwell-perf-test-case` is specifically meant for perf env.; cromwell-instance-role = ""role""; cromwell-perf-test-case = ""perf-test-1""; }; }; ```; The 2 label keys are optional. If specified, each metric will have label(s) added in the form of a (key, value) pair.; So for example, if `cromwell-instance-role = ""backend""` is mentioned in config, each metric data point",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:3626,Energy Efficiency,monitor,monitoring,3626,"mpl.stackdriver.StackdriverInstrumentationServiceActor"". config {; # auth scheme can be `application_default` or `service_account`; auth = ""service-account""; google-project = ""my-project""; # rate at which aggregated metrics will be sent to Stackdriver. It needs to be equal or greater than 1 minute.; # Google's Stackdriver API needs each metric to be sent not more than once per minute.; flush-rate = 1 minute; # below 3 keys are attached as labels to each metric. `cromwell-perf-test-case` is specifically meant for perf env.; cromwell-instance-role = ""role""; cromwell-perf-test-case = ""perf-test-1""; }; }; ```; The 2 label keys are optional. If specified, each metric will have label(s) added in the form of a (key, value) pair.; So for example, if `cromwell-instance-role = ""backend""` is mentioned in config, each metric data point sent to Stackdriver; will have a label (cromwell_instance_role, backend) added to it. There is another optional label that can be added to each metric. `cromwell_id` represents the identifier for different Cromwell instances.; ```hocon; # Unique Cromwell instance identifier; system.cromwell_id = ""cromwell-instance-1""; ```. ##### Metric type and Label keys naming convention; More details on the this can be found [here](https://cloud.google.com/monitoring/api/v3/metrics-details#metric-kinds). You must adhere to the following spelling rules for metric type names:; - You can use upper and lower-case letters, digits, and underscores (_) in the names.; - You can use periods (.) in the domain part of the names.; - You can use forward slashes (/) to separate path elements.; - You can start each path element with a letter or digit.; - The maximum length of a metric type name is 200 characters. You must adhere to the following spelling rules for metric label names:; - You can use upper and lower-case letters, digits, underscores (_) in the names.; - You can start names with a letter or digit.; - The maximum length of a metric label name is 100 characters. ",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:1926,Integrability,depend,depending,1926,"efix = ""my_prefix"" # All metrics will be prefixed by this value if present.; 	 flush-rate = 1 second # Rate at which metrics are sent to the StatsD server; 	}; }; ```. There is also an additional configuration value that can be set: . ```hocon; # Rate at which Cromwell updates its gauge values (number of workflows running, queued, etc...); system.instrumentation-rate = 5 seconds; ```. If you have multiple Cromwell instances, and would like to separate the instrumentation path for each instance, set the `system.cromwell_id` with the unique identifier for each Cromwell instance. For example,; ```hocon; system.cromwell_id = ""cromwell-instance-1""; ```; will prepend all the metrics with path `cromwell.cromwell-instance-1...` for that instance. ##### Metrics. The current StatsD implementation uses metrics-statsd to report instrumentation values.; metrics-statsd reports all metrics with a gauge type.; This means all metrics will be under the gauge section. We might add or remove metrics in the future depending on need and usage.; These are the current high level categories:. * `backend`; * `rest-api`; * `job`; * `workflow`; * `io`. ### Stackdriver. Cromwell now supports sending metrics to [Google's Stackdriver API](https://cloud.google.com/monitoring/api/v3/). To use the Stackdriver instrumentation; specify this in your config:; ```hocon; services.Instrumentation {; class = ""cromwell.services.instrumentation.impl.stackdriver.StackdriverInstrumentationServiceActor"". config {; # auth scheme can be `application_default` or `service_account`; auth = ""service-account""; google-project = ""my-project""; # rate at which aggregated metrics will be sent to Stackdriver. It needs to be equal or greater than 1 minute.; # Google's Stackdriver API needs each metric to be sent not more than once per minute.; flush-rate = 1 minute; # below 3 keys are attached as labels to each metric. `cromwell-perf-test-case` is specifically meant for perf env.; cromwell-instance-role = ""role""; cromwell-per",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:647,Modifiability,config,configure,647,"# Overview. Cromwell's instrumentation support can be useful to collect utilization data in long-running, high-volume; production environments. The default implementation of this ignores these metrics, but Cromwell includes alternate implementations that can forward metrics to a; specific server. ### StatsD. While this instrumentation support can be used in smaller environments it will still require setting up a; [StatsD](https://github.com/etsy/statsd) server outside of Cromwell and it's possible not enough data would be produced to be useful. ; Cromwell collects metrics while running and sends them to an internal service. . Make sure to configure your StatsD service:. ```hocon; services.Instrumentation {; 	class = ""cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActor"". 	config {; 	 hostname = ""localhost"" # Replace with your host; 	 port = 8125 # Replace with your port; 	 # prefix = ""my_prefix"" # All metrics will be prefixed by this value if present.; 	 flush-rate = 1 second # Rate at which metrics are sent to the StatsD server; 	}; }; ```. There is also an additional configuration value that can be set: . ```hocon; # Rate at which Cromwell updates its gauge values (number of workflows running, queued, etc...); system.instrumentation-rate = 5 seconds; ```. If you have multiple Cromwell instances, and would like to separate the instrumentation path for each instance, set the `system.cromwell_id` with the unique identifier for each Cromwell instance. For example,; ```hocon; system.cromwell_id = ""cromwell-instance-1""; ```; will prepend all the metrics with path `cromwell.cromwell-instance-1...` for that instance. ##### Metrics. The current StatsD implementation uses metrics-statsd to report instrumentation values.; metrics-statsd reports all metrics with a gauge type.; This means all metrics will be under the gauge section. We might add or remove metrics in the future depending on need and usage.; These are the current high level categories:.",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:810,Modifiability,config,config,810,"# Overview. Cromwell's instrumentation support can be useful to collect utilization data in long-running, high-volume; production environments. The default implementation of this ignores these metrics, but Cromwell includes alternate implementations that can forward metrics to a; specific server. ### StatsD. While this instrumentation support can be used in smaller environments it will still require setting up a; [StatsD](https://github.com/etsy/statsd) server outside of Cromwell and it's possible not enough data would be produced to be useful. ; Cromwell collects metrics while running and sends them to an internal service. . Make sure to configure your StatsD service:. ```hocon; services.Instrumentation {; 	class = ""cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActor"". 	config {; 	 hostname = ""localhost"" # Replace with your host; 	 port = 8125 # Replace with your port; 	 # prefix = ""my_prefix"" # All metrics will be prefixed by this value if present.; 	 flush-rate = 1 second # Rate at which metrics are sent to the StatsD server; 	}; }; ```. There is also an additional configuration value that can be set: . ```hocon; # Rate at which Cromwell updates its gauge values (number of workflows running, queued, etc...); system.instrumentation-rate = 5 seconds; ```. If you have multiple Cromwell instances, and would like to separate the instrumentation path for each instance, set the `system.cromwell_id` with the unique identifier for each Cromwell instance. For example,; ```hocon; system.cromwell_id = ""cromwell-instance-1""; ```; will prepend all the metrics with path `cromwell.cromwell-instance-1...` for that instance. ##### Metrics. The current StatsD implementation uses metrics-statsd to report instrumentation values.; metrics-statsd reports all metrics with a gauge type.; This means all metrics will be under the gauge section. We might add or remove metrics in the future depending on need and usage.; These are the current high level categories:.",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:1113,Modifiability,config,configuration,1113,"; production environments. The default implementation of this ignores these metrics, but Cromwell includes alternate implementations that can forward metrics to a; specific server. ### StatsD. While this instrumentation support can be used in smaller environments it will still require setting up a; [StatsD](https://github.com/etsy/statsd) server outside of Cromwell and it's possible not enough data would be produced to be useful. ; Cromwell collects metrics while running and sends them to an internal service. . Make sure to configure your StatsD service:. ```hocon; services.Instrumentation {; 	class = ""cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActor"". 	config {; 	 hostname = ""localhost"" # Replace with your host; 	 port = 8125 # Replace with your port; 	 # prefix = ""my_prefix"" # All metrics will be prefixed by this value if present.; 	 flush-rate = 1 second # Rate at which metrics are sent to the StatsD server; 	}; }; ```. There is also an additional configuration value that can be set: . ```hocon; # Rate at which Cromwell updates its gauge values (number of workflows running, queued, etc...); system.instrumentation-rate = 5 seconds; ```. If you have multiple Cromwell instances, and would like to separate the instrumentation path for each instance, set the `system.cromwell_id` with the unique identifier for each Cromwell instance. For example,; ```hocon; system.cromwell_id = ""cromwell-instance-1""; ```; will prepend all the metrics with path `cromwell.cromwell-instance-1...` for that instance. ##### Metrics. The current StatsD implementation uses metrics-statsd to report instrumentation values.; metrics-statsd reports all metrics with a gauge type.; This means all metrics will be under the gauge section. We might add or remove metrics in the future depending on need and usage.; These are the current high level categories:. * `backend`; * `rest-api`; * `job`; * `workflow`; * `io`. ### Stackdriver. Cromwell now supports sending metrics to ",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:2252,Modifiability,config,config,2252,"nning, queued, etc...); system.instrumentation-rate = 5 seconds; ```. If you have multiple Cromwell instances, and would like to separate the instrumentation path for each instance, set the `system.cromwell_id` with the unique identifier for each Cromwell instance. For example,; ```hocon; system.cromwell_id = ""cromwell-instance-1""; ```; will prepend all the metrics with path `cromwell.cromwell-instance-1...` for that instance. ##### Metrics. The current StatsD implementation uses metrics-statsd to report instrumentation values.; metrics-statsd reports all metrics with a gauge type.; This means all metrics will be under the gauge section. We might add or remove metrics in the future depending on need and usage.; These are the current high level categories:. * `backend`; * `rest-api`; * `job`; * `workflow`; * `io`. ### Stackdriver. Cromwell now supports sending metrics to [Google's Stackdriver API](https://cloud.google.com/monitoring/api/v3/). To use the Stackdriver instrumentation; specify this in your config:; ```hocon; services.Instrumentation {; class = ""cromwell.services.instrumentation.impl.stackdriver.StackdriverInstrumentationServiceActor"". config {; # auth scheme can be `application_default` or `service_account`; auth = ""service-account""; google-project = ""my-project""; # rate at which aggregated metrics will be sent to Stackdriver. It needs to be equal or greater than 1 minute.; # Google's Stackdriver API needs each metric to be sent not more than once per minute.; flush-rate = 1 minute; # below 3 keys are attached as labels to each metric. `cromwell-perf-test-case` is specifically meant for perf env.; cromwell-instance-role = ""role""; cromwell-perf-test-case = ""perf-test-1""; }; }; ```; The 2 label keys are optional. If specified, each metric will have label(s) added in the form of a (key, value) pair.; So for example, if `cromwell-instance-role = ""backend""` is mentioned in config, each metric data point sent to Stackdriver; will have a label (cromwell_instanc",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:2400,Modifiability,config,config,2400,"ce. For example,; ```hocon; system.cromwell_id = ""cromwell-instance-1""; ```; will prepend all the metrics with path `cromwell.cromwell-instance-1...` for that instance. ##### Metrics. The current StatsD implementation uses metrics-statsd to report instrumentation values.; metrics-statsd reports all metrics with a gauge type.; This means all metrics will be under the gauge section. We might add or remove metrics in the future depending on need and usage.; These are the current high level categories:. * `backend`; * `rest-api`; * `job`; * `workflow`; * `io`. ### Stackdriver. Cromwell now supports sending metrics to [Google's Stackdriver API](https://cloud.google.com/monitoring/api/v3/). To use the Stackdriver instrumentation; specify this in your config:; ```hocon; services.Instrumentation {; class = ""cromwell.services.instrumentation.impl.stackdriver.StackdriverInstrumentationServiceActor"". config {; # auth scheme can be `application_default` or `service_account`; auth = ""service-account""; google-project = ""my-project""; # rate at which aggregated metrics will be sent to Stackdriver. It needs to be equal or greater than 1 minute.; # Google's Stackdriver API needs each metric to be sent not more than once per minute.; flush-rate = 1 minute; # below 3 keys are attached as labels to each metric. `cromwell-perf-test-case` is specifically meant for perf env.; cromwell-instance-role = ""role""; cromwell-perf-test-case = ""perf-test-1""; }; }; ```; The 2 label keys are optional. If specified, each metric will have label(s) added in the form of a (key, value) pair.; So for example, if `cromwell-instance-role = ""backend""` is mentioned in config, each metric data point sent to Stackdriver; will have a label (cromwell_instance_role, backend) added to it. There is another optional label that can be added to each metric. `cromwell_id` represents the identifier for different Cromwell instances.; ```hocon; # Unique Cromwell instance identifier; system.cromwell_id = ""cromwell-instance-1""",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:3148,Modifiability,config,config,3148,"/monitoring/api/v3/). To use the Stackdriver instrumentation; specify this in your config:; ```hocon; services.Instrumentation {; class = ""cromwell.services.instrumentation.impl.stackdriver.StackdriverInstrumentationServiceActor"". config {; # auth scheme can be `application_default` or `service_account`; auth = ""service-account""; google-project = ""my-project""; # rate at which aggregated metrics will be sent to Stackdriver. It needs to be equal or greater than 1 minute.; # Google's Stackdriver API needs each metric to be sent not more than once per minute.; flush-rate = 1 minute; # below 3 keys are attached as labels to each metric. `cromwell-perf-test-case` is specifically meant for perf env.; cromwell-instance-role = ""role""; cromwell-perf-test-case = ""perf-test-1""; }; }; ```; The 2 label keys are optional. If specified, each metric will have label(s) added in the form of a (key, value) pair.; So for example, if `cromwell-instance-role = ""backend""` is mentioned in config, each metric data point sent to Stackdriver; will have a label (cromwell_instance_role, backend) added to it. There is another optional label that can be added to each metric. `cromwell_id` represents the identifier for different Cromwell instances.; ```hocon; # Unique Cromwell instance identifier; system.cromwell_id = ""cromwell-instance-1""; ```. ##### Metric type and Label keys naming convention; More details on the this can be found [here](https://cloud.google.com/monitoring/api/v3/metrics-details#metric-kinds). You must adhere to the following spelling rules for metric type names:; - You can use upper and lower-case letters, digits, and underscores (_) in the names.; - You can use periods (.) in the domain part of the names.; - You can use forward slashes (/) to separate path elements.; - You can start each path element with a letter or digit.; - The maximum length of a metric type name is 200 characters. You must adhere to the following spelling rules for metric label names:; - You can use upper",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:1242,Performance,queue,queued,1242,"but Cromwell includes alternate implementations that can forward metrics to a; specific server. ### StatsD. While this instrumentation support can be used in smaller environments it will still require setting up a; [StatsD](https://github.com/etsy/statsd) server outside of Cromwell and it's possible not enough data would be produced to be useful. ; Cromwell collects metrics while running and sends them to an internal service. . Make sure to configure your StatsD service:. ```hocon; services.Instrumentation {; 	class = ""cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActor"". 	config {; 	 hostname = ""localhost"" # Replace with your host; 	 port = 8125 # Replace with your port; 	 # prefix = ""my_prefix"" # All metrics will be prefixed by this value if present.; 	 flush-rate = 1 second # Rate at which metrics are sent to the StatsD server; 	}; }; ```. There is also an additional configuration value that can be set: . ```hocon; # Rate at which Cromwell updates its gauge values (number of workflows running, queued, etc...); system.instrumentation-rate = 5 seconds; ```. If you have multiple Cromwell instances, and would like to separate the instrumentation path for each instance, set the `system.cromwell_id` with the unique identifier for each Cromwell instance. For example,; ```hocon; system.cromwell_id = ""cromwell-instance-1""; ```; will prepend all the metrics with path `cromwell.cromwell-instance-1...` for that instance. ##### Metrics. The current StatsD implementation uses metrics-statsd to report instrumentation values.; metrics-statsd reports all metrics with a gauge type.; This means all metrics will be under the gauge section. We might add or remove metrics in the future depending on need and usage.; These are the current high level categories:. * `backend`; * `rest-api`; * `job`; * `workflow`; * `io`. ### Stackdriver. Cromwell now supports sending metrics to [Google's Stackdriver API](https://cloud.google.com/monitoring/api/v3/). To use the S",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:2824,Testability,test,test-case,2824," metrics will be under the gauge section. We might add or remove metrics in the future depending on need and usage.; These are the current high level categories:. * `backend`; * `rest-api`; * `job`; * `workflow`; * `io`. ### Stackdriver. Cromwell now supports sending metrics to [Google's Stackdriver API](https://cloud.google.com/monitoring/api/v3/). To use the Stackdriver instrumentation; specify this in your config:; ```hocon; services.Instrumentation {; class = ""cromwell.services.instrumentation.impl.stackdriver.StackdriverInstrumentationServiceActor"". config {; # auth scheme can be `application_default` or `service_account`; auth = ""service-account""; google-project = ""my-project""; # rate at which aggregated metrics will be sent to Stackdriver. It needs to be equal or greater than 1 minute.; # Google's Stackdriver API needs each metric to be sent not more than once per minute.; flush-rate = 1 minute; # below 3 keys are attached as labels to each metric. `cromwell-perf-test-case` is specifically meant for perf env.; cromwell-instance-role = ""role""; cromwell-perf-test-case = ""perf-test-1""; }; }; ```; The 2 label keys are optional. If specified, each metric will have label(s) added in the form of a (key, value) pair.; So for example, if `cromwell-instance-role = ""backend""` is mentioned in config, each metric data point sent to Stackdriver; will have a label (cromwell_instance_role, backend) added to it. There is another optional label that can be added to each metric. `cromwell_id` represents the identifier for different Cromwell instances.; ```hocon; # Unique Cromwell instance identifier; system.cromwell_id = ""cromwell-instance-1""; ```. ##### Metric type and Label keys naming convention; More details on the this can be found [here](https://cloud.google.com/monitoring/api/v3/metrics-details#metric-kinds). You must adhere to the following spelling rules for metric type names:; - You can use upper and lower-case letters, digits, and underscores (_) in the names.; - You",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:2919,Testability,test,test-case,2919,"pending on need and usage.; These are the current high level categories:. * `backend`; * `rest-api`; * `job`; * `workflow`; * `io`. ### Stackdriver. Cromwell now supports sending metrics to [Google's Stackdriver API](https://cloud.google.com/monitoring/api/v3/). To use the Stackdriver instrumentation; specify this in your config:; ```hocon; services.Instrumentation {; class = ""cromwell.services.instrumentation.impl.stackdriver.StackdriverInstrumentationServiceActor"". config {; # auth scheme can be `application_default` or `service_account`; auth = ""service-account""; google-project = ""my-project""; # rate at which aggregated metrics will be sent to Stackdriver. It needs to be equal or greater than 1 minute.; # Google's Stackdriver API needs each metric to be sent not more than once per minute.; flush-rate = 1 minute; # below 3 keys are attached as labels to each metric. `cromwell-perf-test-case` is specifically meant for perf env.; cromwell-instance-role = ""role""; cromwell-perf-test-case = ""perf-test-1""; }; }; ```; The 2 label keys are optional. If specified, each metric will have label(s) added in the form of a (key, value) pair.; So for example, if `cromwell-instance-role = ""backend""` is mentioned in config, each metric data point sent to Stackdriver; will have a label (cromwell_instance_role, backend) added to it. There is another optional label that can be added to each metric. `cromwell_id` represents the identifier for different Cromwell instances.; ```hocon; # Unique Cromwell instance identifier; system.cromwell_id = ""cromwell-instance-1""; ```. ##### Metric type and Label keys naming convention; More details on the this can be found [here](https://cloud.google.com/monitoring/api/v3/metrics-details#metric-kinds). You must adhere to the following spelling rules for metric type names:; - You can use upper and lower-case letters, digits, and underscores (_) in the names.; - You can use periods (.) in the domain part of the names.; - You can use forward slashes (/) t",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md:2937,Testability,test,test-,2937,"pending on need and usage.; These are the current high level categories:. * `backend`; * `rest-api`; * `job`; * `workflow`; * `io`. ### Stackdriver. Cromwell now supports sending metrics to [Google's Stackdriver API](https://cloud.google.com/monitoring/api/v3/). To use the Stackdriver instrumentation; specify this in your config:; ```hocon; services.Instrumentation {; class = ""cromwell.services.instrumentation.impl.stackdriver.StackdriverInstrumentationServiceActor"". config {; # auth scheme can be `application_default` or `service_account`; auth = ""service-account""; google-project = ""my-project""; # rate at which aggregated metrics will be sent to Stackdriver. It needs to be equal or greater than 1 minute.; # Google's Stackdriver API needs each metric to be sent not more than once per minute.; flush-rate = 1 minute; # below 3 keys are attached as labels to each metric. `cromwell-perf-test-case` is specifically meant for perf env.; cromwell-instance-role = ""role""; cromwell-perf-test-case = ""perf-test-1""; }; }; ```; The 2 label keys are optional. If specified, each metric will have label(s) added in the form of a (key, value) pair.; So for example, if `cromwell-instance-role = ""backend""` is mentioned in config, each metric data point sent to Stackdriver; will have a label (cromwell_instance_role, backend) added to it. There is another optional label that can be added to each metric. `cromwell_id` represents the identifier for different Cromwell instances.; ```hocon; # Unique Cromwell instance identifier; system.cromwell_id = ""cromwell-instance-1""; ```. ##### Metric type and Label keys naming convention; More details on the this can be found [here](https://cloud.google.com/monitoring/api/v3/metrics-details#metric-kinds). You must adhere to the following spelling rules for metric type names:; - You can use upper and lower-case letters, digits, and underscores (_) in the names.; - You can use periods (.) in the domain part of the names.; - You can use forward slashes (/) t",MatchSource.DOCS,docs/developers/Instrumentation.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Instrumentation.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:1505,Deployability,configurat,configuration,1505,"g additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports ",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:2651,Deployability,configurat,configuration,2651,"o/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports must also be configured separately in order to not clash. If persistent database storage is being used, then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:4221,Deployability,configurat,configuration,4221,"then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these Cromwell servers will have access to the same files that any other user has access to. The `refresh_token` or `google_compute_service_account` scheme is the only way to ensure data is protected among multiple users of a Cromwell server, however the aforementioned caveats of authorization for endpoints still applies. . **Protecting Secrets**. Various parts of the Cromwell server [Configuration](Configuring) contain sensitive information, e.g. username and password for your persistent database. It is strongly recommended to protect the configuration files from any untrusted users, for instance by limiting who can access your Cromwell server host or using a technology such as [HashiCorp Vault](https://www.vaultproject.io/). . Additionally, the contents of the Cromwell database can contain sensitive information, so it is recommended to limit the access of the database only to trusted users.",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:434,Modifiability,config,configured,434,"**Warning!**. - Cromwell is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:1505,Modifiability,config,configuration,1505,"g additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports ",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:2022,Modifiability,extend,extends,2022," Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports must also be configured separately in order to not clash. If persistent database storage is being used, then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell se",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:2489,Modifiability,config,configured,2489,"ntication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports must also be configured separately in order to not clash. If persistent database storage is being used, then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentica",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:2607,Modifiability,config,configured,2607,"deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports must also be configured separately in order to not clash. If persistent database storage is being used, then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access fil",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:2651,Modifiability,config,configuration,2651,"o/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports must also be configured separately in order to not clash. If persistent database storage is being used, then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:2674,Modifiability,extend,extended,2674,"o/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e.g. `cromwell1`, `cromwell2` etc so that processes running under one Cromwell instance cannot access the files of another. Different webservice ports must also be configured separately in order to not clash. If persistent database storage is being used, then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:4221,Modifiability,config,configuration,4221,"then each instance must be configured with its own database. The proxy configuration above is extended simply by adding another `Location`:. ```Apache; <Location /cromwell1>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile1; Require user stillanotherperson andanother; ProxyPass http://101.101.234.567:8001; </Location>; ```. **Multiple Tenants in one Cromwell Server**. Even with a proxy in place, a single Cromwell server does not provide _authorization_ for individual users hitting the endpoints. This allows, for instance, one _authenticated_ user to view the metadata for a workflow run by another _authenticated_ user. Due to this limitation, it is important that all users of this Cromwell server be trusted. . With the exception of the use of the `google_compute_service_account` scheme in the [Configuration](Configuring#authentication) for the [Google Cloud backend](backends/Google)), Cromwell servers are setup to access files for use in workflows using a single set of credentials. This means that all users of these Cromwell servers will have access to the same files that any other user has access to. The `refresh_token` or `google_compute_service_account` scheme is the only way to ensure data is protected among multiple users of a Cromwell server, however the aforementioned caveats of authorization for endpoints still applies. . **Protecting Secrets**. Various parts of the Cromwell server [Configuration](Configuring) contain sensitive information, e.g. username and password for your persistent database. It is strongly recommended to protect the configuration files from any untrusted users, for instance by limiting who can access your Cromwell server host or using a technology such as [HashiCorp Vault](https://www.vaultproject.io/). . Additionally, the contents of the Cromwell database can contain sensitive information, so it is recommended to limit the access of the database only to trusted users.",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:45,Security,secur,security,45,"**Warning!**. - Cromwell is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:106,Security,secur,security,106,"**Warning!**. - Cromwell is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:154,Security,secur,security,154,"**Warning!**. - Cromwell is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:250,Security,secur,security,250,"**Warning!**. - Cromwell is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:610,Security,access,accessible,610,"**Warning!**. - Cromwell is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:681,Security,access,access,681,"**Warning!**. - Cromwell is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:909,Security,access,access,909,"**Warning!**. - Cromwell is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:992,Security,access,access,992," is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme ext",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:1016,Security,authenticat,authenticating,1016," is NOT on its own a security appliance!; - Only YOU are responsible for your own security! ; - Please be sure to check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme ext",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:1103,Security,firewall,firewall,1103,"check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If m",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:1153,Security,access,access,1153,"check with your security team before setting up your Cromwell server; - Some recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If m",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:1223,Security,secur,secure,1223," recommendations and suggestions on security can be found below. __This is intended as helpful guidance only, and is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be ",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md:1345,Security,authenticat,authenticated,1345,"nd is not endorsed by the Broad Institute.__. Cromwell running in server mode accepts all connections on the configured webservice port. Without taking additional measures to protect your Cromwell server, this can leave your Cromwell server and therefore all information stored by and accessible by your Cromwell server vulnerable to anyone who is able to access the server. For instance, an unprotected Cromwell server running against the [Google Cloud backend](backends/Google) would leave you vulnerable to outside users running workflows that will cost you money, and potentially access the data files you use within your workflows!. The simplest way to restrict access is by putting an authenticating proxy server in between users and the Cromwell server: . 1. Configure a firewall rule on the Cromwell server host to deny access to the webservice port (e.g. 8000) from all addresses except a secure proxy host. ; 2. Configure `<YourFavoriteWebProxy>` on the proxy host with `<YourFavoriteAuthMechanism>`, to proxy authenticated traffic from the world to the Cromwell server. Using Apache `httpd` web server for example with basic `htpassword` file-based authentication, the configuration might look something like:. ```Apache; <Location /cromwell>; Order deny,allow; Allow from all; AuthType Basic; AuthName ""Password Required""; AuthUserFile /path/to/my/htpasswdfile; Require user someone someoneelse; ProxyPass http://101.101.234.567:8000 # address of cromwell server web service; </Location>; ```. Users now hit `http://my.proxy.org/cromwell` with authenticated requests, and they're forwarded to port 8000 on the Cromwell server host. . **Multiple Servers on one Host**. The above scheme extends easily to multiple Cromwell instances, for use by different groups within an organization, for example. If multiple instances are running on the same host, then each instance should be run as its own dedicated user (such as service account when using a [Google Cloud backend](backends/Google)), e",MatchSource.DOCS,docs/developers/Security.md,broadinstitute,cromwell,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/tree/87/docs/developers/Security.md
