id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:13198,Deployability,pipeline,pipeline-,13198,"Version: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16599,Deployability,pipeline,pipeline,16599,"robeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16608,Deployability,pipeline,pipeline-,16608,"robeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16667,Deployability,pipeline,pipeline,16667,"robeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16676,Deployability,pipeline,pipeline-,16676,"robeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16739,Deployability,pipeline,pipeline,16739,"robeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16748,Deployability,pipeline,pipeline-,16748,"robeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16822,Deployability,pipeline,pipeline,16822,"main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16831,Deployability,pipeline,pipeline-,16831,"main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16900,Deployability,pipeline,pipeline,16900,"; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRU",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16909,Deployability,pipeline,pipeline-,16909,"; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRU",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16980,Deployability,pipeline,pipeline,16980," batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16989,Deployability,pipeline,pipeline-,16989," batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:17054,Deployability,pipeline,pipeline,17054," batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:17063,Deployability,pipeline,pipeline-,17063," batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Re",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:17214,Deployability,pipeline,pipeline,17214,".8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:17223,Deployability,pipeline,pipeline-,17223,".8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:17313,Deployability,pipeline,pipeline,17313,7b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:17322,Deployability,pipeline,pipeline-,17322,7b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:17388,Deployability,pipeline,pipeline,17388,7b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:17397,Deployability,pipeline,pipeline-,17397,7b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:4950,Energy Efficiency,schedul,scheduler,4950,"_message_path': '/dev/termination-log',; 'termination_message_policy': 'File',; 'tty': None,; 'volume_devices': None,; 'volume_mounts': [{'mount_path': '/gsa-key',; 'mount_propagation': None,; 'name': 'gsa-key',; 'read_only': None,; 'sub_path': None},; {'mount_path': '/io',; 'mount_propagation': None,; 'name': 'batch-2554-job-4-8vvgl',; 'read_only': None,; 'sub_path': None},; {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',; 'mount_propagation': None,; 'name': 'default-token-8h99c',; 'read_only': True,; 'sub_path': None}],; 'working_dir': None}],; 'dns_config': None,; 'dns_policy': 'ClusterFirst',; 'enable_service_links': True,; 'host_aliases': None,; 'host_ipc': None,; 'host_network': None,; 'host_pid': None,; 'hostname': None,; 'image_pull_secrets': None,; 'init_containers': None,; 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-4gq2',; 'node_selector': None,; 'priority': 500000,; 'priority_class_name': 'user',; 'readiness_gates': None,; 'restart_policy': 'Never',; 'runtime_class_name': None,; 'scheduler_name': 'default-scheduler',; 'security_context': {'fs_group': None,; 'run_as_group': None,; 'run_as_non_root': None,; 'run_as_user': None,; 'se_linux_options': None,; 'supplemental_groups': None,; 'sysctls': None},; 'service_account': 'default',; 'service_account_name': 'default',; 'share_process_namespace': None,; 'subdomain': None,; 'termination_grace_period_seconds': 30,; 'tolerations': [{'effect': None,; 'key': 'preemptible',; 'operator': None,; 'toleration_seconds': None,; 'value': 'true'},; {'effect': 'NoExecute',; 'key': 'node.kubernetes.io/not-ready',; 'operator': 'Exists',; 'toleration_seconds': 300,; 'value': None},; {'effect': 'NoExecute',; 'key': 'node.kubernetes.io/unreachable',; 'operator': 'Exists',; 'toleration_seconds': 300,; 'value': None}],; 'volumes': [{'aws_elastic_block_store': None,; 'azure_disk': None,; 'azure_file': None,; 'cephfs': None,; 'cinder': None,; 'config_map': None,; 'downward_api': None,; 'empty_dir': None,; 'fc':",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14498,Energy Efficiency,schedul,schedulerName,14498,"__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14521,Energy Efficiency,schedul,scheduler,14521,"__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:19067,Energy Efficiency,Schedul,Scheduled,19067,"gl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 11m default-scheduler Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; Warning FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:19089,Energy Efficiency,schedul,scheduler,19089,"gl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 11m default-scheduler Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; Warning FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:20745,Energy Efficiency,Schedul,Scheduled,20745,"elet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2554-job-4-main-vsk7h; ```; The events; ```; + kubectl get events -n batch-pods --sort-by=.metadata.creationTimestamp; + grep 2554; 12m Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; 11m Normal Scheduled Pod Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; 36s Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:7900,Integrability,message,message,7900,"one,; 'quobyte': None,; 'rbd': None,; 'scale_io': None,; 'secret': None,; 'storageos': None,; 'vsphere_volume': None},; {'aws_elastic_block_store': None,; 'azure_disk': None,; 'azure_file': None,; 'cephfs': None,; 'cinder': None,; 'config_map': None,; 'downward_api': None,; 'empty_dir': None,; 'fc': None,; 'flex_volume': None,; 'flocker': None,; 'gce_persistent_disk': None,; 'git_repo': None,; 'glusterfs': None,; 'host_path': None,; 'iscsi': None,; 'name': 'default-token-8h99c',; 'nfs': None,; 'persistent_volume_claim': None,; 'photon_persistent_disk': None,; 'portworx_volume': None,; 'projected': None,; 'quobyte': None,; 'rbd': None,; 'scale_io': None,; 'secret': {'default_mode': 420,; 'items': None,; 'optional': None,; 'secret_name': 'default-token-8h99c'},; 'storageos': None,; 'vsphere_volume': None}]},; 'status': {'conditions': [{'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'Initialized'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id':",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:8090,Integrability,message,message,8090," 'cinder': None,; 'config_map': None,; 'downward_api': None,; 'empty_dir': None,; 'fc': None,; 'flex_volume': None,; 'flocker': None,; 'gce_persistent_disk': None,; 'git_repo': None,; 'glusterfs': None,; 'host_path': None,; 'iscsi': None,; 'name': 'default-token-8h99c',; 'nfs': None,; 'persistent_volume_claim': None,; 'photon_persistent_disk': None,; 'portworx_volume': None,; 'projected': None,; 'quobyte': None,; 'rbd': None,; 'scale_io': None,; 'secret': {'default_mode': 420,; 'items': None,; 'optional': None,; 'secret_name': 'default-token-8h99c'},; 'storageos': None,; 'vsphere_volume': None}]},; 'status': {'conditions': [{'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'Initialized'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': N",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:8327,Integrability,message,message,8327,"ame': 'default-token-8h99c',; 'nfs': None,; 'persistent_volume_claim': None,; 'photon_persistent_disk': None,; 'portworx_volume': None,; 'projected': None,; 'quobyte': None,; 'rbd': None,; 'scale_io': None,; 'secret': {'default_mode': 420,; 'items': None,; 'optional': None,; 'secret_name': 'default-token-8h99c'},; 'storageos': None,; 'vsphere_volume': None}]},; 'status': {'conditions': [{'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'Initialized'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:8574,Integrability,message,message,8574,"_mode': 420,; 'items': None,; 'optional': None,; 'secret_name': 'default-token-8h99c'},; 'storageos': None,; 'vsphere_volume': None}]},; 'status': {'conditions': [{'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'Initialized'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 53, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurren",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:8991,Integrability,message,message,8991,"'reason': None,; 'status': 'True',; 'type': 'Initialized'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 53, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:9146,Integrability,message,message,9146,": 'Ready'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': 'containers with unready status: [main]',; 'reason': 'ContainersNotReady',; 'status': 'False',; 'type': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 53, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collecti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:9422,Integrability,wrap,wrapped,9422,"': 'ContainersReady'},; {'last_probe_time': None,; 'last_transition_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'message': None,; 'reason': None,; 'status': 'True',; 'type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 53, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:15343,Integrability,message,message,15343,"8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:15519,Integrability,message,message,15519,"8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: ContainersReady; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:19020,Integrability,Message,Message,19020,"gl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 11m default-scheduler Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; Warning FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:9616,Performance,concurren,concurrent,9616,"type': 'PodScheduled'}],; 'container_statuses': [{'container_id': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_id': '',; 'last_state': {'running': None,; 'terminated': None,; 'waiting': None},; 'name': 'main',; 'ready': False,; 'restart_count': 0,; 'state': {'running': None,; 'terminated': {'container_id': None,; 'exit_code': 0,; 'finished_at': None,; 'message': None,; 'reason': None,; 'signal': None,; 'started_at': None},; 'waiting': None}}],; 'host_ip': '10.128.0.8',; 'init_container_statuses': None,; 'message': None,; 'nominated_node_name': None,; 'phase': 'Pending',; 'pod_ip': None,; 'qos_class': 'Burstable',; 'reason': None,; 'start_time': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal())}}; File ""/usr/local/lib/python3.6/dist-packages/batch/k8s.py"", line 53, in wrapped; **kwargs),; File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in blocking_to_async; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run; result = self.fn(*self.args, **self.kwargs); File ""/usr/local/lib/python3.6/dist-packages/batch/blocking_to_async.py"", line 6, in <lambda>; thread_pool, lambda: fun(*args, **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18538, in read_namespaced_pod_log; (data) = self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_clie",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:19403,Safety,timeout,timeout,19403,"gl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersReady False ; PodScheduled True ; Volumes:; gsa-key:; Type: Secret (a volume populated by a Secret); SecretName: konradk-gsa-key; Optional: false; batch-2554-job-4-8vvgl:; Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace); ClaimName: batch-2554-job-4-8vvgl; ReadOnly: false; default-token-8h99c:; Type: Secret (a volume populated by a Secret); SecretName: default-token-8h99c; Optional: false; QoS Class: Burstable; Node-Selectors: <none>; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s; node.kubernetes.io/unreachable:NoExecute for 300s; preemptible=true; Events:; Type Reason Age From Message; ---- ------ ---- ---- -------; Normal Scheduled 11m default-scheduler Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; Warning FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:20494,Safety,timeout,timeout,20494,"elet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2554-job-4-main-vsk7h; ```; The events; ```; + kubectl get events -n batch-pods --sort-by=.metadata.creationTimestamp; + grep 2554; 12m Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; 11m Normal Scheduled Pod Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; 36s Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:21003,Safety,timeout,timeout,21003,"elet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2554-job-4-main-vsk7h; ```; The events; ```; + kubectl get events -n batch-pods --sort-by=.metadata.creationTimestamp; + grep 2554; 12m Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; 11m Normal Scheduled Pod Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; 36s Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14532,Security,secur,securityContext,14532,"__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: 420; secretName: default-token-8h99c; status:; conditions:; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: Initialized; - lastProbeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; message: 'containers with unready status: [main]'; reason: ContainersNotReady; status: ""False""; type: Ready; - lastProbeTime: null; lastTransitionTime: """,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:20145,Security,Access,Access,20145,"FailedMount 36s (x5 over 9m46s) kubelet, gke-vdc-preemptible-pool-9c7148b2-4gq2 Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; + kubectl describe pvc -n batch-pods batch-2554-job-4-8vvgl; Name: batch-2554-job-4-8vvgl; Namespace: batch-pods; StorageClass: batch; Status: Bound; Volume: pvc-32804669-96f6-11e9-8aa3-42010a80015f; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; Annotations: pv.kubernetes.io/bind-completed: yes; pv.kubernetes.io/bound-by-controller: yes; volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/gce-pd; Finalizers: [kubernetes.io/pvc-protection]; Capacity: 1Gi; Access Modes: RWO; VolumeMode: Filesystem; Events: <none>; Mounted By: batch-2554-job-4-main-vsk7h; ```; The events; ```; + kubectl get events -n batch-pods --sort-by=.metadata.creationTimestamp; + grep 2554; 12m Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-cc8d4_batch-pods(968b4ba5-96f6-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-cc8d4"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job-4-8vvgl default-token-8h99c]; 11m Normal Scheduled Pod Successfully assigned batch-pods/batch-2554-job-4-main-vsk7h to gke-vdc-preemptible-pool-9c7148b2-4gq2; 36s Warning FailedMount Pod Unable to mount volumes for pod ""batch-2554-job-4-main-vsk7h_batch-pods(f1d2b3ad-9745-11e9-8aa3-42010a80015f)"": timeout expired waiting for volumes to attach or mount for pod ""batch-pods""/""batch-2554-job-4-main-vsk7h"". list of unmounted volumes=[batch-2554-job-4-8vvgl]. list of unattached volumes=[gsa-key batch-2554-job",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:93,Testability,log,logs,93,"I deleted the pod; ```; # k delete pod batch-2554-job-4-main-cc8d4 -n batch-pods; ```; Batch logs when batch discovered 2554 task 4 ""failed"":; ```; INFO | 2019-06-25 12:37:07,611 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-cc8d4; INFO | 2019-06-25 12:37:07,671 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-cc8d4; INFO | 2019-06-25 12:37:07,671 | batch.py | update_job_with_pod:989 | job (2554, 4) mark complete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipelin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:3931,Testability,log,log,3931,"E_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'env': [{'name': 'POD_IP',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'status.podIP'},; 'resource_field_ref': None,; 'secret_key_ref': None}},; {'name': 'POD_NAME',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'metadata.name'},; 'resource_field_ref': None,; 'secret_key_ref': None}}],; 'env_from': None,; 'image': 'konradjk/saige:0.35.8.2.2',; 'image_pull_policy': 'IfNotPresent',; 'lifecycle': None,; 'liveness_probe': None,; 'name': 'main',; 'ports': None,; 'readiness_probe': None,; 'resources': {'limits': None,; 'requests': {'cpu': '1',; 'memory': '500M'}},; 'security_context': None,; 'stdin': None,; 'stdin_once': None,; 'termination_message_path': '/dev/termination-log',; 'termination_message_policy': 'File',; 'tty': None,; 'volume_devices': None,; 'volume_mounts': [{'mount_path': '/gsa-key',; 'mount_propagation': None,; 'name': 'gsa-key',; 'read_only': None,; 'sub_path': None},; {'mount_path': '/io',; 'mount_propagation': None,; 'name': 'batch-2554-job-4-8vvgl',; 'read_only': None,; 'sub_path': None},; {'mount_path': '/var/run/secrets/kubernetes.io/serviceaccount',; 'mount_propagation': None,; 'name': 'default-token-8h99c',; 'read_only': True,; 'sub_path': None}],; 'working_dir': None}],; 'dns_config': None,; 'dns_policy': 'ClusterFirst',; 'enable_service_links': True,; 'host_aliases': None,; 'host_ipc': None,; 'host_network': None,; 'host_pid': None,; 'hostname': None,; 'image_pull_secrets': None,; 'init_containers': None,; 'node_name': 'gke-vdc-preemptible-pool-9c7148b2-4gq2',; 'node_selector': None,; 'priority': 500000,; 'priority_class_name': 'user',; 'readiness_gates': None,; 'restart_policy': 'Never',; 'runtime_class_name': None,; 'scheduler_name': 'default-schedule",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:10990,Testability,log,logs,10990,"mespace, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/apis/core_v1_api.py"", line 18644, in read_namespaced_pod_log_with_http_info; collection_formats=collection_formats); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); INFO | 2019-06-25 12:37:07,703 | batch.py | mark_complete:501 | no logs for batch-2554-job-4-main-cc8d4 due to previous error, rescheduling pod; INFO | 2019-06-25 12:37:07,730 | batch.py | _create_pod:205 | created pod name: batch-2554-job-4-main-vsk7h for job (2554, 4), task main; INFO | 2019-06-25 12:37:07,788 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instan",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:11510,Testability,log,log,11510,"l_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); INFO | 2019-06-25 12:37:07,703 | batch.py | mark_complete:501 | no logs for batch-2554-job-4-main-cc8d4 due to previous error, rescheduling pod; INFO | 2019-06-25 12:37:07,730 | batch.py | _create_pod:205 | created pod name: batch-2554-job-4-main-vsk7h for job (2554, 4), task main; INFO | 2019-06-25 12:37:07,788 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RES",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:14091,Testability,log,log,14091,"pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key; - mountPath: /io; name: batch-2554-job-4-8vvgl; - mountPath: /var/run/secrets/kubernetes.io/serviceaccount; name: default-token-8h99c; readOnly: true; dnsPolicy: ClusterFirst; enableServiceLinks: true; nodeName: gke-vdc-preemptible-pool-9c7148b2-4gq2; priority: 500000; priorityClassName: user; restartPolicy: Never; schedulerName: default-scheduler; securityContext: {}; serviceAccount: default; serviceAccountName: default; terminationGracePeriodSeconds: 30; tolerations:; - key: preemptible; value: ""true""; - effect: NoExecute; key: node.kubernetes.io/not-ready; operator: Exists; tolerationSeconds: 300; - effect: NoExecute; key: node.kubernetes.io/unreachable; operator: Exists; tolerationSeconds: 300; volumes:; - name: gsa-key; secret:; defaultMode: 420; secretName: konradk-gsa-key; - name: batch-2554-job-4-8vvgl; persistentVolumeClaim:; claimName: batch-2554-job-4-8vvgl; - name: default-token-8h99c; secret:; defaultMode: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649
https://github.com/hail-is/hail/pull/6468#issuecomment-505496680:57,Performance,load,loaded,57,Do you not want to make this a separate script that gets loaded?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6468#issuecomment-505496680
https://github.com/hail-is/hail/pull/6477#issuecomment-505891762:49,Testability,log,logging,49,I'm not sure either. I want to keep the explicit logging of when this happens for now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6477#issuecomment-505891762
https://github.com/hail-is/hail/pull/6482#issuecomment-505873500:222,Availability,down,down,222,"The unoptimized pipeline doesn't have the decorator right now, but I do want to version it so it's easier to find when I decide what we want to do with it. I also think that there's an optimization that will bring it back down to ~a few mins, rather than an hour.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500
https://github.com/hail-is/hail/pull/6482#issuecomment-505873500:16,Deployability,pipeline,pipeline,16,"The unoptimized pipeline doesn't have the decorator right now, but I do want to version it so it's easier to find when I decide what we want to do with it. I also think that there's an optimization that will bring it back down to ~a few mins, rather than an hour.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500
https://github.com/hail-is/hail/pull/6482#issuecomment-505873500:185,Performance,optimiz,optimization,185,"The unoptimized pipeline doesn't have the decorator right now, but I do want to version it so it's easier to find when I decide what we want to do with it. I also think that there's an optimization that will bring it back down to ~a few mins, rather than an hour.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500
https://github.com/hail-is/hail/issues/6483#issuecomment-505884681:233,Deployability,Deploy,Deployment,233,"Prometheus storage was only 10Gb, so it filled up after 14 days. By default, Prometheus deletes logs after 15 days. I increased the storage size to 50Gb accordingly. I also decided to use this opportunity to switch Prometheus from a Deployment to a StatefulSet. This meant turning the PersistentVolume for PrometheusStorage in the monitoring.yaml file to a PersistentVolumeClaim within the Prometheus StatefulSet spec. However, the claim was configured to mount at the same location as the previous PersistentVolume, and I did not first delete the old PersistentVolume. As a result, the new 50Gb disk was not initially allocated. I resolved this by deleting the StatefulSet and the old PersistentVolume, then redeploying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681
https://github.com/hail-is/hail/issues/6483#issuecomment-505884681:331,Energy Efficiency,monitor,monitoring,331,"Prometheus storage was only 10Gb, so it filled up after 14 days. By default, Prometheus deletes logs after 15 days. I increased the storage size to 50Gb accordingly. I also decided to use this opportunity to switch Prometheus from a Deployment to a StatefulSet. This meant turning the PersistentVolume for PrometheusStorage in the monitoring.yaml file to a PersistentVolumeClaim within the Prometheus StatefulSet spec. However, the claim was configured to mount at the same location as the previous PersistentVolume, and I did not first delete the old PersistentVolume. As a result, the new 50Gb disk was not initially allocated. I resolved this by deleting the StatefulSet and the old PersistentVolume, then redeploying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681
https://github.com/hail-is/hail/issues/6483#issuecomment-505884681:619,Energy Efficiency,allocate,allocated,619,"Prometheus storage was only 10Gb, so it filled up after 14 days. By default, Prometheus deletes logs after 15 days. I increased the storage size to 50Gb accordingly. I also decided to use this opportunity to switch Prometheus from a Deployment to a StatefulSet. This meant turning the PersistentVolume for PrometheusStorage in the monitoring.yaml file to a PersistentVolumeClaim within the Prometheus StatefulSet spec. However, the claim was configured to mount at the same location as the previous PersistentVolume, and I did not first delete the old PersistentVolume. As a result, the new 50Gb disk was not initially allocated. I resolved this by deleting the StatefulSet and the old PersistentVolume, then redeploying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681
https://github.com/hail-is/hail/issues/6483#issuecomment-505884681:442,Modifiability,config,configured,442,"Prometheus storage was only 10Gb, so it filled up after 14 days. By default, Prometheus deletes logs after 15 days. I increased the storage size to 50Gb accordingly. I also decided to use this opportunity to switch Prometheus from a Deployment to a StatefulSet. This meant turning the PersistentVolume for PrometheusStorage in the monitoring.yaml file to a PersistentVolumeClaim within the Prometheus StatefulSet spec. However, the claim was configured to mount at the same location as the previous PersistentVolume, and I did not first delete the old PersistentVolume. As a result, the new 50Gb disk was not initially allocated. I resolved this by deleting the StatefulSet and the old PersistentVolume, then redeploying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681
https://github.com/hail-is/hail/issues/6483#issuecomment-505884681:96,Testability,log,logs,96,"Prometheus storage was only 10Gb, so it filled up after 14 days. By default, Prometheus deletes logs after 15 days. I increased the storage size to 50Gb accordingly. I also decided to use this opportunity to switch Prometheus from a Deployment to a StatefulSet. This meant turning the PersistentVolume for PrometheusStorage in the monitoring.yaml file to a PersistentVolumeClaim within the Prometheus StatefulSet spec. However, the claim was configured to mount at the same location as the previous PersistentVolume, and I did not first delete the old PersistentVolume. As a result, the new 50Gb disk was not initially allocated. I resolved this by deleting the StatefulSet and the old PersistentVolume, then redeploying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681
https://github.com/hail-is/hail/issues/6491#issuecomment-513327039:56,Performance,latency,latency,56,#6555 #6560 and #6570 cumulatively address endpoints on latency and request count for batch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6491#issuecomment-513327039
https://github.com/hail-is/hail/issues/6494#issuecomment-506534967:272,Availability,rollback,rollback,272,"Current Plan:. - input pod; exit 0 if any files exist in /io/; if pod is preempted, delete the pvc and start a new pod. - main pod; init container touches /io/some_file_main; if /io/some_file_main exists, exit 0; If pod is preempted, delete pvc and start a new input pod (rollback). - output pod; init container checks /io/some_file_output; if /io/some_file_output exists, exit 0; touch /io/some_file_output at the end of running the output pod successfully",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6494#issuecomment-506534967
https://github.com/hail-is/hail/issues/6494#issuecomment-506534967:272,Deployability,rollback,rollback,272,"Current Plan:. - input pod; exit 0 if any files exist in /io/; if pod is preempted, delete the pvc and start a new pod. - main pod; init container touches /io/some_file_main; if /io/some_file_main exists, exit 0; If pod is preempted, delete pvc and start a new input pod (rollback). - output pod; init container checks /io/some_file_output; if /io/some_file_output exists, exit 0; touch /io/some_file_output at the end of running the output pod successfully",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6494#issuecomment-506534967
https://github.com/hail-is/hail/issues/6494#issuecomment-506730632:145,Availability,failure,failure,145,"Hmm. I think in the main pod file exists case, the init container actually needs to exit non-zero. Batch then needs to distinguish initContainer failure from main container failure. If the initContainer exits 0, the main container will run. For output pod, I think we should just use a normal container, not an init container, so that the above issue isn't present.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6494#issuecomment-506730632
https://github.com/hail-is/hail/issues/6494#issuecomment-506730632:173,Availability,failure,failure,173,"Hmm. I think in the main pod file exists case, the init container actually needs to exit non-zero. Batch then needs to distinguish initContainer failure from main container failure. If the initContainer exits 0, the main container will run. For output pod, I think we should just use a normal container, not an init container, so that the above issue isn't present.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6494#issuecomment-506730632
https://github.com/hail-is/hail/pull/6500#issuecomment-506490357:65,Safety,Unsafe,UnsafeSuite,65,"> No tests for this right now, although I can write . I think an UnsafeSuite test for this should be easy and effective.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6500#issuecomment-506490357
https://github.com/hail-is/hail/pull/6500#issuecomment-506490357:5,Testability,test,tests,5,"> No tests for this right now, although I can write . I think an UnsafeSuite test for this should be easy and effective.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6500#issuecomment-506490357
https://github.com/hail-is/hail/pull/6500#issuecomment-506490357:77,Testability,test,test,77,"> No tests for this right now, although I can write . I think an UnsafeSuite test for this should be easy and effective.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6500#issuecomment-506490357
https://github.com/hail-is/hail/pull/6500#issuecomment-506790438:10,Testability,test,test,10,"I added a test in StagedRegionValue suite, with the rest of the SRVB tests. It only generates structs, but since all of the deepCopy methods will call into each other and will get tested on the struct fields, I cranked the test up to run 1000 times to ensure we were (probably) hitting all the edge cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6500#issuecomment-506790438
https://github.com/hail-is/hail/pull/6500#issuecomment-506790438:69,Testability,test,tests,69,"I added a test in StagedRegionValue suite, with the rest of the SRVB tests. It only generates structs, but since all of the deepCopy methods will call into each other and will get tested on the struct fields, I cranked the test up to run 1000 times to ensure we were (probably) hitting all the edge cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6500#issuecomment-506790438
https://github.com/hail-is/hail/pull/6500#issuecomment-506790438:180,Testability,test,tested,180,"I added a test in StagedRegionValue suite, with the rest of the SRVB tests. It only generates structs, but since all of the deepCopy methods will call into each other and will get tested on the struct fields, I cranked the test up to run 1000 times to ensure we were (probably) hitting all the edge cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6500#issuecomment-506790438
https://github.com/hail-is/hail/pull/6500#issuecomment-506790438:223,Testability,test,test,223,"I added a test in StagedRegionValue suite, with the rest of the SRVB tests. It only generates structs, but since all of the deepCopy methods will call into each other and will get tested on the struct fields, I cranked the test up to run 1000 times to ensure we were (probably) hitting all the edge cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6500#issuecomment-506790438
https://github.com/hail-is/hail/pull/6501#issuecomment-506923400:104,Modifiability,refactor,refactored,104,@patrick-schultz I addressed your comments in the second commit and added sum/count aggregators. I also refactored the tests to test them; they're otherwise still the same.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6501#issuecomment-506923400
https://github.com/hail-is/hail/pull/6501#issuecomment-506923400:119,Testability,test,tests,119,@patrick-schultz I addressed your comments in the second commit and added sum/count aggregators. I also refactored the tests to test them; they're otherwise still the same.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6501#issuecomment-506923400
https://github.com/hail-is/hail/pull/6501#issuecomment-506923400:128,Testability,test,test,128,@patrick-schultz I addressed your comments in the second commit and added sum/count aggregators. I also refactored the tests to test them; they're otherwise still the same.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6501#issuecomment-506923400
https://github.com/hail-is/hail/pull/6510#issuecomment-506860319:8,Availability,failure,failures,8,doctest failures related to `ld_score_regression.sample.mt` -- either we need to mark those as SKIP or check that file in,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6510#issuecomment-506860319
https://github.com/hail-is/hail/pull/6510#issuecomment-510519521:38,Availability,failure,failures,38,"This needs a rebase and has some docs failures, IIRC. I'll push fixes to this branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6510#issuecomment-510519521
https://github.com/hail-is/hail/pull/6519#issuecomment-507063867:147,Deployability,update,updates,147,I implemented all pods and pvcs for a job have the same name. That way we let k8s be responsible for keeping track of the atomicity of pod and pvc updates rather than our database.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6519#issuecomment-507063867
https://github.com/hail-is/hail/pull/6531#issuecomment-509430798:53,Testability,test,test,53,@GreatBrando tomorrow remind me to talk about how to test this,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6531#issuecomment-509430798
https://github.com/hail-is/hail/pull/6534#issuecomment-509260234:40,Testability,test,tests,40,@patrick-schultz looks like some of the tests are failing because the implementation in Emit is missing in this PR. I don't know if you wanted to swap the order of the two PRs and write a manual test or two for the other one to test that the codegen is doing the correct thing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6534#issuecomment-509260234
https://github.com/hail-is/hail/pull/6534#issuecomment-509260234:195,Testability,test,test,195,@patrick-schultz looks like some of the tests are failing because the implementation in Emit is missing in this PR. I don't know if you wanted to swap the order of the two PRs and write a manual test or two for the other one to test that the codegen is doing the correct thing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6534#issuecomment-509260234
https://github.com/hail-is/hail/pull/6534#issuecomment-509260234:228,Testability,test,test,228,@patrick-schultz looks like some of the tests are failing because the implementation in Emit is missing in this PR. I don't know if you wanted to swap the order of the two PRs and write a manual test or two for the other one to test that the codegen is doing the correct thing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6534#issuecomment-509260234
https://github.com/hail-is/hail/pull/6535#issuecomment-508248281:183,Testability,test,tests,183,"Up until this piece, nothing should have changed behavior in production. This piece enables the new path, so we might want to be more careful about merging it. Hopefully it will fail tests, because I know at least prev_nonnull is currently not working, and there may be other bugs left. If it somehow passes, we should probably get more complete tests before merging.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6535#issuecomment-508248281
https://github.com/hail-is/hail/pull/6535#issuecomment-508248281:346,Testability,test,tests,346,"Up until this piece, nothing should have changed behavior in production. This piece enables the new path, so we might want to be more careful about merging it. Hopefully it will fail tests, because I know at least prev_nonnull is currently not working, and there may be other bugs left. If it somehow passes, we should probably get more complete tests before merging.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6535#issuecomment-508248281
https://github.com/hail-is/hail/pull/6535#issuecomment-509383664:235,Availability,error,error,235,"@patrick-schultz @tpoterba I removed the dependence on #6534 and wrote a test to exercise the IR nodes and all the aggregators we had written. This can now be reviewed/go in independently of the other one, which was failing on a match error in Emit. (I'll request changes to block the other one from going in until we've discussed a plan for using it, since that's now the one that will send everything through the new path.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6535#issuecomment-509383664
https://github.com/hail-is/hail/pull/6535#issuecomment-509383664:41,Integrability,depend,dependence,41,"@patrick-schultz @tpoterba I removed the dependence on #6534 and wrote a test to exercise the IR nodes and all the aggregators we had written. This can now be reviewed/go in independently of the other one, which was failing on a match error in Emit. (I'll request changes to block the other one from going in until we've discussed a plan for using it, since that's now the one that will send everything through the new path.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6535#issuecomment-509383664
https://github.com/hail-is/hail/pull/6535#issuecomment-509383664:73,Testability,test,test,73,"@patrick-schultz @tpoterba I removed the dependence on #6534 and wrote a test to exercise the IR nodes and all the aggregators we had written. This can now be reviewed/go in independently of the other one, which was failing on a match error in Emit. (I'll request changes to block the other one from going in until we've discussed a plan for using it, since that's now the one that will send everything through the new path.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6535#issuecomment-509383664
https://github.com/hail-is/hail/pull/6540#issuecomment-508089671:1223,Testability,Test,Test,1223,"The IR has functions so that we don't need a node for every bespoke library function we might want to call out to. There are three types of functions: Relational functions, IR functions, and Code functions. Relational functions take Tables/MatrixTables as arguments. I haven't touched these. IR functions are essentially aliases for IR patterns we expect to be common, and don't necessarily want to inline everywhere. Example: Array median:. ```scala; registerIR(""median"", TArray(tnum(""T"")), tv(""T"")) { array =>; val t = -array.typ.asInstanceOf[TArray].elementType; val v = Ref(genUID(), t); val a = Ref(genUID(), TArray(t)); val size = Ref(genUID(), TInt32()); val lastIdx = size - 1; val midIdx = lastIdx.floorDiv(2); def ref(i: IR) = ArrayRef(a, i); def div(a: IR, b: IR): IR = ApplyBinaryPrimOp(BinaryOp.defaultDivideOp(t), a, b). Let(a.name, ArraySort(ArrayFilter(array, v.name, !IsNA(v))),; If(IsNA(a),; NA(t),; Let(size.name,; ArrayLen(a),; If(size.ceq(0),; NA(t),; If(invoke(""%"", size, 2).cne(0),; ref(midIdx), // odd number of non-missing elements; div(ref(midIdx) + ref(midIdx + 1), Cast(2, t))))))); }; ```. Code functions are functions where we have a handwritten code-generator (not IR). Example: Fisher Exact Test:. ```scala. registerCode(""fisher_exact_test"", TInt32(), TInt32(), TInt32(), TInt32(), fetStruct){ case (r, a, b, c, d) =>; val res = r.mb.newLocal[Array[Double]]; val srvb = new StagedRegionValueBuilder(r, fetStruct.physicalType); Code(; res := Code.invokeScalaObject[Int, Int, Int, Int, Array[Double]](statsPackageClass, ""fisherExactTest"", a, b, c, d),; srvb.start(),; srvb.addDouble(res(0)),; srvb.advance(),; srvb.addDouble(res(1)),; srvb.advance(),; srvb.addDouble(res(2)),; srvb.advance(),; srvb.addDouble(res(3)),; srvb.advance(),; srvb.offset; ); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6540#issuecomment-508089671
https://github.com/hail-is/hail/issues/6545#issuecomment-509396760:447,Testability,test,test,447,"This is likely due to a variety of k8s garbage collection facilities, in particular; ```; --terminated-pod-gc-threshold int32 Default: 12500; Number of terminated pods that can exist before the terminated pod garbage collector starts deleting terminated pods. If <= 0, the terminated pod garbage collector is disabled.; ```; https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/. Is a threshold hit by our 30k test but not Konrad's 3k test. We're addressing this with the unipod strategy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545#issuecomment-509396760
https://github.com/hail-is/hail/issues/6545#issuecomment-509396760:472,Testability,test,test,472,"This is likely due to a variety of k8s garbage collection facilities, in particular; ```; --terminated-pod-gc-threshold int32 Default: 12500; Number of terminated pods that can exist before the terminated pod garbage collector starts deleting terminated pods. If <= 0, the terminated pod garbage collector is disabled.; ```; https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/. Is a threshold hit by our 30k test but not Konrad's 3k test. We're addressing this with the unipod strategy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6545#issuecomment-509396760
https://github.com/hail-is/hail/issues/6546#issuecomment-509397240:71,Testability,test,test,71,We also saw this issue when the Grafana pod got restarted during a 30k test. This appears due to k8s being overwhelmed by the number of requests it is receiving. We saw request rates of 450 to 600 requests per second for almost an hour.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6546#issuecomment-509397240
https://github.com/hail-is/hail/issues/6548#issuecomment-508150362:29,Performance,bottleneck,bottleneck,29,"It appears jinja2 is not the bottleneck:; ```; # seq 0 30000 | jq '{""batch"": {""jobs"": [inputs | {""job_id"": ., ""state"": ""Running"", ""exit_code"": null, ""duration"": null, ""batch_id"": 1}], ""id"": 1}}' > batches; # time j2 -f json ../batch/batch/templates/batch.html batches > foo.html && open foo.html; j2 -f json ../batch/batch/templates/batch.html batches > foo.html 0.30s user 0.06s system 95% cpu 0.374 total; ```. The website does take about ~2 seconds to fully load though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6548#issuecomment-508150362
https://github.com/hail-is/hail/issues/6548#issuecomment-508150362:461,Performance,load,load,461,"It appears jinja2 is not the bottleneck:; ```; # seq 0 30000 | jq '{""batch"": {""jobs"": [inputs | {""job_id"": ., ""state"": ""Running"", ""exit_code"": null, ""duration"": null, ""batch_id"": 1}], ""id"": 1}}' > batches; # time j2 -f json ../batch/batch/templates/batch.html batches > foo.html && open foo.html; j2 -f json ../batch/batch/templates/batch.html batches > foo.html 0.30s user 0.06s system 95% cpu 0.374 total; ```. The website does take about ~2 seconds to fully load though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6548#issuecomment-508150362
https://github.com/hail-is/hail/pull/6552#issuecomment-507804081:31,Deployability,release,release,31,"They weren't documented in any release yet:; https://hail.is/docs/0.2/api.html#top-level-functions. I'm OK removing them, since they won't really break pipelines, only citations. And am happy to be bothered by people asking questions about that!. If you feel strongly, let's add the code aliases back but not the documentation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6552#issuecomment-507804081
https://github.com/hail-is/hail/pull/6552#issuecomment-507804081:152,Deployability,pipeline,pipelines,152,"They weren't documented in any release yet:; https://hail.is/docs/0.2/api.html#top-level-functions. I'm OK removing them, since they won't really break pipelines, only citations. And am happy to be bothered by people asking questions about that!. If you feel strongly, let's add the code aliases back but not the documentation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6552#issuecomment-507804081
https://github.com/hail-is/hail/issues/6556#issuecomment-509397642:33,Deployability,update,updated,33,The cancel endpoint must also be updated.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6556#issuecomment-509397642
https://github.com/hail-is/hail/pull/6557#issuecomment-508094304:22,Testability,test,tests,22,You have some failing tests right now. I'll review once you fix?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6557#issuecomment-508094304
https://github.com/hail-is/hail/pull/6557#issuecomment-513264991:16,Testability,test,test,16,Looks like some test are failing for real though.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6557#issuecomment-513264991
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:576,Availability,avail,available,576,"Finally got a lead on Christina's bug:; ```; # hailctl dataproc submit dk foo.py; Submitting to cluster 'dk'...; gcloud command:; gcloud dataproc jobs submit pyspark foo.py \; --cluster=dk \; --files= \; --py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip \; --properties=; Job [66c1d088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:968,Availability,error,error,968,"Finally got a lead on Christina's bug:; ```; # hailctl dataproc submit dk foo.py; Submitting to cluster 'dk'...; gcloud command:; gcloud dataproc jobs submit pyspark foo.py \; --cluster=dk \; --files= \; --py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip \; --properties=; Job [66c1d088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1195,Availability,error,error,1195,"qxswccs180000gq/T/pyscripts_yg_wzlu0.zip \; --properties=; Job [66c1d088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/pyt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1310,Availability,ERROR,ERROR,1310,"088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1406,Availability,error,error,1406,"d Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1454,Availability,failure,failure,1454,"d Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1475,Availability,avail,available,1475,"kUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'foo.py', '--cluster=dk', '--files=', '--py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip', '--properties=']' ret",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:982,Energy Efficiency,allocate,allocate,982,"Finally got a lead on Christina's bug:; ```; # hailctl dataproc submit dk foo.py; Submitting to cluster 'dk'...; gcloud command:; gcloud dataproc jobs submit pyspark foo.py \; --cluster=dk \; --files= \; --py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip \; --properties=; Job [66c1d088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:733,Testability,LOG,LOGGING,733,"Finally got a lead on Christina's bug:; ```; # hailctl dataproc submit dk foo.py; Submitting to cluster 'dk'...; gcloud command:; gcloud dataproc jobs submit pyspark foo.py \; --cluster=dk \; --files= \; --py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip \; --properties=; Job [66c1d088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:830,Testability,log,log,830,"Finally got a lead on Christina's bug:; ```; # hailctl dataproc submit dk foo.py; Submitting to cluster 'dk'...; gcloud command:; gcloud dataproc jobs submit pyspark foo.py \; --cluster=dk \; --files= \; --py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip \; --properties=; Job [66c1d088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1305,Testability,log,log,1305,"088108948b2b76bb607f61d7b3f] submitted.; Waiting for job output...; Initializing Spark and Hail with default parameters...; using hail jar at /opt/conda/default/lib/python3.6/site-packages/hail/hail-all-spark.jar; Running on Apache Spark version 2.4.3; SparkUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6565#issuecomment-508289815:1466,Testability,log,logs,1466,"kUI available at http://dk-m.c.broad-ctsa.internal:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.16-277ccc7aec45; LOGGING: writing to /tmp/66c1d088108948b2b76bb607f61d7b3f/hail-20190703-2330-0.2.16-277ccc7aec45.log; yo dawg. [Stage 0:> (0 + 1) / 1]OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f2e73b00000, 1035468800, 0) failed; error='Cannot allocate memory' (errno=12); #; # There is insufficient memory for the Java Runtime Environment to continue.; # Native memory allocation (mmap) failed to map 1035468800 bytes for committing reserved memory.; # An error report file with more information is saved as:; # /tmp/66c1d088108948b2b76bb607f61d7b3f/hs_err_pid10896.log; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [66c1d088108948b2b76bb607f61d7b3f] failed with error:; Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found in 'gs://dataproc-7f9e9d5e-03bd-4e95-bea1-fe0321239b35-us/google-cloud-dataproc-metainfo/f03fbc39-c07f-4e3e-8f21-47ffa986058e/jobs/66c1d088108948b2b76bb607f61d7b3f/driveroutput'.; Traceback (most recent call last):; File ""/usr/local/bin/hailctl"", line 10, in <module>; sys.exit(main()); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/__main__.py"", line 91, in main; cli.main(args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/cli.py"", line 99, in main; jmp[args.module].main(args, pass_through_args); File ""/usr/local/lib/python3.7/site-packages/hailtop/hailctl/dataproc/submit.py"", line 72, in main; check_call(cmd); File ""/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py"", line 347, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '['gcloud', 'dataproc', 'jobs', 'submit', 'pyspark', 'foo.py', '--cluster=dk', '--files=', '--py-files=/var/folders/cq/p_l4jm3x72j7wkxqxswccs180000gq/T/pyscripts_yg_wzlu0.zip', '--properties=']' ret",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6565#issuecomment-508289815
https://github.com/hail-is/hail/issues/6566#issuecomment-508267674:479,Performance,race condition,race condition,479,"- starting batch; (nginx timed out, but we got 30k submitted); Should multiplex client sending requests to the server. - close batch should start running all pods, don't create pods before batch has been closed. - wait should not list jobs (separate end point?). - add log statement if we delete a pod. - figure out discrepancy between kubernetes and batch on how many pods have been completed / created. - why were logs accessed twice? Both batch or is one of those fluentd?. - race condition still between accessing the log and deleting the pod?. - See whether our containers are being garbage collected: ; https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/; https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674
https://github.com/hail-is/hail/issues/6566#issuecomment-508267674:421,Security,access,accessed,421,"- starting batch; (nginx timed out, but we got 30k submitted); Should multiplex client sending requests to the server. - close batch should start running all pods, don't create pods before batch has been closed. - wait should not list jobs (separate end point?). - add log statement if we delete a pod. - figure out discrepancy between kubernetes and batch on how many pods have been completed / created. - why were logs accessed twice? Both batch or is one of those fluentd?. - race condition still between accessing the log and deleting the pod?. - See whether our containers are being garbage collected: ; https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/; https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674
https://github.com/hail-is/hail/issues/6566#issuecomment-508267674:508,Security,access,accessing,508,"- starting batch; (nginx timed out, but we got 30k submitted); Should multiplex client sending requests to the server. - close batch should start running all pods, don't create pods before batch has been closed. - wait should not list jobs (separate end point?). - add log statement if we delete a pod. - figure out discrepancy between kubernetes and batch on how many pods have been completed / created. - why were logs accessed twice? Both batch or is one of those fluentd?. - race condition still between accessing the log and deleting the pod?. - See whether our containers are being garbage collected: ; https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/; https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674
https://github.com/hail-is/hail/issues/6566#issuecomment-508267674:269,Testability,log,log,269,"- starting batch; (nginx timed out, but we got 30k submitted); Should multiplex client sending requests to the server. - close batch should start running all pods, don't create pods before batch has been closed. - wait should not list jobs (separate end point?). - add log statement if we delete a pod. - figure out discrepancy between kubernetes and batch on how many pods have been completed / created. - why were logs accessed twice? Both batch or is one of those fluentd?. - race condition still between accessing the log and deleting the pod?. - See whether our containers are being garbage collected: ; https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/; https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674
https://github.com/hail-is/hail/issues/6566#issuecomment-508267674:416,Testability,log,logs,416,"- starting batch; (nginx timed out, but we got 30k submitted); Should multiplex client sending requests to the server. - close batch should start running all pods, don't create pods before batch has been closed. - wait should not list jobs (separate end point?). - add log statement if we delete a pod. - figure out discrepancy between kubernetes and batch on how many pods have been completed / created. - why were logs accessed twice? Both batch or is one of those fluentd?. - race condition still between accessing the log and deleting the pod?. - See whether our containers are being garbage collected: ; https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/; https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674
https://github.com/hail-is/hail/issues/6566#issuecomment-508267674:522,Testability,log,log,522,"- starting batch; (nginx timed out, but we got 30k submitted); Should multiplex client sending requests to the server. - close batch should start running all pods, don't create pods before batch has been closed. - wait should not list jobs (separate end point?). - add log statement if we delete a pod. - figure out discrepancy between kubernetes and batch on how many pods have been completed / created. - why were logs accessed twice? Both batch or is one of those fluentd?. - race condition still between accessing the log and deleting the pod?. - See whether our containers are being garbage collected: ; https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/; https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6566#issuecomment-508267674
https://github.com/hail-is/hail/issues/6567#issuecomment-509626765:84,Deployability,deploy,deployed,84,"This was fixed in #6478, which merged a week ago, but I guess the docs haven't been deployed since then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509626765
https://github.com/hail-is/hail/issues/6567#issuecomment-509642293:65,Deployability,deploy,deploy,65,@patrick-schultz sounds like an opportunity to learn how to do a deploy from Tim 😉,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509642293
https://github.com/hail-is/hail/issues/6567#issuecomment-509642293:47,Usability,learn,learn,47,@patrick-schultz sounds like an opportunity to learn how to do a deploy from Tim 😉,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509642293
https://github.com/hail-is/hail/issues/6567#issuecomment-509642588:18,Deployability,release,release,18,we need to make a release first,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509642588
https://github.com/hail-is/hail/issues/6567#issuecomment-509642724:51,Deployability,deploy,deploying,51,"(I mean I don't think we should be in the habit of deploying docs outside the versioned releases, except in special circumstances)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509642724
https://github.com/hail-is/hail/issues/6567#issuecomment-509642724:88,Deployability,release,releases,88,"(I mean I don't think we should be in the habit of deploying docs outside the versioned releases, except in special circumstances)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509642724
https://github.com/hail-is/hail/issues/6567#issuecomment-509643864:32,Deployability,deploy,deploy,32,"Ah yes, this is what I meant by deploy. Patrick can learn how to do a release / pip deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509643864
https://github.com/hail-is/hail/issues/6567#issuecomment-509643864:70,Deployability,release,release,70,"Ah yes, this is what I meant by deploy. Patrick can learn how to do a release / pip deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509643864
https://github.com/hail-is/hail/issues/6567#issuecomment-509643864:84,Deployability,deploy,deploy,84,"Ah yes, this is what I meant by deploy. Patrick can learn how to do a release / pip deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509643864
https://github.com/hail-is/hail/issues/6567#issuecomment-509643864:52,Usability,learn,learn,52,"Ah yes, this is what I meant by deploy. Patrick can learn how to do a release / pip deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509643864
https://github.com/hail-is/hail/issues/6567#issuecomment-509644515:35,Deployability,deploy,deploy,35,"Sure, I'd like to know how the pip deploy works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509644515
https://github.com/hail-is/hail/pull/6569#issuecomment-510158677:32,Deployability,pipeline,pipeline-ci-roadmap,32,https://dev.hail.is/t/rfc-batch-pipeline-ci-roadmap/136/5,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6569#issuecomment-510158677
https://github.com/hail-is/hail/pull/6580#issuecomment-509490837:85,Deployability,pipeline,pipeline,85,"I swapped out the read/write for a serialize/deserialize thing; got some times for a pipeline that was just read -> densify -> write on the same matrix table and got the following times:. old: [19.249044491007226, 20.367718594003236, 18.515784285002155, 18.116745114995865, 18.104985954996664]; new: [15.649361574003706, 16.205813756998396, 17.644299295003293, 16.90835837100167, 17.12396009400254]. I'd be interested in trying this on a wider matrix table, since I think that's when we start running into issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6580#issuecomment-509490837
https://github.com/hail-is/hail/pull/6581#issuecomment-509610153:34,Modifiability,rewrite,rewrite,34,"> I'm curious if there's a way to rewrite this to be more clear about the state machine?. Yes, this is quite confusing as written, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6581#issuecomment-509610153
https://github.com/hail-is/hail/pull/6581#issuecomment-509610153:58,Usability,clear,clear,58,"> I'm curious if there's a way to rewrite this to be more clear about the state machine?. Yes, this is quite confusing as written, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6581#issuecomment-509610153
https://github.com/hail-is/hail/issues/6582#issuecomment-509627134:182,Availability,error,error,182,"> Did you ssh to CI, start a python session, start a batch client, and delete it in that manner?. Yes. And the CI status (https://ci.hail.is/watched_branches/0/pr/6561) continued to error out with 500s (caused by batch lookup 404s in the logs) even after a heal loop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6582#issuecomment-509627134
https://github.com/hail-is/hail/issues/6582#issuecomment-509627134:238,Testability,log,logs,238,"> Did you ssh to CI, start a python session, start a batch client, and delete it in that manner?. Yes. And the CI status (https://ci.hail.is/watched_branches/0/pr/6561) continued to error out with 500s (caused by batch lookup 404s in the logs) even after a heal loop.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6582#issuecomment-509627134
https://github.com/hail-is/hail/issues/6582#issuecomment-509637189:174,Availability,recover,recover,174,"The UI 500'ed, right? I created https://github.com/hail-is/hail/issues/6587 and gave an example from this morning at 5:52. The only explanation for why the heal loop did not recover is that the batch was already completed. If the batch had already completed (whether in success or in failure), then it won't re-run it. Do you recall if the batch had completed? If a batch completes, bumping is the only way to get another batch to run.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6582#issuecomment-509637189
https://github.com/hail-is/hail/issues/6582#issuecomment-509637189:284,Availability,failure,failure,284,"The UI 500'ed, right? I created https://github.com/hail-is/hail/issues/6587 and gave an example from this morning at 5:52. The only explanation for why the heal loop did not recover is that the batch was already completed. If the batch had already completed (whether in success or in failure), then it won't re-run it. Do you recall if the batch had completed? If a batch completes, bumping is the only way to get another batch to run.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6582#issuecomment-509637189
https://github.com/hail-is/hail/issues/6582#issuecomment-509637189:174,Safety,recover,recover,174,"The UI 500'ed, right? I created https://github.com/hail-is/hail/issues/6587 and gave an example from this morning at 5:52. The only explanation for why the heal loop did not recover is that the batch was already completed. If the batch had already completed (whether in success or in failure), then it won't re-run it. Do you recall if the batch had completed? If a batch completes, bumping is the only way to get another batch to run.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6582#issuecomment-509637189
https://github.com/hail-is/hail/issues/6582#issuecomment-509638201:84,Testability,log,logic,84,"I thought there might be an easier way than changing the database schema / batch/ci logic to have ignored batches, but maybe not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6582#issuecomment-509638201
https://github.com/hail-is/hail/issues/6587#issuecomment-509634461:25,Energy Efficiency,monitor,monitoring,25,"https://internal.hail.is/monitoring/kibana/app/infra#/logs?_g=()&flyoutOptions=(flyoutId:Cs8K1GsBylIYXqvkSfbm,flyoutVisibility:hidden,surroundingLogsId:!n)&logFilter=(expression:'kubernetes.labels.app%20:ci',kind:kuery)&logPosition=(position:(tiebreaker:23440993,time:1562665935824),streamLive:!f)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6587#issuecomment-509634461
https://github.com/hail-is/hail/issues/6587#issuecomment-509634461:54,Testability,log,logs,54,"https://internal.hail.is/monitoring/kibana/app/infra#/logs?_g=()&flyoutOptions=(flyoutId:Cs8K1GsBylIYXqvkSfbm,flyoutVisibility:hidden,surroundingLogsId:!n)&logFilter=(expression:'kubernetes.labels.app%20:ci',kind:kuery)&logPosition=(position:(tiebreaker:23440993,time:1562665935824),streamLive:!f)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6587#issuecomment-509634461
https://github.com/hail-is/hail/issues/6587#issuecomment-509634461:156,Testability,log,logFilter,156,"https://internal.hail.is/monitoring/kibana/app/infra#/logs?_g=()&flyoutOptions=(flyoutId:Cs8K1GsBylIYXqvkSfbm,flyoutVisibility:hidden,surroundingLogsId:!n)&logFilter=(expression:'kubernetes.labels.app%20:ci',kind:kuery)&logPosition=(position:(tiebreaker:23440993,time:1562665935824),streamLive:!f)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6587#issuecomment-509634461
https://github.com/hail-is/hail/issues/6587#issuecomment-509634461:220,Testability,log,logPosition,220,"https://internal.hail.is/monitoring/kibana/app/infra#/logs?_g=()&flyoutOptions=(flyoutId:Cs8K1GsBylIYXqvkSfbm,flyoutVisibility:hidden,surroundingLogsId:!n)&logFilter=(expression:'kubernetes.labels.app%20:ci',kind:kuery)&logPosition=(position:(tiebreaker:23440993,time:1562665935824),streamLive:!f)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6587#issuecomment-509634461
https://github.com/hail-is/hail/pull/6588#issuecomment-509640521:67,Availability,down,down,67,Don't know why the diff looks so weird -- I just dropped benchmark down one level into `benchmark/run`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6588#issuecomment-509640521
https://github.com/hail-is/hail/pull/6588#issuecomment-509640521:57,Testability,benchmark,benchmark,57,Don't know why the diff looks so weird -- I just dropped benchmark down one level into `benchmark/run`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6588#issuecomment-509640521
https://github.com/hail-is/hail/pull/6588#issuecomment-509640521:88,Testability,benchmark,benchmark,88,Don't know why the diff looks so weird -- I just dropped benchmark down one level into `benchmark/run`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6588#issuecomment-509640521
https://github.com/hail-is/hail/pull/6594#issuecomment-512957123:700,Integrability,Wrap,WrappedArray,700,"Tim, would you have any recommendations?. ```sh; Gradle suite > Gradle test > is.hail.expr.ir.IRSuite.regressionTestUnifyBug FAILED; scala.MatchError: +Void (of class is.hail.expr.types.physical.PVoid$); at is.hail.expr.types.physical.PType.setRequired(PType.scala:206); at is.hail.expr.ir.InferPType$.apply(InferPType.scala:209); at is.hail.expr.ir.IR$class.inferSetPType(IR.scala:33); at is.hail.expr.ir.ArrayMap.inferSetPType(IR.scala:232); at is.hail.expr.ir.InferPType$$anonfun$apply$8.apply(InferPType.scala:359); at is.hail.expr.ir.InferPType$$anonfun$apply$8.apply(InferPType.scala:358); ```. This occurs because the IR is the following:. ```sh; ; //a; Literal(array<interval<locus<GRCh37>>>,WrappedArray([20:10277621-20:11898992))); // name; __iruid_11; //body; ApplySpecial(Interval,ArrayBuffer(MakeStruct(ArrayBuffer((locus,ApplySpecial(start,ArrayBuffer(Ref(__iruid_11,interval<locus<GRCh37>>)))))), MakeStruct(ArrayBuffer((locus,ApplySpecial(end,ArrayBuffer(Ref(__iruid_11,interval<locus<GRCh37>>)))))), True(), False())); ```. So I set this IR in the match to return PVoid(). However, in ArrayMap there is a requiredeness setter (which is inspired/copied from InferType's corresponding match:. ```scala; coerce[PStreamable](a.pType2).copyStreamable(body.pType2.setRequired(false)); ```. Which causes a nonsensical operation on PVoid, which is required: true (override final val). As an aside, ApplySpecial seems to allow operations on non-identical types, which would require algebraic types, rather than a singular: any time the types of its`Seq[IR]`'s are different. I have similar issues with other array operations, which want nodes that we currently don't support, including aggregator nodes. Note that I am only calling inferSetPType in Compile (immediately after optimization pass).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-512957123
https://github.com/hail-is/hail/pull/6594#issuecomment-512957123:1784,Performance,optimiz,optimization,1784,"Tim, would you have any recommendations?. ```sh; Gradle suite > Gradle test > is.hail.expr.ir.IRSuite.regressionTestUnifyBug FAILED; scala.MatchError: +Void (of class is.hail.expr.types.physical.PVoid$); at is.hail.expr.types.physical.PType.setRequired(PType.scala:206); at is.hail.expr.ir.InferPType$.apply(InferPType.scala:209); at is.hail.expr.ir.IR$class.inferSetPType(IR.scala:33); at is.hail.expr.ir.ArrayMap.inferSetPType(IR.scala:232); at is.hail.expr.ir.InferPType$$anonfun$apply$8.apply(InferPType.scala:359); at is.hail.expr.ir.InferPType$$anonfun$apply$8.apply(InferPType.scala:358); ```. This occurs because the IR is the following:. ```sh; ; //a; Literal(array<interval<locus<GRCh37>>>,WrappedArray([20:10277621-20:11898992))); // name; __iruid_11; //body; ApplySpecial(Interval,ArrayBuffer(MakeStruct(ArrayBuffer((locus,ApplySpecial(start,ArrayBuffer(Ref(__iruid_11,interval<locus<GRCh37>>)))))), MakeStruct(ArrayBuffer((locus,ApplySpecial(end,ArrayBuffer(Ref(__iruid_11,interval<locus<GRCh37>>)))))), True(), False())); ```. So I set this IR in the match to return PVoid(). However, in ArrayMap there is a requiredeness setter (which is inspired/copied from InferType's corresponding match:. ```scala; coerce[PStreamable](a.pType2).copyStreamable(body.pType2.setRequired(false)); ```. Which causes a nonsensical operation on PVoid, which is required: true (override final val). As an aside, ApplySpecial seems to allow operations on non-identical types, which would require algebraic types, rather than a singular: any time the types of its`Seq[IR]`'s are different. I have similar issues with other array operations, which want nodes that we currently don't support, including aggregator nodes. Note that I am only calling inferSetPType in Compile (immediately after optimization pass).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-512957123
https://github.com/hail-is/hail/pull/6594#issuecomment-512957123:71,Testability,test,test,71,"Tim, would you have any recommendations?. ```sh; Gradle suite > Gradle test > is.hail.expr.ir.IRSuite.regressionTestUnifyBug FAILED; scala.MatchError: +Void (of class is.hail.expr.types.physical.PVoid$); at is.hail.expr.types.physical.PType.setRequired(PType.scala:206); at is.hail.expr.ir.InferPType$.apply(InferPType.scala:209); at is.hail.expr.ir.IR$class.inferSetPType(IR.scala:33); at is.hail.expr.ir.ArrayMap.inferSetPType(IR.scala:232); at is.hail.expr.ir.InferPType$$anonfun$apply$8.apply(InferPType.scala:359); at is.hail.expr.ir.InferPType$$anonfun$apply$8.apply(InferPType.scala:358); ```. This occurs because the IR is the following:. ```sh; ; //a; Literal(array<interval<locus<GRCh37>>>,WrappedArray([20:10277621-20:11898992))); // name; __iruid_11; //body; ApplySpecial(Interval,ArrayBuffer(MakeStruct(ArrayBuffer((locus,ApplySpecial(start,ArrayBuffer(Ref(__iruid_11,interval<locus<GRCh37>>)))))), MakeStruct(ArrayBuffer((locus,ApplySpecial(end,ArrayBuffer(Ref(__iruid_11,interval<locus<GRCh37>>)))))), True(), False())); ```. So I set this IR in the match to return PVoid(). However, in ArrayMap there is a requiredeness setter (which is inspired/copied from InferType's corresponding match:. ```scala; coerce[PStreamable](a.pType2).copyStreamable(body.pType2.setRequired(false)); ```. Which causes a nonsensical operation on PVoid, which is required: true (override final val). As an aside, ApplySpecial seems to allow operations on non-identical types, which would require algebraic types, rather than a singular: any time the types of its`Seq[IR]`'s are different. I have similar issues with other array operations, which want nodes that we currently don't support, including aggregator nodes. Note that I am only calling inferSetPType in Compile (immediately after optimization pass).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-512957123
https://github.com/hail-is/hail/pull/6594#issuecomment-513005812:822,Integrability,message,message,822,"sorry my internet was bad and wasn't reloading the bottom of the page for a while. Can respond now:. > As an aside, ApplySpecial seems to allow operations on non-identical types, which would require algebraic types, rather than a singular: any time the types of itsSeq[IR]'s are different. > I have similar issues with other array operations, which want nodes that we currently don't support, including aggregator nodes. Note that I am only calling inferSetPType in Compile (immediately after optimization pass). The ptype inference for apply methods is handled by some stuff I wrote recently. IRFunction now has a `returnPType` method that takes arg ptypes. > PVoid. Void isn't a catch-all type like Nothing in Scala - it's a specific I-don't-return-anything type used by IRs like TableWrite. The exception in your above message is coming from the Apply node being inferred as a `PVoid` by your `case _ => PVoid` code. Writing the rule for the apply node should fix that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513005812
https://github.com/hail-is/hail/pull/6594#issuecomment-513005812:493,Performance,optimiz,optimization,493,"sorry my internet was bad and wasn't reloading the bottom of the page for a while. Can respond now:. > As an aside, ApplySpecial seems to allow operations on non-identical types, which would require algebraic types, rather than a singular: any time the types of itsSeq[IR]'s are different. > I have similar issues with other array operations, which want nodes that we currently don't support, including aggregator nodes. Note that I am only calling inferSetPType in Compile (immediately after optimization pass). The ptype inference for apply methods is handled by some stuff I wrote recently. IRFunction now has a `returnPType` method that takes arg ptypes. > PVoid. Void isn't a catch-all type like Nothing in Scala - it's a specific I-don't-return-anything type used by IRs like TableWrite. The exception in your above message is coming from the Apply node being inferred as a `PVoid` by your `case _ => PVoid` code. Writing the rule for the apply node should fix that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513005812
https://github.com/hail-is/hail/pull/6594#issuecomment-513007861:72,Integrability,message,message,72,"I figured, noticed the double post :). ```; The exception in your above message is coming from the Apply node being inferred as a PVoid by your case _ => PVoid code. Writing the rule for the apply node should fix that.; ```. Right. It's just that I tried to write the rule, and quickly ran across the fact that Seq[IR] would be inferred such that the first IR had a different type from the 2nd or Nth. This is what I had written:. ```scala; case ApplySpecial(name, irs) => {; val it = irs.iterator; val head = it.next(); head.inferSetPType(env). while(it.hasNext) {; val value = it.next(). value.inferSetPType(env); assert(value.pType2 == head.pType2); }. head.pType2; }; ```. With the result in one case that `head.pType2` was bool, `value.pType2` was something else. Without a type union, it wasn't clear to me what to return. One possibility was that I shouldn't handle this node, so I started with that possibility, which I know understand isn't right. The other was that the implementation was wrong, and my first guess there is that one of the IRs dictates the type (say the first), the 2nd is that there is a simple precedence rule, the 3rd is that the type inference procedure has some branching logic over the collection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513007861
https://github.com/hail-is/hail/pull/6594#issuecomment-513007861:616,Testability,assert,assert,616,"I figured, noticed the double post :). ```; The exception in your above message is coming from the Apply node being inferred as a PVoid by your case _ => PVoid code. Writing the rule for the apply node should fix that.; ```. Right. It's just that I tried to write the rule, and quickly ran across the fact that Seq[IR] would be inferred such that the first IR had a different type from the 2nd or Nth. This is what I had written:. ```scala; case ApplySpecial(name, irs) => {; val it = irs.iterator; val head = it.next(); head.inferSetPType(env). while(it.hasNext) {; val value = it.next(). value.inferSetPType(env); assert(value.pType2 == head.pType2); }. head.pType2; }; ```. With the result in one case that `head.pType2` was bool, `value.pType2` was something else. Without a type union, it wasn't clear to me what to return. One possibility was that I shouldn't handle this node, so I started with that possibility, which I know understand isn't right. The other was that the implementation was wrong, and my first guess there is that one of the IRs dictates the type (say the first), the 2nd is that there is a simple precedence rule, the 3rd is that the type inference procedure has some branching logic over the collection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513007861
https://github.com/hail-is/hail/pull/6594#issuecomment-513007861:1204,Testability,log,logic,1204,"I figured, noticed the double post :). ```; The exception in your above message is coming from the Apply node being inferred as a PVoid by your case _ => PVoid code. Writing the rule for the apply node should fix that.; ```. Right. It's just that I tried to write the rule, and quickly ran across the fact that Seq[IR] would be inferred such that the first IR had a different type from the 2nd or Nth. This is what I had written:. ```scala; case ApplySpecial(name, irs) => {; val it = irs.iterator; val head = it.next(); head.inferSetPType(env). while(it.hasNext) {; val value = it.next(). value.inferSetPType(env); assert(value.pType2 == head.pType2); }. head.pType2; }; ```. With the result in one case that `head.pType2` was bool, `value.pType2` was something else. Without a type union, it wasn't clear to me what to return. One possibility was that I shouldn't handle this node, so I started with that possibility, which I know understand isn't right. The other was that the implementation was wrong, and my first guess there is that one of the IRs dictates the type (say the first), the 2nd is that there is a simple precedence rule, the 3rd is that the type inference procedure has some branching logic over the collection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513007861
https://github.com/hail-is/hail/pull/6594#issuecomment-513007861:801,Usability,clear,clear,801,"I figured, noticed the double post :). ```; The exception in your above message is coming from the Apply node being inferred as a PVoid by your case _ => PVoid code. Writing the rule for the apply node should fix that.; ```. Right. It's just that I tried to write the rule, and quickly ran across the fact that Seq[IR] would be inferred such that the first IR had a different type from the 2nd or Nth. This is what I had written:. ```scala; case ApplySpecial(name, irs) => {; val it = irs.iterator; val head = it.next(); head.inferSetPType(env). while(it.hasNext) {; val value = it.next(). value.inferSetPType(env); assert(value.pType2 == head.pType2); }. head.pType2; }; ```. With the result in one case that `head.pType2` was bool, `value.pType2` was something else. Without a type union, it wasn't clear to me what to return. One possibility was that I shouldn't handle this node, so I started with that possibility, which I know understand isn't right. The other was that the implementation was wrong, and my first guess there is that one of the IRs dictates the type (say the first), the 2nd is that there is a simple precedence rule, the 3rd is that the type inference procedure has some branching logic over the collection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513007861
https://github.com/hail-is/hail/pull/6594#issuecomment-513007861:1116,Usability,simpl,simple,1116,"I figured, noticed the double post :). ```; The exception in your above message is coming from the Apply node being inferred as a PVoid by your case _ => PVoid code. Writing the rule for the apply node should fix that.; ```. Right. It's just that I tried to write the rule, and quickly ran across the fact that Seq[IR] would be inferred such that the first IR had a different type from the 2nd or Nth. This is what I had written:. ```scala; case ApplySpecial(name, irs) => {; val it = irs.iterator; val head = it.next(); head.inferSetPType(env). while(it.hasNext) {; val value = it.next(). value.inferSetPType(env); assert(value.pType2 == head.pType2); }. head.pType2; }; ```. With the result in one case that `head.pType2` was bool, `value.pType2` was something else. Without a type union, it wasn't clear to me what to return. One possibility was that I shouldn't handle this node, so I started with that possibility, which I know understand isn't right. The other was that the implementation was wrong, and my first guess there is that one of the IRs dictates the type (say the first), the 2nd is that there is a simple precedence rule, the 3rd is that the type inference procedure has some branching logic over the collection.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-513007861
https://github.com/hail-is/hail/pull/6594#issuecomment-515061792:37,Integrability,interface,interfaces,37,"Tim, I've added one of the suggested interfaces. To use `protected var _pType2` instead I believe we need to have InferPType extend `IR` inside of IR.scala, e.g `object InferPType extend IR`, by requirement of sealed traits. Anything else you want? Happy to add additional tests, or to move on to the next pType sub-project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515061792
https://github.com/hail-is/hail/pull/6594#issuecomment-515061792:125,Modifiability,extend,extend,125,"Tim, I've added one of the suggested interfaces. To use `protected var _pType2` instead I believe we need to have InferPType extend `IR` inside of IR.scala, e.g `object InferPType extend IR`, by requirement of sealed traits. Anything else you want? Happy to add additional tests, or to move on to the next pType sub-project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515061792
https://github.com/hail-is/hail/pull/6594#issuecomment-515061792:180,Modifiability,extend,extend,180,"Tim, I've added one of the suggested interfaces. To use `protected var _pType2` instead I believe we need to have InferPType extend `IR` inside of IR.scala, e.g `object InferPType extend IR`, by requirement of sealed traits. Anything else you want? Happy to add additional tests, or to move on to the next pType sub-project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515061792
https://github.com/hail-is/hail/pull/6594#issuecomment-515061792:273,Testability,test,tests,273,"Tim, I've added one of the suggested interfaces. To use `protected var _pType2` instead I believe we need to have InferPType extend `IR` inside of IR.scala, e.g `object InferPType extend IR`, by requirement of sealed traits. Anything else you want? Happy to add additional tests, or to move on to the next pType sub-project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515061792
https://github.com/hail-is/hail/pull/6594#issuecomment-515062679:76,Modifiability,extend,extend,76,"> To use protected var _pType2 instead I believe we need to have InferPType extend IR inside of IR.scala, e.g object InferPType extend IR, by requirement of sealed traits. You can use `protected[ir] var _pType2: PType`, which would be fine and let you keep the current structure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515062679
https://github.com/hail-is/hail/pull/6594#issuecomment-515062679:128,Modifiability,extend,extend,128,"> To use protected var _pType2 instead I believe we need to have InferPType extend IR inside of IR.scala, e.g object InferPType extend IR, by requirement of sealed traits. You can use `protected[ir] var _pType2: PType`, which would be fine and let you keep the current structure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515062679
https://github.com/hail-is/hail/pull/6594#issuecomment-515071056:40,Modifiability,variab,variables,40,"This seems tricky! I expected protected variables to be only accessible from child classes. Is the reason this works that adding [ir] hoists the variable to be top-level, when used from an object of type IR, thereby making it once again accessible to sub-packages?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515071056
https://github.com/hail-is/hail/pull/6594#issuecomment-515071056:145,Modifiability,variab,variable,145,"This seems tricky! I expected protected variables to be only accessible from child classes. Is the reason this works that adding [ir] hoists the variable to be top-level, when used from an object of type IR, thereby making it once again accessible to sub-packages?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515071056
https://github.com/hail-is/hail/pull/6594#issuecomment-515071056:61,Security,access,accessible,61,"This seems tricky! I expected protected variables to be only accessible from child classes. Is the reason this works that adding [ir] hoists the variable to be top-level, when used from an object of type IR, thereby making it once again accessible to sub-packages?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515071056
https://github.com/hail-is/hail/pull/6594#issuecomment-515071056:237,Security,access,accessible,237,"This seems tricky! I expected protected variables to be only accessible from child classes. Is the reason this works that adding [ir] hoists the variable to be top-level, when used from an object of type IR, thereby making it once again accessible to sub-packages?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515071056
https://github.com/hail-is/hail/pull/6594#issuecomment-515073290:56,Modifiability,variab,variables,56,"I don't know the exact rules about java/scala protected variables -- but `protected[ir]` means anything in `is.hail.expr.ir` should be able to see it. Which is the whole compiler, basically, so it doesn't do a lot. But I think that's preferable to requiring that InferPType needs to be in the IR file, or extend it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515073290
https://github.com/hail-is/hail/pull/6594#issuecomment-515073290:305,Modifiability,extend,extend,305,"I don't know the exact rules about java/scala protected variables -- but `protected[ir]` means anything in `is.hail.expr.ir` should be able to see it. Which is the whole compiler, basically, so it doesn't do a lot. But I think that's preferable to requiring that InferPType needs to be in the IR file, or extend it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6594#issuecomment-515073290
https://github.com/hail-is/hail/pull/6600#issuecomment-510272888:154,Availability,error,error,154,"@danking Thanks for taking this over! I commented out the mark_unscheduled if the sidecar fails for debugging. The sidecar is running, but I'm getting an error because it's trying to run the top level code in batch.py. Either sidecar.py needs to be separate or we need to reconfigure batch.py so it doesn't run that code. ```; Traceback (most recent call last):; File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.6/dist-packages/batch/sidecar.py"", line 14, in <module>; from .batch import REFRESH_INTERVAL_IN_SECONDS, HAIL_POD_NAMESPACE, KUBERNETES_TIMEOUT_IN_SECONDS; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 83, in <module>; db = BatchDatabase.create_synchronous('/batch-user-secret/sql-config.json'); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 23, in create_synchronous; run_synchronous(cls.__init__(db, config_file)); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 15, in run_synchronous; return loop.run_until_complete(coro); File ""uvloop/loop.pyx"", line 1451, in uvloop.loop.Loop.run_until_complete; File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 210, in __init__; await super().__init__(config_file); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 27, in __init__; with open(config_file, 'r') as f:; FileNotFoundError: [Errno 2] No such file or directory: '/batch-user-secret/sql-config.json'; ```. To see the logs `kubectl -n namespace logs pod_name cleanup`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6600#issuecomment-510272888
https://github.com/hail-is/hail/pull/6600#issuecomment-510272888:871,Modifiability,config,config,871,"@danking Thanks for taking this over! I commented out the mark_unscheduled if the sidecar fails for debugging. The sidecar is running, but I'm getting an error because it's trying to run the top level code in batch.py. Either sidecar.py needs to be separate or we need to reconfigure batch.py so it doesn't run that code. ```; Traceback (most recent call last):; File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.6/dist-packages/batch/sidecar.py"", line 14, in <module>; from .batch import REFRESH_INTERVAL_IN_SECONDS, HAIL_POD_NAMESPACE, KUBERNETES_TIMEOUT_IN_SECONDS; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 83, in <module>; db = BatchDatabase.create_synchronous('/batch-user-secret/sql-config.json'); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 23, in create_synchronous; run_synchronous(cls.__init__(db, config_file)); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 15, in run_synchronous; return loop.run_until_complete(coro); File ""uvloop/loop.pyx"", line 1451, in uvloop.loop.Loop.run_until_complete; File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 210, in __init__; await super().__init__(config_file); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 27, in __init__; with open(config_file, 'r') as f:; FileNotFoundError: [Errno 2] No such file or directory: '/batch-user-secret/sql-config.json'; ```. To see the logs `kubectl -n namespace logs pod_name cleanup`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6600#issuecomment-510272888
https://github.com/hail-is/hail/pull/6600#issuecomment-510272888:1565,Modifiability,config,config,1565,"@danking Thanks for taking this over! I commented out the mark_unscheduled if the sidecar fails for debugging. The sidecar is running, but I'm getting an error because it's trying to run the top level code in batch.py. Either sidecar.py needs to be separate or we need to reconfigure batch.py so it doesn't run that code. ```; Traceback (most recent call last):; File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.6/dist-packages/batch/sidecar.py"", line 14, in <module>; from .batch import REFRESH_INTERVAL_IN_SECONDS, HAIL_POD_NAMESPACE, KUBERNETES_TIMEOUT_IN_SECONDS; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 83, in <module>; db = BatchDatabase.create_synchronous('/batch-user-secret/sql-config.json'); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 23, in create_synchronous; run_synchronous(cls.__init__(db, config_file)); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 15, in run_synchronous; return loop.run_until_complete(coro); File ""uvloop/loop.pyx"", line 1451, in uvloop.loop.Loop.run_until_complete; File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 210, in __init__; await super().__init__(config_file); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 27, in __init__; with open(config_file, 'r') as f:; FileNotFoundError: [Errno 2] No such file or directory: '/batch-user-secret/sql-config.json'; ```. To see the logs `kubectl -n namespace logs pod_name cleanup`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6600#issuecomment-510272888
https://github.com/hail-is/hail/pull/6600#issuecomment-510272888:1595,Testability,log,logs,1595,"@danking Thanks for taking this over! I commented out the mark_unscheduled if the sidecar fails for debugging. The sidecar is running, but I'm getting an error because it's trying to run the top level code in batch.py. Either sidecar.py needs to be separate or we need to reconfigure batch.py so it doesn't run that code. ```; Traceback (most recent call last):; File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.6/dist-packages/batch/sidecar.py"", line 14, in <module>; from .batch import REFRESH_INTERVAL_IN_SECONDS, HAIL_POD_NAMESPACE, KUBERNETES_TIMEOUT_IN_SECONDS; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 83, in <module>; db = BatchDatabase.create_synchronous('/batch-user-secret/sql-config.json'); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 23, in create_synchronous; run_synchronous(cls.__init__(db, config_file)); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 15, in run_synchronous; return loop.run_until_complete(coro); File ""uvloop/loop.pyx"", line 1451, in uvloop.loop.Loop.run_until_complete; File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 210, in __init__; await super().__init__(config_file); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 27, in __init__; with open(config_file, 'r') as f:; FileNotFoundError: [Errno 2] No such file or directory: '/batch-user-secret/sql-config.json'; ```. To see the logs `kubectl -n namespace logs pod_name cleanup`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6600#issuecomment-510272888
https://github.com/hail-is/hail/pull/6600#issuecomment-510272888:1622,Testability,log,logs,1622,"@danking Thanks for taking this over! I commented out the mark_unscheduled if the sidecar fails for debugging. The sidecar is running, but I'm getting an error because it's trying to run the top level code in batch.py. Either sidecar.py needs to be separate or we need to reconfigure batch.py so it doesn't run that code. ```; Traceback (most recent call last):; File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.6/dist-packages/batch/sidecar.py"", line 14, in <module>; from .batch import REFRESH_INTERVAL_IN_SECONDS, HAIL_POD_NAMESPACE, KUBERNETES_TIMEOUT_IN_SECONDS; File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 83, in <module>; db = BatchDatabase.create_synchronous('/batch-user-secret/sql-config.json'); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 23, in create_synchronous; run_synchronous(cls.__init__(db, config_file)); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 15, in run_synchronous; return loop.run_until_complete(coro); File ""uvloop/loop.pyx"", line 1451, in uvloop.loop.Loop.run_until_complete; File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 210, in __init__; await super().__init__(config_file); File ""/usr/local/lib/python3.6/dist-packages/batch/database.py"", line 27, in __init__; with open(config_file, 'r') as f:; FileNotFoundError: [Errno 2] No such file or directory: '/batch-user-secret/sql-config.json'; ```. To see the logs `kubectl -n namespace logs pod_name cleanup`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6600#issuecomment-510272888
https://github.com/hail-is/hail/pull/6600#issuecomment-513258164:129,Testability,test,testing,129,"OK! Let's put this in. I'll continue to clean up batch today and make new PRs to do so. In the meantime, I'll also do some scale testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6600#issuecomment-513258164
https://github.com/hail-is/hail/pull/6605#issuecomment-510677139:619,Availability,error,error,619,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139
https://github.com/hail-is/hail/pull/6605#issuecomment-510677139:726,Availability,error,errors,726,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139
https://github.com/hail-is/hail/pull/6605#issuecomment-510677139:826,Availability,failure,failures,826,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139
https://github.com/hail-is/hail/pull/6605#issuecomment-510677139:1133,Safety,safe,safe,1133,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139
https://github.com/hail-is/hail/pull/6605#issuecomment-510677139:760,Testability,test,tests,760,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139
https://github.com/hail-is/hail/pull/6605#issuecomment-510677139:821,Testability,test,test,821,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139
https://github.com/hail-is/hail/pull/6605#issuecomment-510677139:952,Usability,clear,cleared,952,"This change fixes a huge problem caused by these lines of code and context:. https://github.com/hail-is/hail/pull/6605/files#diff-1278c1788239002cc63ccb82cbef8d76L190. The problem is that in our generated code, every literal is decoded *each time any literal is referenced*. This is **extremely** expensive! . In this change, we instead decode the literals once with the function is constructed from the partition index (used with randomness), by adding a new region argument which the literals are decoded into. This region must live as long as the RegionValues returned by any invocation of the function. The primary error mode I might expect is that we use the wrong region to generate the function, causing use-after-free errors. These are well-covered by tests, since I had a few of these bugs and fixed them due to test failures. The region we *shouldn't* be using is `ctx.region`, which refers to `RVDContext.region`, the per-row region that is cleared after each record. `ctx.r` (the global execution context region) and `ctx.freshRegion` (a partition-owned global region, generally named `globalRegion` or `partRegion`) are safe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6605#issuecomment-510677139
https://github.com/hail-is/hail/pull/6612#issuecomment-510549867:204,Testability,test,test,204,Assigned Dan but feel free to look as well @cseed . Admittedly I'm not 100% sure that the CLI code is correct because I just changed it to make a request instead of just calling the function and I cannot test it until the endpoint exists.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6612#issuecomment-510549867
https://github.com/hail-is/hail/issues/6617#issuecomment-510673350:76,Testability,log,log,76,"However, somehow the job did transition to `Running` even though there's no log of it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6617#issuecomment-510673350
https://github.com/hail-is/hail/pull/6618#issuecomment-510731707:205,Availability,down,down,205,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707
https://github.com/hail-is/hail/pull/6618#issuecomment-510731707:133,Deployability,pipeline,pipeline,133,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707
https://github.com/hail-is/hail/pull/6618#issuecomment-510731707:502,Deployability,update,update,502,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707
https://github.com/hail-is/hail/pull/6618#issuecomment-510731707:304,Testability,log,log,304,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707
https://github.com/hail-is/hail/pull/6618#issuecomment-510731707:604,Usability,clear,clearly,604,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707
https://github.com/hail-is/hail/pull/6619#issuecomment-515501206:361,Deployability,deploy,deployment,361,"> A couple notes. I'm not sure what this should ultimately look like. I think I want `hailctl create user`, which probably creates a batch that does all the creation? That avoids setting up tunnels to the sql server, etc. Sure, if that's the way we want this to work, I will modify to do that. Seems more elegant than having tunnels, which predates our current deployment solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6619#issuecomment-515501206
https://github.com/hail-is/hail/pull/6619#issuecomment-515501206:172,Safety,avoid,avoids,172,"> A couple notes. I'm not sure what this should ultimately look like. I think I want `hailctl create user`, which probably creates a batch that does all the creation? That avoids setting up tunnels to the sql server, etc. Sure, if that's the way we want this to work, I will modify to do that. Seems more elegant than having tunnels, which predates our current deployment solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6619#issuecomment-515501206
https://github.com/hail-is/hail/pull/6628#issuecomment-511217789:148,Testability,test,tests,148,"`PackEncoder` was in the `io` package. Which was imported using `import is.hail.io._` where it was used. I placed it in `is.hail.nativecode` in the tests, making it visible to `is.hail.nativecode` implicitly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6628#issuecomment-511217789
https://github.com/hail-is/hail/pull/6629#issuecomment-511016826:56,Deployability,release,release,56,"oof, this is bad enough we might want to do yet another release",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6629#issuecomment-511016826
https://github.com/hail-is/hail/pull/6629#issuecomment-511021715:13,Usability,undo,undocumented,13,This uses an undocumented feature. It can wait for a little while.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6629#issuecomment-511021715
https://github.com/hail-is/hail/pull/6629#issuecomment-511031021:189,Deployability,pipeline,pipeline,189,"This benchmark is in some ways bad. The real problem is in the compiler/orchestration, not in any execution. I feel like we need a `_do_nothing()` that doesn't execute any code, but runs a pipeline through everything in the compiler and stops short before anything that would submit a spark job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6629#issuecomment-511031021
https://github.com/hail-is/hail/pull/6629#issuecomment-511031021:5,Testability,benchmark,benchmark,5,"This benchmark is in some ways bad. The real problem is in the compiler/orchestration, not in any execution. I feel like we need a `_do_nothing()` that doesn't execute any code, but runs a pipeline through everything in the compiler and stops short before anything that would submit a spark job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6629#issuecomment-511031021
https://github.com/hail-is/hail/pull/6629#issuecomment-511032001:75,Deployability,pipeline,pipelines,75,"yeah, I agree. But we can also get that effect by having tiny data but big pipelines",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6629#issuecomment-511032001
https://github.com/hail-is/hail/pull/6633#issuecomment-511213010:29,Testability,test,test,29,"I'm not entirely sure how to test this. I think I need to create a table with a row beginning exactly on a block boundary, not fully sure how to do that. I've tested this and it does work on the table that I found the issue. I will be making another change soon to prevent this ambiguity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6633#issuecomment-511213010
https://github.com/hail-is/hail/pull/6633#issuecomment-511213010:159,Testability,test,tested,159,"I'm not entirely sure how to test this. I think I need to create a table with a row beginning exactly on a block boundary, not fully sure how to do that. I've tested this and it does work on the table that I found the issue. I will be making another change soon to prevent this ambiguity.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6633#issuecomment-511213010
https://github.com/hail-is/hail/issues/6634#issuecomment-511214882:0,Availability,Error,Error,0,"Error occurs in 0.2.18 and 0.2.17, but not in 0.2.16",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6634#issuecomment-511214882
https://github.com/hail-is/hail/issues/6635#issuecomment-511343816:44,Availability,error,errors,44,This looks a lot like preemption. Are these errors fatal?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635#issuecomment-511343816
https://github.com/hail-is/hail/issues/6635#issuecomment-513887926:8,Availability,error,errors,8,"Yes the errors were fatal, but using non-preemtible instances fixed it, so will close this issue. Thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6635#issuecomment-513887926
https://github.com/hail-is/hail/pull/6640#issuecomment-511460325:19,Integrability,message,messages,19,informative commit messages please!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6640#issuecomment-511460325
https://github.com/hail-is/hail/pull/6644#issuecomment-511512794:32,Testability,log,logging,32,"oops---there's also a couple of logging methods in here that aren't used, but are handy to have for tests/debugging. Can move onto a separate PR if desired, but they're pretty small/straightforward.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6644#issuecomment-511512794
https://github.com/hail-is/hail/pull/6644#issuecomment-511512794:100,Testability,test,tests,100,"oops---there's also a couple of logging methods in here that aren't used, but are handy to have for tests/debugging. Can move onto a separate PR if desired, but they're pretty small/straightforward.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6644#issuecomment-511512794
https://github.com/hail-is/hail/pull/6653#issuecomment-511630081:35,Deployability,update,updates,35,"the most recent numbers, with some updates (this is basically read->densify->force count):; ```; size | new | old; 10 x 25000 | 0m 02.62 | 0m 01.05; 10 x 500000 | 0m 02.39 | 0m 04.91; 200 x 25000 | 0m 00.69 | 0m 05.92; 200 x 500000 | 0m 13.69 | 1m 03.08; 1000 x 25000 | 0m 02.17 | 0m 27.34; 1000 x 500000 | 1m 03.90 | 5m 13.64; 5000 x 25000 | 0m 09.13 | 2m 16.92; 5000 x 500000 | 5m 45.71 | 27m 53.85; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6653#issuecomment-511630081
https://github.com/hail-is/hail/issues/6663#issuecomment-514769232:83,Availability,error,error,83,Is this as easy as d38896d5b3e5160d50070103f7948158af5a5ea1? The commit gives this error instead:; ```; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: ; Index Expressions: int32; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6663#issuecomment-514769232
https://github.com/hail-is/hail/issues/6663#issuecomment-514793080:61,Availability,fault,fault,61,"Yup, it looks like that was the problem. That was totally my fault too. Thanks Milo!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6663#issuecomment-514793080
https://github.com/hail-is/hail/issues/6663#issuecomment-514795368:35,Testability,test,test,35,"Possible. Let's make sure we add a test for this too, @iitalics .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6663#issuecomment-514795368
https://github.com/hail-is/hail/pull/6664#issuecomment-512333398:40,Testability,log,log,40,https://ci.hail.is/batches/1420/jobs/32/log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6664#issuecomment-512333398
https://github.com/hail-is/hail/pull/6664#issuecomment-512843380:15,Availability,toler,tolerations,15,"I think maybe `tolerations` is not currently indented to the same level as `volumes`, causing the error you're getting",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6664#issuecomment-512843380
https://github.com/hail-is/hail/pull/6664#issuecomment-512843380:98,Availability,error,error,98,"I think maybe `tolerations` is not currently indented to the same level as `volumes`, causing the error you're getting",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6664#issuecomment-512843380
https://github.com/hail-is/hail/pull/6666#issuecomment-512274651:30,Testability,benchmark,benchmark,30,too bad I haven't added a PCA benchmark yet :(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6666#issuecomment-512274651
https://github.com/hail-is/hail/pull/6670#issuecomment-512577477:110,Deployability,deploy,deployed,110,I also added some debugging logs to try and figure out why you were getting a batch with no jobs when you dev deployed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6670#issuecomment-512577477
https://github.com/hail-is/hail/pull/6670#issuecomment-512577477:28,Testability,log,logs,28,I also added some debugging logs to try and figure out why you were getting a batch with no jobs when you dev deployed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6670#issuecomment-512577477
https://github.com/hail-is/hail/pull/6676#issuecomment-512932609:15,Testability,test,tests,15,I ran the unit tests locally with no issues - what failed?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6676#issuecomment-512932609
https://github.com/hail-is/hail/pull/6676#issuecomment-512935301:20,Availability,failure,failures,20,"we have some random failures of tests for some of our services related to resource contention, it's a big problem :(. If you push an empty commit, I can rerun the tests and hope they succeed this time...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6676#issuecomment-512935301
https://github.com/hail-is/hail/pull/6676#issuecomment-512935301:32,Testability,test,tests,32,"we have some random failures of tests for some of our services related to resource contention, it's a big problem :(. If you push an empty commit, I can rerun the tests and hope they succeed this time...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6676#issuecomment-512935301
https://github.com/hail-is/hail/pull/6676#issuecomment-512935301:163,Testability,test,tests,163,"we have some random failures of tests for some of our services related to resource contention, it's a big problem :(. If you push an empty commit, I can rerun the tests and hope they succeed this time...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6676#issuecomment-512935301
https://github.com/hail-is/hail/pull/6678#issuecomment-512828240:101,Deployability,configurat,configuration,101,"> CVE-2019-11245 is a vulnerability in k8s that causes some (all?); > containers without a runAsUser configuration to run; > as user id 0, i.e. root. Jupyter refuses to start as root.; > This change enables Jupyter to start successfully. Yeah, causes all containers to run as root upon restart of the container.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6678#issuecomment-512828240
https://github.com/hail-is/hail/pull/6678#issuecomment-512828240:101,Modifiability,config,configuration,101,"> CVE-2019-11245 is a vulnerability in k8s that causes some (all?); > containers without a runAsUser configuration to run; > as user id 0, i.e. root. Jupyter refuses to start as root.; > This change enables Jupyter to start successfully. Yeah, causes all containers to run as root upon restart of the container.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6678#issuecomment-512828240
https://github.com/hail-is/hail/issues/6679#issuecomment-513230208:12,Deployability,upgrade,upgrade,12,Resolved by upgrade and mitigations. Created https://github.com/hail-is/hail/issues/6693 to track the more general issue of containers (non-buggily) running as root in our cluster.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6679#issuecomment-513230208
https://github.com/hail-is/hail/pull/6689#issuecomment-513932854:110,Testability,test,tests,110,"- `render` is already handled by `head_str()` being defined; - 👍just added this in a commit; - i dont see any tests in this suite for any of the other XToTableApply IR's, do you think having the PCRelateSuite is enough?; - i'm looking into this -- i'm not sure how to obtain a ""partitionCounts"" from a blockmatrix; - hm. perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable""?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513932854
https://github.com/hail-is/hail/pull/6689#issuecomment-513935766:223,Modifiability,inherit,inherits,223,"> the TableIR doesn't define partitionCounts. statically known partition counts are used to optimize `.count()` when we know the partition sizes. Here that doesn't apply, so I don't think you need to define that method (it inherits `def partitionCounts = None`). > perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable"". Yes, it was. No need to write a rule for this. Separately, I think we should delete the checklist. It'll never be correct, since it's not checked against the codebase. To add an IR node, one needs to understand the compiler, and we can't adequately document that in a bullet list right now (over time, things should get simpler). . If a node is missing from somewhere it needs to appear, then tests should catch that case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766
https://github.com/hail-is/hail/pull/6689#issuecomment-513935766:92,Performance,optimiz,optimize,92,"> the TableIR doesn't define partitionCounts. statically known partition counts are used to optimize `.count()` when we know the partition sizes. Here that doesn't apply, so I don't think you need to define that method (it inherits `def partitionCounts = None`). > perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable"". Yes, it was. No need to write a rule for this. Separately, I think we should delete the checklist. It'll never be correct, since it's not checked against the codebase. To add an IR node, one needs to understand the compiler, and we can't adequately document that in a bullet list right now (over time, things should get simpler). . If a node is missing from somewhere it needs to appear, then tests should catch that case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766
https://github.com/hail-is/hail/pull/6689#issuecomment-513935766:717,Testability,test,tests,717,"> the TableIR doesn't define partitionCounts. statically known partition counts are used to optimize `.count()` when we know the partition sizes. Here that doesn't apply, so I don't think you need to define that method (it inherits `def partitionCounts = None`). > perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable"". Yes, it was. No need to write a rule for this. Separately, I think we should delete the checklist. It'll never be correct, since it's not checked against the codebase. To add an IR node, one needs to understand the compiler, and we can't adequately document that in a bullet list right now (over time, things should get simpler). . If a node is missing from somewhere it needs to appear, then tests should catch that case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766
https://github.com/hail-is/hail/pull/6689#issuecomment-513935766:644,Usability,simpl,simpler,644,"> the TableIR doesn't define partitionCounts. statically known partition counts are used to optimize `.count()` when we know the partition sizes. Here that doesn't apply, so I don't think you need to define that method (it inherits `def partitionCounts = None`). > perhaps ""LiftLiterals"" was changed to ""LiftNonCompilable"". Yes, it was. No need to write a rule for this. Separately, I think we should delete the checklist. It'll never be correct, since it's not checked against the codebase. To add an IR node, one needs to understand the compiler, and we can't adequately document that in a bullet list right now (over time, things should get simpler). . If a node is missing from somewhere it needs to appear, then tests should catch that case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-513935766
https://github.com/hail-is/hail/pull/6689#issuecomment-522022994:66,Safety,timeout,timeout,66,CI doesn't seem to be retesting this one after it failed due to a timeout issue. If you bump this PR it would probably go in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6689#issuecomment-522022994
https://github.com/hail-is/hail/pull/6694#issuecomment-513396135:17,Testability,test,tested,17,FYI @danking . I tested this is all working by making max_pods = 1 and queue_size = 5 and having a test that created 20 jobs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6694#issuecomment-513396135
https://github.com/hail-is/hail/pull/6694#issuecomment-513396135:99,Testability,test,test,99,FYI @danking . I tested this is all working by making max_pods = 1 and queue_size = 5 and having a test that created 20 jobs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6694#issuecomment-513396135
https://github.com/hail-is/hail/pull/6711#issuecomment-514763348:15,Availability,failure,failures,15,"You have a few failures, rvb.addAnnotation(t, a) needs to be rvb.addAnnotation(t.virtualType, a)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-514763348
https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:1446,Availability,Error,Error,1446,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174
https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:0,Deployability,Update,Updates,0,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174
https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:543,Safety,Safe,SafeRow,543,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174
https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:161,Testability,test,test,161,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174
https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:368,Testability,test,test,368,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174
https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:593,Testability,test,test,593,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174
https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:828,Testability,test,test,828,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174
https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:1063,Testability,test,test,1063,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174
https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:1277,Testability,test,test,1277,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174
https://github.com/hail-is/hail/issues/6715#issuecomment-514336863:102,Deployability,pipeline,pipelines,102,We should probably talk to the SEQR team as well: https://github.com/macarthur-lab/hail-elasticsearch-pipelines/blob/master/download_and_create_reference_datasets/v02/hail_scripts/write_combined_reference_data_ht.py,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6715#issuecomment-514336863
https://github.com/hail-is/hail/pull/6719#issuecomment-514341053:16,Integrability,depend,dependent,16,I think this is dependent on the authorization PR going in so I can test it's working.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6719#issuecomment-514341053
https://github.com/hail-is/hail/pull/6719#issuecomment-514341053:33,Security,authoriz,authorization,33,I think this is dependent on the authorization PR going in so I can test it's working.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6719#issuecomment-514341053
https://github.com/hail-is/hail/pull/6719#issuecomment-514341053:68,Testability,test,test,68,I think this is dependent on the authorization PR going in so I can test it's working.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6719#issuecomment-514341053
https://github.com/hail-is/hail/issues/6720#issuecomment-514358731:29,Security,Secur,Security,29,"SEC; ----. - Web Application Security, A Beginner's Guide https://www.amazon.com/Web-Application-Security-Beginners-Guide/dp/0071776168; - The Web Application Hacker's Handbook: Finding and Exploiting Security Flaws https://www.amazon.com/Web-Application-Hackers-Handbook-Exploiting/dp/1118026470; - http://cryto.net/~joepie91/blog/2016/06/13/stop-using-jwt-for-sessions/; - http://cryto.net/~joepie91/blog/2016/06/19/stop-using-jwt-for-sessions-part-2-why-your-solution-doesnt-work/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720#issuecomment-514358731
https://github.com/hail-is/hail/issues/6720#issuecomment-514358731:97,Security,Secur,Security-Beginners-Guide,97,"SEC; ----. - Web Application Security, A Beginner's Guide https://www.amazon.com/Web-Application-Security-Beginners-Guide/dp/0071776168; - The Web Application Hacker's Handbook: Finding and Exploiting Security Flaws https://www.amazon.com/Web-Application-Hackers-Handbook-Exploiting/dp/1118026470; - http://cryto.net/~joepie91/blog/2016/06/13/stop-using-jwt-for-sessions/; - http://cryto.net/~joepie91/blog/2016/06/19/stop-using-jwt-for-sessions-part-2-why-your-solution-doesnt-work/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720#issuecomment-514358731
https://github.com/hail-is/hail/issues/6720#issuecomment-514358731:201,Security,Secur,Security,201,"SEC; ----. - Web Application Security, A Beginner's Guide https://www.amazon.com/Web-Application-Security-Beginners-Guide/dp/0071776168; - The Web Application Hacker's Handbook: Finding and Exploiting Security Flaws https://www.amazon.com/Web-Application-Hackers-Handbook-Exploiting/dp/1118026470; - http://cryto.net/~joepie91/blog/2016/06/13/stop-using-jwt-for-sessions/; - http://cryto.net/~joepie91/blog/2016/06/19/stop-using-jwt-for-sessions-part-2-why-your-solution-doesnt-work/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720#issuecomment-514358731
https://github.com/hail-is/hail/issues/6720#issuecomment-514358731:52,Usability,Guid,Guide,52,"SEC; ----. - Web Application Security, A Beginner's Guide https://www.amazon.com/Web-Application-Security-Beginners-Guide/dp/0071776168; - The Web Application Hacker's Handbook: Finding and Exploiting Security Flaws https://www.amazon.com/Web-Application-Hackers-Handbook-Exploiting/dp/1118026470; - http://cryto.net/~joepie91/blog/2016/06/13/stop-using-jwt-for-sessions/; - http://cryto.net/~joepie91/blog/2016/06/19/stop-using-jwt-for-sessions-part-2-why-your-solution-doesnt-work/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720#issuecomment-514358731
https://github.com/hail-is/hail/issues/6720#issuecomment-514358731:116,Usability,Guid,Guide,116,"SEC; ----. - Web Application Security, A Beginner's Guide https://www.amazon.com/Web-Application-Security-Beginners-Guide/dp/0071776168; - The Web Application Hacker's Handbook: Finding and Exploiting Security Flaws https://www.amazon.com/Web-Application-Hackers-Handbook-Exploiting/dp/1118026470; - http://cryto.net/~joepie91/blog/2016/06/13/stop-using-jwt-for-sessions/; - http://cryto.net/~joepie91/blog/2016/06/19/stop-using-jwt-for-sessions-part-2-why-your-solution-doesnt-work/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6720#issuecomment-514358731
https://github.com/hail-is/hail/pull/6721#issuecomment-514360904:34,Testability,test,tests,34,It looks like we need more client tests to verify that things like duration and exit codes continue to work. I'll close and add an issue to fix this more thoroughly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6721#issuecomment-514360904
https://github.com/hail-is/hail/pull/6723#issuecomment-515481261:171,Availability,error,errors,171,"While this seems to currently be a functioning improvement on what we had before, it's still slower than I'd like it to be for Jacob's use case and getting unexpected OOM errors, working on it now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6723#issuecomment-515481261
https://github.com/hail-is/hail/pull/6726#issuecomment-515074154:154,Testability,test,test,154,@danking FYI. Let me know if you want to incorporate these fixes into your own branch and close this PR. I made the same fix you made to the list_batches test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6726#issuecomment-515074154
https://github.com/hail-is/hail/pull/6727#issuecomment-516096236:82,Testability,log,logic,82,"So.... this is technically working, but I'm not very happy with how difficult the logic is to parse through. I'm going to work at tidying it up into a more readable format but would love feedback on how to do that. cc @tpoterba @cseed @patrick-schultz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6727#issuecomment-516096236
https://github.com/hail-is/hail/pull/6727#issuecomment-516096236:187,Usability,feedback,feedback,187,"So.... this is technically working, but I'm not very happy with how difficult the logic is to parse through. I'm going to work at tidying it up into a more readable format but would love feedback on how to do that. cc @tpoterba @cseed @patrick-schultz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6727#issuecomment-516096236
https://github.com/hail-is/hail/pull/6727#issuecomment-516690783:63,Testability,test,tests,63,"I broke out the underlying BTree implementation and wrote some tests for it, so this is now stacked on #6771.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6727#issuecomment-516690783
https://github.com/hail-is/hail/pull/6727#issuecomment-521003645:37,Testability,benchmark,benchmark,37,@tpoterba with the table agg counter benchmark:; ```; old aggs	9.915	9.818	0.246; new aggs	9.706	9.501	0.375; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6727#issuecomment-521003645
https://github.com/hail-is/hail/pull/6730#issuecomment-515198365:102,Availability,down,down,102,"I registered the functions inside and outside the actual aggregation to bring the IR size all the way down to about what it was before. It was maybe 140 before I did that, 30 of which belonged to the string concatenation in the error messages :(. . This also hit a bug that I'm fixing in #6740, so it won't be able to go in before that does.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6730#issuecomment-515198365
https://github.com/hail-is/hail/pull/6730#issuecomment-515198365:228,Availability,error,error,228,"I registered the functions inside and outside the actual aggregation to bring the IR size all the way down to about what it was before. It was maybe 140 before I did that, 30 of which belonged to the string concatenation in the error messages :(. . This also hit a bug that I'm fixing in #6740, so it won't be able to go in before that does.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6730#issuecomment-515198365
https://github.com/hail-is/hail/pull/6730#issuecomment-515198365:234,Integrability,message,messages,234,"I registered the functions inside and outside the actual aggregation to bring the IR size all the way down to about what it was before. It was maybe 140 before I did that, 30 of which belonged to the string concatenation in the error messages :(. . This also hit a bug that I'm fixing in #6740, so it won't be able to go in before that does.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6730#issuecomment-515198365
https://github.com/hail-is/hail/pull/6730#issuecomment-515308903:52,Testability,test,test,52,"So this is failing the `test_aggregators_hist_neg0` test, which asserts that a value of -0.0 falls outside of the bins for `[0, 2, 4, 6, 8, 10]`. I really don't see why this is the desired behavior, but this is how it stands as the fix to #5846. I can pretty easily fix this to use `compare` instead of `<=`, but I'd instead like to fix the test so that we consider -0.0 to be in the bin [0.0, 2.0). . @tpoterba @chrisvittal",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6730#issuecomment-515308903
https://github.com/hail-is/hail/pull/6730#issuecomment-515308903:64,Testability,assert,asserts,64,"So this is failing the `test_aggregators_hist_neg0` test, which asserts that a value of -0.0 falls outside of the bins for `[0, 2, 4, 6, 8, 10]`. I really don't see why this is the desired behavior, but this is how it stands as the fix to #5846. I can pretty easily fix this to use `compare` instead of `<=`, but I'd instead like to fix the test so that we consider -0.0 to be in the bin [0.0, 2.0). . @tpoterba @chrisvittal",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6730#issuecomment-515308903
https://github.com/hail-is/hail/pull/6730#issuecomment-515308903:341,Testability,test,test,341,"So this is failing the `test_aggregators_hist_neg0` test, which asserts that a value of -0.0 falls outside of the bins for `[0, 2, 4, 6, 8, 10]`. I really don't see why this is the desired behavior, but this is how it stands as the fix to #5846. I can pretty easily fix this to use `compare` instead of `<=`, but I'd instead like to fix the test so that we consider -0.0 to be in the bin [0.0, 2.0). . @tpoterba @chrisvittal",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6730#issuecomment-515308903
https://github.com/hail-is/hail/pull/6734#issuecomment-514880263:206,Energy Efficiency,schedul,scheduler,206,"How are you running the tests? We have the following in `hail/testng.xml`:. ```; <suite name=""SuiteAll"" verbose=""1"">; <test name=""TestAll""> ; <packages> ; <package name=""is.hail.*"">; <exclude name=""is.hail.scheduler""></exclude>; <exclude name=""is.hail.backend.distributed""></exclude>; 	 </package>; </packages>; </test>; </suite>; ```. These tests are explicitly executed by `scheduler/testng.xml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514880263
https://github.com/hail-is/hail/pull/6734#issuecomment-514880263:376,Energy Efficiency,schedul,scheduler,376,"How are you running the tests? We have the following in `hail/testng.xml`:. ```; <suite name=""SuiteAll"" verbose=""1"">; <test name=""TestAll""> ; <packages> ; <package name=""is.hail.*"">; <exclude name=""is.hail.scheduler""></exclude>; <exclude name=""is.hail.backend.distributed""></exclude>; 	 </package>; </packages>; </test>; </suite>; ```. These tests are explicitly executed by `scheduler/testng.xml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514880263
https://github.com/hail-is/hail/pull/6734#issuecomment-514880263:24,Testability,test,tests,24,"How are you running the tests? We have the following in `hail/testng.xml`:. ```; <suite name=""SuiteAll"" verbose=""1"">; <test name=""TestAll""> ; <packages> ; <package name=""is.hail.*"">; <exclude name=""is.hail.scheduler""></exclude>; <exclude name=""is.hail.backend.distributed""></exclude>; 	 </package>; </packages>; </test>; </suite>; ```. These tests are explicitly executed by `scheduler/testng.xml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514880263
https://github.com/hail-is/hail/pull/6734#issuecomment-514880263:62,Testability,test,testng,62,"How are you running the tests? We have the following in `hail/testng.xml`:. ```; <suite name=""SuiteAll"" verbose=""1"">; <test name=""TestAll""> ; <packages> ; <package name=""is.hail.*"">; <exclude name=""is.hail.scheduler""></exclude>; <exclude name=""is.hail.backend.distributed""></exclude>; 	 </package>; </packages>; </test>; </suite>; ```. These tests are explicitly executed by `scheduler/testng.xml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514880263
https://github.com/hail-is/hail/pull/6734#issuecomment-514880263:119,Testability,test,test,119,"How are you running the tests? We have the following in `hail/testng.xml`:. ```; <suite name=""SuiteAll"" verbose=""1"">; <test name=""TestAll""> ; <packages> ; <package name=""is.hail.*"">; <exclude name=""is.hail.scheduler""></exclude>; <exclude name=""is.hail.backend.distributed""></exclude>; 	 </package>; </packages>; </test>; </suite>; ```. These tests are explicitly executed by `scheduler/testng.xml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514880263
https://github.com/hail-is/hail/pull/6734#issuecomment-514880263:130,Testability,Test,TestAll,130,"How are you running the tests? We have the following in `hail/testng.xml`:. ```; <suite name=""SuiteAll"" verbose=""1"">; <test name=""TestAll""> ; <packages> ; <package name=""is.hail.*"">; <exclude name=""is.hail.scheduler""></exclude>; <exclude name=""is.hail.backend.distributed""></exclude>; 	 </package>; </packages>; </test>; </suite>; ```. These tests are explicitly executed by `scheduler/testng.xml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514880263
https://github.com/hail-is/hail/pull/6734#issuecomment-514880263:314,Testability,test,test,314,"How are you running the tests? We have the following in `hail/testng.xml`:. ```; <suite name=""SuiteAll"" verbose=""1"">; <test name=""TestAll""> ; <packages> ; <package name=""is.hail.*"">; <exclude name=""is.hail.scheduler""></exclude>; <exclude name=""is.hail.backend.distributed""></exclude>; 	 </package>; </packages>; </test>; </suite>; ```. These tests are explicitly executed by `scheduler/testng.xml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514880263
https://github.com/hail-is/hail/pull/6734#issuecomment-514880263:342,Testability,test,tests,342,"How are you running the tests? We have the following in `hail/testng.xml`:. ```; <suite name=""SuiteAll"" verbose=""1"">; <test name=""TestAll""> ; <packages> ; <package name=""is.hail.*"">; <exclude name=""is.hail.scheduler""></exclude>; <exclude name=""is.hail.backend.distributed""></exclude>; 	 </package>; </packages>; </test>; </suite>; ```. These tests are explicitly executed by `scheduler/testng.xml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514880263
https://github.com/hail-is/hail/pull/6734#issuecomment-514880263:386,Testability,test,testng,386,"How are you running the tests? We have the following in `hail/testng.xml`:. ```; <suite name=""SuiteAll"" verbose=""1"">; <test name=""TestAll""> ; <packages> ; <package name=""is.hail.*"">; <exclude name=""is.hail.scheduler""></exclude>; <exclude name=""is.hail.backend.distributed""></exclude>; 	 </package>; </packages>; </test>; </suite>; ```. These tests are explicitly executed by `scheduler/testng.xml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514880263
https://github.com/hail-is/hail/pull/6734#issuecomment-514891047:23,Energy Efficiency,schedul,scheduler,23,`gradle test` runs the scheduler tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514891047
https://github.com/hail-is/hail/pull/6734#issuecomment-514891047:8,Testability,test,test,8,`gradle test` runs the scheduler tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514891047
https://github.com/hail-is/hail/pull/6734#issuecomment-514891047:33,Testability,test,tests,33,`gradle test` runs the scheduler tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514891047
https://github.com/hail-is/hail/pull/6734#issuecomment-514912470:38,Testability,test,test,38,"Then we should exclude them in gradle test, too. These are being run in the `test_scheduler` step of the build process.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-514912470
https://github.com/hail-is/hail/pull/6734#issuecomment-515021721:250,Testability,test,testing,250,"yeah, figured that out after your comment. . I'm a bit bothered by the differences between the build.yaml and the local development process. the build.yaml should be the single source of truth, but it's not in a form that's easy to look to for local testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6734#issuecomment-515021721
https://github.com/hail-is/hail/issues/6738#issuecomment-515162578:43,Deployability,install,install,43,can you include more info? I think a fresh install of conda will include this binary on the path now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6738#issuecomment-515162578
https://github.com/hail-is/hail/issues/6738#issuecomment-515163173:95,Deployability,install,install,95,"Related (but for bash scripts): https://github.com/conda/conda/issues/7980. I haven't tried to install anaconda for some months, but when I first tried, ~6mo ago, definitely had this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6738#issuecomment-515163173
https://github.com/hail-is/hail/issues/6738#issuecomment-515167477:25,Deployability,install,installer,25,Confirmed that Miniconda installer appends the necessary to bash_profile (both GUI package installer and bash installer),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6738#issuecomment-515167477
https://github.com/hail-is/hail/issues/6738#issuecomment-515167477:91,Deployability,install,installer,91,Confirmed that Miniconda installer appends the necessary to bash_profile (both GUI package installer and bash installer),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6738#issuecomment-515167477
https://github.com/hail-is/hail/issues/6738#issuecomment-515167477:110,Deployability,install,installer,110,Confirmed that Miniconda installer appends the necessary to bash_profile (both GUI package installer and bash installer),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6738#issuecomment-515167477
https://github.com/hail-is/hail/pull/6742#issuecomment-515218879:11,Testability,test,test,11,"hm. python test is failing because max can't handle float32's, but the code i based it off of (`SumAggregator`) only handled int64 and float64. . should i insert cases to handle every kind of number?. also should i assign someone in particular to this PR or choose randomly from the scorecard?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6742#issuecomment-515218879
https://github.com/hail-is/hail/pull/6742#issuecomment-519546529:35,Availability,failure,failure,35,needs a bump. There's also a weird failure in the tutorial docs - seems like something is generating an `inf` that Python doesn't expect,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6742#issuecomment-519546529
https://github.com/hail-is/hail/pull/6742#issuecomment-519554156:77,Availability,failure,failure,77,"yep, run `./gradlew makeDocs`. It could be somehow related to another random failure (no space left on device)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6742#issuecomment-519554156
https://github.com/hail-is/hail/pull/6742#issuecomment-520512516:136,Availability,failure,failures,136,"`test_summarize_run` is segfaulting on the CI server, but it runs fine for me locally (in fact, I can run all of `test_expr.py` with no failures). can somebody else try cloning this branch and running tests locally?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6742#issuecomment-520512516
https://github.com/hail-is/hail/pull/6742#issuecomment-520512516:201,Testability,test,tests,201,"`test_summarize_run` is segfaulting on the CI server, but it runs fine for me locally (in fact, I can run all of `test_expr.py` with no failures). can somebody else try cloning this branch and running tests locally?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6742#issuecomment-520512516
https://github.com/hail-is/hail/issues/6745#issuecomment-515479734:54,Modifiability,rewrite,rewrite,54,"I think we should totally remove these tutorials, and rewrite them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6745#issuecomment-515479734
https://github.com/hail-is/hail/issues/6747#issuecomment-515511257:29,Deployability,install,install,29,Hail requires Java 8. Please install Java 8: https://hail.is/docs/0.2/getting_started.html#requirements,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6747#issuecomment-515511257
https://github.com/hail-is/hail/pull/6748#issuecomment-527900542:81,Modifiability,refactor,refactoring,81,"I've raised the dead whale and rebased against master. Also reverted some of the refactoring changes, so the diff is ~600 lines shorter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6748#issuecomment-527900542
https://github.com/hail-is/hail/pull/6757#issuecomment-516101033:46,Performance,optimiz,optimization,46,maybe we should make an issue to support this optimization for non-ascending orders?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6757#issuecomment-516101033
https://github.com/hail-is/hail/pull/6757#issuecomment-516102881:48,Performance,optimiz,optimization,48,> maybe we should make an issue to support this optimization for non-ascending orders?. good idea.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6757#issuecomment-516102881
https://github.com/hail-is/hail/issues/6762#issuecomment-516427230:35,Deployability,install,installations,35,"We've hit issues like this on some installations of Python, but I still have no idea what is the root cause inside Python. It looks like sometimes the submodule `bar` needs to be in the `__all__` of `foo` to import like as `import foo.bar.baz as bazz`, and sometimes not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-516427230
https://github.com/hail-is/hail/issues/6762#issuecomment-522487262:97,Deployability,install,installation,97,"@pyousefi could you help me replicate this? What system are you using, and which specific Python installation?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-522487262
https://github.com/hail-is/hail/issues/6762#issuecomment-533266332:43,Deployability,install,install,43,"I think the issue is that I used `pip3` to install `hail`, following the docs. Unfortunately, `conda` only installs `pip` into the `hail` environment and doesn't create the `pip3` alias, so I was using my system-wide `pip3` to install `hail`. When I use `pip` to install `hail` within the `hail` conda environment, everything works fine. Maybe just update the docs to use `pip` instead of `pip3`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533266332
https://github.com/hail-is/hail/issues/6762#issuecomment-533266332:107,Deployability,install,installs,107,"I think the issue is that I used `pip3` to install `hail`, following the docs. Unfortunately, `conda` only installs `pip` into the `hail` environment and doesn't create the `pip3` alias, so I was using my system-wide `pip3` to install `hail`. When I use `pip` to install `hail` within the `hail` conda environment, everything works fine. Maybe just update the docs to use `pip` instead of `pip3`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533266332
https://github.com/hail-is/hail/issues/6762#issuecomment-533266332:227,Deployability,install,install,227,"I think the issue is that I used `pip3` to install `hail`, following the docs. Unfortunately, `conda` only installs `pip` into the `hail` environment and doesn't create the `pip3` alias, so I was using my system-wide `pip3` to install `hail`. When I use `pip` to install `hail` within the `hail` conda environment, everything works fine. Maybe just update the docs to use `pip` instead of `pip3`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533266332
https://github.com/hail-is/hail/issues/6762#issuecomment-533266332:263,Deployability,install,install,263,"I think the issue is that I used `pip3` to install `hail`, following the docs. Unfortunately, `conda` only installs `pip` into the `hail` environment and doesn't create the `pip3` alias, so I was using my system-wide `pip3` to install `hail`. When I use `pip` to install `hail` within the `hail` conda environment, everything works fine. Maybe just update the docs to use `pip` instead of `pip3`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533266332
https://github.com/hail-is/hail/issues/6762#issuecomment-533266332:349,Deployability,update,update,349,"I think the issue is that I used `pip3` to install `hail`, following the docs. Unfortunately, `conda` only installs `pip` into the `hail` environment and doesn't create the `pip3` alias, so I was using my system-wide `pip3` to install `hail`. When I use `pip` to install `hail` within the `hail` conda environment, everything works fine. Maybe just update the docs to use `pip` instead of `pip3`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533266332
https://github.com/hail-is/hail/issues/6762#issuecomment-533269724:5,Deployability,update,update,5,I'll update them to run `python3 -m pip`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533269724
https://github.com/hail-is/hail/issues/6762#issuecomment-533279733:79,Availability,error,error,79,"Hmmmm, I still don't totally understand why we're hitting this specific import error. The system pip should still be able to install and run hail, I think -- I'd expect either an import error saying that `hail` cannot be found (if it's installed somewhere not on the Python path), or success. I still want to replicate in a docker, will report back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733
https://github.com/hail-is/hail/issues/6762#issuecomment-533279733:186,Availability,error,error,186,"Hmmmm, I still don't totally understand why we're hitting this specific import error. The system pip should still be able to install and run hail, I think -- I'd expect either an import error saying that `hail` cannot be found (if it's installed somewhere not on the Python path), or success. I still want to replicate in a docker, will report back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733
https://github.com/hail-is/hail/issues/6762#issuecomment-533279733:125,Deployability,install,install,125,"Hmmmm, I still don't totally understand why we're hitting this specific import error. The system pip should still be able to install and run hail, I think -- I'd expect either an import error saying that `hail` cannot be found (if it's installed somewhere not on the Python path), or success. I still want to replicate in a docker, will report back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733
https://github.com/hail-is/hail/issues/6762#issuecomment-533279733:236,Deployability,install,installed,236,"Hmmmm, I still don't totally understand why we're hitting this specific import error. The system pip should still be able to install and run hail, I think -- I'd expect either an import error saying that `hail` cannot be found (if it's installed somewhere not on the Python path), or success. I still want to replicate in a docker, will report back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733
https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275:94,Availability,Error,Error,94,"Hello developers,; Sorry for resurrecting the issue. I have the same problem with a different Error. ```; conda create -n hail2; conda activate hail2; conda install -c bioconda hail; pip install gnomad; ```. Installation has no issues. But below command throws error. This is the same on two different machines that I have tried so far. Removing conda env, or changing env location, has not helped me so far. ```; import hail; from gnomad.sample_qc.ancestry import assign_population_pcs. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/sample_qc/ancestry.py"", line 9, in <module>; from gnomad.utils.filtering import filter_to_autosomes; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/utils/filtering.py"", line 9, in <module>; from gnomad.resources.resource_utils import DataException; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/__init__.py"", line 3, in <module>; from .resource_utils import *; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 162, in <module>; class VariantDatasetResource(BaseResource):; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 173, in VariantDatasetResource; def vds(self, force_import: bool = False) -> hl.vds.VariantDataset:; AttributeError: module 'hail' has no attribute 'vds'; ```. Could anyone please recommend me how to circumvent this?; Thanks for the amazing package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275
https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275:261,Availability,error,error,261,"Hello developers,; Sorry for resurrecting the issue. I have the same problem with a different Error. ```; conda create -n hail2; conda activate hail2; conda install -c bioconda hail; pip install gnomad; ```. Installation has no issues. But below command throws error. This is the same on two different machines that I have tried so far. Removing conda env, or changing env location, has not helped me so far. ```; import hail; from gnomad.sample_qc.ancestry import assign_population_pcs. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/sample_qc/ancestry.py"", line 9, in <module>; from gnomad.utils.filtering import filter_to_autosomes; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/utils/filtering.py"", line 9, in <module>; from gnomad.resources.resource_utils import DataException; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/__init__.py"", line 3, in <module>; from .resource_utils import *; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 162, in <module>; class VariantDatasetResource(BaseResource):; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 173, in VariantDatasetResource; def vds(self, force_import: bool = False) -> hl.vds.VariantDataset:; AttributeError: module 'hail' has no attribute 'vds'; ```. Could anyone please recommend me how to circumvent this?; Thanks for the amazing package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275
https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275:157,Deployability,install,install,157,"Hello developers,; Sorry for resurrecting the issue. I have the same problem with a different Error. ```; conda create -n hail2; conda activate hail2; conda install -c bioconda hail; pip install gnomad; ```. Installation has no issues. But below command throws error. This is the same on two different machines that I have tried so far. Removing conda env, or changing env location, has not helped me so far. ```; import hail; from gnomad.sample_qc.ancestry import assign_population_pcs. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/sample_qc/ancestry.py"", line 9, in <module>; from gnomad.utils.filtering import filter_to_autosomes; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/utils/filtering.py"", line 9, in <module>; from gnomad.resources.resource_utils import DataException; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/__init__.py"", line 3, in <module>; from .resource_utils import *; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 162, in <module>; class VariantDatasetResource(BaseResource):; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 173, in VariantDatasetResource; def vds(self, force_import: bool = False) -> hl.vds.VariantDataset:; AttributeError: module 'hail' has no attribute 'vds'; ```. Could anyone please recommend me how to circumvent this?; Thanks for the amazing package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275
https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275:187,Deployability,install,install,187,"Hello developers,; Sorry for resurrecting the issue. I have the same problem with a different Error. ```; conda create -n hail2; conda activate hail2; conda install -c bioconda hail; pip install gnomad; ```. Installation has no issues. But below command throws error. This is the same on two different machines that I have tried so far. Removing conda env, or changing env location, has not helped me so far. ```; import hail; from gnomad.sample_qc.ancestry import assign_population_pcs. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/sample_qc/ancestry.py"", line 9, in <module>; from gnomad.utils.filtering import filter_to_autosomes; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/utils/filtering.py"", line 9, in <module>; from gnomad.resources.resource_utils import DataException; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/__init__.py"", line 3, in <module>; from .resource_utils import *; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 162, in <module>; class VariantDatasetResource(BaseResource):; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 173, in VariantDatasetResource; def vds(self, force_import: bool = False) -> hl.vds.VariantDataset:; AttributeError: module 'hail' has no attribute 'vds'; ```. Could anyone please recommend me how to circumvent this?; Thanks for the amazing package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275
https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275:208,Deployability,Install,Installation,208,"Hello developers,; Sorry for resurrecting the issue. I have the same problem with a different Error. ```; conda create -n hail2; conda activate hail2; conda install -c bioconda hail; pip install gnomad; ```. Installation has no issues. But below command throws error. This is the same on two different machines that I have tried so far. Removing conda env, or changing env location, has not helped me so far. ```; import hail; from gnomad.sample_qc.ancestry import assign_population_pcs. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/sample_qc/ancestry.py"", line 9, in <module>; from gnomad.utils.filtering import filter_to_autosomes; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/utils/filtering.py"", line 9, in <module>; from gnomad.resources.resource_utils import DataException; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/__init__.py"", line 3, in <module>; from .resource_utils import *; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 162, in <module>; class VariantDatasetResource(BaseResource):; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 173, in VariantDatasetResource; def vds(self, force_import: bool = False) -> hl.vds.VariantDataset:; AttributeError: module 'hail' has no attribute 'vds'; ```. Could anyone please recommend me how to circumvent this?; Thanks for the amazing package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275
https://github.com/hail-is/hail/issues/6762#issuecomment-1262357111:182,Deployability,install,install,182,"Hail doesn't have a conda package -- the bioconda package there was not uploaded by the Hail Team (could even be malware -- we don't know). It's certainly a very old version. If you install Hail with pip, you should pick up the latest version 0.2.100 and have access to hl.vds, which is somewhat recent functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262357111
https://github.com/hail-is/hail/issues/6762#issuecomment-1262357111:260,Security,access,access,260,"Hail doesn't have a conda package -- the bioconda package there was not uploaded by the Hail Team (could even be malware -- we don't know). It's certainly a very old version. If you install Hail with pip, you should pick up the latest version 0.2.100 and have access to hl.vds, which is somewhat recent functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262357111
https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759:357,Availability,error,error,357,"Hi Tim,; Sorry for the bother again. I am following this short tutorial as described [here](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/). The code snippet was working properly with earlier. Now that I have installed hail from pip, I have this error while running RF model. ```; ht, rf_model = assign_population_pcs(; ... ht,; ... pc_cols=ht.scores,; ... fit=fit,; ... ). 2022-09-29 14:55:46 Hail: INFO: Coerced sorted dataset (0 + 1) / 1]; INFO (gnomad.sample_qc.ancestry 224): Found the following sample count after population assignment: sas: 1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/bioinfoRD/ARCdata/Projects_AMT/conda_envs/hail/lib/python3.10/site-packages/gnomad/sample_qc/ancestry.py"", line 235, in assign_population_pcs; min_assignment_prob=min_prob, error_rate=error_rate; UnboundLocalError: local variable 'error_rate' referenced before assignment; ```. Could you please suggest what might be happening here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759
https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759:320,Deployability,install,installed,320,"Hi Tim,; Sorry for the bother again. I am following this short tutorial as described [here](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/). The code snippet was working properly with earlier. Now that I have installed hail from pip, I have this error while running RF model. ```; ht, rf_model = assign_population_pcs(; ... ht,; ... pc_cols=ht.scores,; ... fit=fit,; ... ). 2022-09-29 14:55:46 Hail: INFO: Coerced sorted dataset (0 + 1) / 1]; INFO (gnomad.sample_qc.ancestry 224): Found the following sample count after population assignment: sas: 1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/bioinfoRD/ARCdata/Projects_AMT/conda_envs/hail/lib/python3.10/site-packages/gnomad/sample_qc/ancestry.py"", line 235, in assign_population_pcs; min_assignment_prob=min_prob, error_rate=error_rate; UnboundLocalError: local variable 'error_rate' referenced before assignment; ```. Could you please suggest what might be happening here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759
https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759:963,Modifiability,variab,variable,963,"Hi Tim,; Sorry for the bother again. I am following this short tutorial as described [here](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/). The code snippet was working properly with earlier. Now that I have installed hail from pip, I have this error while running RF model. ```; ht, rf_model = assign_population_pcs(; ... ht,; ... pc_cols=ht.scores,; ... fit=fit,; ... ). 2022-09-29 14:55:46 Hail: INFO: Coerced sorted dataset (0 + 1) / 1]; INFO (gnomad.sample_qc.ancestry 224): Found the following sample count after population assignment: sas: 1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/bioinfoRD/ARCdata/Projects_AMT/conda_envs/hail/lib/python3.10/site-packages/gnomad/sample_qc/ancestry.py"", line 235, in assign_population_pcs; min_assignment_prob=min_prob, error_rate=error_rate; UnboundLocalError: local variable 'error_rate' referenced before assignment; ```. Could you please suggest what might be happening here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759
https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759:195,Performance,load,loadings-and-random-forest-classifier-on-your-dataset,195,"Hi Tim,; Sorry for the bother again. I am following this short tutorial as described [here](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/). The code snippet was working properly with earlier. Now that I have installed hail from pip, I have this error while running RF model. ```; ht, rf_model = assign_population_pcs(; ... ht,; ... pc_cols=ht.scores,; ... fit=fit,; ... ). 2022-09-29 14:55:46 Hail: INFO: Coerced sorted dataset (0 + 1) / 1]; INFO (gnomad.sample_qc.ancestry 224): Found the following sample count after population assignment: sas: 1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/bioinfoRD/ARCdata/Projects_AMT/conda_envs/hail/lib/python3.10/site-packages/gnomad/sample_qc/ancestry.py"", line 235, in assign_population_pcs; min_assignment_prob=min_prob, error_rate=error_rate; UnboundLocalError: local variable 'error_rate' referenced before assignment; ```. Could you please suggest what might be happening here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759
https://github.com/hail-is/hail/pull/6764#issuecomment-516533758:55,Testability,test,test,55,"This is admittedly untested, since I don't know how to test CI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6764#issuecomment-516533758
https://github.com/hail-is/hail/pull/6765#issuecomment-530559600:33,Availability,ping,ping,33,"@catoverdrive can you rebase and ping me when done? This isn't stacked anymore, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6765#issuecomment-530559600
https://github.com/hail-is/hail/issues/6773#issuecomment-517021923:95,Availability,failure,failure,95,"Yeah. Really odd that prometheus doesn't somehow signal that it needs more memory, or exits as failure. It just sits and spins.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6773#issuecomment-517021923
https://github.com/hail-is/hail/pull/6774#issuecomment-518771521:131,Testability,log,logging,131,"The nodes only have 30Gi total. If I make it exactly 30 I imagine there will be issues since the nodes also have to run docker and logging stuff and whatnot, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-518771521
https://github.com/hail-is/hail/pull/6774#issuecomment-518774162:120,Availability,toler,tolerate,120,"Also, now that we are not going with the ""run a highmem node premptible pool"" approach, should we still have Prometheus tolerate preemptibles? It would probably be better if it was on one of our always running nodes, especially since it has a nontrivial amount of startup time in my experience",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-518774162
https://github.com/hail-is/hail/pull/6774#issuecomment-519120787:379,Performance,load,loaded,379,"Non preemptible for that reason seems fine. . However, I think I have a fundamental misunderstanding. In my experience thus far, Prometheus will need more than 30GB of RAM if anyone runs thousands of pods on our cluster for an hour or more. Is that not your understanding? Last time I ran the test Prometheus wasn’t able to start after crashing. Also the per-pod Graphana graphs loaded noticeably slower than everything else.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-519120787
https://github.com/hail-is/hail/pull/6774#issuecomment-519120787:293,Testability,test,test,293,"Non preemptible for that reason seems fine. . However, I think I have a fundamental misunderstanding. In my experience thus far, Prometheus will need more than 30GB of RAM if anyone runs thousands of pods on our cluster for an hour or more. Is that not your understanding? Last time I ran the test Prometheus wasn’t able to start after crashing. Also the per-pod Graphana graphs loaded noticeably slower than everything else.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-519120787
https://github.com/hail-is/hail/pull/6774#issuecomment-519178381:74,Testability,test,tests,74,"I think I had the misunderstanding. I know Prometheus dies when we do big tests, but I thought that was just because we requested no resources at all in the past. I didn't understand that it consistently uses 30Gb + of memory at those times. In that case, it seems constantly running a bigger node is unavoidable",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-519178381
https://github.com/hail-is/hail/pull/6774#issuecomment-519249271:230,Energy Efficiency,schedul,scheduled,230,"Agreed. We tend to idle at 5 n1-standard-8 non-preemptible machines. An additional pool of n1-standard-16's or 32's would be fine. Prometheus could take, say, 45 GB, and a bunch of compute-hungry but memory-lean tasks will get co-scheduled.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-519249271
https://github.com/hail-is/hail/pull/6774#issuecomment-519250516:86,Usability,Clear,Clearly,86,"Hrm. One issue: I do not fully understand the minimum number of nodes allowed in GKE. Clearly, we need at least three master nodes to reach consensus. I'm not sure if any similar requirements exist for worker pools.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-519250516
https://github.com/hail-is/hail/pull/6774#issuecomment-521758111:139,Performance,load,load,139,"Final conclusion here: Since batch 1 is discontinued in favor of batch 2, and we don't know how prometheus will react to batch 2's logging load (should be better since there's not constant kubernetes junk), let's not start a new node pool for now, and request 20 Gbs of memory as done here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-521758111
https://github.com/hail-is/hail/pull/6774#issuecomment-521758111:131,Testability,log,logging,131,"Final conclusion here: Since batch 1 is discontinued in favor of batch 2, and we don't know how prometheus will react to batch 2's logging load (should be better since there's not constant kubernetes junk), let's not start a new node pool for now, and request 20 Gbs of memory as done here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-521758111
https://github.com/hail-is/hail/pull/6789#issuecomment-517703256:2,Availability,down,downloaded,2,I downloaded from the artifact url ; ```; gsutil cp gs://hail-ci-nt3qc/build/ec2176d5842192a57afea55fe102e32c/www.tar.gz .; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-517703256
https://github.com/hail-is/hail/pull/6789#issuecomment-522140293:12,Availability,down,down,12,"Dan, I went down a rabbit hole with this one. Updated bootstrap (XSS exploit protection, not EOL), jQuery (a bunch of security patches), focused on using flex box for layout, and fixed many of the inconsistencies I found on the docs page (the way the header was laid out, namely lack of element alignment with rest of docs and odd centering, the weirdness of having two home buttons named Hail, Annotation Database didn't scope styles so changed docs nav layout, broken navbar menu, etc).; - Also removes navbar code duplication in docs. Also after speaking with Jackie, restored fixed navbar on docs (so that it stays in place during scrolling). This may cause issues with (especially older) mobile devices, but those probably aren't spending much time on the docs page anyway. Works great on narrow views as well. Since 0.1 doesn't appear to be built, if these changes can affect that will need to be addressed. Before: https://youtu.be/I-Awgx3spnQ; After: https://youtu.be/ff1387vDsQ8. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-522140293
https://github.com/hail-is/hail/pull/6789#issuecomment-522140293:46,Deployability,Update,Updated,46,"Dan, I went down a rabbit hole with this one. Updated bootstrap (XSS exploit protection, not EOL), jQuery (a bunch of security patches), focused on using flex box for layout, and fixed many of the inconsistencies I found on the docs page (the way the header was laid out, namely lack of element alignment with rest of docs and odd centering, the weirdness of having two home buttons named Hail, Annotation Database didn't scope styles so changed docs nav layout, broken navbar menu, etc).; - Also removes navbar code duplication in docs. Also after speaking with Jackie, restored fixed navbar on docs (so that it stays in place during scrolling). This may cause issues with (especially older) mobile devices, but those probably aren't spending much time on the docs page anyway. Works great on narrow views as well. Since 0.1 doesn't appear to be built, if these changes can affect that will need to be addressed. Before: https://youtu.be/I-Awgx3spnQ; After: https://youtu.be/ff1387vDsQ8. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-522140293
https://github.com/hail-is/hail/pull/6789#issuecomment-522140293:127,Deployability,patch,patches,127,"Dan, I went down a rabbit hole with this one. Updated bootstrap (XSS exploit protection, not EOL), jQuery (a bunch of security patches), focused on using flex box for layout, and fixed many of the inconsistencies I found on the docs page (the way the header was laid out, namely lack of element alignment with rest of docs and odd centering, the weirdness of having two home buttons named Hail, Annotation Database didn't scope styles so changed docs nav layout, broken navbar menu, etc).; - Also removes navbar code duplication in docs. Also after speaking with Jackie, restored fixed navbar on docs (so that it stays in place during scrolling). This may cause issues with (especially older) mobile devices, but those probably aren't spending much time on the docs page anyway. Works great on narrow views as well. Since 0.1 doesn't appear to be built, if these changes can affect that will need to be addressed. Before: https://youtu.be/I-Awgx3spnQ; After: https://youtu.be/ff1387vDsQ8. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-522140293
https://github.com/hail-is/hail/pull/6789#issuecomment-522140293:65,Security,XSS,XSS,65,"Dan, I went down a rabbit hole with this one. Updated bootstrap (XSS exploit protection, not EOL), jQuery (a bunch of security patches), focused on using flex box for layout, and fixed many of the inconsistencies I found on the docs page (the way the header was laid out, namely lack of element alignment with rest of docs and odd centering, the weirdness of having two home buttons named Hail, Annotation Database didn't scope styles so changed docs nav layout, broken navbar menu, etc).; - Also removes navbar code duplication in docs. Also after speaking with Jackie, restored fixed navbar on docs (so that it stays in place during scrolling). This may cause issues with (especially older) mobile devices, but those probably aren't spending much time on the docs page anyway. Works great on narrow views as well. Since 0.1 doesn't appear to be built, if these changes can affect that will need to be addressed. Before: https://youtu.be/I-Awgx3spnQ; After: https://youtu.be/ff1387vDsQ8. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-522140293
https://github.com/hail-is/hail/pull/6789#issuecomment-522140293:118,Security,secur,security,118,"Dan, I went down a rabbit hole with this one. Updated bootstrap (XSS exploit protection, not EOL), jQuery (a bunch of security patches), focused on using flex box for layout, and fixed many of the inconsistencies I found on the docs page (the way the header was laid out, namely lack of element alignment with rest of docs and odd centering, the weirdness of having two home buttons named Hail, Annotation Database didn't scope styles so changed docs nav layout, broken navbar menu, etc).; - Also removes navbar code duplication in docs. Also after speaking with Jackie, restored fixed navbar on docs (so that it stays in place during scrolling). This may cause issues with (especially older) mobile devices, but those probably aren't spending much time on the docs page anyway. Works great on narrow views as well. Since 0.1 doesn't appear to be built, if these changes can affect that will need to be addressed. Before: https://youtu.be/I-Awgx3spnQ; After: https://youtu.be/ff1387vDsQ8. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-522140293
https://github.com/hail-is/hail/pull/6789#issuecomment-523032505:525,Testability,log,log,525,"bump, should be good to go. I do wonder if we need the navbar on docs page at all. I think it would be sensible to restore the doc home button above the left-nav searchbar, and make the docs links in hail.is navbar be target=""_blank"". It seems this is more the norm with sphinx-generated docs. On the other hand, I do at some point want to have the documentation feel like a native part of our apparent site (may be split up across domains/subdomains different pages, but should feel coherent, and give people the ability to log in from anywhere, ala: https://www.tensorflow.org/api_docs/python/tf )",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-523032505
https://github.com/hail-is/hail/pull/6794#issuecomment-518700128:20,Testability,benchmark,benchmarks,20,bump. I want to run benchmarks again.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6794#issuecomment-518700128
https://github.com/hail-is/hail/pull/6797#issuecomment-517734418:70,Safety,avoid,avoid,70,"the sidecars are going away in another pull request, so I'd prefer to avoid the merge conflict.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6797#issuecomment-517734418
https://github.com/hail-is/hail/pull/6816#issuecomment-519535998:186,Integrability,depend,depending,186,"Reassigned to Patrick since Arcturus is OOO. The primary concern Arcturus had is that I've called aggregator functions in my Python implementation which need to be consistently agg/scan depending on whether `hardy_weinberg_test` is called as an agg or scan. I'm now convinced we get this for free. The way it works is that a global AggFunc singleton object contains a flag `_as_scan`, which is essentially set in a context manager whenever a function is called as `hl.scan`. This means that if I call `agg.filter` inside `hardy_weinberg_test`, it will look at the AggFunc, which properly has the flag set. It would be incorrect to call `hl.scan.xxx` inside a function in the agg module, but it's perfectly correct to call `hl.agg.xxx`, since the outer context is already managed properly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6816#issuecomment-519535998
https://github.com/hail-is/hail/pull/6817#issuecomment-522016103:19,Testability,test,tests,19,This PR is failing tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6817#issuecomment-522016103
https://github.com/hail-is/hail/pull/6817#issuecomment-522115406:72,Availability,toler,tolerated,72,This change uncovered a problem with lowering -- `AggLet` nodes are not tolerated inside MatrixMapCols/MatrixMapRows. I'll follow up with a PR to fix this for MatrixMapRows and add targeted tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6817#issuecomment-522115406
https://github.com/hail-is/hail/pull/6817#issuecomment-522115406:190,Testability,test,tests,190,This change uncovered a problem with lowering -- `AggLet` nodes are not tolerated inside MatrixMapCols/MatrixMapRows. I'll follow up with a PR to fix this for MatrixMapRows and add targeted tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6817#issuecomment-522115406
https://github.com/hail-is/hail/pull/6822#issuecomment-531877691:18,Availability,ping,ping,18,@iitalics can you ping me once this is rebased?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6822#issuecomment-531877691
https://github.com/hail-is/hail/pull/6824#issuecomment-518855752:540,Testability,test,test,540,"@chrisvittal I hit a segfault on this PR in the decoder trying to do; ```; mt = hl.read_matrix_table(filename); summarized = hl.experimental.densify(mt). summarized.write(output1, overwrite=True); summarized2 = hl.read_matrix_table(output1). def test1():; 	summarized._force_count_rows(). def test2():; 	summarized2._force_count_rows(); 	; print(timeit.repeat(test1, number=1, repeat=5)); print(timeit.repeat(test2, number=1, repeat=5)); ```; The weird thing is that it appears to be sporadic, since it successfully ran all 5 iterations of test 1 and was on the last iteration of test2, judging by the number of spark stages, and it ran successfully when I just ran it a second time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6824#issuecomment-518855752
https://github.com/hail-is/hail/pull/6825#issuecomment-519985328:19,Availability,failure,failure,19,"you have a doctest failure:; ```. =================================== FAILURES ===================================; ______________ [doctest] hail.experimental.ldscsim.get_cov_matrix ______________; 034 trait 1 & trait 4 :math:`r_g` = 0; 035 trait 2 & trait 3 :math:`r_g` = 0.9; 036 trait 2 & trait 4 :math:`r_g` = 0.15; 037 trait 3 & trait 4 :math:`r_g` = 1; 038 To obtain the covariance matrix corresponding to this scenario :math:`h^2` values are; 039 ordered according to user specification and :math:`r_g` values are ordered by the ; 040 order in which the corresponding genetic covariance terms will appear in the ; 041 covariance matrix, reading lines in the upper triangular matrix from left to; 042 right, top to bottom (read first row left to right, read second row left to ; 043 right, etc.), exluding the diagonal.; Differences (unified diff with -expected +actual):; @@ -1,4 +1,12 @@; -array([[0.1 , 0.06928203, 0.09899495, 0. ],; - [0.06928203, 0.3 , 0.22045408, 0.06363961],; - [0.09899495, 0.22045408, 0.2 , 0.34641016],; - [0. , 0.06363961, 0.34641016, 0.6 ]]); +covariance matrix is not positive semidefinite.; +adjusting rg values to make covariance matrix positive semidefinite; +0.4 -> 0.42023852645344634; +0.7 -> 0.5623351779264535; +0.0 -> 0.04812102869845767; +0.9 -> 0.7179252549815345; +0.15 -> 0.20103397396043554; +1.0 -> 0.7534523335539473; +(array([[0.1 , 0.07278745, 0.0795262 , 0.0117872 ],; + [0.07278745, 0.3 , 0.17585505, 0.08529149],; + [0.0795262 , 0.17585505, 0.2 , 0.26100354],; + [0.0117872 , 0.08529149, 0.26100354, 0.6 ]]), [0.42023852645344634, 0.5623351779264535, 0.04812102869845767, 0.7179252549815345, 0.20103397396043554, 0.7534523335539473]). ```. And you'll need to update `ldscsim.rst` because you changed the list of exposed functions, it looks like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328
https://github.com/hail-is/hail/pull/6825#issuecomment-519985328:70,Availability,FAILURE,FAILURES,70,"you have a doctest failure:; ```. =================================== FAILURES ===================================; ______________ [doctest] hail.experimental.ldscsim.get_cov_matrix ______________; 034 trait 1 & trait 4 :math:`r_g` = 0; 035 trait 2 & trait 3 :math:`r_g` = 0.9; 036 trait 2 & trait 4 :math:`r_g` = 0.15; 037 trait 3 & trait 4 :math:`r_g` = 1; 038 To obtain the covariance matrix corresponding to this scenario :math:`h^2` values are; 039 ordered according to user specification and :math:`r_g` values are ordered by the ; 040 order in which the corresponding genetic covariance terms will appear in the ; 041 covariance matrix, reading lines in the upper triangular matrix from left to; 042 right, top to bottom (read first row left to right, read second row left to ; 043 right, etc.), exluding the diagonal.; Differences (unified diff with -expected +actual):; @@ -1,4 +1,12 @@; -array([[0.1 , 0.06928203, 0.09899495, 0. ],; - [0.06928203, 0.3 , 0.22045408, 0.06363961],; - [0.09899495, 0.22045408, 0.2 , 0.34641016],; - [0. , 0.06363961, 0.34641016, 0.6 ]]); +covariance matrix is not positive semidefinite.; +adjusting rg values to make covariance matrix positive semidefinite; +0.4 -> 0.42023852645344634; +0.7 -> 0.5623351779264535; +0.0 -> 0.04812102869845767; +0.9 -> 0.7179252549815345; +0.15 -> 0.20103397396043554; +1.0 -> 0.7534523335539473; +(array([[0.1 , 0.07278745, 0.0795262 , 0.0117872 ],; + [0.07278745, 0.3 , 0.17585505, 0.08529149],; + [0.0795262 , 0.17585505, 0.2 , 0.26100354],; + [0.0117872 , 0.08529149, 0.26100354, 0.6 ]]), [0.42023852645344634, 0.5623351779264535, 0.04812102869845767, 0.7179252549815345, 0.20103397396043554, 0.7534523335539473]). ```. And you'll need to update `ldscsim.rst` because you changed the list of exposed functions, it looks like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328
https://github.com/hail-is/hail/pull/6825#issuecomment-519985328:1716,Deployability,update,update,1716,"you have a doctest failure:; ```. =================================== FAILURES ===================================; ______________ [doctest] hail.experimental.ldscsim.get_cov_matrix ______________; 034 trait 1 & trait 4 :math:`r_g` = 0; 035 trait 2 & trait 3 :math:`r_g` = 0.9; 036 trait 2 & trait 4 :math:`r_g` = 0.15; 037 trait 3 & trait 4 :math:`r_g` = 1; 038 To obtain the covariance matrix corresponding to this scenario :math:`h^2` values are; 039 ordered according to user specification and :math:`r_g` values are ordered by the ; 040 order in which the corresponding genetic covariance terms will appear in the ; 041 covariance matrix, reading lines in the upper triangular matrix from left to; 042 right, top to bottom (read first row left to right, read second row left to ; 043 right, etc.), exluding the diagonal.; Differences (unified diff with -expected +actual):; @@ -1,4 +1,12 @@; -array([[0.1 , 0.06928203, 0.09899495, 0. ],; - [0.06928203, 0.3 , 0.22045408, 0.06363961],; - [0.09899495, 0.22045408, 0.2 , 0.34641016],; - [0. , 0.06363961, 0.34641016, 0.6 ]]); +covariance matrix is not positive semidefinite.; +adjusting rg values to make covariance matrix positive semidefinite; +0.4 -> 0.42023852645344634; +0.7 -> 0.5623351779264535; +0.0 -> 0.04812102869845767; +0.9 -> 0.7179252549815345; +0.15 -> 0.20103397396043554; +1.0 -> 0.7534523335539473; +(array([[0.1 , 0.07278745, 0.0795262 , 0.0117872 ],; + [0.07278745, 0.3 , 0.17585505, 0.08529149],; + [0.0795262 , 0.17585505, 0.2 , 0.26100354],; + [0.0117872 , 0.08529149, 0.26100354, 0.6 ]]), [0.42023852645344634, 0.5623351779264535, 0.04812102869845767, 0.7179252549815345, 0.20103397396043554, 0.7534523335539473]). ```. And you'll need to update `ldscsim.rst` because you changed the list of exposed functions, it looks like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328
https://github.com/hail-is/hail/pull/6825#issuecomment-519985328:1769,Security,expose,exposed,1769,"you have a doctest failure:; ```. =================================== FAILURES ===================================; ______________ [doctest] hail.experimental.ldscsim.get_cov_matrix ______________; 034 trait 1 & trait 4 :math:`r_g` = 0; 035 trait 2 & trait 3 :math:`r_g` = 0.9; 036 trait 2 & trait 4 :math:`r_g` = 0.15; 037 trait 3 & trait 4 :math:`r_g` = 1; 038 To obtain the covariance matrix corresponding to this scenario :math:`h^2` values are; 039 ordered according to user specification and :math:`r_g` values are ordered by the ; 040 order in which the corresponding genetic covariance terms will appear in the ; 041 covariance matrix, reading lines in the upper triangular matrix from left to; 042 right, top to bottom (read first row left to right, read second row left to ; 043 right, etc.), exluding the diagonal.; Differences (unified diff with -expected +actual):; @@ -1,4 +1,12 @@; -array([[0.1 , 0.06928203, 0.09899495, 0. ],; - [0.06928203, 0.3 , 0.22045408, 0.06363961],; - [0.09899495, 0.22045408, 0.2 , 0.34641016],; - [0. , 0.06363961, 0.34641016, 0.6 ]]); +covariance matrix is not positive semidefinite.; +adjusting rg values to make covariance matrix positive semidefinite; +0.4 -> 0.42023852645344634; +0.7 -> 0.5623351779264535; +0.0 -> 0.04812102869845767; +0.9 -> 0.7179252549815345; +0.15 -> 0.20103397396043554; +1.0 -> 0.7534523335539473; +(array([[0.1 , 0.07278745, 0.0795262 , 0.0117872 ],; + [0.07278745, 0.3 , 0.17585505, 0.08529149],; + [0.0795262 , 0.17585505, 0.2 , 0.26100354],; + [0.0117872 , 0.08529149, 0.26100354, 0.6 ]]), [0.42023852645344634, 0.5623351779264535, 0.04812102869845767, 0.7179252549815345, 0.20103397396043554, 0.7534523335539473]). ```. And you'll need to update `ldscsim.rst` because you changed the list of exposed functions, it looks like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328
https://github.com/hail-is/hail/pull/6825#issuecomment-521290991:5,Security,authoriz,authorized,5,I've authorized another test of this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-521290991
https://github.com/hail-is/hail/pull/6825#issuecomment-521290991:24,Testability,test,test,24,I've authorized another test of this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-521290991
https://github.com/hail-is/hail/pull/6825#issuecomment-529527257:2,Testability,test,testing,2,👍 testing now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-529527257
https://github.com/hail-is/hail/pull/6825#issuecomment-529649760:13,Availability,failure,failures,13,Getting more failures :(. Let's sit down tomorrow and I'll show you how to build docs locally,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-529649760
https://github.com/hail-is/hail/pull/6825#issuecomment-529649760:36,Availability,down,down,36,Getting more failures :(. Let's sit down tomorrow and I'll show you how to build docs locally,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-529649760
https://github.com/hail-is/hail/pull/6837#issuecomment-519629507:279,Performance,perform,performance,279,"> ApproxCDF should probably be marked non-commutative. Isn't it also non-associative? We're assuming everything is associative. > isn't TakeBy also non-commutative?. I don't think we guarantee a stable sort, so I'm inclined to leave it as an unstable sort for now in the name of performance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6837#issuecomment-519629507
https://github.com/hail-is/hail/pull/6837#issuecomment-519633134:608,Deployability,pipeline,pipeline,608,"> Isn't it also non-associative? We're assuming everything is associative. I was thinking there was still some benefit to using the associative combiner, but I take it back. ApproxCDF is *approximately* commutative, so if we aren't achieving determinism anyways, you're right the commutative combiner is fine. > I don't think we guarantee a stable sort, so I'm inclined to leave it as an unstable sort for now in the name of performance. Even if we don't guarantee stable sort, we might hope to guarantee determinism (no promise what answer you get, but you'll get the same answer each time you run the same pipeline). I think the associative combiner *might* be enough to ensure determinism. Though if you want to leave it nondeterministic for now, and wait for the better fix of guaranteeing stable sort by appending indices to the sort keys, that's fine with me.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6837#issuecomment-519633134
https://github.com/hail-is/hail/pull/6837#issuecomment-519633134:425,Performance,perform,performance,425,"> Isn't it also non-associative? We're assuming everything is associative. I was thinking there was still some benefit to using the associative combiner, but I take it back. ApproxCDF is *approximately* commutative, so if we aren't achieving determinism anyways, you're right the commutative combiner is fine. > I don't think we guarantee a stable sort, so I'm inclined to leave it as an unstable sort for now in the name of performance. Even if we don't guarantee stable sort, we might hope to guarantee determinism (no promise what answer you get, but you'll get the same answer each time you run the same pipeline). I think the associative combiner *might* be enough to ensure determinism. Though if you want to leave it nondeterministic for now, and wait for the better fix of guaranteeing stable sort by appending indices to the sort keys, that's fine with me.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6837#issuecomment-519633134
https://github.com/hail-is/hail/pull/6837#issuecomment-519635282:172,Deployability,pipeline,pipeline,172,"> Even if we don't guarantee stable sort, we might hope to guarantee determinism (no promise what answer you get, but you'll get the same answer each time you run the same pipeline). I think the associative combiner might be enough to ensure determinism. Though if you want to leave it nondeterministic for now, and wait for the better fix of guaranteeing stable sort by appending indices to the sort keys, that's fine with me. Ah, you're right, yeah.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6837#issuecomment-519635282
https://github.com/hail-is/hail/pull/6841#issuecomment-519709334:27,Testability,test,tests,27,"@tpoterba the doctests are tests, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6841#issuecomment-519709334
https://github.com/hail-is/hail/pull/6841#issuecomment-520638350:9,Testability,benchmark,benchmarks,9,Where do benchmarks go?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6841#issuecomment-520638350
https://github.com/hail-is/hail/pull/6841#issuecomment-520638993:57,Availability,robust,robust,57,"Still need to add row, col, and table tests. Have pretty robust entry tests, so those should go quick tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6841#issuecomment-520638993
https://github.com/hail-is/hail/pull/6841#issuecomment-520638993:38,Testability,test,tests,38,"Still need to add row, col, and table tests. Have pretty robust entry tests, so those should go quick tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6841#issuecomment-520638993
https://github.com/hail-is/hail/pull/6841#issuecomment-520638993:70,Testability,test,tests,70,"Still need to add row, col, and table tests. Have pretty robust entry tests, so those should go quick tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6841#issuecomment-520638993
https://github.com/hail-is/hail/pull/6841#issuecomment-521651919:14,Testability,test,tests,14,"OK, plenty of tests. Just need to know where the benchmarks go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6841#issuecomment-521651919
https://github.com/hail-is/hail/pull/6841#issuecomment-521651919:49,Testability,benchmark,benchmarks,49,"OK, plenty of tests. Just need to know where the benchmarks go.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6841#issuecomment-521651919
https://github.com/hail-is/hail/pull/6841#issuecomment-521656677:4,Testability,benchmark,benchmarks,4,"OK, benchmarks added for row, col, and entry export on an MT.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6841#issuecomment-521656677
https://github.com/hail-is/hail/issues/6849#issuecomment-520013973:41,Testability,test,test,41,"how should i er, make a nondeterministic test? just run it a bunch of times?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6849#issuecomment-520013973
https://github.com/hail-is/hail/pull/6863#issuecomment-521659615:127,Testability,test,tests,127,"I removed `test_hail_java` parallelism since the array ordering both fails & segfaults when multiple threads are executing the tests. This seems really bad, but I also don't care to fix it in this PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6863#issuecomment-521659615
https://github.com/hail-is/hail/pull/6863#issuecomment-521764182:1,Performance,concurren,concurrency,1,#concurrency,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6863#issuecomment-521764182
https://github.com/hail-is/hail/pull/6863#issuecomment-524927093:44,Safety,safe,safe,44,ughhhhhhhhh `index_bgen` is very not thread safe.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6863#issuecomment-524927093
https://github.com/hail-is/hail/pull/6874#issuecomment-521662062:31,Availability,failure,failures,31,"Currently there are some tests failures, but they are stemming from me running more tests than I expect to it would seem (i.e. trying to run the NDArray write tests in JVM byte code world). General review of the byt ecode generation stuff would still be appreciated, I'll debug the testing stuff when I get a chance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-521662062
https://github.com/hail-is/hail/pull/6874#issuecomment-521662062:25,Testability,test,tests,25,"Currently there are some tests failures, but they are stemming from me running more tests than I expect to it would seem (i.e. trying to run the NDArray write tests in JVM byte code world). General review of the byt ecode generation stuff would still be appreciated, I'll debug the testing stuff when I get a chance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-521662062
https://github.com/hail-is/hail/pull/6874#issuecomment-521662062:84,Testability,test,tests,84,"Currently there are some tests failures, but they are stemming from me running more tests than I expect to it would seem (i.e. trying to run the NDArray write tests in JVM byte code world). General review of the byt ecode generation stuff would still be appreciated, I'll debug the testing stuff when I get a chance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-521662062
https://github.com/hail-is/hail/pull/6874#issuecomment-521662062:159,Testability,test,tests,159,"Currently there are some tests failures, but they are stemming from me running more tests than I expect to it would seem (i.e. trying to run the NDArray write tests in JVM byte code world). General review of the byt ecode generation stuff would still be appreciated, I'll debug the testing stuff when I get a chance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-521662062
https://github.com/hail-is/hail/pull/6874#issuecomment-521662062:282,Testability,test,testing,282,"Currently there are some tests failures, but they are stemming from me running more tests than I expect to it would seem (i.e. trying to run the NDArray write tests in JVM byte code world). General review of the byt ecode generation stuff would still be appreciated, I'll debug the testing stuff when I get a chance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-521662062
https://github.com/hail-is/hail/pull/6874#issuecomment-523600895:170,Testability,test,test,170,"Alright, I've addressed all of the above, you were right I was able to move things up to PContainer and simplify some of the code there. I didn't do the one decorator to test cxx and java in this PR because the cxx shape test ended up using some things I haven't implemented on jvm side yet so for now I just made a separate test. I'll add that in a separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-523600895
https://github.com/hail-is/hail/pull/6874#issuecomment-523600895:221,Testability,test,test,221,"Alright, I've addressed all of the above, you were right I was able to move things up to PContainer and simplify some of the code there. I didn't do the one decorator to test cxx and java in this PR because the cxx shape test ended up using some things I haven't implemented on jvm side yet so for now I just made a separate test. I'll add that in a separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-523600895
https://github.com/hail-is/hail/pull/6874#issuecomment-523600895:325,Testability,test,test,325,"Alright, I've addressed all of the above, you were right I was able to move things up to PContainer and simplify some of the code there. I didn't do the one decorator to test cxx and java in this PR because the cxx shape test ended up using some things I haven't implemented on jvm side yet so for now I just made a separate test. I'll add that in a separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-523600895
https://github.com/hail-is/hail/pull/6874#issuecomment-523600895:104,Usability,simpl,simplify,104,"Alright, I've addressed all of the above, you were right I was able to move things up to PContainer and simplify some of the code there. I didn't do the one decorator to test cxx and java in this PR because the cxx shape test ended up using some things I haven't implemented on jvm side yet so for now I just made a separate test. I'll add that in a separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-523600895
https://github.com/hail-is/hail/pull/6874#issuecomment-523602549:20,Testability,test,tests,20,And I think all the tests are passing now as well,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6874#issuecomment-523602549
https://github.com/hail-is/hail/pull/6879#issuecomment-525345763:554,Performance,perform,performance,554,"Works for me. ```; $ gcloud version ; Google Cloud SDK 259.0.0; beta 2019.05.17; bq 2.0.46; core 2019.08.16; gsutil 4.42; $ gcloud dataproc clusters create cdv --master-machine-type n1-standard-1 --worker-machine-type n1-standard-1 --max-idle 10m --zone us-central1-b; Waiting on operation [projects/broad-mpg-gnomad/regions/global/operations/22271968-933b-3629-9d06-02f6a3039ddd].; Waiting for cluster creation operation...⠛ ; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done. ; Created [https://dataproc.googleapis.com/v1/projects/broad-mpg-gnomad/regions/global/clusters/cdv] Cluster placed in zone [us-central1-b]. $ gcloud dataproc clusters list; NAME WORKER_COUNT PREEMPTIBLE_WORKER_COUNT STATUS ZONE SCHEDULED_DELETE; cdv 2 RUNNING us-central1-b enabled; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6879#issuecomment-525345763
https://github.com/hail-is/hail/pull/6879#issuecomment-525345763:615,Performance,perform,performance,615,"Works for me. ```; $ gcloud version ; Google Cloud SDK 259.0.0; beta 2019.05.17; bq 2.0.46; core 2019.08.16; gsutil 4.42; $ gcloud dataproc clusters create cdv --master-machine-type n1-standard-1 --worker-machine-type n1-standard-1 --max-idle 10m --zone us-central1-b; Waiting on operation [projects/broad-mpg-gnomad/regions/global/operations/22271968-933b-3629-9d06-02f6a3039ddd].; Waiting for cluster creation operation...⠛ ; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done. ; Created [https://dataproc.googleapis.com/v1/projects/broad-mpg-gnomad/regions/global/clusters/cdv] Cluster placed in zone [us-central1-b]. $ gcloud dataproc clusters list; NAME WORKER_COUNT PREEMPTIBLE_WORKER_COUNT STATUS ZONE SCHEDULED_DELETE; cdv 2 RUNNING us-central1-b enabled; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6879#issuecomment-525345763
https://github.com/hail-is/hail/pull/6879#issuecomment-525345763:655,Performance,perform,performance,655,"Works for me. ```; $ gcloud version ; Google Cloud SDK 259.0.0; beta 2019.05.17; bq 2.0.46; core 2019.08.16; gsutil 4.42; $ gcloud dataproc clusters create cdv --master-machine-type n1-standard-1 --worker-machine-type n1-standard-1 --max-idle 10m --zone us-central1-b; Waiting on operation [projects/broad-mpg-gnomad/regions/global/operations/22271968-933b-3629-9d06-02f6a3039ddd].; Waiting for cluster creation operation...⠛ ; WARNING: For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.; Waiting for cluster creation operation...done. ; Created [https://dataproc.googleapis.com/v1/projects/broad-mpg-gnomad/regions/global/clusters/cdv] Cluster placed in zone [us-central1-b]. $ gcloud dataproc clusters list; NAME WORKER_COUNT PREEMPTIBLE_WORKER_COUNT STATUS ZONE SCHEDULED_DELETE; cdv 2 RUNNING us-central1-b enabled; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6879#issuecomment-525345763
https://github.com/hail-is/hail/pull/6887#issuecomment-531924944:13,Deployability,update,updated,13,"@johnc1231 I updated this, after discovering that HadoopFS does mkdir -p, but only during writes. I also made some small changes related to backend.py and ServiceBackend, to make it at least formally possible to use ServiceBackend (issue opened to this effect). Given the state of ServiceBackend and apiserver, I did not update test_google_fs_utils.py to explicitly check mkdir -p behavior, but this is implicitly checked in the local version of the file creation test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6887#issuecomment-531924944
https://github.com/hail-is/hail/pull/6887#issuecomment-531924944:321,Deployability,update,update,321,"@johnc1231 I updated this, after discovering that HadoopFS does mkdir -p, but only during writes. I also made some small changes related to backend.py and ServiceBackend, to make it at least formally possible to use ServiceBackend (issue opened to this effect). Given the state of ServiceBackend and apiserver, I did not update test_google_fs_utils.py to explicitly check mkdir -p behavior, but this is implicitly checked in the local version of the file creation test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6887#issuecomment-531924944
https://github.com/hail-is/hail/pull/6887#issuecomment-531924944:464,Testability,test,test,464,"@johnc1231 I updated this, after discovering that HadoopFS does mkdir -p, but only during writes. I also made some small changes related to backend.py and ServiceBackend, to make it at least formally possible to use ServiceBackend (issue opened to this effect). Given the state of ServiceBackend and apiserver, I did not update test_google_fs_utils.py to explicitly check mkdir -p behavior, but this is implicitly checked in the local version of the file creation test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6887#issuecomment-531924944
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:154,Deployability,deploy,deploy,154,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:312,Deployability,deploy,deploy,312,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:354,Deployability,deploy,deploy,354,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1600,Deployability,deploy,deploy,1600,"ials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev de",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1983,Deployability,configurat,configuration,1983,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:2261,Deployability,configurat,configuration,2261,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:2475,Deployability,deploy,deploy,2475,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:2605,Deployability,deploy,deploy,2605,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:2689,Deployability,deploy,deployed,2689,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:171,Integrability,rout,router-resolver,171,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:557,Integrability,mediat,mediated,557,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:944,Modifiability,config,config,944,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1607,Modifiability,config,config,1607,"ials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev de",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1983,Modifiability,config,configuration,1983,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:2261,Modifiability,config,configuration,2261,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:2448,Modifiability,config,config,2448,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:2482,Modifiability,config,config,2482,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:2108,Performance,load,loaded,2108,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:535,Security,access,access,535,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1167,Security,encrypt,encrypted,1167,"uter-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:138,Testability,test,tested,138,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:202,Testability,test,tested,202,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:209,Testability,log,login,209,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:215,Testability,log,logout,215,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:248,Testability,log,login,248,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:256,Testability,log,logout,256,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:290,Testability,log,login,290,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:296,Testability,log,logout,296,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:438,Testability,test,test,438,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:443,Testability,log,login,443,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:449,Testability,log,logout,449,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:800,Testability,log,login,800,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:884,Testability,test,tested,884,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:933,Testability,test,tested,933,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1030,Testability,log,login,1030,". Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaul",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1036,Testability,log,logout,1036,". Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaul",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1972,Usability,Simpl,Simplified,1972,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251
https://github.com/hail-is/hail/pull/6892#issuecomment-528373663:58,Deployability,deploy,deployment,58,"Cotton, mostly looks great, I haven't taken a look at the deployment configs yet. I would like to do that, and if you're ok with this from a time standpoint, spin up a locally deployed version to play with. . Nice work!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528373663
https://github.com/hail-is/hail/pull/6892#issuecomment-528373663:176,Deployability,deploy,deployed,176,"Cotton, mostly looks great, I haven't taken a look at the deployment configs yet. I would like to do that, and if you're ok with this from a time standpoint, spin up a locally deployed version to play with. . Nice work!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528373663
https://github.com/hail-is/hail/pull/6892#issuecomment-528373663:69,Modifiability,config,configs,69,"Cotton, mostly looks great, I haven't taken a look at the deployment configs yet. I would like to do that, and if you're ok with this from a time standpoint, spin up a locally deployed version to play with. . Nice work!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528373663
https://github.com/hail-is/hail/pull/6892#issuecomment-528876155:51,Testability,test,test-apiserver-pod,51,"HAIL_TOKEN_FILE still exists in . ```sh; apiserver/test-apiserver-pod.yaml; 21: - name: HAIL_TOKEN_FILE. notebook2/notebook/notebook.py; 73: env=[kube.client.V1EnvVar(name='HAIL_TOKEN_FILE',. notebook2/notebook/kubeclient.py; 46: env=[kube.client.V1EnvVar(name='HAIL_TOKEN_FILE',. hail/python/hail/backend/backend.py; 216: os.environ.get('HAIL_TOKEN_FILE') or; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528876155
https://github.com/hail-is/hail/pull/6892#issuecomment-528921065:60,Testability,test,tests,60,"Im also happy to let this in as mostly is (so no additional tests, api changes), and set the remainder that you find valid as issues. No need to stall something that isn't very user facing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528921065
https://github.com/hail-is/hail/pull/6892#issuecomment-528993572:477,Deployability,deploy,deployed,477,"OK, I think I addressed all the comments. Here is a summary of the code changes I made:; - use *-tokens instead of *-jwt for the session tokens; - db event to clean up sessions,; - there was a bunch of legacy garbage in batch/ and batch/Makefile that I nuked,; - added hailtop test for deploy_config,; - fixed up hail ServiceBackend (which isn't actually tested now); - create_user => insert_user. I think there is still some legacy garbage in apiserver/, but that's not being deployed right now so I just left it. Let me know if I missed anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528993572
https://github.com/hail-is/hail/pull/6892#issuecomment-528993572:277,Testability,test,test,277,"OK, I think I addressed all the comments. Here is a summary of the code changes I made:; - use *-tokens instead of *-jwt for the session tokens; - db event to clean up sessions,; - there was a bunch of legacy garbage in batch/ and batch/Makefile that I nuked,; - added hailtop test for deploy_config,; - fixed up hail ServiceBackend (which isn't actually tested now); - create_user => insert_user. I think there is still some legacy garbage in apiserver/, but that's not being deployed right now so I just left it. Let me know if I missed anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528993572
https://github.com/hail-is/hail/pull/6892#issuecomment-528993572:355,Testability,test,tested,355,"OK, I think I addressed all the comments. Here is a summary of the code changes I made:; - use *-tokens instead of *-jwt for the session tokens; - db event to clean up sessions,; - there was a bunch of legacy garbage in batch/ and batch/Makefile that I nuked,; - added hailtop test for deploy_config,; - fixed up hail ServiceBackend (which isn't actually tested now); - create_user => insert_user. I think there is still some legacy garbage in apiserver/, but that's not being deployed right now so I just left it. Let me know if I missed anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528993572
https://github.com/hail-is/hail/pull/6894#issuecomment-522472904:18,Modifiability,flexible,flexible,18,"regex is way more flexible (it was string containment before). Chris added this feature as a quick one-off to help with some stuff he was working on, but being able to write inclusion/exclusion regexes has been very useful!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6894#issuecomment-522472904
https://github.com/hail-is/hail/pull/6902#issuecomment-522672211:92,Modifiability,refactor,refactor,92,I also eliminated a useless internal function. I think this was an artifact leftover from a refactor.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6902#issuecomment-522672211
https://github.com/hail-is/hail/pull/6908#issuecomment-525915270:89,Availability,failure,failure,89,bump. can we get this in? I want to PR the stacked changes so that Dan can see the batch failure modes,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6908#issuecomment-525915270
https://github.com/hail-is/hail/pull/6912#issuecomment-524057804:0,Availability,ping,ping,0,"ping @catoverdrive , should be addressed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6912#issuecomment-524057804
https://github.com/hail-is/hail/pull/6917#issuecomment-523633712:1,Deployability,update,updated,1,(updated just now to what I *think* is more correct behavior),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6917#issuecomment-523633712
https://github.com/hail-is/hail/pull/6918#issuecomment-523662443:114,Integrability,rout,routes,114,It might be easier to invent a new domain name for VPC-internal clients. `batch.hail.internal`. Add the right DNS routes. Add new root certs to the worker nodes. That ensures our DNS changes don't break anyone else who is trying to talk to `batch.hail.is`. We should maybe talk more about what these DNS changes are breaking so we can plan our network infrastructure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-523662443
https://github.com/hail-is/hail/pull/6918#issuecomment-524103114:36,Deployability,configurat,configuration,36,"Something still isn't right with my configuration. The service and deployment are all up. I can curl to the internal gateway and it shows up in the logs. However, I'm getting 404 with this query: `http://hail.internal/jigold/batch2/healthcheck`. I set up cloud dns to route hail.internal to the internal gateway. I verified no traffic is recorded in the jigold router. I tried adding and removing the `gateway` service account but it made no difference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114
https://github.com/hail-is/hail/pull/6918#issuecomment-524103114:67,Deployability,deploy,deployment,67,"Something still isn't right with my configuration. The service and deployment are all up. I can curl to the internal gateway and it shows up in the logs. However, I'm getting 404 with this query: `http://hail.internal/jigold/batch2/healthcheck`. I set up cloud dns to route hail.internal to the internal gateway. I verified no traffic is recorded in the jigold router. I tried adding and removing the `gateway` service account but it made no difference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114
https://github.com/hail-is/hail/pull/6918#issuecomment-524103114:268,Integrability,rout,route,268,"Something still isn't right with my configuration. The service and deployment are all up. I can curl to the internal gateway and it shows up in the logs. However, I'm getting 404 with this query: `http://hail.internal/jigold/batch2/healthcheck`. I set up cloud dns to route hail.internal to the internal gateway. I verified no traffic is recorded in the jigold router. I tried adding and removing the `gateway` service account but it made no difference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114
https://github.com/hail-is/hail/pull/6918#issuecomment-524103114:361,Integrability,rout,router,361,"Something still isn't right with my configuration. The service and deployment are all up. I can curl to the internal gateway and it shows up in the logs. However, I'm getting 404 with this query: `http://hail.internal/jigold/batch2/healthcheck`. I set up cloud dns to route hail.internal to the internal gateway. I verified no traffic is recorded in the jigold router. I tried adding and removing the `gateway` service account but it made no difference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114
https://github.com/hail-is/hail/pull/6918#issuecomment-524103114:36,Modifiability,config,configuration,36,"Something still isn't right with my configuration. The service and deployment are all up. I can curl to the internal gateway and it shows up in the logs. However, I'm getting 404 with this query: `http://hail.internal/jigold/batch2/healthcheck`. I set up cloud dns to route hail.internal to the internal gateway. I verified no traffic is recorded in the jigold router. I tried adding and removing the `gateway` service account but it made no difference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114
https://github.com/hail-is/hail/pull/6918#issuecomment-524103114:148,Testability,log,logs,148,"Something still isn't right with my configuration. The service and deployment are all up. I can curl to the internal gateway and it shows up in the logs. However, I'm getting 404 with this query: `http://hail.internal/jigold/batch2/healthcheck`. I set up cloud dns to route hail.internal to the internal gateway. I verified no traffic is recorded in the jigold router. I tried adding and removing the `gateway` service account but it made no difference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114
https://github.com/hail-is/hail/pull/6923#issuecomment-524017142:73,Integrability,depend,depend,73,"Grr. I realized that our build.yaml `build_hail` step did not explicitly depend on setting up the python package correctly, it just relied on `make jars` doing that as a side effect. I've added an explicit rule for setting up the version files and I call it in build.yaml. The better answer is to change build.yaml to ship around a wheel file. I don't want to do this right now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524017142
https://github.com/hail-is/hail/pull/6923#issuecomment-524203427:150,Deployability,pipeline,pipelines,150,"#6927 Should fix the issue I was seeing with the warnings. The issue was that to the `make` invocations that we were making in the compilation of C++ pipelines, it looked like they were in jobserver mode. But they we didn't preserve the right state to execute them. . I think the real solution is unsetting `MAKEFLAGS` in the environment in `pgradle`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524203427
https://github.com/hail-is/hail/pull/6923#issuecomment-524446104:315,Availability,error,error,315,"> Your tool should also examine the first word of the MAKEFLAGS variable and look for the character n. If this character is present then make was invoked with the ‘-n’ option and your tool should stop without performing any operations. Added. > Your tool should be sure to write back the tokens it read, even under error conditions. This includes not only errors in your tool but also outside influences such as interrupts (SIGINT), etc. You may want to install signal handlers to manage this write-back. I mean, I doubt anyone is sending signals other than SIGKILL to our build system, but I added some signal handlers that just `sys.exit(0)` which triggers the finally (I checked). > We also get a lot of ‘warning: jobserver unavailable: using -j1. Add +' to parent make rule.’warnings when runningmake jvm-test`. This is because our C++ backend uses make to drive compilation (wtf‽). I strip MAKEFLAGS before calling gradle now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104
https://github.com/hail-is/hail/pull/6923#issuecomment-524446104:356,Availability,error,errors,356,"> Your tool should also examine the first word of the MAKEFLAGS variable and look for the character n. If this character is present then make was invoked with the ‘-n’ option and your tool should stop without performing any operations. Added. > Your tool should be sure to write back the tokens it read, even under error conditions. This includes not only errors in your tool but also outside influences such as interrupts (SIGINT), etc. You may want to install signal handlers to manage this write-back. I mean, I doubt anyone is sending signals other than SIGKILL to our build system, but I added some signal handlers that just `sys.exit(0)` which triggers the finally (I checked). > We also get a lot of ‘warning: jobserver unavailable: using -j1. Add +' to parent make rule.’warnings when runningmake jvm-test`. This is because our C++ backend uses make to drive compilation (wtf‽). I strip MAKEFLAGS before calling gradle now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104
https://github.com/hail-is/hail/pull/6923#issuecomment-524446104:454,Deployability,install,install,454,"> Your tool should also examine the first word of the MAKEFLAGS variable and look for the character n. If this character is present then make was invoked with the ‘-n’ option and your tool should stop without performing any operations. Added. > Your tool should be sure to write back the tokens it read, even under error conditions. This includes not only errors in your tool but also outside influences such as interrupts (SIGINT), etc. You may want to install signal handlers to manage this write-back. I mean, I doubt anyone is sending signals other than SIGKILL to our build system, but I added some signal handlers that just `sys.exit(0)` which triggers the finally (I checked). > We also get a lot of ‘warning: jobserver unavailable: using -j1. Add +' to parent make rule.’warnings when runningmake jvm-test`. This is because our C++ backend uses make to drive compilation (wtf‽). I strip MAKEFLAGS before calling gradle now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104
https://github.com/hail-is/hail/pull/6923#issuecomment-524446104:64,Modifiability,variab,variable,64,"> Your tool should also examine the first word of the MAKEFLAGS variable and look for the character n. If this character is present then make was invoked with the ‘-n’ option and your tool should stop without performing any operations. Added. > Your tool should be sure to write back the tokens it read, even under error conditions. This includes not only errors in your tool but also outside influences such as interrupts (SIGINT), etc. You may want to install signal handlers to manage this write-back. I mean, I doubt anyone is sending signals other than SIGKILL to our build system, but I added some signal handlers that just `sys.exit(0)` which triggers the finally (I checked). > We also get a lot of ‘warning: jobserver unavailable: using -j1. Add +' to parent make rule.’warnings when runningmake jvm-test`. This is because our C++ backend uses make to drive compilation (wtf‽). I strip MAKEFLAGS before calling gradle now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104
https://github.com/hail-is/hail/pull/6923#issuecomment-524446104:209,Performance,perform,performing,209,"> Your tool should also examine the first word of the MAKEFLAGS variable and look for the character n. If this character is present then make was invoked with the ‘-n’ option and your tool should stop without performing any operations. Added. > Your tool should be sure to write back the tokens it read, even under error conditions. This includes not only errors in your tool but also outside influences such as interrupts (SIGINT), etc. You may want to install signal handlers to manage this write-back. I mean, I doubt anyone is sending signals other than SIGKILL to our build system, but I added some signal handlers that just `sys.exit(0)` which triggers the finally (I checked). > We also get a lot of ‘warning: jobserver unavailable: using -j1. Add +' to parent make rule.’warnings when runningmake jvm-test`. This is because our C++ backend uses make to drive compilation (wtf‽). I strip MAKEFLAGS before calling gradle now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104
https://github.com/hail-is/hail/pull/6923#issuecomment-524446104:809,Testability,test,test,809,"> Your tool should also examine the first word of the MAKEFLAGS variable and look for the character n. If this character is present then make was invoked with the ‘-n’ option and your tool should stop without performing any operations. Added. > Your tool should be sure to write back the tokens it read, even under error conditions. This includes not only errors in your tool but also outside influences such as interrupts (SIGINT), etc. You may want to install signal handlers to manage this write-back. I mean, I doubt anyone is sending signals other than SIGKILL to our build system, but I added some signal handlers that just `sys.exit(0)` which triggers the finally (I checked). > We also get a lot of ‘warning: jobserver unavailable: using -j1. Add +' to parent make rule.’warnings when runningmake jvm-test`. This is because our C++ backend uses make to drive compilation (wtf‽). I strip MAKEFLAGS before calling gradle now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104
https://github.com/hail-is/hail/pull/6925#issuecomment-524130051:24,Testability,test,tests,24,It looks like the batch tests failed unrelated to this. You'll need to bump it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6925#issuecomment-524130051
https://github.com/hail-is/hail/pull/6926#issuecomment-524323940:0,Testability,Test,Tested,0,"Tested this by hand, it works now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6926#issuecomment-524323940
https://github.com/hail-is/hail/pull/6927#issuecomment-524446990:94,Modifiability,portab,portability,94,Also could someone please make sure this works on macOS and there aren't any dumb linux/glibc portability issues. I tried to keep it pretty POSIX-y.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6927#issuecomment-524446990
https://github.com/hail-is/hail/pull/6928#issuecomment-524334117:14,Deployability,deploy,deployed,14,".. and I hand-deployed the router and it looks good. Most of our existing services can't handle being located at internal.hail.is/ns/svc, so I'm going to make a series of changes to fix that, possibly folded into my auth changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928#issuecomment-524334117
https://github.com/hail-is/hail/pull/6928#issuecomment-524334117:27,Integrability,rout,router,27,".. and I hand-deployed the router and it looks good. Most of our existing services can't handle being located at internal.hail.is/ns/svc, so I'm going to make a series of changes to fix that, possibly folded into my auth changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928#issuecomment-524334117
https://github.com/hail-is/hail/pull/6928#issuecomment-524357498:51,Integrability,rout,router,51,Although let me try again now that you changed the router code.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6928#issuecomment-524357498
https://github.com/hail-is/hail/pull/6934#issuecomment-524422872:24,Testability,log,logging,24,Also added google-cloud-logging,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6934#issuecomment-524422872
https://github.com/hail-is/hail/pull/6937#issuecomment-524832178:54,Availability,failure,failures,54,"We have a CI problem leading to random erroneous test failures, and the system isn't designed to run the tests twice for a single commit. Can you add a new commit to the PR with `git commit -m ""bump"" --allow-empty`?; Sorry!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6937#issuecomment-524832178
https://github.com/hail-is/hail/pull/6937#issuecomment-524832178:49,Testability,test,test,49,"We have a CI problem leading to random erroneous test failures, and the system isn't designed to run the tests twice for a single commit. Can you add a new commit to the PR with `git commit -m ""bump"" --allow-empty`?; Sorry!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6937#issuecomment-524832178
https://github.com/hail-is/hail/pull/6937#issuecomment-524832178:105,Testability,test,tests,105,"We have a CI problem leading to random erroneous test failures, and the system isn't designed to run the tests twice for a single commit. Can you add a new commit to the PR with `git commit -m ""bump"" --allow-empty`?; Sorry!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6937#issuecomment-524832178
https://github.com/hail-is/hail/pull/6940#issuecomment-524914202:55,Availability,avail,available,55,I also checked and made sure the `$(dir)` function was available on make 3.81.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6940#issuecomment-524914202
https://github.com/hail-is/hail/pull/6942#issuecomment-525928431:22,Testability,benchmark,benchmark,22,"oh damn!. ```python; @benchmark; def table_aggregate_take_by_strings():; ht = hl.read_table(resource('many_strings_table.ht')); ht.aggregate(hl.tuple([hl.agg.take(ht['f18'], 25, ordering=ht[f'f{i}']) for i in range(12)])); ```. master:; ```; [1/1] Running table_aggregate_take_by...; run 1: 13.09; run 2: 13.19; run 3: 12.18; run 4: 11.57; run 5: 11.48; table_aggregate_take_by	12.302	12.181	0.725; ```. PR:; ```; [1/1] Running table_aggregate_take_by...; run 1: 4.84; run 2: 3.97; run 3: 3.81; run 4: 4.23; run 5: 4.12; table_aggregate_take_by	4.194	4.123	0.351; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-525928431
https://github.com/hail-is/hail/pull/6942#issuecomment-525946036:26,Testability,test,test,26,I added a separate stress test of the aggregator state.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-525946036
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:203,Deployability,configurat,configurations,203,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:1281,Deployability,configurat,configuration,1281,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:203,Modifiability,config,configurations,203,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:1281,Modifiability,config,configuration,1281,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:763,Safety,sanity check,sanity checking,763,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:73,Testability,test,test,73,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:268,Testability,test,test,268,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:306,Testability,test,test,306,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:355,Testability,test,tested,355,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:483,Testability,test,test,483,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:586,Testability,test,test,586,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:690,Testability,test,tests,690,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:837,Testability,test,test,837,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:876,Testability,test,test,876,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:1000,Testability,test,test,1000,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:298,Usability,simpl,simpler,298,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:579,Usability,simpl,simple,579,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530
https://github.com/hail-is/hail/pull/6943#issuecomment-525037589:42,Energy Efficiency,reduce,reduce,42,"I'm confused by the stack depth problem. `reduce` isn't recursive, it forwards to `reduceLeft`:; ```scala; def reduceLeft[B >: A](op: (B, A) => B): B = {; if (isEmpty); throw new UnsupportedOperationException(""empty.reduceLeft""). var first = true; var acc: B = 0.asInstanceOf[B]. for (x <- self) {; if (first) {; acc = x; first = false; }; else acc = op(acc, x); }; acc; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6943#issuecomment-525037589
https://github.com/hail-is/hail/pull/6943#issuecomment-525037589:83,Energy Efficiency,reduce,reduceLeft,83,"I'm confused by the stack depth problem. `reduce` isn't recursive, it forwards to `reduceLeft`:; ```scala; def reduceLeft[B >: A](op: (B, A) => B): B = {; if (isEmpty); throw new UnsupportedOperationException(""empty.reduceLeft""). var first = true; var acc: B = 0.asInstanceOf[B]. for (x <- self) {; if (first) {; acc = x; first = false; }; else acc = op(acc, x); }; acc; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6943#issuecomment-525037589
https://github.com/hail-is/hail/pull/6943#issuecomment-525037589:111,Energy Efficiency,reduce,reduceLeft,111,"I'm confused by the stack depth problem. `reduce` isn't recursive, it forwards to `reduceLeft`:; ```scala; def reduceLeft[B >: A](op: (B, A) => B): B = {; if (isEmpty); throw new UnsupportedOperationException(""empty.reduceLeft""). var first = true; var acc: B = 0.asInstanceOf[B]. for (x <- self) {; if (first) {; acc = x; first = false; }; else acc = op(acc, x); }; acc; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6943#issuecomment-525037589
https://github.com/hail-is/hail/pull/6943#issuecomment-525037589:216,Energy Efficiency,reduce,reduceLeft,216,"I'm confused by the stack depth problem. `reduce` isn't recursive, it forwards to `reduceLeft`:; ```scala; def reduceLeft[B >: A](op: (B, A) => B): B = {; if (isEmpty); throw new UnsupportedOperationException(""empty.reduceLeft""). var first = true; var acc: B = 0.asInstanceOf[B]. for (x <- self) {; if (first) {; acc = x; first = false; }; else acc = op(acc, x); }; acc; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6943#issuecomment-525037589
https://github.com/hail-is/hail/pull/6947#issuecomment-525030201:85,Availability,redundant,redundant,85,@jigold sorry I pushed a change literally as you approved it! It was just removing a redundant cast of `this.asInstanceOf[ReferenceGenome]` that was called on a ReferenceGenome object.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6947#issuecomment-525030201
https://github.com/hail-is/hail/pull/6947#issuecomment-525030201:85,Safety,redund,redundant,85,@jigold sorry I pushed a change literally as you approved it! It was just removing a redundant cast of `this.asInstanceOf[ReferenceGenome]` that was called on a ReferenceGenome object.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6947#issuecomment-525030201
https://github.com/hail-is/hail/issues/6952#issuecomment-526639135:92,Performance,Load,Load,92,"Got it, will make sure next time I put prio:high label, the work gets done within <=2 days. Load balancing with other work, and will find a better balance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6952#issuecomment-526639135
https://github.com/hail-is/hail/issues/6952#issuecomment-573314993:132,Performance,perform,performing,132,"Once the PR's for Coalesce and Die go in, the only remaining item for the InferPType pass is CastRename...then I think we can start performing this pass during compilation/interpretation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6952#issuecomment-573314993
https://github.com/hail-is/hail/pull/6956#issuecomment-525798414:14,Availability,echo,echo,14,"FWIW:; ```sh; echo 'this is \; a test'; this is \; a test; # vs; echo ""this is \; a test""; this is a test; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525798414
https://github.com/hail-is/hail/pull/6956#issuecomment-525798414:65,Availability,echo,echo,65,"FWIW:; ```sh; echo 'this is \; a test'; this is \; a test; # vs; echo ""this is \; a test""; this is a test; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525798414
https://github.com/hail-is/hail/pull/6956#issuecomment-525798414:33,Testability,test,test,33,"FWIW:; ```sh; echo 'this is \; a test'; this is \; a test; # vs; echo ""this is \; a test""; this is a test; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525798414
https://github.com/hail-is/hail/pull/6956#issuecomment-525798414:53,Testability,test,test,53,"FWIW:; ```sh; echo 'this is \; a test'; this is \; a test; # vs; echo ""this is \; a test""; this is a test; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525798414
https://github.com/hail-is/hail/pull/6956#issuecomment-525798414:84,Testability,test,test,84,"FWIW:; ```sh; echo 'this is \; a test'; this is \; a test; # vs; echo ""this is \; a test""; this is a test; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525798414
https://github.com/hail-is/hail/pull/6956#issuecomment-525798414:101,Testability,test,test,101,"FWIW:; ```sh; echo 'this is \; a test'; this is \; a test; # vs; echo ""this is \; a test""; this is a test; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525798414
https://github.com/hail-is/hail/pull/6956#issuecomment-525801961:110,Availability,echo,echo,110,"Ah, my Make is interpreting the slashes it self, which is what I would have expected from Make. . ```; # make echo; echo 'hello \; foo'; hello foo; # echo 'hello \; quote> foo'; hello \; foo; # cat Makefile; echo:; 	echo 'hello \; foo'. # ; ```. I guess newer GNU Make doesn't interpret the slashes before sending it to sh?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525801961
https://github.com/hail-is/hail/pull/6956#issuecomment-525801961:116,Availability,echo,echo,116,"Ah, my Make is interpreting the slashes it self, which is what I would have expected from Make. . ```; # make echo; echo 'hello \; foo'; hello foo; # echo 'hello \; quote> foo'; hello \; foo; # cat Makefile; echo:; 	echo 'hello \; foo'. # ; ```. I guess newer GNU Make doesn't interpret the slashes before sending it to sh?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525801961
https://github.com/hail-is/hail/pull/6956#issuecomment-525801961:150,Availability,echo,echo,150,"Ah, my Make is interpreting the slashes it self, which is what I would have expected from Make. . ```; # make echo; echo 'hello \; foo'; hello foo; # echo 'hello \; quote> foo'; hello \; foo; # cat Makefile; echo:; 	echo 'hello \; foo'. # ; ```. I guess newer GNU Make doesn't interpret the slashes before sending it to sh?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525801961
https://github.com/hail-is/hail/pull/6956#issuecomment-525801961:208,Availability,echo,echo,208,"Ah, my Make is interpreting the slashes it self, which is what I would have expected from Make. . ```; # make echo; echo 'hello \; foo'; hello foo; # echo 'hello \; quote> foo'; hello \; foo; # cat Makefile; echo:; 	echo 'hello \; foo'. # ; ```. I guess newer GNU Make doesn't interpret the slashes before sending it to sh?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525801961
https://github.com/hail-is/hail/pull/6956#issuecomment-525801961:216,Availability,echo,echo,216,"Ah, my Make is interpreting the slashes it self, which is what I would have expected from Make. . ```; # make echo; echo 'hello \; foo'; hello foo; # echo 'hello \; quote> foo'; hello \; foo; # cat Makefile; echo:; 	echo 'hello \; foo'. # ; ```. I guess newer GNU Make doesn't interpret the slashes before sending it to sh?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6956#issuecomment-525801961
https://github.com/hail-is/hail/pull/6960#issuecomment-526828725:827,Performance,perform,performance,827,"master:; ```; 2019-08-31 07:54:03,500: INFO: [1/1] Running import_bgen_info_score...; 2019-08-31 07:55:45,687: INFO: burn in: 102.19s; 2019-08-31 07:57:25,618: INFO: run 1: 99.93s; 2019-08-31 07:59:05,480: INFO: run 2: 99.86s; 2019-08-31 08:00:45,759: INFO: run 3: 100.28s; ```. PR:; ```; 2019-08-31 07:14:26,080: INFO: [1/1] Running import_bgen_info_score...; 2019-08-31 07:16:14,620: INFO: burn in: 108.54s; 2019-08-31 07:17:58,944: INFO: run 1: 104.32s; 2019-08-31 07:19:42,457: INFO: run 2: 103.51s; 2019-08-31 07:21:25,404: INFO: run 3: 102.95s; ```. Since just importing and force-counting the BGEN takes about 80s, thisi PR is ~10-15% slower than master. Note that this is an `annotate_rows` aggregation which uses **old** aggs, so if this doesn't get faster when we switch to new aggs, we should be concerned about the performance of the primitive aggregators (sum, etc).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6960#issuecomment-526828725
https://github.com/hail-is/hail/issues/6963#issuecomment-563994619:129,Availability,error,error-prone,129,"Sorry this got missed! We should have responded, at least. This is generally intended -- scientific notation is one of the least error-prone way to represent floating-point values, and the format we use is a standard one that most tools should handle. However, we do intend to expose an option to parameterize the format of floating point values in export_vcf (though scientific notation will probably always be the default).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6963#issuecomment-563994619
https://github.com/hail-is/hail/issues/6963#issuecomment-563994619:297,Modifiability,parameteriz,parameterize,297,"Sorry this got missed! We should have responded, at least. This is generally intended -- scientific notation is one of the least error-prone way to represent floating-point values, and the format we use is a standard one that most tools should handle. However, we do intend to expose an option to parameterize the format of floating point values in export_vcf (though scientific notation will probably always be the default).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6963#issuecomment-563994619
https://github.com/hail-is/hail/issues/6963#issuecomment-563994619:277,Security,expose,expose,277,"Sorry this got missed! We should have responded, at least. This is generally intended -- scientific notation is one of the least error-prone way to represent floating-point values, and the format we use is a standard one that most tools should handle. However, we do intend to expose an option to parameterize the format of floating point values in export_vcf (though scientific notation will probably always be the default).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/6963#issuecomment-563994619
https://github.com/hail-is/hail/pull/6971#issuecomment-526604943:66,Performance,perform,performance,66,can we merge this after #6969 goes in? That resolves a pretty big performance regression in stats/mean/corr/hist aggregators.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-526604943
https://github.com/hail-is/hail/pull/6971#issuecomment-526623710:23,Availability,ping,ping,23,"I'm good with waiting, ping here when ready",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-526623710
https://github.com/hail-is/hail/pull/6971#issuecomment-527601168:73,Deployability,release,release,73,Arcturus has been without internet and unable to review #6969 - OK if we release tomorrow?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527601168
https://github.com/hail-is/hail/pull/6971#issuecomment-527603482:21,Deployability,release,release,21,"actually, let's just release now. We can make another release at the end of the week -- there are a couple of major performance improvements queued up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527603482
https://github.com/hail-is/hail/pull/6971#issuecomment-527603482:54,Deployability,release,release,54,"actually, let's just release now. We can make another release at the end of the week -- there are a couple of major performance improvements queued up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527603482
https://github.com/hail-is/hail/pull/6971#issuecomment-527603482:116,Performance,perform,performance,116,"actually, let's just release now. We can make another release at the end of the week -- there are a couple of major performance improvements queued up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527603482
https://github.com/hail-is/hail/pull/6971#issuecomment-527603482:141,Performance,queue,queued,141,"actually, let's just release now. We can make another release at the end of the week -- there are a couple of major performance improvements queued up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527603482
https://github.com/hail-is/hail/pull/6971#issuecomment-527603770:44,Deployability,release,release,44,"oops I did tell Beryl we'd get #6978 in the release, though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527603770
https://github.com/hail-is/hail/pull/6971#issuecomment-527604266:91,Deployability,update,update,91,This branch is up to date with master's `69ba846a3`. Let me know when #6978 lands and I'll update again. Maybe nudge @iitalics to review that?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527604266
https://github.com/hail-is/hail/pull/6973#issuecomment-526692251:110,Deployability,deploy,deploys,110,"I'm good with this, I'll approve whenever you're ready to handle CI maybe breaking. Don't want to mess up dev deploys somehow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6973#issuecomment-526692251
https://github.com/hail-is/hail/pull/6973#issuecomment-526698057:50,Deployability,deploy,deployed,50,"@johnc1231 You can approve. It's actually already deployed, I did that to test it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6973#issuecomment-526698057
https://github.com/hail-is/hail/pull/6973#issuecomment-526698057:74,Testability,test,test,74,"@johnc1231 You can approve. It's actually already deployed, I did that to test it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6973#issuecomment-526698057
https://github.com/hail-is/hail/pull/6980#issuecomment-527418027:97,Performance,perform,performance,97,"This is almost all coming from the Python change. the gqFromPL change has a negligible effect on performance, but still seems good to include.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6980#issuecomment-527418027
https://github.com/hail-is/hail/pull/6980#issuecomment-527418968:31,Usability,simpl,simple,31,more improvements (though less simple) to come.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6980#issuecomment-527418968
https://github.com/hail-is/hail/pull/6981#issuecomment-527467578:44,Testability,Benchmark,Benchmark,44,"Ported `ArrayFunctions.mean` to ArrayFold2. Benchmark:; ```python; @benchmark; def table_range_means():; ht = hl.utils.range_table(10_000_000, 16); ht = ht.annotate(m = hl.mean(hl.range(0, ht.idx % 1111))); ht._force_count(); ```. Master:; ```; 2019-09-03 09:39:05,777: INFO: [1/1] Running table_range_means...; 2019-09-03 09:40:52,557: INFO: burn in: 106.78s; 2019-09-03 09:42:34,333: INFO: run 1: 101.78s; 2019-09-03 09:44:14,982: INFO: run 2: 100.65s; 2019-09-03 09:45:53,590: INFO: run 3: 98.61s; ```. PR:; ```; 2019-09-03 09:47:26,110: INFO: [1/1] Running table_range_means...; 2019-09-03 09:47:29,465: INFO: burn in: 3.35s; 2019-09-03 09:47:32,615: INFO: run 1: 3.15s; 2019-09-03 09:47:35,703: INFO: run 2: 3.09s; 2019-09-03 09:47:38,840: INFO: run 3: 3.14s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6981#issuecomment-527467578
https://github.com/hail-is/hail/pull/6981#issuecomment-527467578:68,Testability,benchmark,benchmark,68,"Ported `ArrayFunctions.mean` to ArrayFold2. Benchmark:; ```python; @benchmark; def table_range_means():; ht = hl.utils.range_table(10_000_000, 16); ht = ht.annotate(m = hl.mean(hl.range(0, ht.idx % 1111))); ht._force_count(); ```. Master:; ```; 2019-09-03 09:39:05,777: INFO: [1/1] Running table_range_means...; 2019-09-03 09:40:52,557: INFO: burn in: 106.78s; 2019-09-03 09:42:34,333: INFO: run 1: 101.78s; 2019-09-03 09:44:14,982: INFO: run 2: 100.65s; 2019-09-03 09:45:53,590: INFO: run 3: 98.61s; ```. PR:; ```; 2019-09-03 09:47:26,110: INFO: [1/1] Running table_range_means...; 2019-09-03 09:47:29,465: INFO: burn in: 3.35s; 2019-09-03 09:47:32,615: INFO: run 1: 3.15s; 2019-09-03 09:47:35,703: INFO: run 2: 3.09s; 2019-09-03 09:47:38,840: INFO: run 3: 3.14s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6981#issuecomment-527467578
https://github.com/hail-is/hail/pull/6984#issuecomment-527605115:53,Availability,error,error,53,I feel like rebinding an argument should be a syntax error in scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6984#issuecomment-527605115
https://github.com/hail-is/hail/pull/6984#issuecomment-527605644:55,Availability,error,error,55,> I feel like rebinding an argument should be a syntax error in scala. It certainly should warrant some big red intellij squiggles.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6984#issuecomment-527605644
https://github.com/hail-is/hail/pull/6987#issuecomment-529001365:521,Availability,error,error,521,"@chrisvittal I took the plunge and also created a `MatrixHybridReader`. Changes (including above):. - FunctionBuilder now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists; - `TextMatrixReader` exists which mimics `TextTableReader`; these should get unified at some point; - minor documentation fixes to `import_matrix_table`; - better error messages wrt using the name `row_id`, which is reserved for use by `import_matrix_table` when there are no keys specified; - several new tests, including:; - extensive testing of the product space of `header`, `delimiter`, `header`, and `entry_type` (including such weird things as using `9` as the missing value); - a pathological file: `9`-separated values with `8` representing the missing value; - several tests that trigger the pruner; - rename `LoadMatrix.scala` to `TextMatrixReader.scala`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365
https://github.com/hail-is/hail/pull/6987#issuecomment-529001365:527,Integrability,message,messages,527,"@chrisvittal I took the plunge and also created a `MatrixHybridReader`. Changes (including above):. - FunctionBuilder now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists; - `TextMatrixReader` exists which mimics `TextTableReader`; these should get unified at some point; - minor documentation fixes to `import_matrix_table`; - better error messages wrt using the name `row_id`, which is reserved for use by `import_matrix_table` when there are no keys specified; - several new tests, including:; - extensive testing of the product space of `header`, `delimiter`, `header`, and `entry_type` (including such weird things as using `9` as the missing value); - a pathological file: `9`-separated values with `8` representing the missing value; - several tests that trigger the pruner; - rename `LoadMatrix.scala` to `TextMatrixReader.scala`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365
https://github.com/hail-is/hail/pull/6987#issuecomment-529001365:978,Performance,Load,LoadMatrix,978,"@chrisvittal I took the plunge and also created a `MatrixHybridReader`. Changes (including above):. - FunctionBuilder now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists; - `TextMatrixReader` exists which mimics `TextTableReader`; these should get unified at some point; - minor documentation fixes to `import_matrix_table`; - better error messages wrt using the name `row_id`, which is reserved for use by `import_matrix_table` when there are no keys specified; - several new tests, including:; - extensive testing of the product space of `header`, `delimiter`, `header`, and `entry_type` (including such weird things as using `9` as the missing value); - a pathological file: `9`-separated values with `8` representing the missing value; - several tests that trigger the pruner; - rename `LoadMatrix.scala` to `TextMatrixReader.scala`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365
https://github.com/hail-is/hail/pull/6987#issuecomment-529001365:664,Testability,test,tests,664,"@chrisvittal I took the plunge and also created a `MatrixHybridReader`. Changes (including above):. - FunctionBuilder now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists; - `TextMatrixReader` exists which mimics `TextTableReader`; these should get unified at some point; - minor documentation fixes to `import_matrix_table`; - better error messages wrt using the name `row_id`, which is reserved for use by `import_matrix_table` when there are no keys specified; - several new tests, including:; - extensive testing of the product space of `header`, `delimiter`, `header`, and `entry_type` (including such weird things as using `9` as the missing value); - a pathological file: `9`-separated values with `8` representing the missing value; - several tests that trigger the pruner; - rename `LoadMatrix.scala` to `TextMatrixReader.scala`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365
https://github.com/hail-is/hail/pull/6987#issuecomment-529001365:695,Testability,test,testing,695,"@chrisvittal I took the plunge and also created a `MatrixHybridReader`. Changes (including above):. - FunctionBuilder now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists; - `TextMatrixReader` exists which mimics `TextTableReader`; these should get unified at some point; - minor documentation fixes to `import_matrix_table`; - better error messages wrt using the name `row_id`, which is reserved for use by `import_matrix_table` when there are no keys specified; - several new tests, including:; - extensive testing of the product space of `header`, `delimiter`, `header`, and `entry_type` (including such weird things as using `9` as the missing value); - a pathological file: `9`-separated values with `8` representing the missing value; - several tests that trigger the pruner; - rename `LoadMatrix.scala` to `TextMatrixReader.scala`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365
https://github.com/hail-is/hail/pull/6987#issuecomment-529001365:937,Testability,test,tests,937,"@chrisvittal I took the plunge and also created a `MatrixHybridReader`. Changes (including above):. - FunctionBuilder now accepts `Code[Unit]` to be added to the `init` method of the function object; - SRVB now has an `init` method that should be called in the `init` method of a function object if many methods will share the SRVB; - `CodeChar` now exists; - `TextMatrixReader` exists which mimics `TextTableReader`; these should get unified at some point; - minor documentation fixes to `import_matrix_table`; - better error messages wrt using the name `row_id`, which is reserved for use by `import_matrix_table` when there are no keys specified; - several new tests, including:; - extensive testing of the product space of `header`, `delimiter`, `header`, and `entry_type` (including such weird things as using `9` as the missing value); - a pathological file: `9`-separated values with `8` representing the missing value; - several tests that trigger the pruner; - rename `LoadMatrix.scala` to `TextMatrixReader.scala`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6987#issuecomment-529001365
https://github.com/hail-is/hail/pull/6990#issuecomment-530868509:16,Deployability,integrat,integration,16,"Tim, I left the integration tests for now. I propose if you want them out, that we leave 2 cases for each of the type-combinations, so that we can inductively prove that our code can infer the correct unified type across nested IR (without the Ref shortcut)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6990#issuecomment-530868509
https://github.com/hail-is/hail/pull/6990#issuecomment-530868509:16,Integrability,integrat,integration,16,"Tim, I left the integration tests for now. I propose if you want them out, that we leave 2 cases for each of the type-combinations, so that we can inductively prove that our code can infer the correct unified type across nested IR (without the Ref shortcut)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6990#issuecomment-530868509
https://github.com/hail-is/hail/pull/6990#issuecomment-530868509:28,Testability,test,tests,28,"Tim, I left the integration tests for now. I propose if you want them out, that we leave 2 cases for each of the type-combinations, so that we can inductively prove that our code can infer the correct unified type across nested IR (without the Ref shortcut)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6990#issuecomment-530868509
https://github.com/hail-is/hail/pull/6990#issuecomment-532417330:30,Testability,test,tests,30,"@tpoterba did a quick pass of tests. Goal for the testGetNestedElement* was to test the minimal set that would prove correctness: 1) base case with 1 element, 2) collection of elements, 3) collection of nested elements, since from this we can deduce that we can handle any nested depth (although we cannot guarantee this; the tests don't account for stopping conditions by depth)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6990#issuecomment-532417330
https://github.com/hail-is/hail/pull/6990#issuecomment-532417330:50,Testability,test,testGetNestedElement,50,"@tpoterba did a quick pass of tests. Goal for the testGetNestedElement* was to test the minimal set that would prove correctness: 1) base case with 1 element, 2) collection of elements, 3) collection of nested elements, since from this we can deduce that we can handle any nested depth (although we cannot guarantee this; the tests don't account for stopping conditions by depth)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6990#issuecomment-532417330
https://github.com/hail-is/hail/pull/6990#issuecomment-532417330:79,Testability,test,test,79,"@tpoterba did a quick pass of tests. Goal for the testGetNestedElement* was to test the minimal set that would prove correctness: 1) base case with 1 element, 2) collection of elements, 3) collection of nested elements, since from this we can deduce that we can handle any nested depth (although we cannot guarantee this; the tests don't account for stopping conditions by depth)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6990#issuecomment-532417330
https://github.com/hail-is/hail/pull/6990#issuecomment-532417330:326,Testability,test,tests,326,"@tpoterba did a quick pass of tests. Goal for the testGetNestedElement* was to test the minimal set that would prove correctness: 1) base case with 1 element, 2) collection of elements, 3) collection of nested elements, since from this we can deduce that we can handle any nested depth (although we cannot guarantee this; the tests don't account for stopping conditions by depth)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6990#issuecomment-532417330
https://github.com/hail-is/hail/pull/6991#issuecomment-528461237:17,Deployability,deploy,deploy,17,"I needed this to deploy 0.2.21. `make deploy` relies on the docs having been deployed (by CI) but there's no guarantee that happens -- we went a week without any successful deploys of master commits, for instance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6991#issuecomment-528461237
https://github.com/hail-is/hail/pull/6991#issuecomment-528461237:38,Deployability,deploy,deploy,38,"I needed this to deploy 0.2.21. `make deploy` relies on the docs having been deployed (by CI) but there's no guarantee that happens -- we went a week without any successful deploys of master commits, for instance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6991#issuecomment-528461237
https://github.com/hail-is/hail/pull/6991#issuecomment-528461237:77,Deployability,deploy,deployed,77,"I needed this to deploy 0.2.21. `make deploy` relies on the docs having been deployed (by CI) but there's no guarantee that happens -- we went a week without any successful deploys of master commits, for instance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6991#issuecomment-528461237
https://github.com/hail-is/hail/pull/6991#issuecomment-528461237:173,Deployability,deploy,deploys,173,"I needed this to deploy 0.2.21. `make deploy` relies on the docs having been deployed (by CI) but there's no guarantee that happens -- we went a week without any successful deploys of master commits, for instance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6991#issuecomment-528461237
https://github.com/hail-is/hail/pull/6996#issuecomment-528542699:6,Modifiability,parameteriz,parameterize,6,could parameterize this to not run if running in no-test mode:; https://github.com/hail-is/hail/blob/836baa3604ab05fb6a603cfce80c94e55e988a43/hail/python/hail/docs/Makefile#L53,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6996#issuecomment-528542699
https://github.com/hail-is/hail/pull/6996#issuecomment-528542699:52,Testability,test,test,52,could parameterize this to not run if running in no-test mode:; https://github.com/hail-is/hail/blob/836baa3604ab05fb6a603cfce80c94e55e988a43/hail/python/hail/docs/Makefile#L53,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6996#issuecomment-528542699
https://github.com/hail-is/hail/pull/6996#issuecomment-530058611:67,Deployability,install,installed,67,"wait, I take back my earlier comment -- of course hail needs to be installed to build the docs. The docs are built by navigating the module hierarchy and docstrings of the installed Hail module.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6996#issuecomment-530058611
https://github.com/hail-is/hail/pull/6996#issuecomment-530058611:172,Deployability,install,installed,172,"wait, I take back my earlier comment -- of course hail needs to be installed to build the docs. The docs are built by navigating the module hierarchy and docstrings of the installed Hail module.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/6996#issuecomment-530058611
https://github.com/hail-is/hail/pull/7002#issuecomment-528628851:35,Testability,test,tests,35,Maybe just comment out the ndarray tests? I'm going to use them in near future,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7002#issuecomment-528628851
https://github.com/hail-is/hail/pull/7002#issuecomment-528642262:119,Integrability,message,message,119,"The python ones. I didn't actually read the diff because I was on my phone, just saw ""remove ndarraywrite tesr"" commit message. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7002#issuecomment-528642262
https://github.com/hail-is/hail/pull/7004#issuecomment-528714248:7,Testability,log,log,7,Commit log is the description of what changed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7004#issuecomment-528714248
https://github.com/hail-is/hail/issues/7008#issuecomment-529008947:88,Deployability,configurat,configuration,88,"Better place to post things like this would be discuss.hail.is (because it's probably a configuration issue with your cluster and not a bug in hail). I'd guess you don't have BLAS installed on your cluster. If you check the hail.log file, do you have lines like. ```; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; ```. ?. See here: https://hail.is/docs/0.2/getting_started.html#common-installation-issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947
https://github.com/hail-is/hail/issues/7008#issuecomment-529008947:180,Deployability,install,installed,180,"Better place to post things like this would be discuss.hail.is (because it's probably a configuration issue with your cluster and not a bug in hail). I'd guess you don't have BLAS installed on your cluster. If you check the hail.log file, do you have lines like. ```; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; ```. ?. See here: https://hail.is/docs/0.2/getting_started.html#common-installation-issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947
https://github.com/hail-is/hail/issues/7008#issuecomment-529008947:499,Deployability,install,installation-issues,499,"Better place to post things like this would be discuss.hail.is (because it's probably a configuration issue with your cluster and not a bug in hail). I'd guess you don't have BLAS installed on your cluster. If you check the hail.log file, do you have lines like. ```; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; ```. ?. See here: https://hail.is/docs/0.2/getting_started.html#common-installation-issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947
https://github.com/hail-is/hail/issues/7008#issuecomment-529008947:88,Modifiability,config,configuration,88,"Better place to post things like this would be discuss.hail.is (because it's probably a configuration issue with your cluster and not a bug in hail). I'd guess you don't have BLAS installed on your cluster. If you check the hail.log file, do you have lines like. ```; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; ```. ?. See here: https://hail.is/docs/0.2/getting_started.html#common-installation-issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947
https://github.com/hail-is/hail/issues/7008#issuecomment-529008947:278,Performance,load,load,278,"Better place to post things like this would be discuss.hail.is (because it's probably a configuration issue with your cluster and not a bug in hail). I'd guess you don't have BLAS installed on your cluster. If you check the hail.log file, do you have lines like. ```; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; ```. ?. See here: https://hail.is/docs/0.2/getting_started.html#common-installation-issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947
https://github.com/hail-is/hail/issues/7008#issuecomment-529008947:359,Performance,load,load,359,"Better place to post things like this would be discuss.hail.is (because it's probably a configuration issue with your cluster and not a bug in hail). I'd guess you don't have BLAS installed on your cluster. If you check the hail.log file, do you have lines like. ```; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; ```. ?. See here: https://hail.is/docs/0.2/getting_started.html#common-installation-issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947
https://github.com/hail-is/hail/issues/7008#issuecomment-529008947:229,Testability,log,log,229,"Better place to post things like this would be discuss.hail.is (because it's probably a configuration issue with your cluster and not a bug in hail). I'd guess you don't have BLAS installed on your cluster. If you check the hail.log file, do you have lines like. ```; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; ```. ?. See here: https://hail.is/docs/0.2/getting_started.html#common-installation-issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947
https://github.com/hail-is/hail/issues/7008#issuecomment-529740667:17,Deployability,install,installed,17,"BLAS is actually installed in /usr/lib64/atlas. Spark was not finding the lib for some reason. ; ; The solution was to add the following config to the spark-submit command line. --conf spark.executor.extraClassPath=""/usr/lib64/atlas"" . It would be useful to add this to the documentation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529740667
https://github.com/hail-is/hail/issues/7008#issuecomment-529740667:137,Modifiability,config,config,137,"BLAS is actually installed in /usr/lib64/atlas. Spark was not finding the lib for some reason. ; ; The solution was to add the following config to the spark-submit command line. --conf spark.executor.extraClassPath=""/usr/lib64/atlas"" . It would be useful to add this to the documentation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529740667
https://github.com/hail-is/hail/pull/7009#issuecomment-531305458:36,Testability,test,tests,36,will look again with high prio when tests are passing,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7009#issuecomment-531305458
https://github.com/hail-is/hail/pull/7009#issuecomment-531331340:107,Modifiability,variab,variables,107,"I made `define_function` go through the CSE path, and it broke because CSE was assuming an IR with no free variables. I fixed CSE to take a list of free variables.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7009#issuecomment-531331340
https://github.com/hail-is/hail/pull/7009#issuecomment-531331340:153,Modifiability,variab,variables,153,"I made `define_function` go through the CSE path, and it broke because CSE was assuming an IR with no free variables. I fixed CSE to take a list of free variables.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7009#issuecomment-531331340
https://github.com/hail-is/hail/pull/7015#issuecomment-539530177:51,Integrability,rout,routing,51,This should use distinct server blocks rather than routing based on the URL.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539530177
https://github.com/hail-is/hail/pull/7015#issuecomment-539818744:19,Deployability,deploy,deploy,19,Got blocked on dev deploy not working (500). Will try again in the morning.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539818744
https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:3,Availability,error,error,3,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584
https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:103,Availability,error,error,103,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584
https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:172,Availability,error,error,172,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584
https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:134,Deployability,deploy,deploy,134,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584
https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:249,Deployability,deploy,deploy,249,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584
https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:256,Energy Efficiency,monitor,monitoring,256,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584
https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:225,Testability,log,log,225,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584
https://github.com/hail-is/hail/pull/7015#issuecomment-540028930:5,Availability,error,error,5,"> So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. I should fix it!. edit: Yeah, I could see it in the logs, I just didn't understand some of the syntax, so dug around CI and deploy codebase, figured it out. Monitoring wasn't expected, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930
https://github.com/hail-is/hail/pull/7015#issuecomment-540028930:105,Availability,error,error,105,"> So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. I should fix it!. edit: Yeah, I could see it in the logs, I just didn't understand some of the syntax, so dug around CI and deploy codebase, figured it out. Monitoring wasn't expected, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930
https://github.com/hail-is/hail/pull/7015#issuecomment-540028930:174,Availability,error,error,174,"> So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. I should fix it!. edit: Yeah, I could see it in the logs, I just didn't understand some of the syntax, so dug around CI and deploy codebase, figured it out. Monitoring wasn't expected, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930
https://github.com/hail-is/hail/pull/7015#issuecomment-540028930:136,Deployability,deploy,deploy,136,"> So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. I should fix it!. edit: Yeah, I could see it in the logs, I just didn't understand some of the syntax, so dug around CI and deploy codebase, figured it out. Monitoring wasn't expected, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930
https://github.com/hail-is/hail/pull/7015#issuecomment-540028930:356,Deployability,deploy,deploy,356,"> So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. I should fix it!. edit: Yeah, I could see it in the logs, I just didn't understand some of the syntax, so dug around CI and deploy codebase, figured it out. Monitoring wasn't expected, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930
https://github.com/hail-is/hail/pull/7015#issuecomment-540028930:389,Energy Efficiency,Monitor,Monitoring,389,"> So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. I should fix it!. edit: Yeah, I could see it in the logs, I just didn't understand some of the syntax, so dug around CI and deploy codebase, figured it out. Monitoring wasn't expected, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930
https://github.com/hail-is/hail/pull/7015#issuecomment-540028930:227,Testability,log,log,227,"> So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. I should fix it!. edit: Yeah, I could see it in the logs, I just didn't understand some of the syntax, so dug around CI and deploy codebase, figured it out. Monitoring wasn't expected, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930
https://github.com/hail-is/hail/pull/7015#issuecomment-540028930:284,Testability,log,logs,284,"> So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. I should fix it!. edit: Yeah, I could see it in the logs, I just didn't understand some of the syntax, so dug around CI and deploy codebase, figured it out. Monitoring wasn't expected, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930
https://github.com/hail-is/hail/pull/7015#issuecomment-540031541:544,Availability,down,down,544,"> I should fix it!. That'd be awesome!. Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541
https://github.com/hail-is/hail/pull/7015#issuecomment-540031541:145,Deployability,deploy,deployed,145,"> I should fix it!. That'd be awesome!. Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541
https://github.com/hail-is/hail/pull/7015#issuecomment-540031541:302,Testability,test,tested,302,"> I should fix it!. That'd be awesome!. Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541
https://github.com/hail-is/hail/pull/7015#issuecomment-540031541:437,Testability,test,testing,437,"> I should fix it!. That'd be awesome!. Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541
https://github.com/hail-is/hail/pull/7015#issuecomment-540031541:492,Testability,test,testing,492,"> I should fix it!. That'd be awesome!. Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541
https://github.com/hail-is/hail/pull/7015#issuecomment-540031541:553,Testability,test,test,553,"> I should fix it!. That'd be awesome!. Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541
https://github.com/hail-is/hail/pull/7015#issuecomment-540035311:558,Availability,down,down,558,"> > I should fix it!; > ; > That'd be awesome!; > ; > Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet. Thank you for the background. Makes sense",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311
https://github.com/hail-is/hail/pull/7015#issuecomment-540035311:159,Deployability,deploy,deployed,159,"> > I should fix it!; > ; > That'd be awesome!; > ; > Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet. Thank you for the background. Makes sense",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311
https://github.com/hail-is/hail/pull/7015#issuecomment-540035311:316,Testability,test,tested,316,"> > I should fix it!; > ; > That'd be awesome!; > ; > Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet. Thank you for the background. Makes sense",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311
https://github.com/hail-is/hail/pull/7015#issuecomment-540035311:451,Testability,test,testing,451,"> > I should fix it!; > ; > That'd be awesome!; > ; > Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet. Thank you for the background. Makes sense",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311
https://github.com/hail-is/hail/pull/7015#issuecomment-540035311:506,Testability,test,testing,506,"> > I should fix it!; > ; > That'd be awesome!; > ; > Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet. Thank you for the background. Makes sense",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311
https://github.com/hail-is/hail/pull/7015#issuecomment-540035311:567,Testability,test,test,567,"> > I should fix it!; > ; > That'd be awesome!; > ; > Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet. Thank you for the background. Makes sense",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:1396,Deployability,deploy,deployed,1396,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:499,Energy Efficiency,monitor,monitoring,499,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:561,Energy Efficiency,monitor,monitoring,561,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:665,Energy Efficiency,monitor,monitoring,665,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:728,Energy Efficiency,monitor,monitoring,728,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:773,Energy Efficiency,monitor,monitoring,773,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:830,Energy Efficiency,monitor,monitoring,830,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:889,Energy Efficiency,monitor,monitoring,889,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:510,Integrability,rout,router,510,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:534,Integrability,rout,router-,534,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:685,Integrability,rout,routes,685,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:1154,Integrability,rout,routes,1154,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:759,Modifiability,rewrite,rewrite,759,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:866,Modifiability,rewrite,rewrite,866,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:880,Modifiability,rewrite,rewrite,880,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:1165,Modifiability,rewrite,rewrite,1165,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:117,Testability,log,logs,117,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:529,Testability,log,logs,529,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516
https://github.com/hail-is/hail/pull/7015#issuecomment-540622757:74,Energy Efficiency,monitor,monitoring,74,"Here is an example of the trailing slash issue:. What kibana sees:. ""GET /monitoring/kibana/ui/fonts/inter_ui/Inter-UI-Bold.woff2 HTTP/1.1"" 200 94840 ""https://internal.hail.is/monitoring/kibana/app/kibana"". config:. ```; location /monitoring/kibana/ {; proxy_pass http://kibana/;; }; ```. It may not be inconsistent with trailing / on proxy_pass stripping the url, but it sure is confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540622757
https://github.com/hail-is/hail/pull/7015#issuecomment-540622757:176,Energy Efficiency,monitor,monitoring,176,"Here is an example of the trailing slash issue:. What kibana sees:. ""GET /monitoring/kibana/ui/fonts/inter_ui/Inter-UI-Bold.woff2 HTTP/1.1"" 200 94840 ""https://internal.hail.is/monitoring/kibana/app/kibana"". config:. ```; location /monitoring/kibana/ {; proxy_pass http://kibana/;; }; ```. It may not be inconsistent with trailing / on proxy_pass stripping the url, but it sure is confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540622757
https://github.com/hail-is/hail/pull/7015#issuecomment-540622757:231,Energy Efficiency,monitor,monitoring,231,"Here is an example of the trailing slash issue:. What kibana sees:. ""GET /monitoring/kibana/ui/fonts/inter_ui/Inter-UI-Bold.woff2 HTTP/1.1"" 200 94840 ""https://internal.hail.is/monitoring/kibana/app/kibana"". config:. ```; location /monitoring/kibana/ {; proxy_pass http://kibana/;; }; ```. It may not be inconsistent with trailing / on proxy_pass stripping the url, but it sure is confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540622757
https://github.com/hail-is/hail/pull/7015#issuecomment-540622757:207,Modifiability,config,config,207,"Here is an example of the trailing slash issue:. What kibana sees:. ""GET /monitoring/kibana/ui/fonts/inter_ui/Inter-UI-Bold.woff2 HTTP/1.1"" 200 94840 ""https://internal.hail.is/monitoring/kibana/app/kibana"". config:. ```; location /monitoring/kibana/ {; proxy_pass http://kibana/;; }; ```. It may not be inconsistent with trailing / on proxy_pass stripping the url, but it sure is confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540622757
https://github.com/hail-is/hail/pull/7015#issuecomment-540628849:151,Integrability,rout,router,151,"New config should be correct. I have left the trailing slash on the location path, because without it requests don't appear to make it to the internal router. I think the issue is upstream.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540628849
https://github.com/hail-is/hail/pull/7015#issuecomment-540628849:4,Modifiability,config,config,4,"New config should be correct. I have left the trailing slash on the location path, because without it requests don't appear to make it to the internal router. I think the issue is upstream.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540628849
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:57,Deployability,deploy,deployed,57,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:106,Energy Efficiency,monitor,monitoring,106,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:170,Energy Efficiency,monitor,monitoring,170,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:266,Energy Efficiency,monitor,monitoring,266,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:351,Energy Efficiency,monitor,monitoring,351,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:437,Energy Efficiency,monitor,monitoring,437,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:230,Integrability,Rout,Routing,230,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:308,Integrability,rout,router,308,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:324,Integrability,rout,router-,324,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:400,Integrability,rout,router-,400,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:585,Integrability,rout,router,585,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:49,Modifiability,config,config,49,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:319,Testability,log,logs,319,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336
https://github.com/hail-is/hail/pull/7015#issuecomment-541102715:96,Deployability,deploy,deployed,96,"> > New config should be correct.; > > As an example of this slash issue, the following config (deployed right now) doesn't work.; > ; > I'm confused, is it correct, or does it not work? If there's an issue upstream, it needs to get fixed, too. The new config addresses the requested changes on the config. When this pr was created there was a claim that the redirection to service.internal was not found in gateway. That appears to be not the case (or at least the internal router doesn't seem to be receiving those requests) and still needs to be investigated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715
https://github.com/hail-is/hail/pull/7015#issuecomment-541102715:475,Integrability,rout,router,475,"> > New config should be correct.; > > As an example of this slash issue, the following config (deployed right now) doesn't work.; > ; > I'm confused, is it correct, or does it not work? If there's an issue upstream, it needs to get fixed, too. The new config addresses the requested changes on the config. When this pr was created there was a claim that the redirection to service.internal was not found in gateway. That appears to be not the case (or at least the internal router doesn't seem to be receiving those requests) and still needs to be investigated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715
https://github.com/hail-is/hail/pull/7015#issuecomment-541102715:8,Modifiability,config,config,8,"> > New config should be correct.; > > As an example of this slash issue, the following config (deployed right now) doesn't work.; > ; > I'm confused, is it correct, or does it not work? If there's an issue upstream, it needs to get fixed, too. The new config addresses the requested changes on the config. When this pr was created there was a claim that the redirection to service.internal was not found in gateway. That appears to be not the case (or at least the internal router doesn't seem to be receiving those requests) and still needs to be investigated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715
https://github.com/hail-is/hail/pull/7015#issuecomment-541102715:88,Modifiability,config,config,88,"> > New config should be correct.; > > As an example of this slash issue, the following config (deployed right now) doesn't work.; > ; > I'm confused, is it correct, or does it not work? If there's an issue upstream, it needs to get fixed, too. The new config addresses the requested changes on the config. When this pr was created there was a claim that the redirection to service.internal was not found in gateway. That appears to be not the case (or at least the internal router doesn't seem to be receiving those requests) and still needs to be investigated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715
https://github.com/hail-is/hail/pull/7015#issuecomment-541102715:253,Modifiability,config,config,253,"> > New config should be correct.; > > As an example of this slash issue, the following config (deployed right now) doesn't work.; > ; > I'm confused, is it correct, or does it not work? If there's an issue upstream, it needs to get fixed, too. The new config addresses the requested changes on the config. When this pr was created there was a claim that the redirection to service.internal was not found in gateway. That appears to be not the case (or at least the internal router doesn't seem to be receiving those requests) and still needs to be investigated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715
https://github.com/hail-is/hail/pull/7015#issuecomment-541102715:299,Modifiability,config,config,299,"> > New config should be correct.; > > As an example of this slash issue, the following config (deployed right now) doesn't work.; > ; > I'm confused, is it correct, or does it not work? If there's an issue upstream, it needs to get fixed, too. The new config addresses the requested changes on the config. When this pr was created there was a claim that the redirection to service.internal was not found in gateway. That appears to be not the case (or at least the internal router doesn't seem to be receiving those requests) and still needs to be investigated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715
https://github.com/hail-is/hail/pull/7015#issuecomment-541102899:24,Testability,test,test,24,What is the best way to test gateway?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541102899
https://github.com/hail-is/hail/pull/7015#issuecomment-541104185:10,Modifiability,config,config,10,> The new config addresses the requested changes on the config. I don't know what this means. You didn't answer the question.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541104185
https://github.com/hail-is/hail/pull/7015#issuecomment-541104185:56,Modifiability,config,config,56,> The new config addresses the requested changes on the config. I don't know what this means. You didn't answer the question.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541104185
https://github.com/hail-is/hail/pull/7015#issuecomment-541104331:41,Deployability,Deploy,Deploy,41,"> What is the best way to test gateway?. Deploy the new gateway. Yes, it's live. Yes, it might break stuff. Good luck.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541104331
https://github.com/hail-is/hail/pull/7015#issuecomment-541104331:26,Testability,test,test,26,"> What is the best way to test gateway?. Deploy the new gateway. Yes, it's live. Yes, it might break stuff. Good luck.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541104331
https://github.com/hail-is/hail/pull/7015#issuecomment-541331762:221,Modifiability,rewrite,rewrite,221,"Regex isn't used to detect the optional slash because doing so doesn't really save any lines of code, because nginx does not directly allow proxy_pass with a trailing slash inside of regex-containing locations, without a rewrite rule (and that rewrite rule costs 1 line). https://serverfault.com/questions/649151/nginx-location-regex-doesnt-work-with-proxy-pass",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541331762
https://github.com/hail-is/hail/pull/7015#issuecomment-541331762:244,Modifiability,rewrite,rewrite,244,"Regex isn't used to detect the optional slash because doing so doesn't really save any lines of code, because nginx does not directly allow proxy_pass with a trailing slash inside of regex-containing locations, without a rewrite rule (and that rewrite rule costs 1 line). https://serverfault.com/questions/649151/nginx-location-regex-doesnt-work-with-proxy-pass",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541331762
https://github.com/hail-is/hail/pull/7015#issuecomment-541331762:20,Safety,detect,detect,20,"Regex isn't used to detect the optional slash because doing so doesn't really save any lines of code, because nginx does not directly allow proxy_pass with a trailing slash inside of regex-containing locations, without a rewrite rule (and that rewrite rule costs 1 line). https://serverfault.com/questions/649151/nginx-location-regex-doesnt-work-with-proxy-pass",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541331762
https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:278,Energy Efficiency,monitor,monitoring,278,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:647,Energy Efficiency,reduce,reduces,647,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:289,Integrability,rout,router,289,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:393,Performance,cache,cache,393,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:149,Testability,log,logs,149,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:222,Testability,log,logs,222,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:457,Testability,test,testing,457,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:601,Testability,test,tested,601,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:377,Usability,Clear,Cleared,377,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
https://github.com/hail-is/hail/pull/7015#issuecomment-541336418:760,Usability,clear,clear,760,"I also now suspect that the strange behavior I was seeing was due to caching on Chrome's side. This would explain why I was seeing nothing in server logs, and why behavior was inconsistent between browsers. I even watched logs of all 6 gateways (3 gateway, 3 internal), and the monitoring router, nothing. I also saw differences in redirect behavior between Safari and Chrome. Cleared browser cache (hard refreshes weren't doing anything), and started also testing in Firefox. Lastly, the proxy_set_header Host does not appear to be needed for Grafana or Prometheus to operate, so I have excluded it (tested with the Cluster dashboard). This also reduces the number of places we need to specify which external domain Grafana/Prometheus sit behind. edit: To be clear I also tried to find documentation on the use of GF_SERVER_DOMAIN and could not. GF_SERVER_DOMAIN doesn't even appear in Grafan's repository (at least, GitHub search doesn't find it, although it does find GF_SERVER_ROOT_URL)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541336418
https://github.com/hail-is/hail/pull/7015#issuecomment-554498117:156,Integrability,rout,router,156,"Assigned John, I think @cseed is busy. John, this PR has 2 features:; 1) Remove catch-all server block, for service-specific blocks, in better keeping with router.nginx.conf. 2) Allow prefix matches only on the exact, slash-less url. Meaning /prometheusss$haxor doesn't work, but /prometheus does. This is most easily accomplished with an exact match location block, because by Nginx semantics, regex-containing locations cannot be elided with a root proxy_pass (one with a trailing /), because nginx wants a static prefix to remove, and regex prevents that. So to accomplish this with regex would require more LOC, namely a URL rewrite rule inside the location block.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-554498117
https://github.com/hail-is/hail/pull/7015#issuecomment-554498117:629,Modifiability,rewrite,rewrite,629,"Assigned John, I think @cseed is busy. John, this PR has 2 features:; 1) Remove catch-all server block, for service-specific blocks, in better keeping with router.nginx.conf. 2) Allow prefix matches only on the exact, slash-less url. Meaning /prometheusss$haxor doesn't work, but /prometheus does. This is most easily accomplished with an exact match location block, because by Nginx semantics, regex-containing locations cannot be elided with a root proxy_pass (one with a trailing /), because nginx wants a static prefix to remove, and regex prevents that. So to accomplish this with regex would require more LOC, namely a URL rewrite rule inside the location block.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-554498117
https://github.com/hail-is/hail/pull/7015#issuecomment-555152983:1425,Energy Efficiency,monitor,monitoring,1425,"> I still generally find nginx and rules about trailing slashes confusing, but as far as I can tell this seems fine. What do you find confusing?. edit:; Not sure if this helps (if not, let me know, I should be able to clearly answer any questions you have):. Normal location blocks (meaning `location /path/to` with no modifiers or regex) match on prefix, which means that Nginx checks whether the specified `/path/to` exists entirely as a word in the longer url, starting from the beginning of that url (after the domain). A `location = /path/to` (with `=` modifier), is similar in that it is checked against the beginning of the url, with the change that the matched prefix must match exactly (there cannot be anything after `/path/to`). The proxy_pass rule with regard to slashes: Say I have `location /path/to/foo { proxy_pass http://127.0.0.1; }`. Upon matching, Nginx will redirect the request to `127.0.0.1/path/to/foo`, because in the absence of a trailing slash, the entire url (after the domain) is appended. If the proxy_pass directive has a trailing slash, the entire matched prefix is dropped, and only the uri after the prefix is appended. Ex: if the url was `http://domain.com/path/to/foo`, the redirect would be `http://127.0.0.1/`. If the url was `http://domain.com/path/to/foo/bar` the redirect would be to `http://127.0.0.1/bar`. In this PR, when no trailing slash is provided to `https://internal.hail.is/monitoring/service`, the exact location block matches, since the inexact match block has a trailing slash, and therefore doesn't match the slash-less url (moot point anyway: if we made both the inexact and exact location blocks have no slashes, the exact one would take precedence). Then we specify that the proxy_pass has a trailing slash for each block, so that the root of the internal website is `/` rather than `/monitoring/{service}`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-555152983
https://github.com/hail-is/hail/pull/7015#issuecomment-555152983:1843,Energy Efficiency,monitor,monitoring,1843,"> I still generally find nginx and rules about trailing slashes confusing, but as far as I can tell this seems fine. What do you find confusing?. edit:; Not sure if this helps (if not, let me know, I should be able to clearly answer any questions you have):. Normal location blocks (meaning `location /path/to` with no modifiers or regex) match on prefix, which means that Nginx checks whether the specified `/path/to` exists entirely as a word in the longer url, starting from the beginning of that url (after the domain). A `location = /path/to` (with `=` modifier), is similar in that it is checked against the beginning of the url, with the change that the matched prefix must match exactly (there cannot be anything after `/path/to`). The proxy_pass rule with regard to slashes: Say I have `location /path/to/foo { proxy_pass http://127.0.0.1; }`. Upon matching, Nginx will redirect the request to `127.0.0.1/path/to/foo`, because in the absence of a trailing slash, the entire url (after the domain) is appended. If the proxy_pass directive has a trailing slash, the entire matched prefix is dropped, and only the uri after the prefix is appended. Ex: if the url was `http://domain.com/path/to/foo`, the redirect would be `http://127.0.0.1/`. If the url was `http://domain.com/path/to/foo/bar` the redirect would be to `http://127.0.0.1/bar`. In this PR, when no trailing slash is provided to `https://internal.hail.is/monitoring/service`, the exact location block matches, since the inexact match block has a trailing slash, and therefore doesn't match the slash-less url (moot point anyway: if we made both the inexact and exact location blocks have no slashes, the exact one would take precedence). Then we specify that the proxy_pass has a trailing slash for each block, so that the root of the internal website is `/` rather than `/monitoring/{service}`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-555152983
https://github.com/hail-is/hail/pull/7015#issuecomment-555152983:218,Usability,clear,clearly,218,"> I still generally find nginx and rules about trailing slashes confusing, but as far as I can tell this seems fine. What do you find confusing?. edit:; Not sure if this helps (if not, let me know, I should be able to clearly answer any questions you have):. Normal location blocks (meaning `location /path/to` with no modifiers or regex) match on prefix, which means that Nginx checks whether the specified `/path/to` exists entirely as a word in the longer url, starting from the beginning of that url (after the domain). A `location = /path/to` (with `=` modifier), is similar in that it is checked against the beginning of the url, with the change that the matched prefix must match exactly (there cannot be anything after `/path/to`). The proxy_pass rule with regard to slashes: Say I have `location /path/to/foo { proxy_pass http://127.0.0.1; }`. Upon matching, Nginx will redirect the request to `127.0.0.1/path/to/foo`, because in the absence of a trailing slash, the entire url (after the domain) is appended. If the proxy_pass directive has a trailing slash, the entire matched prefix is dropped, and only the uri after the prefix is appended. Ex: if the url was `http://domain.com/path/to/foo`, the redirect would be `http://127.0.0.1/`. If the url was `http://domain.com/path/to/foo/bar` the redirect would be to `http://127.0.0.1/bar`. In this PR, when no trailing slash is provided to `https://internal.hail.is/monitoring/service`, the exact location block matches, since the inexact match block has a trailing slash, and therefore doesn't match the slash-less url (moot point anyway: if we made both the inexact and exact location blocks have no slashes, the exact one would take precedence). Then we specify that the proxy_pass has a trailing slash for each block, so that the root of the internal website is `/` rather than `/monitoring/{service}`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-555152983
https://github.com/hail-is/hail/issues/7016#issuecomment-529142368:2853,Availability,alive,alive,2853,"Pod spec.initContainers{setup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 11s 11s 1 batch-12728-job-287-742170.15c2403c041fbfef Pod spec.containers{main} Normal Pulling kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f pulling image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 10s 10s 1 batch-12728-job-287-742170.15c2403c1523483f Pod spec.containers{main} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Successfully pulled image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 7s 7s 1 batch-12728-job-287-742170.15c2403ccfdae1f7 Pod spec.containers{main} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 7s 7s 1 batch-12728-job-287-742170.15c2403cef202da1 Pod spec.containers{main} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 7s 7s 1 batch-12728-job-287-742170.15c2403cef7b3cef Pod spec.containers{cleanup} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""gcr.io/hail-vdc/batch:tg0py3mel4jf"" already present on machine; 7s 7s 1 batch-12728-job-287-742170.15c2403cf8c02f7f Pod spec.containers{cleanup} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 6s 6s 1 batch-12728-job-287-742170.15c2403d0a5a03fb Pod spec.containers{cleanup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 6s 6s 1 batch-12728-job-287-742170.15c2403d0a914c20 Pod spec.containers{keep-alive} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""gcr.io/hail-vdc/batch:tg0py3mel4jf"" already present on machine; 4s 4s 1 batch-12728-job-287-742170.15c2403d8c542f61 Pod spec.containers{keep-alive} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 4s 4s 1 batch-12728-job-287-742170.15c2403da37c4943 Pod spec.containers{keep-alive} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368
https://github.com/hail-is/hail/issues/7016#issuecomment-529142368:3084,Availability,alive,alive,3084,"Pod spec.initContainers{setup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 11s 11s 1 batch-12728-job-287-742170.15c2403c041fbfef Pod spec.containers{main} Normal Pulling kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f pulling image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 10s 10s 1 batch-12728-job-287-742170.15c2403c1523483f Pod spec.containers{main} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Successfully pulled image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 7s 7s 1 batch-12728-job-287-742170.15c2403ccfdae1f7 Pod spec.containers{main} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 7s 7s 1 batch-12728-job-287-742170.15c2403cef202da1 Pod spec.containers{main} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 7s 7s 1 batch-12728-job-287-742170.15c2403cef7b3cef Pod spec.containers{cleanup} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""gcr.io/hail-vdc/batch:tg0py3mel4jf"" already present on machine; 7s 7s 1 batch-12728-job-287-742170.15c2403cf8c02f7f Pod spec.containers{cleanup} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 6s 6s 1 batch-12728-job-287-742170.15c2403d0a5a03fb Pod spec.containers{cleanup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 6s 6s 1 batch-12728-job-287-742170.15c2403d0a914c20 Pod spec.containers{keep-alive} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""gcr.io/hail-vdc/batch:tg0py3mel4jf"" already present on machine; 4s 4s 1 batch-12728-job-287-742170.15c2403d8c542f61 Pod spec.containers{keep-alive} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 4s 4s 1 batch-12728-job-287-742170.15c2403da37c4943 Pod spec.containers{keep-alive} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368
https://github.com/hail-is/hail/issues/7016#issuecomment-529142368:3254,Availability,alive,alive,3254,"Pod spec.initContainers{setup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 11s 11s 1 batch-12728-job-287-742170.15c2403c041fbfef Pod spec.containers{main} Normal Pulling kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f pulling image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 10s 10s 1 batch-12728-job-287-742170.15c2403c1523483f Pod spec.containers{main} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Successfully pulled image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 7s 7s 1 batch-12728-job-287-742170.15c2403ccfdae1f7 Pod spec.containers{main} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 7s 7s 1 batch-12728-job-287-742170.15c2403cef202da1 Pod spec.containers{main} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 7s 7s 1 batch-12728-job-287-742170.15c2403cef7b3cef Pod spec.containers{cleanup} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""gcr.io/hail-vdc/batch:tg0py3mel4jf"" already present on machine; 7s 7s 1 batch-12728-job-287-742170.15c2403cf8c02f7f Pod spec.containers{cleanup} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 6s 6s 1 batch-12728-job-287-742170.15c2403d0a5a03fb Pod spec.containers{cleanup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 6s 6s 1 batch-12728-job-287-742170.15c2403d0a914c20 Pod spec.containers{keep-alive} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""gcr.io/hail-vdc/batch:tg0py3mel4jf"" already present on machine; 4s 4s 1 batch-12728-job-287-742170.15c2403d8c542f61 Pod spec.containers{keep-alive} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 4s 4s 1 batch-12728-job-287-742170.15c2403da37c4943 Pod spec.containers{keep-alive} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368
https://github.com/hail-is/hail/issues/7016#issuecomment-529142368:159,Energy Efficiency,schedul,scheduler,159,"I deleted the pod in question, and then I see these k8s events:; ```; 1m 1m 1 batch-12728-job-287-742170.15c2402851e4fd08 Pod Warning FailedScheduling default-scheduler Binding rejected: Operation cannot be fulfilled on pods/binding ""batch-12728-job-287-742170"": pod batch-12728-job-287-742170 is being deleted, cannot be assigned to a host; 1m 1m 1 batch-12728-job-287-742170.15c2402851ed033d Pod Warning FailedScheduling default-scheduler skip schedule deleting pod: batch-pods/batch-12728-job-287-742170; 1m 1m 1 batch-12728-job-287-742170.15c2402853df7089 Pod Normal Scheduled default-scheduler Successfully assigned batch-pods/batch-12728-job-287-742170 to gke-vdc-non-preemptible-pool-0106a51b-qz7f; 1m 1m 1 batch-12728-job-287-742170.15c24029e9da8a8e Pod Normal SuccessfulAttachVolume attachdetach-controller AttachVolume.Attach succeeded for volume ""pvc-167a0df6-d011-11e9-92a9-42010a800041"" ; 1m 1m 1 batch-12728-job-287-742170.15c2402aa05ad63c Pod spec.initContainers{setup} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""google/cloud-sdk:237.0.0-alpine"" already present on machine; 1m 1m 1 batch-12728-job-287-742170.15c2402aa4bf829c Pod spec.initContainers{setup} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 1m 1m 1 batch-12728-job-287-742170.15c2402aaf3284fb Pod spec.initContainers{setup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 11s 11s 1 batch-12728-job-287-742170.15c2403c041fbfef Pod spec.containers{main} Normal Pulling kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f pulling image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 10s 10s 1 batch-12728-job-287-742170.15c2403c1523483f Pod spec.containers{main} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Successfully pulled image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 7s 7s 1 batch-12728-job-287-742170.15c2403ccfdae1f7 Pod spec.containers{main} Normal Created kubelet, gke-vdc-non-p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368
https://github.com/hail-is/hail/issues/7016#issuecomment-529142368:431,Energy Efficiency,schedul,scheduler,431,"I deleted the pod in question, and then I see these k8s events:; ```; 1m 1m 1 batch-12728-job-287-742170.15c2402851e4fd08 Pod Warning FailedScheduling default-scheduler Binding rejected: Operation cannot be fulfilled on pods/binding ""batch-12728-job-287-742170"": pod batch-12728-job-287-742170 is being deleted, cannot be assigned to a host; 1m 1m 1 batch-12728-job-287-742170.15c2402851ed033d Pod Warning FailedScheduling default-scheduler skip schedule deleting pod: batch-pods/batch-12728-job-287-742170; 1m 1m 1 batch-12728-job-287-742170.15c2402853df7089 Pod Normal Scheduled default-scheduler Successfully assigned batch-pods/batch-12728-job-287-742170 to gke-vdc-non-preemptible-pool-0106a51b-qz7f; 1m 1m 1 batch-12728-job-287-742170.15c24029e9da8a8e Pod Normal SuccessfulAttachVolume attachdetach-controller AttachVolume.Attach succeeded for volume ""pvc-167a0df6-d011-11e9-92a9-42010a800041"" ; 1m 1m 1 batch-12728-job-287-742170.15c2402aa05ad63c Pod spec.initContainers{setup} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""google/cloud-sdk:237.0.0-alpine"" already present on machine; 1m 1m 1 batch-12728-job-287-742170.15c2402aa4bf829c Pod spec.initContainers{setup} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 1m 1m 1 batch-12728-job-287-742170.15c2402aaf3284fb Pod spec.initContainers{setup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 11s 11s 1 batch-12728-job-287-742170.15c2403c041fbfef Pod spec.containers{main} Normal Pulling kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f pulling image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 10s 10s 1 batch-12728-job-287-742170.15c2403c1523483f Pod spec.containers{main} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Successfully pulled image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 7s 7s 1 batch-12728-job-287-742170.15c2403ccfdae1f7 Pod spec.containers{main} Normal Created kubelet, gke-vdc-non-p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368
https://github.com/hail-is/hail/issues/7016#issuecomment-529142368:446,Energy Efficiency,schedul,schedule,446,"I deleted the pod in question, and then I see these k8s events:; ```; 1m 1m 1 batch-12728-job-287-742170.15c2402851e4fd08 Pod Warning FailedScheduling default-scheduler Binding rejected: Operation cannot be fulfilled on pods/binding ""batch-12728-job-287-742170"": pod batch-12728-job-287-742170 is being deleted, cannot be assigned to a host; 1m 1m 1 batch-12728-job-287-742170.15c2402851ed033d Pod Warning FailedScheduling default-scheduler skip schedule deleting pod: batch-pods/batch-12728-job-287-742170; 1m 1m 1 batch-12728-job-287-742170.15c2402853df7089 Pod Normal Scheduled default-scheduler Successfully assigned batch-pods/batch-12728-job-287-742170 to gke-vdc-non-preemptible-pool-0106a51b-qz7f; 1m 1m 1 batch-12728-job-287-742170.15c24029e9da8a8e Pod Normal SuccessfulAttachVolume attachdetach-controller AttachVolume.Attach succeeded for volume ""pvc-167a0df6-d011-11e9-92a9-42010a800041"" ; 1m 1m 1 batch-12728-job-287-742170.15c2402aa05ad63c Pod spec.initContainers{setup} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""google/cloud-sdk:237.0.0-alpine"" already present on machine; 1m 1m 1 batch-12728-job-287-742170.15c2402aa4bf829c Pod spec.initContainers{setup} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 1m 1m 1 batch-12728-job-287-742170.15c2402aaf3284fb Pod spec.initContainers{setup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 11s 11s 1 batch-12728-job-287-742170.15c2403c041fbfef Pod spec.containers{main} Normal Pulling kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f pulling image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 10s 10s 1 batch-12728-job-287-742170.15c2403c1523483f Pod spec.containers{main} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Successfully pulled image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 7s 7s 1 batch-12728-job-287-742170.15c2403ccfdae1f7 Pod spec.containers{main} Normal Created kubelet, gke-vdc-non-p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368
https://github.com/hail-is/hail/issues/7016#issuecomment-529142368:571,Energy Efficiency,Schedul,Scheduled,571,"I deleted the pod in question, and then I see these k8s events:; ```; 1m 1m 1 batch-12728-job-287-742170.15c2402851e4fd08 Pod Warning FailedScheduling default-scheduler Binding rejected: Operation cannot be fulfilled on pods/binding ""batch-12728-job-287-742170"": pod batch-12728-job-287-742170 is being deleted, cannot be assigned to a host; 1m 1m 1 batch-12728-job-287-742170.15c2402851ed033d Pod Warning FailedScheduling default-scheduler skip schedule deleting pod: batch-pods/batch-12728-job-287-742170; 1m 1m 1 batch-12728-job-287-742170.15c2402853df7089 Pod Normal Scheduled default-scheduler Successfully assigned batch-pods/batch-12728-job-287-742170 to gke-vdc-non-preemptible-pool-0106a51b-qz7f; 1m 1m 1 batch-12728-job-287-742170.15c24029e9da8a8e Pod Normal SuccessfulAttachVolume attachdetach-controller AttachVolume.Attach succeeded for volume ""pvc-167a0df6-d011-11e9-92a9-42010a800041"" ; 1m 1m 1 batch-12728-job-287-742170.15c2402aa05ad63c Pod spec.initContainers{setup} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""google/cloud-sdk:237.0.0-alpine"" already present on machine; 1m 1m 1 batch-12728-job-287-742170.15c2402aa4bf829c Pod spec.initContainers{setup} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 1m 1m 1 batch-12728-job-287-742170.15c2402aaf3284fb Pod spec.initContainers{setup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 11s 11s 1 batch-12728-job-287-742170.15c2403c041fbfef Pod spec.containers{main} Normal Pulling kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f pulling image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 10s 10s 1 batch-12728-job-287-742170.15c2403c1523483f Pod spec.containers{main} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Successfully pulled image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 7s 7s 1 batch-12728-job-287-742170.15c2403ccfdae1f7 Pod spec.containers{main} Normal Created kubelet, gke-vdc-non-p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368
https://github.com/hail-is/hail/issues/7016#issuecomment-529142368:589,Energy Efficiency,schedul,scheduler,589,"I deleted the pod in question, and then I see these k8s events:; ```; 1m 1m 1 batch-12728-job-287-742170.15c2402851e4fd08 Pod Warning FailedScheduling default-scheduler Binding rejected: Operation cannot be fulfilled on pods/binding ""batch-12728-job-287-742170"": pod batch-12728-job-287-742170 is being deleted, cannot be assigned to a host; 1m 1m 1 batch-12728-job-287-742170.15c2402851ed033d Pod Warning FailedScheduling default-scheduler skip schedule deleting pod: batch-pods/batch-12728-job-287-742170; 1m 1m 1 batch-12728-job-287-742170.15c2402853df7089 Pod Normal Scheduled default-scheduler Successfully assigned batch-pods/batch-12728-job-287-742170 to gke-vdc-non-preemptible-pool-0106a51b-qz7f; 1m 1m 1 batch-12728-job-287-742170.15c24029e9da8a8e Pod Normal SuccessfulAttachVolume attachdetach-controller AttachVolume.Attach succeeded for volume ""pvc-167a0df6-d011-11e9-92a9-42010a800041"" ; 1m 1m 1 batch-12728-job-287-742170.15c2402aa05ad63c Pod spec.initContainers{setup} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Container image ""google/cloud-sdk:237.0.0-alpine"" already present on machine; 1m 1m 1 batch-12728-job-287-742170.15c2402aa4bf829c Pod spec.initContainers{setup} Normal Created kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Created container; 1m 1m 1 batch-12728-job-287-742170.15c2402aaf3284fb Pod spec.initContainers{setup} Normal Started kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Started container; 11s 11s 1 batch-12728-job-287-742170.15c2403c041fbfef Pod spec.containers{main} Normal Pulling kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f pulling image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 10s 10s 1 batch-12728-job-287-742170.15c2403c1523483f Pod spec.containers{main} Normal Pulled kubelet, gke-vdc-non-preemptible-pool-0106a51b-qz7f Successfully pulled image ""gcr.io/broad-ctsa/benchmark_wang:latest""; 7s 7s 1 batch-12728-job-287-742170.15c2403ccfdae1f7 Pod spec.containers{main} Normal Created kubelet, gke-vdc-non-p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529142368
https://github.com/hail-is/hail/issues/7016#issuecomment-529558144:20,Safety,avoid,avoid,20,"I don't know how to avoid this, closing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7016#issuecomment-529558144
https://github.com/hail-is/hail/pull/7020#issuecomment-529151909:33,Testability,log,logging,33,grr. I see this prevents me from logging in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7020#issuecomment-529151909
https://github.com/hail-is/hail/pull/7020#issuecomment-529152351:67,Testability,log,logging,67,"OK, this should do it. I'm not sure why we passed the headers when logging in, we asked to login so the old creds are irrelevant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7020#issuecomment-529152351
https://github.com/hail-is/hail/pull/7020#issuecomment-529152351:91,Testability,log,login,91,"OK, this should do it. I'm not sure why we passed the headers when logging in, we asked to login so the old creds are irrelevant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7020#issuecomment-529152351
https://github.com/hail-is/hail/pull/7020#issuecomment-529559470:23,Availability,error,error,23,"OK, I will PR improved error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7020#issuecomment-529559470
https://github.com/hail-is/hail/pull/7020#issuecomment-529559470:29,Integrability,message,messages,29,"OK, I will PR improved error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7020#issuecomment-529559470
https://github.com/hail-is/hail/pull/7022#issuecomment-529491736:155,Performance,perform,performance,155,"I don't quite understand this. To take advantage of this, it seems like we need also parallelize the tests. Where's the test parallelism set? Can we get a performance comparison, e.g. this PR test vs master?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7022#issuecomment-529491736
https://github.com/hail-is/hail/pull/7022#issuecomment-529491736:101,Testability,test,tests,101,"I don't quite understand this. To take advantage of this, it seems like we need also parallelize the tests. Where's the test parallelism set? Can we get a performance comparison, e.g. this PR test vs master?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7022#issuecomment-529491736
https://github.com/hail-is/hail/pull/7022#issuecomment-529491736:120,Testability,test,test,120,"I don't quite understand this. To take advantage of this, it seems like we need also parallelize the tests. Where's the test parallelism set? Can we get a performance comparison, e.g. this PR test vs master?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7022#issuecomment-529491736
https://github.com/hail-is/hail/pull/7022#issuecomment-529491736:192,Testability,test,test,192,"I don't quite understand this. To take advantage of this, it seems like we need also parallelize the tests. Where's the test parallelism set? Can we get a performance comparison, e.g. this PR test vs master?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7022#issuecomment-529491736
https://github.com/hail-is/hail/pull/7022#issuecomment-529556683:38,Testability,test,tests,38,"Every run_image_step that was changed tests hail using `hl.init(master='local[2]')`. This means they all use at least two cores (for some reason, some appear to use >2 cores, I don't know why, I'll investigate as I gather more data about this). This change makes the pod's CPU requests reflect their real usage. This change was motivated by watching the Grafana metric for ""CPU Oversubscription"" and seeing these pods constantly using more than their request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7022#issuecomment-529556683
https://github.com/hail-is/hail/pull/7024#issuecomment-529229251:119,Availability,down,download,119,"We don't use near line or cold line storage, so this change should be okay. However, there is a requirement on who can download the files that I am concerned about. > Note that for such uploads, crcmod is required for downloading regardless of whether the parallel composite upload option is on or not. For some distributions this is easy (e.g., it comes pre-installed on macOS), but in other cases some users have found it difficult. Because of this, at present parallel composite uploads are disabled by default. Google is actively working with a number of the Linux distributions to get crcmod included with the stock distribution. Once that is done we will re-enable parallel composite uploads by default in gsutil.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529229251
https://github.com/hail-is/hail/pull/7024#issuecomment-529229251:218,Availability,down,downloading,218,"We don't use near line or cold line storage, so this change should be okay. However, there is a requirement on who can download the files that I am concerned about. > Note that for such uploads, crcmod is required for downloading regardless of whether the parallel composite upload option is on or not. For some distributions this is easy (e.g., it comes pre-installed on macOS), but in other cases some users have found it difficult. Because of this, at present parallel composite uploads are disabled by default. Google is actively working with a number of the Linux distributions to get crcmod included with the stock distribution. Once that is done we will re-enable parallel composite uploads by default in gsutil.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529229251
https://github.com/hail-is/hail/pull/7024#issuecomment-529229251:359,Deployability,install,installed,359,"We don't use near line or cold line storage, so this change should be okay. However, there is a requirement on who can download the files that I am concerned about. > Note that for such uploads, crcmod is required for downloading regardless of whether the parallel composite upload option is on or not. For some distributions this is easy (e.g., it comes pre-installed on macOS), but in other cases some users have found it difficult. Because of this, at present parallel composite uploads are disabled by default. Google is actively working with a number of the Linux distributions to get crcmod included with the stock distribution. Once that is done we will re-enable parallel composite uploads by default in gsutil.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529229251
https://github.com/hail-is/hail/pull/7024#issuecomment-529304476:69,Deployability,install,installed,69,"Where does this command get run? We need to make sure that crcmod is installed for the version of python that is running the gsutil command, not the version that is running batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529304476
https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:145,Availability,down,downloads,145,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965
https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:192,Availability,down,downloads,192,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965
https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:594,Availability,Down,Downloaded,594,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965
https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:860,Availability,avail,available,860,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965
https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:1044,Deployability,install,installed,1044,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965
https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:1091,Deployability,install,install,1091,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965
https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:945,Modifiability,config,config,945,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965
https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:964,Modifiability,config,config,964,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965
https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:676,Security,checksum,checksum,676,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965
https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:90,Testability,test,tests,90,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965
https://github.com/hail-is/hail/pull/7024#issuecomment-529590702:18,Testability,test,tests,18,"Erm, actually our tests never upload such a large file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529590702
https://github.com/hail-is/hail/pull/7024#issuecomment-529650160:148,Deployability,install,install,148,```; Building dependency tree...; Reading state information...; [91mE: Unable to locate package python-setuptools; The command '/bin/sh -c apt-get install gcc python-setuptools && pip uninstall crcmod && pip install --no-cache-dir -U crcmod' returned a non-zero code: 100; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529650160
https://github.com/hail-is/hail/pull/7024#issuecomment-529650160:209,Deployability,install,install,209,```; Building dependency tree...; Reading state information...; [91mE: Unable to locate package python-setuptools; The command '/bin/sh -c apt-get install gcc python-setuptools && pip uninstall crcmod && pip install --no-cache-dir -U crcmod' returned a non-zero code: 100; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529650160
https://github.com/hail-is/hail/pull/7024#issuecomment-529650160:14,Integrability,depend,dependency,14,```; Building dependency tree...; Reading state information...; [91mE: Unable to locate package python-setuptools; The command '/bin/sh -c apt-get install gcc python-setuptools && pip uninstall crcmod && pip install --no-cache-dir -U crcmod' returned a non-zero code: 100; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529650160
https://github.com/hail-is/hail/pull/7024#issuecomment-529650160:222,Performance,cache,cache-dir,222,```; Building dependency tree...; Reading state information...; [91mE: Unable to locate package python-setuptools; The command '/bin/sh -c apt-get install gcc python-setuptools && pip uninstall crcmod && pip install --no-cache-dir -U crcmod' returned a non-zero code: 100; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529650160
https://github.com/hail-is/hail/pull/7029#issuecomment-530395992:345,Usability,clear,clearer,345,"@konradjk I fixed the instructions. I remembered investigating this a month ago when a user asked how to do this and I didn't know off the top of my head what the semantics were. And I remembered the ""--"" thing that's used to pass arguments to gcloud and got them mixed up. So this is really not a big docs change now, but I think it's a little clearer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7029#issuecomment-530395992
https://github.com/hail-is/hail/pull/7031#issuecomment-529671940:208,Modifiability,config,config,208,"OK, I think this is ready. The goal was to fix the problem of client and server components being mixed together inside hailtop.gear. Changes:. - break out the client part of hailtop.gear into hailtop.{utils, config, auth}; - move the server parts of hailtop.gear to a toplevel gear module; - added service_base_image with hailtop and gear; - moved async_to_blocking and blocking_to_async to hailtop.utils; - also some bits of cleanup. In particular this fixes the aiomysql issue by moving the database stuff into gear.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-529671940
https://github.com/hail-is/hail/pull/7031#issuecomment-530112882:272,Deployability,deploy,deploying,272,"This seems right to me. In this vein of work, there's one more thing I want: we should build this Dockerfile with the hail wheel and then execute `pylint hail && pylint hailtop`. Pylint will look for uninstalled modules. This will save us from checking in (and eventually deploying) a hail package with bad dependencies. We should probably also run the python tests against this version of hail. This is a tru, local-mode user environment. ```; FROM ubuntu:18.04. ENV LANG C.UTF-8. RUN apt-get update && \; apt-get -y install \; openjdk-8-jdk-headless \; python3 python3-pip && \; rm -rf /var/lib/apt/lists/*. COPY hail.whl pylintrc ./; RUN pip install --no-cache-dir ./hail.whl; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882
https://github.com/hail-is/hail/pull/7031#issuecomment-530112882:494,Deployability,update,update,494,"This seems right to me. In this vein of work, there's one more thing I want: we should build this Dockerfile with the hail wheel and then execute `pylint hail && pylint hailtop`. Pylint will look for uninstalled modules. This will save us from checking in (and eventually deploying) a hail package with bad dependencies. We should probably also run the python tests against this version of hail. This is a tru, local-mode user environment. ```; FROM ubuntu:18.04. ENV LANG C.UTF-8. RUN apt-get update && \; apt-get -y install \; openjdk-8-jdk-headless \; python3 python3-pip && \; rm -rf /var/lib/apt/lists/*. COPY hail.whl pylintrc ./; RUN pip install --no-cache-dir ./hail.whl; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882
https://github.com/hail-is/hail/pull/7031#issuecomment-530112882:518,Deployability,install,install,518,"This seems right to me. In this vein of work, there's one more thing I want: we should build this Dockerfile with the hail wheel and then execute `pylint hail && pylint hailtop`. Pylint will look for uninstalled modules. This will save us from checking in (and eventually deploying) a hail package with bad dependencies. We should probably also run the python tests against this version of hail. This is a tru, local-mode user environment. ```; FROM ubuntu:18.04. ENV LANG C.UTF-8. RUN apt-get update && \; apt-get -y install \; openjdk-8-jdk-headless \; python3 python3-pip && \; rm -rf /var/lib/apt/lists/*. COPY hail.whl pylintrc ./; RUN pip install --no-cache-dir ./hail.whl; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882
https://github.com/hail-is/hail/pull/7031#issuecomment-530112882:645,Deployability,install,install,645,"This seems right to me. In this vein of work, there's one more thing I want: we should build this Dockerfile with the hail wheel and then execute `pylint hail && pylint hailtop`. Pylint will look for uninstalled modules. This will save us from checking in (and eventually deploying) a hail package with bad dependencies. We should probably also run the python tests against this version of hail. This is a tru, local-mode user environment. ```; FROM ubuntu:18.04. ENV LANG C.UTF-8. RUN apt-get update && \; apt-get -y install \; openjdk-8-jdk-headless \; python3 python3-pip && \; rm -rf /var/lib/apt/lists/*. COPY hail.whl pylintrc ./; RUN pip install --no-cache-dir ./hail.whl; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882
https://github.com/hail-is/hail/pull/7031#issuecomment-530112882:307,Integrability,depend,dependencies,307,"This seems right to me. In this vein of work, there's one more thing I want: we should build this Dockerfile with the hail wheel and then execute `pylint hail && pylint hailtop`. Pylint will look for uninstalled modules. This will save us from checking in (and eventually deploying) a hail package with bad dependencies. We should probably also run the python tests against this version of hail. This is a tru, local-mode user environment. ```; FROM ubuntu:18.04. ENV LANG C.UTF-8. RUN apt-get update && \; apt-get -y install \; openjdk-8-jdk-headless \; python3 python3-pip && \; rm -rf /var/lib/apt/lists/*. COPY hail.whl pylintrc ./; RUN pip install --no-cache-dir ./hail.whl; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882
https://github.com/hail-is/hail/pull/7031#issuecomment-530112882:658,Performance,cache,cache-dir,658,"This seems right to me. In this vein of work, there's one more thing I want: we should build this Dockerfile with the hail wheel and then execute `pylint hail && pylint hailtop`. Pylint will look for uninstalled modules. This will save us from checking in (and eventually deploying) a hail package with bad dependencies. We should probably also run the python tests against this version of hail. This is a tru, local-mode user environment. ```; FROM ubuntu:18.04. ENV LANG C.UTF-8. RUN apt-get update && \; apt-get -y install \; openjdk-8-jdk-headless \; python3 python3-pip && \; rm -rf /var/lib/apt/lists/*. COPY hail.whl pylintrc ./; RUN pip install --no-cache-dir ./hail.whl; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882
https://github.com/hail-is/hail/pull/7031#issuecomment-530112882:360,Testability,test,tests,360,"This seems right to me. In this vein of work, there's one more thing I want: we should build this Dockerfile with the hail wheel and then execute `pylint hail && pylint hailtop`. Pylint will look for uninstalled modules. This will save us from checking in (and eventually deploying) a hail package with bad dependencies. We should probably also run the python tests against this version of hail. This is a tru, local-mode user environment. ```; FROM ubuntu:18.04. ENV LANG C.UTF-8. RUN apt-get update && \; apt-get -y install \; openjdk-8-jdk-headless \; python3 python3-pip && \; rm -rf /var/lib/apt/lists/*. COPY hail.whl pylintrc ./; RUN pip install --no-cache-dir ./hail.whl; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882
https://github.com/hail-is/hail/pull/7033#issuecomment-529665181:25,Testability,benchmark,benchmarks,25,Still waiting on getting benchmarks working.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7033#issuecomment-529665181
https://github.com/hail-is/hail/pull/7047#issuecomment-530825854:20,Availability,down,downloaded,20,"looks good to me, I downloaded the docs and poked around. Much improved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7047#issuecomment-530825854
https://github.com/hail-is/hail/pull/7055#issuecomment-531377155:56,Modifiability,polymorphi,polymorphism,56,just pushed some changes to remove the need for all the polymorphism,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7055#issuecomment-531377155
https://github.com/hail-is/hail/pull/7055#issuecomment-531960975:160,Availability,error,errors,160,"added some logic to allow `CallCC` code to be emitted more than once in the same function (see `testDuplicateCallCC`, which used to cause bytecode verification errors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7055#issuecomment-531960975
https://github.com/hail-is/hail/pull/7055#issuecomment-531960975:11,Testability,log,logic,11,"added some logic to allow `CallCC` code to be emitted more than once in the same function (see `testDuplicateCallCC`, which used to cause bytecode verification errors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7055#issuecomment-531960975
https://github.com/hail-is/hail/pull/7055#issuecomment-531960975:96,Testability,test,testDuplicateCallCC,96,"added some logic to allow `CallCC` code to be emitted more than once in the same function (see `testDuplicateCallCC`, which used to cause bytecode verification errors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7055#issuecomment-531960975
https://github.com/hail-is/hail/pull/7060#issuecomment-533674187:542,Modifiability,rewrite,rewrite,542,"I want to preempt before you go much further and suggest that we not change the *implementation* of the decoders/encoders right now. Instead, we should just be copying code from EmitPackEncoder/EmitPackDecoder into the ETypes. I think there should be an internal method on EType that roughly looks like ; ```; def emitDecoder(; t: PType,; mb: MethodBuilder,; in: Code[InputBuffer],; srvb: StagedRegionValueBuilder): Code[Unit] = {; ```. And probably a top level thing on EType that mimics EmitPackDecoder.decode. I don't think I'm OK doing a rewrite of all the codegen in the same PR as a reorganization.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7060#issuecomment-533674187
https://github.com/hail-is/hail/pull/7060#issuecomment-534278690:33,Testability,test,tests,33,This should be done. I'm running tests locally and in ci now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7060#issuecomment-534278690
https://github.com/hail-is/hail/pull/7060#issuecomment-535127711:8,Testability,benchmark,benchmark,8,Want to benchmark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7060#issuecomment-535127711
https://github.com/hail-is/hail/pull/7060#issuecomment-535609382:26,Deployability,pipeline,pipeline,26,"Haven't been able to have pipeline benchmarks finish, but from the looks of things this change does not make things significantly slower or faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7060#issuecomment-535609382
https://github.com/hail-is/hail/pull/7060#issuecomment-535609382:35,Testability,benchmark,benchmarks,35,"Haven't been able to have pipeline benchmarks finish, but from the looks of things this change does not make things significantly slower or faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7060#issuecomment-535609382
https://github.com/hail-is/hail/pull/7064#issuecomment-531938132:25,Integrability,depend,dependencies,25,> gidgethub. Will remove dependencies in separate PR so I don't want to wait for all the images to rebuild. Can't quite get rid of Flask yet as it is used in callback tests. Could you help me understand what the benefit of gidgethub is over our previous strategy? Since we generally prefer minimal deps,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531938132
https://github.com/hail-is/hail/pull/7064#issuecomment-531938132:167,Testability,test,tests,167,> gidgethub. Will remove dependencies in separate PR so I don't want to wait for all the images to rebuild. Can't quite get rid of Flask yet as it is used in callback tests. Could you help me understand what the benefit of gidgethub is over our previous strategy? Since we generally prefer minimal deps,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531938132
https://github.com/hail-is/hail/pull/7064#issuecomment-531942870:599,Deployability,install,installation,599,"> Could you help me understand what the benefit of gidgethub is over our previous strategy? Since we generally prefer minimal deps. This isn't more deps. I switched gidgethub for PyGithub. gidgethub is async, PyGithub is not. Now that we're using aiohttp over Flask, if we don't use an async library to query Github handling web requests will hang while the synchronous library blocks waiting for requests. Actually, this is fewer deps. We already use gidgethub in CI, so overall this removes the PyGithub dependency. Since both CI and scorecard depend on gidgethub, I moved it from the CI-specific installation to the service base image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531942870
https://github.com/hail-is/hail/pull/7064#issuecomment-531942870:506,Integrability,depend,dependency,506,"> Could you help me understand what the benefit of gidgethub is over our previous strategy? Since we generally prefer minimal deps. This isn't more deps. I switched gidgethub for PyGithub. gidgethub is async, PyGithub is not. Now that we're using aiohttp over Flask, if we don't use an async library to query Github handling web requests will hang while the synchronous library blocks waiting for requests. Actually, this is fewer deps. We already use gidgethub in CI, so overall this removes the PyGithub dependency. Since both CI and scorecard depend on gidgethub, I moved it from the CI-specific installation to the service base image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531942870
https://github.com/hail-is/hail/pull/7064#issuecomment-531942870:546,Integrability,depend,depend,546,"> Could you help me understand what the benefit of gidgethub is over our previous strategy? Since we generally prefer minimal deps. This isn't more deps. I switched gidgethub for PyGithub. gidgethub is async, PyGithub is not. Now that we're using aiohttp over Flask, if we don't use an async library to query Github handling web requests will hang while the synchronous library blocks waiting for requests. Actually, this is fewer deps. We already use gidgethub in CI, so overall this removes the PyGithub dependency. Since both CI and scorecard depend on gidgethub, I moved it from the CI-specific installation to the service base image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531942870
https://github.com/hail-is/hail/pull/7064#issuecomment-531991835:18,Deployability,deploy,deploy,18,"I fixed scorecard deploy stuff and now it is working with `dev deploy`. I also pushed some CSS changes:; - body { margin: 0; } that removes the extra header spacing; - but added an 0 8px 8px 8px margin to #content; - simplified the header layout CSS; - fixed the header item clickable area, should be bigger and uniform across header items; - fixed the misalignment on Safari. I'm pretty happy with this for this iteration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531991835
https://github.com/hail-is/hail/pull/7064#issuecomment-531991835:63,Deployability,deploy,deploy,63,"I fixed scorecard deploy stuff and now it is working with `dev deploy`. I also pushed some CSS changes:; - body { margin: 0; } that removes the extra header spacing; - but added an 0 8px 8px 8px margin to #content; - simplified the header layout CSS; - fixed the header item clickable area, should be bigger and uniform across header items; - fixed the misalignment on Safari. I'm pretty happy with this for this iteration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531991835
https://github.com/hail-is/hail/pull/7064#issuecomment-531991835:217,Usability,simpl,simplified,217,"I fixed scorecard deploy stuff and now it is working with `dev deploy`. I also pushed some CSS changes:; - body { margin: 0; } that removes the extra header spacing; - but added an 0 8px 8px 8px margin to #content; - simplified the header layout CSS; - fixed the header item clickable area, should be bigger and uniform across header items; - fixed the misalignment on Safari. I'm pretty happy with this for this iteration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531991835
https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:1652,Availability,Error,Error,1652,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733
https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:50,Deployability,install,install,50,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733
https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:150,Deployability,deploy,deploy,150,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733
https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:621,Deployability,deploy,deploy,621,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733
https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:882,Deployability,deploy,deploy,882,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733
https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:1048,Deployability,deploy,deploy,1048,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733
https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:1627,Integrability,message,message,1627,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733
https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:13,Testability,test,test,13,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733
https://github.com/hail-is/hail/pull/7064#issuecomment-532038340:131,Testability,log,logo,131,"Checked https://internal.hail.is/cseed/notebook2/notebook. Crash on trying to create a notebook. Minor styling fixes as well (Hail logo is tiny), can be delayed for future PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532038340
https://github.com/hail-is/hail/pull/7064#issuecomment-532254006:128,Availability,failure,failure,128,Did you copy the the oath2 key to your namespace? You need to do that for the auth service to boot. This is probably a cascaded failure because auth is down.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532254006
https://github.com/hail-is/hail/pull/7064#issuecomment-532254006:152,Availability,down,down,152,Did you copy the the oath2 key to your namespace? You need to do that for the auth service to boot. This is probably a cascaded failure because auth is down.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532254006
https://github.com/hail-is/hail/pull/7064#issuecomment-532254581:36,Testability,log,logo,36,"> Minor styling fixes as well (Hail logo is tiny). Where is the Hail logo tiny? You mean the ""Hail"" in the center the / page?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532254581
https://github.com/hail-is/hail/pull/7064#issuecomment-532254581:69,Testability,log,logo,69,"> Minor styling fixes as well (Hail logo is tiny). Where is the Hail logo tiny? You mean the ""Hail"" in the center the / page?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532254581
https://github.com/hail-is/hail/pull/7064#issuecomment-532256223:130,Availability,failure,failure,130,"> Did you copy the the oath2 key to your namespace? You need to do that for the auth service to boot. This is probably a cascaded failure because auth is down. I think I have what I need. I have a ~/.hail/token and ~/.hail/tokens.json. The token file looks like a jwt, decodes to ""email"": ""ako"" followed by some low-ascii characters. . Before I ran this I logged in using hailctl auth login.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532256223
https://github.com/hail-is/hail/pull/7064#issuecomment-532256223:154,Availability,down,down,154,"> Did you copy the the oath2 key to your namespace? You need to do that for the auth service to boot. This is probably a cascaded failure because auth is down. I think I have what I need. I have a ~/.hail/token and ~/.hail/tokens.json. The token file looks like a jwt, decodes to ""email"": ""ako"" followed by some low-ascii characters. . Before I ran this I logged in using hailctl auth login.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532256223
https://github.com/hail-is/hail/pull/7064#issuecomment-532256223:356,Testability,log,logged,356,"> Did you copy the the oath2 key to your namespace? You need to do that for the auth service to boot. This is probably a cascaded failure because auth is down. I think I have what I need. I have a ~/.hail/token and ~/.hail/tokens.json. The token file looks like a jwt, decodes to ""email"": ""ako"" followed by some low-ascii characters. . Before I ran this I logged in using hailctl auth login.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532256223
https://github.com/hail-is/hail/pull/7064#issuecomment-532256223:385,Testability,log,login,385,"> Did you copy the the oath2 key to your namespace? You need to do that for the auth service to boot. This is probably a cascaded failure because auth is down. I think I have what I need. I have a ~/.hail/token and ~/.hail/tokens.json. The token file looks like a jwt, decodes to ""email"": ""ako"" followed by some low-ascii characters. . Before I ran this I logged in using hailctl auth login.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532256223
https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:454,Availability,failure,failures,454,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285
https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:52,Deployability,install,install,52,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285
https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:141,Deployability,deploy,deploy,141,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285
https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:318,Deployability,deploy,deploy,318,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285
https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:339,Deployability,deploy,deploy,339,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285
https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:363,Deployability,configurat,configuration,363,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285
https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:416,Deployability,deploy,deploy,416,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285
https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:471,Deployability,deploy,deployed,471,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285
https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:363,Modifiability,config,configuration,363,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285
https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:15,Testability,test,test,15,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285
https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:665,Testability,log,log,665,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285
https://github.com/hail-is/hail/pull/7064#issuecomment-532273328:152,Testability,log,login,152,"> Crash on trying to create a notebook. I looked into this. The notebook launch failed because your account in my namespace isn't complete, it can only login, but doesn't have the other secrets necessary to launch a notebook (e.g. gsa-key).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532273328
https://github.com/hail-is/hail/pull/7064#issuecomment-532274736:33,Availability,failure,failure,33,"Oops, sorry, I misread where the failure was happening. You need `cseed/hail:batch-web`, not `cseed:batch-web`. I created an issue to fix this: https://github.com/hail-is/hail/issues/7074",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532274736
https://github.com/hail-is/hail/issues/7068#issuecomment-531915477:78,Deployability,deploy,deployed,78,"ServiceBackend/apiserver is known to not be working right now. It isn't being deployed or maintained. @johnc1231 and @catoverdrive were working on some tasks related to this. Few tasks:; - the global reference state in the JVM backend has to go, and needs to be stored in the Python client. This means reference information needs to be including along with queries.; - Table => CollectDArray lowering needs to be finished so apiserver can use the new `scheduler` to execute pipelines.; - Need to implement GoogleFS on the JVM side. I think someone just needs to take on ""get service backend working again"". As per our quarterly planning discussion, it might make sense to focus on upstream tasks for now (lowering, batch).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7068#issuecomment-531915477
https://github.com/hail-is/hail/issues/7068#issuecomment-531915477:474,Deployability,pipeline,pipelines,474,"ServiceBackend/apiserver is known to not be working right now. It isn't being deployed or maintained. @johnc1231 and @catoverdrive were working on some tasks related to this. Few tasks:; - the global reference state in the JVM backend has to go, and needs to be stored in the Python client. This means reference information needs to be including along with queries.; - Table => CollectDArray lowering needs to be finished so apiserver can use the new `scheduler` to execute pipelines.; - Need to implement GoogleFS on the JVM side. I think someone just needs to take on ""get service backend working again"". As per our quarterly planning discussion, it might make sense to focus on upstream tasks for now (lowering, batch).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7068#issuecomment-531915477
https://github.com/hail-is/hail/issues/7068#issuecomment-531915477:452,Energy Efficiency,schedul,scheduler,452,"ServiceBackend/apiserver is known to not be working right now. It isn't being deployed or maintained. @johnc1231 and @catoverdrive were working on some tasks related to this. Few tasks:; - the global reference state in the JVM backend has to go, and needs to be stored in the Python client. This means reference information needs to be including along with queries.; - Table => CollectDArray lowering needs to be finished so apiserver can use the new `scheduler` to execute pipelines.; - Need to implement GoogleFS on the JVM side. I think someone just needs to take on ""get service backend working again"". As per our quarterly planning discussion, it might make sense to focus on upstream tasks for now (lowering, batch).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7068#issuecomment-531915477
https://github.com/hail-is/hail/pull/7069#issuecomment-538660862:193,Testability,test,tests,193,"Think I've addressed all your comments. I addressed missingness in reindex because you explicitly mentioned it, but the ndarray code overall is going to need a missingness overhaul where I add tests for missing values. I'll do that in a subsequent PR. I wasn't sure what your comment about `EmitArrayTriplet` was about, that's not something I use.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7069#issuecomment-538660862
https://github.com/hail-is/hail/pull/7071#issuecomment-534279756:41,Performance,optimiz,optimizer,41,aha! Looks like this caught a *separate* optimizer bug related to keying :(. Will fix this test and then fix that bug afterwars.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7071#issuecomment-534279756
https://github.com/hail-is/hail/pull/7071#issuecomment-534279756:91,Testability,test,test,91,aha! Looks like this caught a *separate* optimizer bug related to keying :(. Will fix this test and then fix that bug afterwars.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7071#issuecomment-534279756
https://github.com/hail-is/hail/pull/7073#issuecomment-532176730:4,Testability,benchmark,benchmark,4,"The benchmark goes from ~5m to ~40s. This is still wayyyyy too long! Force-count from read takes about 3s. In the PR, profiling indicates it spends all its time in the RegionValueCollect aggregator, so when TableKeyByAndAggregate starts using new aggs and your collect reimplementation goes in, I expect this to get *much* better.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7073#issuecomment-532176730
https://github.com/hail-is/hail/issues/7080#issuecomment-536709685:333,Deployability,release,releases,333,"Is there a possibility of adding an exception for users who need to pass a SparkContext to `hl.init`? . In our use case, our notebooks use the sparkmagic kernel to communicate with livy running on multiple clusters. Sparkmagic automatically creates a SparkContext when connecting to livy on the cluster master node (AWS EMR). And in releases prior to 0.2.20 we were passing that SparkContext to `hl.init`. . In our case, the JAR is in our class path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-536709685
https://github.com/hail-is/hail/issues/7080#issuecomment-536711994:45,Deployability,configurat,configuration,45,essentially you need to add the following as configuration in Spark:. ```; HAIL_JAR_LOCATION=/path/to/python/site-packages/hail/hail-all-spark.jar; spark.jars=${HAIL_JAR_LOCATION}; spark.driver.extraClassPath=${HAIL_JAR_LOCATION}; spark.executor.extraClassPath=./hail-all-spark.jar; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-536711994
https://github.com/hail-is/hail/issues/7080#issuecomment-536711994:45,Modifiability,config,configuration,45,essentially you need to add the following as configuration in Spark:. ```; HAIL_JAR_LOCATION=/path/to/python/site-packages/hail/hail-all-spark.jar; spark.jars=${HAIL_JAR_LOCATION}; spark.driver.extraClassPath=${HAIL_JAR_LOCATION}; spark.executor.extraClassPath=./hail-all-spark.jar; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-536711994
https://github.com/hail-is/hail/issues/7080#issuecomment-536736873:73,Testability,test,test,73,Thanks for the quick help! I've just build a new image from master. I'll test and report back.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-536736873
https://github.com/hail-is/hail/issues/7080#issuecomment-537048154:5,Security,validat,validated,5,I've validated our setup has those requirements and we're just hitting a FatalError from a commit a few weeks ago. https://github.com/hail-is/hail/blob/a0e8eb81e0f4d7ad446723e7cc04d4c6ac4ad066/hail/python/hail/context.py#L59-L67. If I revert this file we're able to pass in the existing SparkContext with the expected `hl.init(sc=sc)`. As general feedback it may be better to warn here than force exit. I may be wrong but I don't see a way around this for people using remote notebooks to talk to Spark via Livy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-537048154
https://github.com/hail-is/hail/issues/7080#issuecomment-537048154:347,Usability,feedback,feedback,347,I've validated our setup has those requirements and we're just hitting a FatalError from a commit a few weeks ago. https://github.com/hail-is/hail/blob/a0e8eb81e0f4d7ad446723e7cc04d4c6ac4ad066/hail/python/hail/context.py#L59-L67. If I revert this file we're able to pass in the existing SparkContext with the expected `hl.init(sc=sc)`. As general feedback it may be better to warn here than force exit. I may be wrong but I don't see a way around this for people using remote notebooks to talk to Spark via Livy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-537048154
https://github.com/hail-is/hail/issues/7080#issuecomment-537100468:0,Testability,Test,Tested,0,Tested locally via Livy and my notebook is happy. I really appreciate your help here!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-537100468
https://github.com/hail-is/hail/pull/7082#issuecomment-532727074:17,Testability,test,test,17,"er, let me add a test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7082#issuecomment-532727074
https://github.com/hail-is/hail/pull/7082#issuecomment-532783880:14,Testability,test,tests,14,fixed + added tests. ready for review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7082#issuecomment-532783880
https://github.com/hail-is/hail/pull/7087#issuecomment-532861246:37,Deployability,install,install-hailctl,37,"I checked out your branch, ran `make install-hailctl`, started a cluster, connected to a notebook, and ran `hl.utils.range_table(1_000_000, 10000)._force_count()`. Did not see any monitor UI show up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532861246
https://github.com/hail-is/hail/pull/7087#issuecomment-532861246:180,Energy Efficiency,monitor,monitor,180,"I checked out your branch, ran `make install-hailctl`, started a cluster, connected to a notebook, and ran `hl.utils.range_table(1_000_000, 10000)._force_count()`. Did not see any monitor UI show up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532861246
https://github.com/hail-is/hail/pull/7087#issuecomment-532865823:572,Modifiability,variab,variable,572,"Though running: `pprint(dict(os.environ.items()))`, yielded:. ```; {'CLICOLOR': '1',; 'GIT_PAGER': 'cat',; 'HOME': '/root',; 'INVOCATION_ID': '0faec80a970f4cf29ce69112519fe641',; 'JOURNAL_STREAM': '8:38888',; 'JPY_PARENT_PID': '5858',; 'LANG': 'en_US.UTF-8',; 'LOGNAME': 'root',; 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',; 'PAGER': 'cat',; 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',; 'SHELL': '/bin/sh',; 'SPARKMONITOR_KERNEL_PORT': '38853',; 'TERM': 'xterm-color',; 'USER': 'root'}; ```. which does not include the environment variable you added saying to use the new thing, though that's clearly present in `init_notebook.py`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532865823
https://github.com/hail-is/hail/pull/7087#issuecomment-532865823:261,Testability,LOG,LOGNAME,261,"Though running: `pprint(dict(os.environ.items()))`, yielded:. ```; {'CLICOLOR': '1',; 'GIT_PAGER': 'cat',; 'HOME': '/root',; 'INVOCATION_ID': '0faec80a970f4cf29ce69112519fe641',; 'JOURNAL_STREAM': '8:38888',; 'JPY_PARENT_PID': '5858',; 'LANG': 'en_US.UTF-8',; 'LOGNAME': 'root',; 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',; 'PAGER': 'cat',; 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',; 'SHELL': '/bin/sh',; 'SPARKMONITOR_KERNEL_PORT': '38853',; 'TERM': 'xterm-color',; 'USER': 'root'}; ```. which does not include the environment variable you added saying to use the new thing, though that's clearly present in `init_notebook.py`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532865823
https://github.com/hail-is/hail/pull/7087#issuecomment-532865823:634,Usability,clear,clearly,634,"Though running: `pprint(dict(os.environ.items()))`, yielded:. ```; {'CLICOLOR': '1',; 'GIT_PAGER': 'cat',; 'HOME': '/root',; 'INVOCATION_ID': '0faec80a970f4cf29ce69112519fe641',; 'JOURNAL_STREAM': '8:38888',; 'JPY_PARENT_PID': '5858',; 'LANG': 'en_US.UTF-8',; 'LOGNAME': 'root',; 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',; 'PAGER': 'cat',; 'PATH': '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',; 'SHELL': '/bin/sh',; 'SPARKMONITOR_KERNEL_PORT': '38853',; 'TERM': 'xterm-color',; 'USER': 'root'}; ```. which does not include the environment variable you added saying to use the new thing, though that's clearly present in `init_notebook.py`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532865823
https://github.com/hail-is/hail/pull/7087#issuecomment-532874446:12,Testability,test,tested,12,huh. I just tested again and it worked!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532874446
https://github.com/hail-is/hail/pull/7087#issuecomment-532883549:122,Availability,reliab,reliable,122,"bad news, though -- after playing with the thing a bit longer and trying to bump the partition number up, it's really not reliable. At all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532883549
https://github.com/hail-is/hail/pull/7087#issuecomment-532900121:102,Security,attack,attack,102,aha it gets a push for every task end! Running even a 5000-partition short job is a denial of service attack against the extension,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532900121
https://github.com/hail-is/hail/pull/7087#issuecomment-532900229:52,Testability,log,log,52,"after running one 5000-partition job, the extension log is 300k lines",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532900229
https://github.com/hail-is/hail/pull/7088#issuecomment-532938355:44,Availability,error,error,44,"Hmm, I realized one problem with this - the error message will return that of coalesce rather than or_else and might be a little confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7088#issuecomment-532938355
https://github.com/hail-is/hail/pull/7088#issuecomment-532938355:50,Integrability,message,message,50,"Hmm, I realized one problem with this - the error message will return that of coalesce rather than or_else and might be a little confusing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7088#issuecomment-532938355
https://github.com/hail-is/hail/pull/7088#issuecomment-533099406:142,Availability,redundant,redundant,142,"Yeah but it would really mess with debugging. I think probably still worth keeping the `unify_exprs` and `TypeError` lines (it'll be a little redundant, but it's just type checking so shouldn't be slow)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7088#issuecomment-533099406
https://github.com/hail-is/hail/pull/7088#issuecomment-533099406:142,Safety,redund,redundant,142,"Yeah but it would really mess with debugging. I think probably still worth keeping the `unify_exprs` and `TypeError` lines (it'll be a little redundant, but it's just type checking so shouldn't be slow)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7088#issuecomment-533099406
https://github.com/hail-is/hail/pull/7092#issuecomment-533282019:9,Availability,failure,failure,9,got test failure: https://ci.hail.is/batches/13670/jobs/33/log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7092#issuecomment-533282019
https://github.com/hail-is/hail/pull/7092#issuecomment-533282019:4,Testability,test,test,4,got test failure: https://ci.hail.is/batches/13670/jobs/33/log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7092#issuecomment-533282019
https://github.com/hail-is/hail/pull/7092#issuecomment-533282019:59,Testability,log,log,59,got test failure: https://ci.hail.is/batches/13670/jobs/33/log,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7092#issuecomment-533282019
https://github.com/hail-is/hail/pull/7101#issuecomment-534707540:44,Testability,test,testing,44,I'm closing this PR for now so that I'm not testing it for every commit I make.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7101#issuecomment-534707540
https://github.com/hail-is/hail/pull/7106#issuecomment-533892685:6,Testability,test,test,6,lgtm. test maybe?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7106#issuecomment-533892685
https://github.com/hail-is/hail/issues/7107#issuecomment-533924688:41,Integrability,interface,interface,41,can't do this now as it's technically an interface change./,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7107#issuecomment-533924688
https://github.com/hail-is/hail/pull/7108#issuecomment-534253455:5,Testability,benchmark,benchmarks,5,also benchmarks are moved to `/benchmark/python/benchmark_hail` now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108#issuecomment-534253455
https://github.com/hail-is/hail/pull/7108#issuecomment-534253455:31,Testability,benchmark,benchmark,31,also benchmarks are moved to `/benchmark/python/benchmark_hail` now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7108#issuecomment-534253455
https://github.com/hail-is/hail/pull/7112#issuecomment-534267706:220,Security,validat,validate,220,"Obviously, look forward to feedback on the UI and let me know if you run into any UI bugs. Another todo that I've started:; - write a UI testing playbook to enumerate all the UI interactions we want to test (by hand) to validate this code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534267706
https://github.com/hail-is/hail/pull/7112#issuecomment-534267706:137,Testability,test,testing,137,"Obviously, look forward to feedback on the UI and let me know if you run into any UI bugs. Another todo that I've started:; - write a UI testing playbook to enumerate all the UI interactions we want to test (by hand) to validate this code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534267706
https://github.com/hail-is/hail/pull/7112#issuecomment-534267706:202,Testability,test,test,202,"Obviously, look forward to feedback on the UI and let me know if you run into any UI bugs. Another todo that I've started:; - write a UI testing playbook to enumerate all the UI interactions we want to test (by hand) to validate this code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534267706
https://github.com/hail-is/hail/pull/7112#issuecomment-534267706:27,Usability,feedback,feedback,27,"Obviously, look forward to feedback on the UI and let me know if you run into any UI bugs. Another todo that I've started:; - write a UI testing playbook to enumerate all the UI interactions we want to test (by hand) to validate this code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534267706
https://github.com/hail-is/hail/pull/7112#issuecomment-534276842:655,Availability,error,error,655,"One final comment, the goal here was separate the normal user notebook flow from the workshop guest notebook flow, while sharing the main logic without impacting logic outside notebook. I think that was largely successful. I think the only impact outside was to layout.html in web_common, it checks a `workshop` variable to load the workshop header instead of the default one. This is necessary because you can't override a block in a included file from the file that includes it. The other design I considered was have auth support a guest user for workshops which was represented just like any other user, but this seemed both more complicated and more error prone from the security perspective. As we have other use cases for guest users (e.g. free tier), let's revisit this decision.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842
https://github.com/hail-is/hail/pull/7112#issuecomment-534276842:312,Modifiability,variab,variable,312,"One final comment, the goal here was separate the normal user notebook flow from the workshop guest notebook flow, while sharing the main logic without impacting logic outside notebook. I think that was largely successful. I think the only impact outside was to layout.html in web_common, it checks a `workshop` variable to load the workshop header instead of the default one. This is necessary because you can't override a block in a included file from the file that includes it. The other design I considered was have auth support a guest user for workshops which was represented just like any other user, but this seemed both more complicated and more error prone from the security perspective. As we have other use cases for guest users (e.g. free tier), let's revisit this decision.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842
https://github.com/hail-is/hail/pull/7112#issuecomment-534276842:324,Performance,load,load,324,"One final comment, the goal here was separate the normal user notebook flow from the workshop guest notebook flow, while sharing the main logic without impacting logic outside notebook. I think that was largely successful. I think the only impact outside was to layout.html in web_common, it checks a `workshop` variable to load the workshop header instead of the default one. This is necessary because you can't override a block in a included file from the file that includes it. The other design I considered was have auth support a guest user for workshops which was represented just like any other user, but this seemed both more complicated and more error prone from the security perspective. As we have other use cases for guest users (e.g. free tier), let's revisit this decision.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842
https://github.com/hail-is/hail/pull/7112#issuecomment-534276842:676,Security,secur,security,676,"One final comment, the goal here was separate the normal user notebook flow from the workshop guest notebook flow, while sharing the main logic without impacting logic outside notebook. I think that was largely successful. I think the only impact outside was to layout.html in web_common, it checks a `workshop` variable to load the workshop header instead of the default one. This is necessary because you can't override a block in a included file from the file that includes it. The other design I considered was have auth support a guest user for workshops which was represented just like any other user, but this seemed both more complicated and more error prone from the security perspective. As we have other use cases for guest users (e.g. free tier), let's revisit this decision.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842
https://github.com/hail-is/hail/pull/7112#issuecomment-534276842:138,Testability,log,logic,138,"One final comment, the goal here was separate the normal user notebook flow from the workshop guest notebook flow, while sharing the main logic without impacting logic outside notebook. I think that was largely successful. I think the only impact outside was to layout.html in web_common, it checks a `workshop` variable to load the workshop header instead of the default one. This is necessary because you can't override a block in a included file from the file that includes it. The other design I considered was have auth support a guest user for workshops which was represented just like any other user, but this seemed both more complicated and more error prone from the security perspective. As we have other use cases for guest users (e.g. free tier), let's revisit this decision.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842
https://github.com/hail-is/hail/pull/7112#issuecomment-534276842:162,Testability,log,logic,162,"One final comment, the goal here was separate the normal user notebook flow from the workshop guest notebook flow, while sharing the main logic without impacting logic outside notebook. I think that was largely successful. I think the only impact outside was to layout.html in web_common, it checks a `workshop` variable to load the workshop header instead of the default one. This is necessary because you can't override a block in a included file from the file that includes it. The other design I considered was have auth support a guest user for workshops which was represented just like any other user, but this seemed both more complicated and more error prone from the security perspective. As we have other use cases for guest users (e.g. free tier), let's revisit this decision.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534276842
https://github.com/hail-is/hail/pull/7112#issuecomment-534569631:27,Testability,test,test,27,"FYI, I'm getting the scale test working and pushing some additional changes. I will follow up when it is stable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534569631
https://github.com/hail-is/hail/pull/7112#issuecomment-534603185:156,Security,password,password,156,"OK, code is stable again, scale tests are working. Run with:. ```; ~/hail/notebook $ PYTHONPATH=../hail/python:../gear python3 scale-test.py 10 <workshop> <password>; ```. ```; successes: 10 / 10 = 1.0; mean time: 2.3504347085952757; max time: 3.135228157043457; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534603185
https://github.com/hail-is/hail/pull/7112#issuecomment-534603185:32,Testability,test,tests,32,"OK, code is stable again, scale tests are working. Run with:. ```; ~/hail/notebook $ PYTHONPATH=../hail/python:../gear python3 scale-test.py 10 <workshop> <password>; ```. ```; successes: 10 / 10 = 1.0; mean time: 2.3504347085952757; max time: 3.135228157043457; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534603185
https://github.com/hail-is/hail/pull/7112#issuecomment-534603185:133,Testability,test,test,133,"OK, code is stable again, scale tests are working. Run with:. ```; ~/hail/notebook $ PYTHONPATH=../hail/python:../gear python3 scale-test.py 10 <workshop> <password>; ```. ```; successes: 10 / 10 = 1.0; mean time: 2.3504347085952757; max time: 3.135228157043457; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534603185
https://github.com/hail-is/hail/pull/7112#issuecomment-534707944:114,Deployability,update,updated,114,"Oops, I had a bug where the readiness check was hitting the notebook service, not the actual notebook. Here is an updated scale test:. ```; successes: 10 / 10 = 1.0; mean time: 6.920137214660644; max time: 14.664504528045654; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534707944
https://github.com/hail-is/hail/pull/7112#issuecomment-534707944:128,Testability,test,test,128,"Oops, I had a bug where the readiness check was hitting the notebook service, not the actual notebook. Here is an updated scale test:. ```; successes: 10 / 10 = 1.0; mean time: 6.920137214660644; max time: 14.664504528045654; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534707944
https://github.com/hail-is/hail/pull/7112#issuecomment-535220649:11,Availability,error,errors,11,Seeing 500 errors on create (maybe latest not deployed to your namespace),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-535220649
https://github.com/hail-is/hail/pull/7112#issuecomment-535220649:46,Deployability,deploy,deployed,46,Seeing 500 errors on create (maybe latest not deployed to your namespace),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-535220649
https://github.com/hail-is/hail/pull/7112#issuecomment-535317771:202,Availability,error,errors,202,"Good comments, thanks. I think I addressed everything. There were some pending bugs from moving things to workshop.hail.is. My namespace should be up to date now, let me know if you're still seeing 500 errors. I'm still working on the remaining todo items, but those are independent and I think this is ready to go in (when you approve).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-535317771
https://github.com/hail-is/hail/pull/7113#issuecomment-534322797:35,Testability,benchmark,benchmark,35,"oh, nice, I didn't see this. Let's benchmark!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7113#issuecomment-534322797
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:195,Deployability,patch,patch,195,"This PR _explicitly_ changes the defaults. It optionally accepts the old values in an _attempt_ to not break current users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:225,Deployability,patch,patch,225,"This PR _explicitly_ changes the defaults. It optionally accepts the old values in an _attempt_ to not break current users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:410,Deployability,PATCH,PATCH,410,"This PR _explicitly_ changes the defaults. It optionally accepts the old values in an _attempt_ to not break current users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2938,Deployability,Deploy,DeployConfig,2938,"ONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2977,Deployability,Deploy,DeployConfig,2977,"ONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3142,Deployability,deploy,deploy-config,3142,"/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3225,Deployability,deploy,deploy-config,3225,"hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4744,Deployability,deploy,deploy-config,4744,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4827,Deployability,deploy,deploy-config,4827,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:458,Modifiability,config,config,458,"This PR _explicitly_ changes the defaults. It optionally accepts the old values in an _attempt_ to not break current users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:618,Modifiability,config,config,618,"This PR _explicitly_ changes the defaults. It optionally accepts the old values in an _attempt_ to not break current users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1075,Modifiability,config,config,1075,"he old values in an _attempt_ to not break current users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(se",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1123,Modifiability,config,config,1123,"users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__ini",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1244,Modifiability,config,config,1244,"patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailto",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1607,Modifiability,config,config,1607," The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1654,Modifiability,config,config,1654," spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/pyt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2106,Modifiability,config,config,2106,"__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2147,Modifiability,config,config,2147,"g.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2228,Modifiability,config,config,2228,"il/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CO",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2274,Modifiability,config,config,2274," 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2516,Modifiability,config,config,2516,"/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; inde",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2562,Modifiability,config,config,2562,"ys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2648,Modifiability,config,config,2648,"ig; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2699,Modifiability,config,config,2699,"y_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deplo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2905,Modifiability,config,config,2905,"ONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3149,Modifiability,config,config,3149,"/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3232,Modifiability,config,config,3232,"hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3316,Modifiability,config,config,3316,"@@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; in",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3672,Modifiability,config,config,3672,"top/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_D",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3719,Modifiability,config,config,3719,"ging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4230,Modifiability,config,config,4230,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4278,Modifiability,config,config,4278,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4366,Modifiability,config,config,4366,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4419,Modifiability,config,config,4419,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4489,Modifiability,config,config,4489,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4536,Modifiability,config,config,4536,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4751,Modifiability,config,config,4751,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4834,Modifiability,config,config,4834,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4904,Modifiability,config,config,4904,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3330,Performance,load,loads,3330,"port get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:175,Testability,log,login,175,"This PR _explicitly_ changes the defaults. It optionally accepts the old values in an _attempt_ to not break current users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1190,Testability,log,login,1190,"auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; ind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1584,Testability,log,logging,1584,"bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1706,Testability,log,log,1706," spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/pyt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:1712,Testability,log,logging,1712," spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/pyt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2747,Testability,log,logging,2747," class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deplo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2783,Testability,log,log,2783," class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deplo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2789,Testability,log,logging,2789," class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deplo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3393,Testability,log,login,3393,"loy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/con",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3437,Testability,log,login,3437," ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/co",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3521,Testability,log,login,3521,"/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3570,Testability,log,login,3570,"on/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HA",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902
https://github.com/hail-is/hail/pull/7125#issuecomment-535660700:204,Deployability,configurat,configuration,204,"Namely, you can solve this. > I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control. without making your new configuration the default, right? The 2nd change you made restored the default behavior for the case of tokens.json, which, it seems to me, gives you inconsistent behavior for 'deploy-config.json', or if we ever changes tokens.json to some other name.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535660700
https://github.com/hail-is/hail/pull/7125#issuecomment-535660700:381,Deployability,deploy,deploy-config,381,"Namely, you can solve this. > I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control. without making your new configuration the default, right? The 2nd change you made restored the default behavior for the case of tokens.json, which, it seems to me, gives you inconsistent behavior for 'deploy-config.json', or if we ever changes tokens.json to some other name.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535660700
https://github.com/hail-is/hail/pull/7125#issuecomment-535660700:204,Modifiability,config,configuration,204,"Namely, you can solve this. > I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control. without making your new configuration the default, right? The 2nd change you made restored the default behavior for the case of tokens.json, which, it seems to me, gives you inconsistent behavior for 'deploy-config.json', or if we ever changes tokens.json to some other name.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535660700
https://github.com/hail-is/hail/pull/7125#issuecomment-535660700:388,Modifiability,config,config,388,"Namely, you can solve this. > I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control. without making your new configuration the default, right? The 2nd change you made restored the default behavior for the case of tokens.json, which, it seems to me, gives you inconsistent behavior for 'deploy-config.json', or if we ever changes tokens.json to some other name.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535660700
https://github.com/hail-is/hail/pull/7125#issuecomment-540740392:194,Modifiability,config,config,194,"Closing this for now. I'll redo it in a way I think will be acceptable (or not, it's really just a small hill I care about, but I would like to point out that `gcloud` stores its credential and config data in `.config/`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-540740392
https://github.com/hail-is/hail/pull/7125#issuecomment-540740392:211,Modifiability,config,config,211,"Closing this for now. I'll redo it in a way I think will be acceptable (or not, it's really just a small hill I care about, but I would like to point out that `gcloud` stores its credential and config data in `.config/`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-540740392
https://github.com/hail-is/hail/pull/7131#issuecomment-539046228:30,Availability,down,downsample,30,actually will reopen when the downsample PR goes in,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7131#issuecomment-539046228
https://github.com/hail-is/hail/issues/7132#issuecomment-535488533:43,Security,secur,secure,43,"A way to do this: https://pypi.org/project/secure/0.1.6/. ```python; secure.SecureCookie.aiohttp(; response,; value=""ABC123"",; samesite=False,; path=""/secure"",; expires=24,; ); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7132#issuecomment-535488533
https://github.com/hail-is/hail/issues/7132#issuecomment-535488533:69,Security,secur,secure,69,"A way to do this: https://pypi.org/project/secure/0.1.6/. ```python; secure.SecureCookie.aiohttp(; response,; value=""ABC123"",; samesite=False,; path=""/secure"",; expires=24,; ); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7132#issuecomment-535488533
https://github.com/hail-is/hail/issues/7132#issuecomment-535488533:76,Security,Secur,SecureCookie,76,"A way to do this: https://pypi.org/project/secure/0.1.6/. ```python; secure.SecureCookie.aiohttp(; response,; value=""ABC123"",; samesite=False,; path=""/secure"",; expires=24,; ); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7132#issuecomment-535488533
https://github.com/hail-is/hail/issues/7132#issuecomment-535488533:151,Security,secur,secure,151,"A way to do this: https://pypi.org/project/secure/0.1.6/. ```python; secure.SecureCookie.aiohttp(; response,; value=""ABC123"",; samesite=False,; path=""/secure"",; expires=24,; ); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7132#issuecomment-535488533
https://github.com/hail-is/hail/pull/7134#issuecomment-535987529:35,Availability,failure,failures,35,also have rebase conflict and test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-535987529
https://github.com/hail-is/hail/pull/7134#issuecomment-535987529:30,Testability,test,test,30,also have rebase conflict and test failures,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-535987529
https://github.com/hail-is/hail/pull/7134#issuecomment-536038333:28,Testability,benchmark,benchmark,28,"Oops, still have to add the benchmark.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-536038333
https://github.com/hail-is/hail/pull/7134#issuecomment-539702199:11,Integrability,wrap,wrapped,11,"Sorry, got wrapped up in other work. I've got things primed to run benchmark comparisons in the morning. Here's some crazy numbers just running one iteration (from this pr):; ```; 2019-10-08 16:46:31,396: INFO: [1/6] Running table_aggregate_linreg20...; 2019-10-08 16:46:44,645: INFO: burn in: 13.24s; 2019-10-08 16:46:55,367: INFO: run 1: 10.72s; 2019-10-08 16:46:55,367: INFO: [2/6] Running table_aggregate_linreg21...; 2019-10-08 16:47:07,836: INFO: burn in: 12.47s; 2019-10-08 16:47:19,376: INFO: run 1: 11.54s; 2019-10-08 16:47:19,376: INFO: [3/6] Running table_aggregate_linreg22...; 2019-10-08 16:47:32,728: INFO: burn in: 13.35s; 2019-10-08 16:47:44,571: INFO: run 1: 11.84s; 2019-10-08 16:47:44,571: INFO: [4/6] Running table_aggregate_linreg23...; 2019-10-08 16:48:37,183: INFO: burn in: 52.61s; 2019-10-08 16:49:28,580: INFO: run 1: 51.40s; 2019-10-08 16:49:28,580: INFO: [5/6] Running table_aggregate_linreg24...; 2019-10-08 16:50:25,410: INFO: burn in: 56.83s; 2019-10-08 16:51:20,184: INFO: run 1: 54.77s; 2019-10-08 16:51:20,184: INFO: [6/6] Running table_aggregate_linreg25...; 2019-10-08 16:52:16,778: INFO: burn in: 56.59s; 2019-10-08 16:53:21,050: INFO: run 1: 64.27s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-539702199
https://github.com/hail-is/hail/pull/7134#issuecomment-539702199:67,Testability,benchmark,benchmark,67,"Sorry, got wrapped up in other work. I've got things primed to run benchmark comparisons in the morning. Here's some crazy numbers just running one iteration (from this pr):; ```; 2019-10-08 16:46:31,396: INFO: [1/6] Running table_aggregate_linreg20...; 2019-10-08 16:46:44,645: INFO: burn in: 13.24s; 2019-10-08 16:46:55,367: INFO: run 1: 10.72s; 2019-10-08 16:46:55,367: INFO: [2/6] Running table_aggregate_linreg21...; 2019-10-08 16:47:07,836: INFO: burn in: 12.47s; 2019-10-08 16:47:19,376: INFO: run 1: 11.54s; 2019-10-08 16:47:19,376: INFO: [3/6] Running table_aggregate_linreg22...; 2019-10-08 16:47:32,728: INFO: burn in: 13.35s; 2019-10-08 16:47:44,571: INFO: run 1: 11.84s; 2019-10-08 16:47:44,571: INFO: [4/6] Running table_aggregate_linreg23...; 2019-10-08 16:48:37,183: INFO: burn in: 52.61s; 2019-10-08 16:49:28,580: INFO: run 1: 51.40s; 2019-10-08 16:49:28,580: INFO: [5/6] Running table_aggregate_linreg24...; 2019-10-08 16:50:25,410: INFO: burn in: 56.83s; 2019-10-08 16:51:20,184: INFO: run 1: 54.77s; 2019-10-08 16:51:20,184: INFO: [6/6] Running table_aggregate_linreg25...; 2019-10-08 16:52:16,778: INFO: burn in: 56.59s; 2019-10-08 16:53:21,050: INFO: run 1: 64.27s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-539702199
https://github.com/hail-is/hail/pull/7134#issuecomment-540058423:14,Testability,test,tests,14,"Two different tests: first running N linregs as separate aggregations, and the second running N linregs inside an array_agg. I suspect you're right that the first one is running into some kind of method size limit in the jit. Time 1 is master, Time 2 is this branch. ```python; @benchmark; def table_aggregate_linregN():; ht = hl.read_table(resource('many_ints_table.ht')); ht.aggregate(hl.tuple([hl.agg.linreg(ht[f'i{i%5}'],; [ht[f'i{(i+1)%5}'], ht[f'i{(i+2)%5}']]); for i in range(N)])); ```. ```quote; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_linreg20 101.2% 11.622 11.765; table_aggregate_linreg21 98.0% 11.964 11.730; table_aggregate_linreg22 22.2% 54.272 12.068; table_aggregate_linreg23 99.9% 54.989 54.939; table_aggregate_linreg24 101.7% 54.753 55.661; table_aggregate_linreg25 102.7% 55.902 57.422; ----------------------; Geometric mean: 78.3%; Simple mean: 87.6%; Median: 100.6%; ```. ```python; @benchmark; def table_aggregate_linregN():; ht = hl.read_table(resource('many_ints_table.ht')); ht.aggregate(hl.agg.array_agg(lambda i: hl.agg.linreg(ht.i0 + i, [ht.i1, ht.i2]),; hl.range(N))); ```. ```quote; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_linreg22 79.1% 10.609 8.389; table_aggregate_linreg23 79.0% 11.006 8.695; table_aggregate_linreg21 78.6% 10.559 8.295; table_aggregate_linreg25 77.2% 11.579 8.937; table_aggregate_linreg24 76.3% 11.520 8.793; table_aggregate_linreg20 75.7% 10.792 8.173; ----------------------; Geometric mean: 77.6%; Simple mean: 77.6%; Median: 77.9%; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-540058423
https://github.com/hail-is/hail/pull/7134#issuecomment-540058423:279,Testability,benchmark,benchmark,279,"Two different tests: first running N linregs as separate aggregations, and the second running N linregs inside an array_agg. I suspect you're right that the first one is running into some kind of method size limit in the jit. Time 1 is master, Time 2 is this branch. ```python; @benchmark; def table_aggregate_linregN():; ht = hl.read_table(resource('many_ints_table.ht')); ht.aggregate(hl.tuple([hl.agg.linreg(ht[f'i{i%5}'],; [ht[f'i{(i+1)%5}'], ht[f'i{(i+2)%5}']]); for i in range(N)])); ```. ```quote; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_linreg20 101.2% 11.622 11.765; table_aggregate_linreg21 98.0% 11.964 11.730; table_aggregate_linreg22 22.2% 54.272 12.068; table_aggregate_linreg23 99.9% 54.989 54.939; table_aggregate_linreg24 101.7% 54.753 55.661; table_aggregate_linreg25 102.7% 55.902 57.422; ----------------------; Geometric mean: 78.3%; Simple mean: 87.6%; Median: 100.6%; ```. ```python; @benchmark; def table_aggregate_linregN():; ht = hl.read_table(resource('many_ints_table.ht')); ht.aggregate(hl.agg.array_agg(lambda i: hl.agg.linreg(ht.i0 + i, [ht.i1, ht.i2]),; hl.range(N))); ```. ```quote; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_linreg22 79.1% 10.609 8.389; table_aggregate_linreg23 79.0% 11.006 8.695; table_aggregate_linreg21 78.6% 10.559 8.295; table_aggregate_linreg25 77.2% 11.579 8.937; table_aggregate_linreg24 76.3% 11.520 8.793; table_aggregate_linreg20 75.7% 10.792 8.173; ----------------------; Geometric mean: 77.6%; Simple mean: 77.6%; Median: 77.9%; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-540058423
https://github.com/hail-is/hail/pull/7134#issuecomment-540058423:936,Testability,benchmark,benchmark,936,"Two different tests: first running N linregs as separate aggregations, and the second running N linregs inside an array_agg. I suspect you're right that the first one is running into some kind of method size limit in the jit. Time 1 is master, Time 2 is this branch. ```python; @benchmark; def table_aggregate_linregN():; ht = hl.read_table(resource('many_ints_table.ht')); ht.aggregate(hl.tuple([hl.agg.linreg(ht[f'i{i%5}'],; [ht[f'i{(i+1)%5}'], ht[f'i{(i+2)%5}']]); for i in range(N)])); ```. ```quote; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_linreg20 101.2% 11.622 11.765; table_aggregate_linreg21 98.0% 11.964 11.730; table_aggregate_linreg22 22.2% 54.272 12.068; table_aggregate_linreg23 99.9% 54.989 54.939; table_aggregate_linreg24 101.7% 54.753 55.661; table_aggregate_linreg25 102.7% 55.902 57.422; ----------------------; Geometric mean: 78.3%; Simple mean: 87.6%; Median: 100.6%; ```. ```python; @benchmark; def table_aggregate_linregN():; ht = hl.read_table(resource('many_ints_table.ht')); ht.aggregate(hl.agg.array_agg(lambda i: hl.agg.linreg(ht.i0 + i, [ht.i1, ht.i2]),; hl.range(N))); ```. ```quote; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_linreg22 79.1% 10.609 8.389; table_aggregate_linreg23 79.0% 11.006 8.695; table_aggregate_linreg21 78.6% 10.559 8.295; table_aggregate_linreg25 77.2% 11.579 8.937; table_aggregate_linreg24 76.3% 11.520 8.793; table_aggregate_linreg20 75.7% 10.792 8.173; ----------------------; Geometric mean: 77.6%; Simple mean: 77.6%; Median: 77.9%; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-540058423
https://github.com/hail-is/hail/pull/7134#issuecomment-540058423:883,Usability,Simpl,Simple,883,"Two different tests: first running N linregs as separate aggregations, and the second running N linregs inside an array_agg. I suspect you're right that the first one is running into some kind of method size limit in the jit. Time 1 is master, Time 2 is this branch. ```python; @benchmark; def table_aggregate_linregN():; ht = hl.read_table(resource('many_ints_table.ht')); ht.aggregate(hl.tuple([hl.agg.linreg(ht[f'i{i%5}'],; [ht[f'i{(i+1)%5}'], ht[f'i{(i+2)%5}']]); for i in range(N)])); ```. ```quote; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_linreg20 101.2% 11.622 11.765; table_aggregate_linreg21 98.0% 11.964 11.730; table_aggregate_linreg22 22.2% 54.272 12.068; table_aggregate_linreg23 99.9% 54.989 54.939; table_aggregate_linreg24 101.7% 54.753 55.661; table_aggregate_linreg25 102.7% 55.902 57.422; ----------------------; Geometric mean: 78.3%; Simple mean: 87.6%; Median: 100.6%; ```. ```python; @benchmark; def table_aggregate_linregN():; ht = hl.read_table(resource('many_ints_table.ht')); ht.aggregate(hl.agg.array_agg(lambda i: hl.agg.linreg(ht.i0 + i, [ht.i1, ht.i2]),; hl.range(N))); ```. ```quote; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_linreg22 79.1% 10.609 8.389; table_aggregate_linreg23 79.0% 11.006 8.695; table_aggregate_linreg21 78.6% 10.559 8.295; table_aggregate_linreg25 77.2% 11.579 8.937; table_aggregate_linreg24 76.3% 11.520 8.793; table_aggregate_linreg20 75.7% 10.792 8.173; ----------------------; Geometric mean: 77.6%; Simple mean: 77.6%; Median: 77.9%; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-540058423
https://github.com/hail-is/hail/pull/7134#issuecomment-540058423:1513,Usability,Simpl,Simple,1513,"Two different tests: first running N linregs as separate aggregations, and the second running N linregs inside an array_agg. I suspect you're right that the first one is running into some kind of method size limit in the jit. Time 1 is master, Time 2 is this branch. ```python; @benchmark; def table_aggregate_linregN():; ht = hl.read_table(resource('many_ints_table.ht')); ht.aggregate(hl.tuple([hl.agg.linreg(ht[f'i{i%5}'],; [ht[f'i{(i+1)%5}'], ht[f'i{(i+2)%5}']]); for i in range(N)])); ```. ```quote; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_linreg20 101.2% 11.622 11.765; table_aggregate_linreg21 98.0% 11.964 11.730; table_aggregate_linreg22 22.2% 54.272 12.068; table_aggregate_linreg23 99.9% 54.989 54.939; table_aggregate_linreg24 101.7% 54.753 55.661; table_aggregate_linreg25 102.7% 55.902 57.422; ----------------------; Geometric mean: 78.3%; Simple mean: 87.6%; Median: 100.6%; ```. ```python; @benchmark; def table_aggregate_linregN():; ht = hl.read_table(resource('many_ints_table.ht')); ht.aggregate(hl.agg.array_agg(lambda i: hl.agg.linreg(ht.i0 + i, [ht.i1, ht.i2]),; hl.range(N))); ```. ```quote; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_linreg22 79.1% 10.609 8.389; table_aggregate_linreg23 79.0% 11.006 8.695; table_aggregate_linreg21 78.6% 10.559 8.295; table_aggregate_linreg25 77.2% 11.579 8.937; table_aggregate_linreg24 76.3% 11.520 8.793; table_aggregate_linreg20 75.7% 10.792 8.173; ----------------------; Geometric mean: 77.6%; Simple mean: 77.6%; Median: 77.9%; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-540058423
https://github.com/hail-is/hail/pull/7134#issuecomment-542277918:33,Testability,test,tests,33,@patrick-schultz Looks like some tests are failing / docs conflicts,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-542277918
https://github.com/hail-is/hail/pull/7134#issuecomment-542281084:32,Availability,failure,failures,32,"I started looking into the test failures last week, but I can't reproduce them locally; I'm very confused. Anyways, I'm working on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-542281084
https://github.com/hail-is/hail/pull/7134#issuecomment-542281084:27,Testability,test,test,27,"I started looking into the test failures last week, but I can't reproduce them locally; I'm very confused. Anyways, I'm working on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-542281084
https://github.com/hail-is/hail/pull/7134#issuecomment-555510546:66,Testability,test,tests,66,"I think scorecard has been broken for a while w.r.t. the ""failing tests"" category. That's just not showing up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-555510546
https://github.com/hail-is/hail/pull/7134#issuecomment-558180275:52,Testability,test,tests,52,"Okay, I think this is ready for re-review (assuming tests finally pass). Note the changes to `ld_score_regression`, which makes it generate smaller IR by using `AggArrayPerElement` instead of unrolled loops. That solved the blown stack in the parser and the class size issues, so I was able to put the linreg aggregator back the way it was (with `n` and `yty` computed in separate aggregations).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-558180275
https://github.com/hail-is/hail/pull/7134#issuecomment-558211252:20,Availability,error,error,20,looks like a rebase error?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7134#issuecomment-558211252
https://github.com/hail-is/hail/issues/7136#issuecomment-535590377:341,Deployability,install,installation,341,"Sorry you hit this -- I think I understand what happened. Since we don't include jupyter as a Hail package dependency (it's a large dependency and pulls in a host of transitive dependencies as well), when you ran `jupyter` you picked up a different `jupyter` (probably the conda base environment one, which uses an entirely different Python installation). . I don't want to add jupyter as a dependency, but we can certainly add a note in the tutorials landing page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7136#issuecomment-535590377
https://github.com/hail-is/hail/issues/7136#issuecomment-535590377:107,Integrability,depend,dependency,107,"Sorry you hit this -- I think I understand what happened. Since we don't include jupyter as a Hail package dependency (it's a large dependency and pulls in a host of transitive dependencies as well), when you ran `jupyter` you picked up a different `jupyter` (probably the conda base environment one, which uses an entirely different Python installation). . I don't want to add jupyter as a dependency, but we can certainly add a note in the tutorials landing page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7136#issuecomment-535590377
https://github.com/hail-is/hail/issues/7136#issuecomment-535590377:132,Integrability,depend,dependency,132,"Sorry you hit this -- I think I understand what happened. Since we don't include jupyter as a Hail package dependency (it's a large dependency and pulls in a host of transitive dependencies as well), when you ran `jupyter` you picked up a different `jupyter` (probably the conda base environment one, which uses an entirely different Python installation). . I don't want to add jupyter as a dependency, but we can certainly add a note in the tutorials landing page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7136#issuecomment-535590377
https://github.com/hail-is/hail/issues/7136#issuecomment-535590377:177,Integrability,depend,dependencies,177,"Sorry you hit this -- I think I understand what happened. Since we don't include jupyter as a Hail package dependency (it's a large dependency and pulls in a host of transitive dependencies as well), when you ran `jupyter` you picked up a different `jupyter` (probably the conda base environment one, which uses an entirely different Python installation). . I don't want to add jupyter as a dependency, but we can certainly add a note in the tutorials landing page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7136#issuecomment-535590377
https://github.com/hail-is/hail/issues/7136#issuecomment-535590377:391,Integrability,depend,dependency,391,"Sorry you hit this -- I think I understand what happened. Since we don't include jupyter as a Hail package dependency (it's a large dependency and pulls in a host of transitive dependencies as well), when you ran `jupyter` you picked up a different `jupyter` (probably the conda base environment one, which uses an entirely different Python installation). . I don't want to add jupyter as a dependency, but we can certainly add a note in the tutorials landing page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7136#issuecomment-535590377
https://github.com/hail-is/hail/pull/7141#issuecomment-536620808:55,Usability,simpl,simplify,55,"addressed your comment (good catch). I commented out a simplify rule that is currently invalid, but will be possible to introduce soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7141#issuecomment-536620808
https://github.com/hail-is/hail/issues/7144#issuecomment-535749300:80,Performance,optimiz,optimization,80,"there will definitely be a regime where that's faster, yeah. But that's just an optimization to order_by, yes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7144#issuecomment-535749300
https://github.com/hail-is/hail/issues/7144#issuecomment-535750439:34,Performance,optimiz,optimization,34,"qq uses key_by, but yeah, it's an optimization. I mean, there's just so few cases where you actually want to use a spark shuffle. I don't think that many people are creating qq plots with 1 billion points.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7144#issuecomment-535750439
https://github.com/hail-is/hail/issues/7144#issuecomment-570306267:40,Usability,simpl,simplify,40,`TableCollect(TableOrderBy(...))` has a simplify rule that uses a local sort,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7144#issuecomment-570306267
https://github.com/hail-is/hail/pull/7145#issuecomment-536150091:849,Availability,avail,available,849,"> I'm seeing issues creating notebooks (500). I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). > reaching CI and Batch (502). Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. > Notebook2 link should be changed to redirect to notebook 1. There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536150091
https://github.com/hail-is/hail/pull/7145#issuecomment-536150091:336,Deployability,deploy,deployed,336,"> I'm seeing issues creating notebooks (500). I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). > reaching CI and Batch (502). Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. > Notebook2 link should be changed to redirect to notebook 1. There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536150091
https://github.com/hail-is/hail/pull/7145#issuecomment-536150091:373,Testability,test,test,373,"> I'm seeing issues creating notebooks (500). I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). > reaching CI and Batch (502). Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. > Notebook2 link should be changed to redirect to notebook 1. There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536150091
https://github.com/hail-is/hail/pull/7145#issuecomment-536197945:334,Availability,avail,available,334,"> There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2. I just mean we should have a gateway/router redirect, or have 404's issued. Right now some kind of routing happens, resulting in a certificate error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536197945
https://github.com/hail-is/hail/pull/7145#issuecomment-536197945:518,Availability,error,error,518,"> There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2. I just mean we should have a gateway/router redirect, or have 404's issued. Right now some kind of routing happens, resulting in a certificate error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536197945
https://github.com/hail-is/hail/pull/7145#issuecomment-536197945:412,Integrability,rout,router,412,"> There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2. I just mean we should have a gateway/router redirect, or have 404's issued. Right now some kind of routing happens, resulting in a certificate error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536197945
https://github.com/hail-is/hail/pull/7145#issuecomment-536197945:474,Integrability,rout,routing,474,"> There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2. I just mean we should have a gateway/router redirect, or have 404's issued. Right now some kind of routing happens, resulting in a certificate error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536197945
https://github.com/hail-is/hail/pull/7145#issuecomment-536197945:506,Security,certificate,certificate,506,"> There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2. I just mean we should have a gateway/router redirect, or have 404's issued. Right now some kind of routing happens, resulting in a certificate error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536197945
https://github.com/hail-is/hail/pull/7145#issuecomment-536222416:273,Availability,error,error,273,"Ah. This has nothing to do with notebook2. We have a wildcard DNS entry for *.hail.is so we don't have to modify DNS every time we add/remove a service. However, Let's Encrypt didn't support wildcard certificates when I wrote that code. So anything.hail.is will get a cert error. To fix this we either need to get a wildcard cert or fix the subdomains we use in DNS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536222416
https://github.com/hail-is/hail/pull/7145#issuecomment-536222416:168,Security,Encrypt,Encrypt,168,"Ah. This has nothing to do with notebook2. We have a wildcard DNS entry for *.hail.is so we don't have to modify DNS every time we add/remove a service. However, Let's Encrypt didn't support wildcard certificates when I wrote that code. So anything.hail.is will get a cert error. To fix this we either need to get a wildcard cert or fix the subdomains we use in DNS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536222416
https://github.com/hail-is/hail/pull/7145#issuecomment-536222416:200,Security,certificate,certificates,200,"Ah. This has nothing to do with notebook2. We have a wildcard DNS entry for *.hail.is so we don't have to modify DNS every time we add/remove a service. However, Let's Encrypt didn't support wildcard certificates when I wrote that code. So anything.hail.is will get a cert error. To fix this we either need to get a wildcard cert or fix the subdomains we use in DNS.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536222416
https://github.com/hail-is/hail/pull/7145#issuecomment-536244757:234,Deployability,update,update,234,"> I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). Makes sense. Could you update the branch in your namespace? If not I can check tomorrow in mine. > Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. Got it. It's just that these changes affect CI and Batch, so I would like to quickly glance at a working example. Again, can do this tomorrow in my namespace. Not entirely comfortable with dev deploy yet, so want to try it when I'm less tired. Sounds like this will go in tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536244757
https://github.com/hail-is/hail/pull/7145#issuecomment-536244757:360,Deployability,deploy,deployed,360,"> I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). Makes sense. Could you update the branch in your namespace? If not I can check tomorrow in mine. > Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. Got it. It's just that these changes affect CI and Batch, so I would like to quickly glance at a working example. Again, can do this tomorrow in my namespace. Not entirely comfortable with dev deploy yet, so want to try it when I'm less tired. Sounds like this will go in tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536244757
https://github.com/hail-is/hail/pull/7145#issuecomment-536244757:672,Deployability,deploy,deploy,672,"> I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). Makes sense. Could you update the branch in your namespace? If not I can check tomorrow in mine. > Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. Got it. It's just that these changes affect CI and Batch, so I would like to quickly glance at a working example. Again, can do this tomorrow in my namespace. Not entirely comfortable with dev deploy yet, so want to try it when I'm less tired. Sounds like this will go in tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536244757
https://github.com/hail-is/hail/pull/7145#issuecomment-536244757:397,Testability,test,test,397,"> I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). Makes sense. Could you update the branch in your namespace? If not I can check tomorrow in mine. > Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. Got it. It's just that these changes affect CI and Batch, so I would like to quickly glance at a working example. Again, can do this tomorrow in my namespace. Not entirely comfortable with dev deploy yet, so want to try it when I'm less tired. Sounds like this will go in tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536244757
https://github.com/hail-is/hail/pull/7145#issuecomment-536245619:40,Deployability,deploy,deployed,40,"This branch, notebook, ci and batch are deployed in my namespace now. Note, batch is tested pretty well, and we test CI comes up (which is about all you can do in a dev deploy).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245619
https://github.com/hail-is/hail/pull/7145#issuecomment-536245619:169,Deployability,deploy,deploy,169,"This branch, notebook, ci and batch are deployed in my namespace now. Note, batch is tested pretty well, and we test CI comes up (which is about all you can do in a dev deploy).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245619
https://github.com/hail-is/hail/pull/7145#issuecomment-536245619:85,Testability,test,tested,85,"This branch, notebook, ci and batch are deployed in my namespace now. Note, batch is tested pretty well, and we test CI comes up (which is about all you can do in a dev deploy).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245619
https://github.com/hail-is/hail/pull/7145#issuecomment-536245619:112,Testability,test,test,112,"This branch, notebook, ci and batch are deployed in my namespace now. Note, batch is tested pretty well, and we test CI comes up (which is about all you can do in a dev deploy).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245619
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:4,Availability,error,error,4,"The error, some resources are missing. . {""levelname"": ""ERROR"", ""asctime"": ""2019-09-29 03:43:05,260"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory\n response = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/gear/csrf.py\"", line 20, in wrapped\n return await fun(request, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 77, in wrapped\n return await fun(request, userdata, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 417, in post_notebook\n return await _post_notebook('notebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:56,Availability,ERROR,ERROR,56,"The error, some resources are missing. . {""levelname"": ""ERROR"", ""asctime"": ""2019-09-29 03:43:05,260"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory\n response = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/gear/csrf.py\"", line 20, in wrapped\n return await fun(request, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 77, in wrapped\n return await fun(request, userdata, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 417, in post_notebook\n return await _post_notebook('notebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:185,Availability,Error,Error,185,"The error, some resources are missing. . {""levelname"": ""ERROR"", ""asctime"": ""2019-09-29 03:43:05,260"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory\n response = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/gear/csrf.py\"", line 20, in wrapped\n return await fun(request, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 77, in wrapped\n return await fun(request, userdata, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 417, in post_notebook\n return await _post_notebook('notebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:2548,Availability,Failure,Failure,2548,"ebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\nHTTP response headers: <CIMultiDictProxy('Audit-Id': '16158e81-8543-457e-8bea-5d5e1a8c39f3', 'Content-Type': 'application/json', 'Date': 'Sun, 29 Sep 2019 03:43:05 GMT', 'Content-Length': '901')>\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""Pod \\\""notebook-worker-9z6tg\\\"" is invalid: [spec.volumes[1].secret.secretName: Required value, spec.volumes[2].secret.secretName: Required value, spec.containers[0].volumeMounts[1].name: Not found: \\\""gsa-key\\\"", spec.containers[0].volumeMounts[2].name: Not found: \\\""user-tokens\\\""]\"",\""reason\"":\""Invalid\"",\""details\"":{\""name\"":\""notebook-worker-9z6tg\"",\""kind\"":\""Pod\"",\""causes\"":[{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[1].secret.secretName\""},{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[2].secret.secretName\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""gsa-key\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[1].name\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""user-tokens\\\""\"",\""fiel",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:174,Integrability,message,message,174,"The error, some resources are missing. . {""levelname"": ""ERROR"", ""asctime"": ""2019-09-29 03:43:05,260"", ""filename"": ""web_protocol.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory\n response = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/gear/csrf.py\"", line 20, in wrapped\n return await fun(request, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 77, in wrapped\n return await fun(request, userdata, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 417, in post_notebook\n return await _post_notebook('notebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:1091,Integrability,wrap,wrapped,1091,"col.py"", ""funcNameAndLine"": ""log_exception:355"", ""message"": ""Error handling request"", ""exc_info"": ""Traceback (most recent call last):\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory\n response = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/gear/csrf.py\"", line 20, in wrapped\n return await fun(request, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 77, in wrapped\n return await fun(request, userdata, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 417, in post_notebook\n return await _post_notebook('notebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:1220,Integrability,wrap,wrapped,1220,"n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_protocol.py\"", line 418, in start\n resp = await task\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_app.py\"", line 458, in _handle\n resp = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp/web_middlewares.py\"", line 119, in impl\n return await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/aiohttp_session/__init__.py\"", line 152, in factory\n response = await handler(request)\n File \""/usr/local/lib/python3.6/dist-packages/gear/csrf.py\"", line 20, in wrapped\n return await fun(request, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/gear/auth.py\"", line 77, in wrapped\n return await fun(request, userdata, *args, **kwargs)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 417, in post_notebook\n return await _post_notebook('notebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\n",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:2560,Integrability,message,message,2560,"ebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\nHTTP response headers: <CIMultiDictProxy('Audit-Id': '16158e81-8543-457e-8bea-5d5e1a8c39f3', 'Content-Type': 'application/json', 'Date': 'Sun, 29 Sep 2019 03:43:05 GMT', 'Content-Length': '901')>\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""Pod \\\""notebook-worker-9z6tg\\\"" is invalid: [spec.volumes[1].secret.secretName: Required value, spec.volumes[2].secret.secretName: Required value, spec.containers[0].volumeMounts[1].name: Not found: \\\""gsa-key\\\"", spec.containers[0].volumeMounts[2].name: Not found: \\\""user-tokens\\\""]\"",\""reason\"":\""Invalid\"",\""details\"":{\""name\"":\""notebook-worker-9z6tg\"",\""kind\"":\""Pod\"",\""causes\"":[{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[1].secret.secretName\""},{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[2].secret.secretName\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""gsa-key\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[1].name\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""user-tokens\\\""\"",\""fiel",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:3002,Integrability,message,message,3002,"ages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\nHTTP response headers: <CIMultiDictProxy('Audit-Id': '16158e81-8543-457e-8bea-5d5e1a8c39f3', 'Content-Type': 'application/json', 'Date': 'Sun, 29 Sep 2019 03:43:05 GMT', 'Content-Length': '901')>\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""Pod \\\""notebook-worker-9z6tg\\\"" is invalid: [spec.volumes[1].secret.secretName: Required value, spec.volumes[2].secret.secretName: Required value, spec.containers[0].volumeMounts[1].name: Not found: \\\""gsa-key\\\"", spec.containers[0].volumeMounts[2].name: Not found: \\\""user-tokens\\\""]\"",\""reason\"":\""Invalid\"",\""details\"":{\""name\"":\""notebook-worker-9z6tg\"",\""kind\"":\""Pod\"",\""causes\"":[{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[1].secret.secretName\""},{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[2].secret.secretName\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""gsa-key\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[1].name\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""user-tokens\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[2].name\""}]},\""code\"":422}\n\n""}",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:3117,Integrability,message,message,3117,"ages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\nHTTP response headers: <CIMultiDictProxy('Audit-Id': '16158e81-8543-457e-8bea-5d5e1a8c39f3', 'Content-Type': 'application/json', 'Date': 'Sun, 29 Sep 2019 03:43:05 GMT', 'Content-Length': '901')>\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""Pod \\\""notebook-worker-9z6tg\\\"" is invalid: [spec.volumes[1].secret.secretName: Required value, spec.volumes[2].secret.secretName: Required value, spec.containers[0].volumeMounts[1].name: Not found: \\\""gsa-key\\\"", spec.containers[0].volumeMounts[2].name: Not found: \\\""user-tokens\\\""]\"",\""reason\"":\""Invalid\"",\""details\"":{\""name\"":\""notebook-worker-9z6tg\"",\""kind\"":\""Pod\"",\""causes\"":[{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[1].secret.secretName\""},{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[2].secret.secretName\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""gsa-key\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[1].name\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""user-tokens\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[2].name\""}]},\""code\"":422}\n\n""}",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:3232,Integrability,message,message,3232,"ages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\nHTTP response headers: <CIMultiDictProxy('Audit-Id': '16158e81-8543-457e-8bea-5d5e1a8c39f3', 'Content-Type': 'application/json', 'Date': 'Sun, 29 Sep 2019 03:43:05 GMT', 'Content-Length': '901')>\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""Pod \\\""notebook-worker-9z6tg\\\"" is invalid: [spec.volumes[1].secret.secretName: Required value, spec.volumes[2].secret.secretName: Required value, spec.containers[0].volumeMounts[1].name: Not found: \\\""gsa-key\\\"", spec.containers[0].volumeMounts[2].name: Not found: \\\""user-tokens\\\""]\"",\""reason\"":\""Invalid\"",\""details\"":{\""name\"":\""notebook-worker-9z6tg\"",\""kind\"":\""Pod\"",\""causes\"":[{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[1].secret.secretName\""},{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[2].secret.secretName\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""gsa-key\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[1].name\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""user-tokens\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[2].name\""}]},\""code\"":422}\n\n""}",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:3365,Integrability,message,message,3365,"ages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\nHTTP response headers: <CIMultiDictProxy('Audit-Id': '16158e81-8543-457e-8bea-5d5e1a8c39f3', 'Content-Type': 'application/json', 'Date': 'Sun, 29 Sep 2019 03:43:05 GMT', 'Content-Length': '901')>\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""Pod \\\""notebook-worker-9z6tg\\\"" is invalid: [spec.volumes[1].secret.secretName: Required value, spec.volumes[2].secret.secretName: Required value, spec.containers[0].volumeMounts[1].name: Not found: \\\""gsa-key\\\"", spec.containers[0].volumeMounts[2].name: Not found: \\\""user-tokens\\\""]\"",\""reason\"":\""Invalid\"",\""details\"":{\""name\"":\""notebook-worker-9z6tg\"",\""kind\"":\""Pod\"",\""causes\"":[{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[1].secret.secretName\""},{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[2].secret.secretName\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""gsa-key\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[1].name\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""user-tokens\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[2].name\""}]},\""code\"":422}\n\n""}",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536245851:2301,Security,Audit,Audit-Id,2301,"ebook', request, userdata)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 278, in _post_notebook\n pod = await start_pod(k8s, service, userdata, notebook_token, jupyter_token)\n File \""/usr/local/lib/python3.6/dist-packages/notebook/notebook.py\"", line 167, in start_pod\n _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/api_client.py\"", line 166, in __call_api\n _request_timeout=_request_timeout)\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 230, in POST\n body=body))\n File \""/usr/local/lib/python3.6/dist-packages/kubernetes_asyncio/client/rest.py\"", line 181, in request\n raise ApiException(http_resp=r)\nkubernetes_asyncio.client.rest.ApiException: (422)\nReason: Unprocessable Entity\nHTTP response headers: <CIMultiDictProxy('Audit-Id': '16158e81-8543-457e-8bea-5d5e1a8c39f3', 'Content-Type': 'application/json', 'Date': 'Sun, 29 Sep 2019 03:43:05 GMT', 'Content-Length': '901')>\nHTTP response body: {\""kind\"":\""Status\"",\""apiVersion\"":\""v1\"",\""metadata\"":{},\""status\"":\""Failure\"",\""message\"":\""Pod \\\""notebook-worker-9z6tg\\\"" is invalid: [spec.volumes[1].secret.secretName: Required value, spec.volumes[2].secret.secretName: Required value, spec.containers[0].volumeMounts[1].name: Not found: \\\""gsa-key\\\"", spec.containers[0].volumeMounts[2].name: Not found: \\\""user-tokens\\\""]\"",\""reason\"":\""Invalid\"",\""details\"":{\""name\"":\""notebook-worker-9z6tg\"",\""kind\"":\""Pod\"",\""causes\"":[{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[1].secret.secretName\""},{\""reason\"":\""FieldValueRequired\"",\""message\"":\""Required value\"",\""field\"":\""spec.volumes[2].secret.secretName\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""gsa-key\\\""\"",\""field\"":\""spec.containers[0].volumeMounts[1].name\""},{\""reason\"":\""FieldValueNotFound\"",\""message\"":\""Not found: \\\""user-tokens\\\""\"",\""fiel",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245851
https://github.com/hail-is/hail/pull/7145#issuecomment-536246164:33,Energy Efficiency,monitor,monitoring,33,"Oops, sorry, your PR was for the monitoring namespace. I see the problem, CI doesn't have a route for `''`. I'll push a fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246164
https://github.com/hail-is/hail/pull/7145#issuecomment-536246164:92,Integrability,rout,route,92,"Oops, sorry, your PR was for the monitoring namespace. I see the problem, CI doesn't have a route for `''`. I'll push a fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246164
https://github.com/hail-is/hail/pull/7145#issuecomment-536246245:35,Energy Efficiency,monitor,monitoring,35,"> Oops, sorry, your PR was for the monitoring namespace. I see the problem, CI doesn't have a route for `''`. I'll push a fix. Yeah, monitoring, thought you meant it was a similar issue (exact route not matching). Sounds good, I wasn't looking for the """", such a weird route.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246245
https://github.com/hail-is/hail/pull/7145#issuecomment-536246245:133,Energy Efficiency,monitor,monitoring,133,"> Oops, sorry, your PR was for the monitoring namespace. I see the problem, CI doesn't have a route for `''`. I'll push a fix. Yeah, monitoring, thought you meant it was a similar issue (exact route not matching). Sounds good, I wasn't looking for the """", such a weird route.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246245
https://github.com/hail-is/hail/pull/7145#issuecomment-536246245:94,Integrability,rout,route,94,"> Oops, sorry, your PR was for the monitoring namespace. I see the problem, CI doesn't have a route for `''`. I'll push a fix. Yeah, monitoring, thought you meant it was a similar issue (exact route not matching). Sounds good, I wasn't looking for the """", such a weird route.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246245
https://github.com/hail-is/hail/pull/7145#issuecomment-536246245:193,Integrability,rout,route,193,"> Oops, sorry, your PR was for the monitoring namespace. I see the problem, CI doesn't have a route for `''`. I'll push a fix. Yeah, monitoring, thought you meant it was a similar issue (exact route not matching). Sounds good, I wasn't looking for the """", such a weird route.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246245
https://github.com/hail-is/hail/pull/7145#issuecomment-536246245:269,Integrability,rout,route,269,"> Oops, sorry, your PR was for the monitoring namespace. I see the problem, CI doesn't have a route for `''`. I'll push a fix. Yeah, monitoring, thought you meant it was a similar issue (exact route not matching). Sounds good, I wasn't looking for the """", such a weird route.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246245
https://github.com/hail-is/hail/pull/7145#issuecomment-536246943:14,Deployability,deploy,deployed,14,"OK, fixed and deployed. It works now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246943
https://github.com/hail-is/hail/pull/7146#issuecomment-536017411:123,Testability,test,tests,123,"Hmm. @johnc1231, We should probably pick some time to go through all the BM functionality and verify that we have adequate tests for it all. It feels like we've had a never ending series of bugs discovered by users.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7146#issuecomment-536017411
https://github.com/hail-is/hail/pull/7146#issuecomment-536022939:103,Availability,error,error,103,"Yeah, agreed. I've never taken inventory of the full test suite. This was the first bug that caused an error message though, previous ones have just been about things being too slow. So we'll need benchmarks to catch that stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7146#issuecomment-536022939
https://github.com/hail-is/hail/pull/7146#issuecomment-536022939:109,Integrability,message,message,109,"Yeah, agreed. I've never taken inventory of the full test suite. This was the first bug that caused an error message though, previous ones have just been about things being too slow. So we'll need benchmarks to catch that stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7146#issuecomment-536022939
https://github.com/hail-is/hail/pull/7146#issuecomment-536022939:53,Testability,test,test,53,"Yeah, agreed. I've never taken inventory of the full test suite. This was the first bug that caused an error message though, previous ones have just been about things being too slow. So we'll need benchmarks to catch that stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7146#issuecomment-536022939
https://github.com/hail-is/hail/pull/7146#issuecomment-536022939:197,Testability,benchmark,benchmarks,197,"Yeah, agreed. I've never taken inventory of the full test suite. This was the first bug that caused an error message though, previous ones have just been about things being too slow. So we'll need benchmarks to catch that stuff.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7146#issuecomment-536022939
https://github.com/hail-is/hail/issues/7147#issuecomment-535985237:31,Integrability,wrap,wrap,31,pretty sure the solution is to wrap the links in angle brackets,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7147#issuecomment-535985237
https://github.com/hail-is/hail/pull/7155#issuecomment-536244530:25,Testability,test,test,25,"Sounds fine, but I can't test it, still getting 500's in your namespace when clicking on the create button. Guessing an issue on my end, but want to be sure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7155#issuecomment-536244530
https://github.com/hail-is/hail/pull/7162#issuecomment-536317934:152,Security,validat,validation,152,"What happens? I just tested it and it works fine for me. Although we probably shouldn't have spaces in workshop names, maybe I'll do that if/when I add validation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7162#issuecomment-536317934
https://github.com/hail-is/hail/pull/7162#issuecomment-536317934:21,Testability,test,tested,21,"What happens? I just tested it and it works fine for me. Although we probably shouldn't have spaces in workshop names, maybe I'll do that if/when I add validation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7162#issuecomment-536317934
https://github.com/hail-is/hail/pull/7162#issuecomment-536318084:154,Security,validat,validation,154,"> What happens? I just tested it and it works fine for me. Although we probably shouldn't have spaces in workshop names, maybe I'll do that if/when I add validation. It would claim deletion without deleting. Must have been resolved or caused by a separate issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7162#issuecomment-536318084
https://github.com/hail-is/hail/pull/7162#issuecomment-536318084:23,Testability,test,tested,23,"> What happens? I just tested it and it works fine for me. Although we probably shouldn't have spaces in workshop names, maybe I'll do that if/when I add validation. It would claim deletion without deleting. Must have been resolved or caused by a separate issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7162#issuecomment-536318084
https://github.com/hail-is/hail/pull/7162#issuecomment-536320928:294,Integrability,message,message,294,"> Must have been resolved or caused by a separate issue. Can you reproduce it? I looked over the code path and this is very hard to understand for two reasons: nothing treats spaces specifically, so it probably doesn't have anything to do with spaces, and I don't see how you can get a success message but the deletion didn't happen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7162#issuecomment-536320928
https://github.com/hail-is/hail/pull/7162#issuecomment-536323352:303,Integrability,message,message,303,"> > Must have been resolved or caused by a separate issue.; > ; > Can you reproduce it? I looked over the code path and this is very hard to understand for two reasons: nothing treats spaces specifically, so it probably doesn't have anything to do with spaces, and I don't see how you can get a success message but the deletion didn't happen. Nope, just tried, works great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7162#issuecomment-536323352
https://github.com/hail-is/hail/issues/7168#issuecomment-536808587:1048,Availability,down,downstream,1048,"This approach to joining doesn't work for me:; ```; In [1]: import hail as hl . In [2]: t = hl.utils.range_table(1) . In [3]: t2 = t.key_by(idx=t.idx, idx2=t.idx) . In [4]: t.annotate(foo=t2[t.key]) ; Traceback (most recent call last):; File ""<ipython-input-4-85e676382c80>"", line 1, in <module>; t.annotate(foo=t2[t.key]); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 368, in __getitem__; return self.index(*wrap_to_tuple(item)); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 1536, in index; raise ExpressionException(f""Key type mismatch: cannot index table with given expressions:\n""; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: int32, int32; Index Expressions: int32; ```. And since the annotation db is just built on joins, it wouldn't work in that setting either. Moreover, the annotation db needs to be careful with uniqueness of the key-row relationship. I try to avoid unnecessarily using `all_matches=True` which introduces arrays that complicate downstream work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7168#issuecomment-536808587
https://github.com/hail-is/hail/issues/7168#issuecomment-536808587:963,Safety,avoid,avoid,963,"This approach to joining doesn't work for me:; ```; In [1]: import hail as hl . In [2]: t = hl.utils.range_table(1) . In [3]: t2 = t.key_by(idx=t.idx, idx2=t.idx) . In [4]: t.annotate(foo=t2[t.key]) ; Traceback (most recent call last):; File ""<ipython-input-4-85e676382c80>"", line 1, in <module>; t.annotate(foo=t2[t.key]); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 368, in __getitem__; return self.index(*wrap_to_tuple(item)); File ""/usr/local/lib/python3.7/site-packages/hail/table.py"", line 1536, in index; raise ExpressionException(f""Key type mismatch: cannot index table with given expressions:\n""; ExpressionException: Key type mismatch: cannot index table with given expressions:; Table key: int32, int32; Index Expressions: int32; ```. And since the annotation db is just built on joins, it wouldn't work in that setting either. Moreover, the annotation db needs to be careful with uniqueness of the key-row relationship. I try to avoid unnecessarily using `all_matches=True` which introduces arrays that complicate downstream work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7168#issuecomment-536808587
https://github.com/hail-is/hail/issues/7173#issuecomment-570312503:45,Modifiability,parameteriz,parameterized,45,do we need loop?. don't we just need another parameterized aggregator?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7173#issuecomment-570312503
https://github.com/hail-is/hail/issues/7173#issuecomment-570318532:18,Security,expose,expose,18,though we need to expose it in python,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7173#issuecomment-570318532
https://github.com/hail-is/hail/pull/7178#issuecomment-542408914:12,Deployability,install,installed,12,"OK, the pip installed version uses a local copy of the annotation db JSON file. I cleaned up the lens thing a bit put it in a separate file and used ABCs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178#issuecomment-542408914
https://github.com/hail-is/hail/pull/7178#issuecomment-545569253:144,Deployability,update,updated,144,"@tpoterba when I changed ttuple from a mapping to a sequence, I lost `.values()` as a uniform way to get the types of a tstruct or ttuple. I've updated this PR to modify tstruct to have a `types()` which is analogous to ttuple's `types()`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7178#issuecomment-545569253
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:343,Deployability,configurat,configuration,343,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:681,Deployability,integrat,integrated,681,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:681,Integrability,integrat,integrated,681,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:853,Integrability,wrap,wrapping,853,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:343,Modifiability,config,configuration,343,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:985,Performance,perform,perform,985,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:1322,Performance,perform,perform,1322,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:153,Testability,test,tests,153,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:311,Testability,benchmark,benchmarking,311,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:666,Testability,benchmark,benchmark,666,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:877,Testability,test,test,877,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:1176,Testability,test,tests,1176,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:1013,Availability,Down,Download,1013,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:746,Deployability,install,installs,746,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:594,Modifiability,config,config,594,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:663,Modifiability,config,config,663,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:830,Modifiability,config,config,830,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:874,Modifiability,variab,variables,874,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:557,Security,password,password,557,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:58,Testability,benchmark,benchmarks,58,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:221,Testability,benchmark,benchmark,221,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:634,Testability,benchmark,benchmark,634,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:724,Testability,benchmark,benchmark,724,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:1022,Testability,log,logs,1022,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:1122,Testability,benchmark,benchmark,1122,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:1483,Usability,simpl,simpler,1483,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:298,Deployability,install,install,298,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:27,Testability,test,test,27,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:149,Testability,benchmark,benchmark,149,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:282,Testability,benchmark,benchmark,282,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:316,Testability,benchmark,benchmark,316,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:360,Testability,test,test-,360,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:438,Testability,benchmark,benchmark,438,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:452,Testability,test,test-,452,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:561,Testability,benchmark,benchmark,561,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:582,Testability,test,test-,582,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:620,Testability,log,logs,620,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:632,Testability,benchmark,benchmark,632,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:642,Testability,log,log,642,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:713,Testability,test,testing,713,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807
https://github.com/hail-is/hail/pull/7182#issuecomment-537763919:81,Availability,Error,Error,81,"ref: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20importing.20dbsnp.20VCF/near/177209550. I have confirmed that the test I added fails with the same error laurent saw in the linked conversation, and passes with the change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7182#issuecomment-537763919
https://github.com/hail-is/hail/pull/7182#issuecomment-537763919:188,Availability,error,error,188,"ref: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20importing.20dbsnp.20VCF/near/177209550. I have confirmed that the test I added fails with the same error laurent saw in the linked conversation, and passes with the change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7182#issuecomment-537763919
https://github.com/hail-is/hail/pull/7182#issuecomment-537763919:155,Testability,test,test,155,"ref: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Error.20importing.20dbsnp.20VCF/near/177209550. I have confirmed that the test I added fails with the same error laurent saw in the linked conversation, and passes with the change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7182#issuecomment-537763919
https://github.com/hail-is/hail/pull/7194#issuecomment-538159223:12,Testability,benchmark,benchmark,12,added a new benchmark:; ```; wm2b0-b9b:hail wang$ hail-bench compare old.json new.json --metric median; Name Ratio Time 1 Time 2; ---- ----- ------ ------; group_by_take_rekey 23.5% 23.902 5.612; ----------------------; Geometric mean: 23.5%; Simple mean: 23.5%; Median: 23.5%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7194#issuecomment-538159223
https://github.com/hail-is/hail/pull/7194#issuecomment-538159223:243,Usability,Simpl,Simple,243,added a new benchmark:; ```; wm2b0-b9b:hail wang$ hail-bench compare old.json new.json --metric median; Name Ratio Time 1 Time 2; ---- ----- ------ ------; group_by_take_rekey 23.5% 23.902 5.612; ----------------------; Geometric mean: 23.5%; Simple mean: 23.5%; Median: 23.5%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7194#issuecomment-538159223
https://github.com/hail-is/hail/pull/7195#issuecomment-538148467:265,Usability,Simpl,Simple,265,assuming I haven't messed something up:; ```; wm2b0-b9b:hail wang$ hail-bench compare old.json new.json --metric median; Name Ratio Time 1 Time 2; ---- ----- ------ ------; group_by_collect_per_row 10.0% 38.441 3.852; ----------------------; Geometric mean: 10.0%; Simple mean: 10.0%; Median: 10.0%; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7195#issuecomment-538148467
https://github.com/hail-is/hail/pull/7197#issuecomment-538549413:197,Performance,perform,performance,197,"I looked closer at the logic in the old combiner and realized it was permitting things within 1 window-size of the window to be binned. I wasn't doing this at all, so implemented it. This has nice performance properties, but uses more memory than the user requests, so I'm using just a 25% ""grace window"" plus the buffer to have both good performance and low memory usage. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_downsample_worst_case 39.5% 57.617 22.773; table_aggregate_downsample_dense 26.6% 127.079 33.843; ----------------------; Geometric mean: 32.4%; Simple mean: 33.1%; Median: 33.1%; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7197#issuecomment-538549413
https://github.com/hail-is/hail/pull/7197#issuecomment-538549413:339,Performance,perform,performance,339,"I looked closer at the logic in the old combiner and realized it was permitting things within 1 window-size of the window to be binned. I wasn't doing this at all, so implemented it. This has nice performance properties, but uses more memory than the user requests, so I'm using just a 25% ""grace window"" plus the buffer to have both good performance and low memory usage. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_downsample_worst_case 39.5% 57.617 22.773; table_aggregate_downsample_dense 26.6% 127.079 33.843; ----------------------; Geometric mean: 32.4%; Simple mean: 33.1%; Median: 33.1%; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7197#issuecomment-538549413
https://github.com/hail-is/hail/pull/7197#issuecomment-538549413:23,Testability,log,logic,23,"I looked closer at the logic in the old combiner and realized it was permitting things within 1 window-size of the window to be binned. I wasn't doing this at all, so implemented it. This has nice performance properties, but uses more memory than the user requests, so I'm using just a 25% ""grace window"" plus the buffer to have both good performance and low memory usage. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_downsample_worst_case 39.5% 57.617 22.773; table_aggregate_downsample_dense 26.6% 127.079 33.843; ----------------------; Geometric mean: 32.4%; Simple mean: 33.1%; Median: 33.1%; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7197#issuecomment-538549413
https://github.com/hail-is/hail/pull/7197#issuecomment-538549413:591,Usability,Simpl,Simple,591,"I looked closer at the logic in the old combiner and realized it was permitting things within 1 window-size of the window to be binned. I wasn't doing this at all, so implemented it. This has nice performance properties, but uses more memory than the user requests, so I'm using just a 25% ""grace window"" plus the buffer to have both good performance and low memory usage. ```; Name Ratio Time 1 Time 2; ---- ----- ------ ------; table_aggregate_downsample_worst_case 39.5% 57.617 22.773; table_aggregate_downsample_dense 26.6% 127.079 33.843; ----------------------; Geometric mean: 32.4%; Simple mean: 33.1%; Median: 33.1%; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7197#issuecomment-538549413
https://github.com/hail-is/hail/pull/7208#issuecomment-539151226:21,Deployability,install,install,21,"I like this, `uvloop.install()` has always made me a bit unsettled.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7208#issuecomment-539151226
https://github.com/hail-is/hail/pull/7209#issuecomment-539184173:8,Testability,test,test,8,Lack of test was me being lazy since I moved all the tests in my other ndarray PR. I'll just add one here and address the merge conflict in either direction.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7209#issuecomment-539184173
https://github.com/hail-is/hail/pull/7209#issuecomment-539184173:53,Testability,test,tests,53,Lack of test was me being lazy since I moved all the tests in my other ndarray PR. I'll just add one here and address the merge conflict in either direction.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7209#issuecomment-539184173
https://github.com/hail-is/hail/pull/7209#issuecomment-539190745:22,Testability,test,tests,22,"Agh, mostly unrelated tests are failing because of a bug that I've fixed in a different PR. I'll wait for that to go in (should go in tomorrow), then rebase and dismiss",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7209#issuecomment-539190745
https://github.com/hail-is/hail/pull/7213#issuecomment-539510518:14,Testability,test,tests,14,Failing flake tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7213#issuecomment-539510518
https://github.com/hail-is/hail/pull/7216#issuecomment-539671078:106,Performance,cache,cache,106,"I rebuilt the disk image from scratch and it's 2.1 GB. I'm not sure we can do much better than that if we cache the batch2 image on there. However, I found if we don't do that, it adds up to an extra minute.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7216#issuecomment-539671078
https://github.com/hail-is/hail/pull/7226#issuecomment-539972072:617,Availability,toler,tolerate,617,"OK, this is passing. Sorry for another big PR, I think I'm getting close to converging and I'll start spreading things around. Summary of changes:; - break batch into two services: batch2 and batch2-driver; - batch2 handles connections to the client, but has no driver; - driver has the driver and all the control loops; - workers now connect to batch2-driver, not batch2; - batch2 hits batch2-driver after close, cancel and delete to actually handle the jobs; - also removed abort and jsonify from utils and prefer the native aiohttp interfaces. I'll change the batch2 deployment in a later PR to run in triplicate, tolerate preemptibles and have a horizontal autoscaler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072
https://github.com/hail-is/hail/pull/7226#issuecomment-539972072:570,Deployability,deploy,deployment,570,"OK, this is passing. Sorry for another big PR, I think I'm getting close to converging and I'll start spreading things around. Summary of changes:; - break batch into two services: batch2 and batch2-driver; - batch2 handles connections to the client, but has no driver; - driver has the driver and all the control loops; - workers now connect to batch2-driver, not batch2; - batch2 hits batch2-driver after close, cancel and delete to actually handle the jobs; - also removed abort and jsonify from utils and prefer the native aiohttp interfaces. I'll change the batch2 deployment in a later PR to run in triplicate, tolerate preemptibles and have a horizontal autoscaler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072
https://github.com/hail-is/hail/pull/7226#issuecomment-539972072:535,Integrability,interface,interfaces,535,"OK, this is passing. Sorry for another big PR, I think I'm getting close to converging and I'll start spreading things around. Summary of changes:; - break batch into two services: batch2 and batch2-driver; - batch2 handles connections to the client, but has no driver; - driver has the driver and all the control loops; - workers now connect to batch2-driver, not batch2; - batch2 hits batch2-driver after close, cancel and delete to actually handle the jobs; - also removed abort and jsonify from utils and prefer the native aiohttp interfaces. I'll change the batch2 deployment in a later PR to run in triplicate, tolerate preemptibles and have a horizontal autoscaler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072
https://github.com/hail-is/hail/pull/7226#issuecomment-539972072:476,Safety,abort,abort,476,"OK, this is passing. Sorry for another big PR, I think I'm getting close to converging and I'll start spreading things around. Summary of changes:; - break batch into two services: batch2 and batch2-driver; - batch2 handles connections to the client, but has no driver; - driver has the driver and all the control loops; - workers now connect to batch2-driver, not batch2; - batch2 hits batch2-driver after close, cancel and delete to actually handle the jobs; - also removed abort and jsonify from utils and prefer the native aiohttp interfaces. I'll change the batch2 deployment in a later PR to run in triplicate, tolerate preemptibles and have a horizontal autoscaler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072
https://github.com/hail-is/hail/pull/7240#issuecomment-540264157:35,Usability,feedback,feedback,35,"yes, sure. I'll ask Kumar for more feedback tomorrow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7240#issuecomment-540264157
https://github.com/hail-is/hail/pull/7241#issuecomment-541195142:108,Usability,clear,clear,108,"What do you mean specifically? Code-wise? If you're asking how I feel about the code, I think the change is clear now, but it's a bit verbose. I think it was more readable without all of the `_mcpu` extensions, but was also more confusing that way. . I had an idea that I tried to implement earlier where we had a class `Cores` that internally represented the cores as mCPU but then printed everything in CPU when referenced. That could have worked but I had to be careful with how I overrode all of the mathematical operations. I thought this was overkill for the problem so I abandoned it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7241#issuecomment-541195142
https://github.com/hail-is/hail/pull/7241#issuecomment-541225212:185,Usability,clear,clearest,185,"Yeah, code-wise. I wanted to see how you felt my suggestion(s) were playing out. I agree with you. Having seen the code, I think I still like this best since it is the least confusing (clearest?) A `Cores` class if we used it consistently is an interesting option, although I also like that everyone knows how integers work and there's no conceptual overhead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7241#issuecomment-541225212
https://github.com/hail-is/hail/pull/7247#issuecomment-540742636:120,Integrability,depend,dependencies,120,"yes, but it does use modified functionality (namely info_to_keep is now potentially different). (which means transitive dependencies would no longer work in the same way; and VarDP=hl.cond(row.info.VarDP > vardp_outlier is totally different since that argument is removed)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7247#issuecomment-540742636
https://github.com/hail-is/hail/pull/7247#issuecomment-540747610:265,Availability,down,downstream,265,"The only place outside of tests where `combine_gvcfs` is called is `drive_combiner`. And no user would have had the opportunity to use the old parameter, which was a workaround for a bug in GATK that's no longer relevant with the way the combiner output is used by downstream. Also this is still _highly experimental_ functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7247#issuecomment-540747610
https://github.com/hail-is/hail/pull/7247#issuecomment-540747610:26,Testability,test,tests,26,"The only place outside of tests where `combine_gvcfs` is called is `drive_combiner`. And no user would have had the opportunity to use the old parameter, which was a workaround for a bug in GATK that's no longer relevant with the way the combiner output is used by downstream. Also this is still _highly experimental_ functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7247#issuecomment-540747610
https://github.com/hail-is/hail/pull/7252#issuecomment-540783673:62,Performance,load,loadField,62,"you'll get even more benefit from staging, to get rid of the `loadField` overhead. please follow up with DSP for the 1kg gvcfs so we can add a benchmark!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7252#issuecomment-540783673
https://github.com/hail-is/hail/pull/7252#issuecomment-540783673:143,Testability,benchmark,benchmark,143,"you'll get even more benefit from staging, to get rid of the `loadField` overhead. please follow up with DSP for the 1kg gvcfs so we can add a benchmark!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7252#issuecomment-540783673
https://github.com/hail-is/hail/pull/7252#issuecomment-540784159:62,Performance,load,loadField,62,"you'll get even more benefit from staging, to get rid of the `loadField` overhead. please follow up with DSP for the 1kg gvcfs so we can add a benchmark!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7252#issuecomment-540784159
https://github.com/hail-is/hail/pull/7252#issuecomment-540784159:143,Testability,benchmark,benchmark,143,"you'll get even more benefit from staging, to get rid of the `loadField` overhead. please follow up with DSP for the 1kg gvcfs so we can add a benchmark!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7252#issuecomment-540784159
https://github.com/hail-is/hail/pull/7252#issuecomment-540788545:43,Testability,test,test,43,"We will. But good news! Old 100 exome chr1 test: 38 min, new test: 15 min.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7252#issuecomment-540788545
https://github.com/hail-is/hail/pull/7252#issuecomment-540788545:61,Testability,test,test,61,"We will. But good news! Old 100 exome chr1 test: 38 min, new test: 15 min.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7252#issuecomment-540788545
https://github.com/hail-is/hail/pull/7252#issuecomment-540803531:45,Testability,test,test,45,"> We will. But good news! Old 100 exome chr1 test: 38 min, new test: 15 min. Shhhhh don't tell whoever pays our cloud bill",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7252#issuecomment-540803531
https://github.com/hail-is/hail/pull/7252#issuecomment-540803531:63,Testability,test,test,63,"> We will. But good news! Old 100 exome chr1 test: 38 min, new test: 15 min. Shhhhh don't tell whoever pays our cloud bill",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7252#issuecomment-540803531
https://github.com/hail-is/hail/pull/7261#issuecomment-543191626:42,Testability,test,tests,42,I think we may need to rethink our compat tests at some point -- a few more bumps and that test will take minutes to run,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7261#issuecomment-543191626
https://github.com/hail-is/hail/pull/7261#issuecomment-543191626:91,Testability,test,test,91,I think we may need to rethink our compat tests at some point -- a few more bumps and that test will take minutes to run,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7261#issuecomment-543191626
https://github.com/hail-is/hail/pull/7270#issuecomment-541263068:23,Testability,test,tests,23,"I had to remove the ui tests for now. The ui is protected by a session ID in the aiohttp session cookie (not the auth header I was trying to use), and I need a bit more infrastructure to cook up a valid one of those in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7270#issuecomment-541263068
https://github.com/hail-is/hail/pull/7270#issuecomment-541263068:223,Testability,test,tests,223,"I had to remove the ui tests for now. The ui is protected by a session ID in the aiohttp session cookie (not the auth header I was trying to use), and I need a bit more infrastructure to cook up a valid one of those in the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7270#issuecomment-541263068
https://github.com/hail-is/hail/pull/7287#issuecomment-541333109:109,Availability,toler,tolerations,109,"Pushed a couple more changes:; - Make apiVersions consistent, and bring them up to date; - Removed incorrect tolerations on CI and batch. A toleration means you can tolerate the given taint. So CI and batch were being scheduled on preemptibles which I didn't think we want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-541333109
https://github.com/hail-is/hail/pull/7287#issuecomment-541333109:140,Availability,toler,toleration,140,"Pushed a couple more changes:; - Make apiVersions consistent, and bring them up to date; - Removed incorrect tolerations on CI and batch. A toleration means you can tolerate the given taint. So CI and batch were being scheduled on preemptibles which I didn't think we want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-541333109
https://github.com/hail-is/hail/pull/7287#issuecomment-541333109:165,Availability,toler,tolerate,165,"Pushed a couple more changes:; - Make apiVersions consistent, and bring them up to date; - Removed incorrect tolerations on CI and batch. A toleration means you can tolerate the given taint. So CI and batch were being scheduled on preemptibles which I didn't think we want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-541333109
https://github.com/hail-is/hail/pull/7287#issuecomment-541333109:218,Energy Efficiency,schedul,scheduled,218,"Pushed a couple more changes:; - Make apiVersions consistent, and bring them up to date; - Removed incorrect tolerations on CI and batch. A toleration means you can tolerate the given taint. So CI and batch were being scheduled on preemptibles which I didn't think we want.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-541333109
https://github.com/hail-is/hail/pull/7287#issuecomment-542216720:18,Availability,error,error,18,Failed with a 500 error from batch 2.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-542216720
https://github.com/hail-is/hail/pull/7287#issuecomment-542232729:43,Availability,error,error,43,"Yes, I looked into this, it is a transient error that should be retried. I will change batch2 to use the retry infrastructure after this PR goes in: https://github.com/hail-is/hail/pull/7284",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7287#issuecomment-542232729
https://github.com/hail-is/hail/pull/7289#issuecomment-541355452:93,Availability,failure,failures,93,"Shoot, this was branched from the resource-link branch, which hasn't merged yet due to batch failures. Will reissue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7289#issuecomment-541355452
https://github.com/hail-is/hail/issues/7290#issuecomment-545056325:12,Usability,simpl,simpler,12,"Not needed, simpler js-based solution in https://github.com/hail-is/hail/pull/7334",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7290#issuecomment-545056325
https://github.com/hail-is/hail/pull/7291#issuecomment-541356252:47,Safety,avoid,avoid,47,"Also, the rules are as specific as they are to avoid using `!important` tags. These rules have slightly higher specificity than the rules they're overriding, so no `!important` tag is needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7291#issuecomment-541356252
https://github.com/hail-is/hail/issues/7292#issuecomment-558279772:326,Usability,undo,undocumented,326,Let me look over the docs again. There were a number of methods with no documentation when this was created. I don’t don’t quite understand what you mean by too broad (could you clarify?) I just mean that anything that doesn’t have an explanation should not be in the online doc. If you’re saying that there are no longer any undocumented methods then yes let’s close.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7292#issuecomment-558279772
https://github.com/hail-is/hail/issues/7292#issuecomment-558282593:112,Usability,undo,undocumented,112,"> But I actually think we are in decent shape now. I agree that we’re in decent shape. At the same time, having undocumented methods in our public documentation is not excellent, and we should strive for excellence/complete coverage",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7292#issuecomment-558282593
https://github.com/hail-is/hail/issues/7292#issuecomment-558284093:70,Availability,down,down,70,"Oh I see, I actually didn't realize / forgot you could have functions down below that were not present in the summary at the top. Agree we should fix this. I'm going to keep a running list of missing functions in the issue description",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7292#issuecomment-558284093
https://github.com/hail-is/hail/issues/7298#issuecomment-542178553:38,Deployability,install,installed,38,"I'm pretty sure that Hail isn't being installed correctly. But since you're not the one installing it, we can't really help. Let's continue discussion on the forum: https://discuss.hail.is/t/using-hail0-2-on-azure-databrick/1128",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7298#issuecomment-542178553
https://github.com/hail-is/hail/issues/7298#issuecomment-542178553:88,Deployability,install,installing,88,"I'm pretty sure that Hail isn't being installed correctly. But since you're not the one installing it, we can't really help. Let's continue discussion on the forum: https://discuss.hail.is/t/using-hail0-2-on-azure-databrick/1128",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7298#issuecomment-542178553
https://github.com/hail-is/hail/issues/7299#issuecomment-542180948:165,Deployability,update,update,165,"Sometimes new versions of packages introduce breaking changes, which is why we have version pins. I'll look at pandas shortly and see if there's any reason we can't update the upper bound on that range though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542180948
https://github.com/hail-is/hail/issues/7299#issuecomment-542183134:136,Deployability,update,updates,136,"Python versioning is a huge problem. Basically every time we have used unbounded dependency versions, we've gotten burned (some package updates and now Hail is broken for anyone who tries to install it). John could find out that 0.24 is supported, but then we'd have to pin at `<0.25`, so this doesn't solve the problem generally. I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542183134
https://github.com/hail-is/hail/issues/7299#issuecomment-542183134:191,Deployability,install,install,191,"Python versioning is a huge problem. Basically every time we have used unbounded dependency versions, we've gotten burned (some package updates and now Hail is broken for anyone who tries to install it). John could find out that 0.24 is supported, but then we'd have to pin at `<0.25`, so this doesn't solve the problem generally. I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542183134
https://github.com/hail-is/hail/issues/7299#issuecomment-542183134:81,Integrability,depend,dependency,81,"Python versioning is a huge problem. Basically every time we have used unbounded dependency versions, we've gotten burned (some package updates and now Hail is broken for anyone who tries to install it). John could find out that 0.24 is supported, but then we'd have to pin at `<0.25`, so this doesn't solve the problem generally. I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542183134
https://github.com/hail-is/hail/issues/7299#issuecomment-542194340:413,Deployability,continuous,continuous,413,"My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. > I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**. Wow, that's bad. I did not know conda removes old packages. Are you using some continuous integration?; We are solving these issues by some policy that pull requests have to pass CI building. ; If there is a new version of e.g. Pandas, CI will install it when somebody pushes a commit.; In case the new package version breaks something, CI fails and we get a notification to fix it. This way, we can in general keep up to date with the latest package versions and only rise the minimum version number of dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542194340
https://github.com/hail-is/hail/issues/7299#issuecomment-542194340:424,Deployability,integrat,integration,424,"My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. > I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**. Wow, that's bad. I did not know conda removes old packages. Are you using some continuous integration?; We are solving these issues by some policy that pull requests have to pass CI building. ; If there is a new version of e.g. Pandas, CI will install it when somebody pushes a commit.; In case the new package version breaks something, CI fails and we get a notification to fix it. This way, we can in general keep up to date with the latest package versions and only rise the minimum version number of dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542194340
https://github.com/hail-is/hail/issues/7299#issuecomment-542194340:578,Deployability,install,install,578,"My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. > I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**. Wow, that's bad. I did not know conda removes old packages. Are you using some continuous integration?; We are solving these issues by some policy that pull requests have to pass CI building. ; If there is a new version of e.g. Pandas, CI will install it when somebody pushes a commit.; In case the new package version breaks something, CI fails and we get a notification to fix it. This way, we can in general keep up to date with the latest package versions and only rise the minimum version number of dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542194340
https://github.com/hail-is/hail/issues/7299#issuecomment-542194340:424,Integrability,integrat,integration,424,"My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. > I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**. Wow, that's bad. I did not know conda removes old packages. Are you using some continuous integration?; We are solving these issues by some policy that pull requests have to pass CI building. ; If there is a new version of e.g. Pandas, CI will install it when somebody pushes a commit.; In case the new package version breaks something, CI fails and we get a notification to fix it. This way, we can in general keep up to date with the latest package versions and only rise the minimum version number of dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542194340
https://github.com/hail-is/hail/issues/7299#issuecomment-542194340:838,Integrability,depend,dependencies,838,"My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. > I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**. Wow, that's bad. I did not know conda removes old packages. Are you using some continuous integration?; We are solving these issues by some policy that pull requests have to pass CI building. ; If there is a new version of e.g. Pandas, CI will install it when somebody pushes a commit.; In case the new package version breaks something, CI fails and we get a notification to fix it. This way, we can in general keep up to date with the latest package versions and only rise the minimum version number of dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542194340
https://github.com/hail-is/hail/issues/7299#issuecomment-542204913:372,Deployability,continuous,continuous,372,"> My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. Yeah, I'm sympathetic to this. Our current pinned-version approach is partially a [over-]reaction to builds breaking without any internal change, especially because many new Hail users are also new Python users and not capable of debugging Python dependency hell. > Are you using some continuous integration?. We do use CI, but I'm not exactly sure what you're proposing. An automatic find-the-latest-version-that-works step? It's true that if we were to use unpinned versions, our CI would start failing if a package introduces a breaking change, and we can fix it. However, I think that's too late -- now all old Hail versions will break from fresh installs! I feel that reproducibility demands we pin versions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542204913
https://github.com/hail-is/hail/issues/7299#issuecomment-542204913:383,Deployability,integrat,integration,383,"> My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. Yeah, I'm sympathetic to this. Our current pinned-version approach is partially a [over-]reaction to builds breaking without any internal change, especially because many new Hail users are also new Python users and not capable of debugging Python dependency hell. > Are you using some continuous integration?. We do use CI, but I'm not exactly sure what you're proposing. An automatic find-the-latest-version-that-works step? It's true that if we were to use unpinned versions, our CI would start failing if a package introduces a breaking change, and we can fix it. However, I think that's too late -- now all old Hail versions will break from fresh installs! I feel that reproducibility demands we pin versions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542204913
https://github.com/hail-is/hail/issues/7299#issuecomment-542204913:738,Deployability,install,installs,738,"> My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. Yeah, I'm sympathetic to this. Our current pinned-version approach is partially a [over-]reaction to builds breaking without any internal change, especially because many new Hail users are also new Python users and not capable of debugging Python dependency hell. > Are you using some continuous integration?. We do use CI, but I'm not exactly sure what you're proposing. An automatic find-the-latest-version-that-works step? It's true that if we were to use unpinned versions, our CI would start failing if a package introduces a breaking change, and we can fix it. However, I think that's too late -- now all old Hail versions will break from fresh installs! I feel that reproducibility demands we pin versions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542204913
https://github.com/hail-is/hail/issues/7299#issuecomment-542204913:334,Integrability,depend,dependency,334,"> My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. Yeah, I'm sympathetic to this. Our current pinned-version approach is partially a [over-]reaction to builds breaking without any internal change, especially because many new Hail users are also new Python users and not capable of debugging Python dependency hell. > Are you using some continuous integration?. We do use CI, but I'm not exactly sure what you're proposing. An automatic find-the-latest-version-that-works step? It's true that if we were to use unpinned versions, our CI would start failing if a package introduces a breaking change, and we can fix it. However, I think that's too late -- now all old Hail versions will break from fresh installs! I feel that reproducibility demands we pin versions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542204913
https://github.com/hail-is/hail/issues/7299#issuecomment-542204913:383,Integrability,integrat,integration,383,"> My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. Yeah, I'm sympathetic to this. Our current pinned-version approach is partially a [over-]reaction to builds breaking without any internal change, especially because many new Hail users are also new Python users and not capable of debugging Python dependency hell. > Are you using some continuous integration?. We do use CI, but I'm not exactly sure what you're proposing. An automatic find-the-latest-version-that-works step? It's true that if we were to use unpinned versions, our CI would start failing if a package introduces a breaking change, and we can fix it. However, I think that's too late -- now all old Hail versions will break from fresh installs! I feel that reproducibility demands we pin versions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542204913
https://github.com/hail-is/hail/issues/7299#issuecomment-542205189:35,Integrability,depend,dependency,35,"(that's not to say we shouldn't do dependency audits every few months, bumping the pinned version to latest)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542205189
https://github.com/hail-is/hail/issues/7299#issuecomment-542205189:46,Security,audit,audits,46,"(that's not to say we shouldn't do dependency audits every few months, bumping the pinned version to latest)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542205189
https://github.com/hail-is/hail/issues/7299#issuecomment-542208997:30,Integrability,depend,dependency,30,"I'd propose to do an implicit dependency audit every time you push a new commit. You can still pin versions on published packages, but use unpinned dependencies for CI testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542208997
https://github.com/hail-is/hail/issues/7299#issuecomment-542208997:148,Integrability,depend,dependencies,148,"I'd propose to do an implicit dependency audit every time you push a new commit. You can still pin versions on published packages, but use unpinned dependencies for CI testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542208997
https://github.com/hail-is/hail/issues/7299#issuecomment-542208997:41,Security,audit,audit,41,"I'd propose to do an implicit dependency audit every time you push a new commit. You can still pin versions on published packages, but use unpinned dependencies for CI testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542208997
https://github.com/hail-is/hail/issues/7299#issuecomment-542208997:168,Testability,test,testing,168,"I'd propose to do an implicit dependency audit every time you push a new commit. You can still pin versions on published packages, but use unpinned dependencies for CI testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542208997
https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:515,Deployability,update,update,515,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446
https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:32,Integrability,depend,dependency,32,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446
https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:152,Integrability,depend,dependencies,152,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446
https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:279,Integrability,depend,depend,279,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446
https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:522,Integrability,depend,dependencies,522,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446
https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:43,Security,audit,audit,43,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446
https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:172,Testability,test,testing,172,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446
https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:222,Testability,test,test,222,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446
https://github.com/hail-is/hail/pull/7306#issuecomment-543238818:16,Testability,test,test,16,I added a basic test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7306#issuecomment-543238818
https://github.com/hail-is/hail/pull/7307#issuecomment-542398239:179,Energy Efficiency,reduce,reduce,179,I couldn't figure out how to get the gcloud and gsutil binaries into the docker container. This adds about 400 MB to the docker image. I'll make a to-do item to figure out how to reduce the image size further.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7307#issuecomment-542398239
https://github.com/hail-is/hail/pull/7324#issuecomment-543186317:134,Deployability,deploy,deployed,134,"Changes to the batch format (specifically around resources, but also a bug related to unused secrets namespace field) broke CI. I had deployed this as CI, and it looks it will be able to self-build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7324#issuecomment-543186317
https://github.com/hail-is/hail/pull/7324#issuecomment-543208801:416,Availability,error,error,416,"Pushed two more changes:; - Fixed bug in construction of volumes/volumeMounts in pod spec; - Increased resource limits on some build steps. Note, my change from yesterday set requests = limits (since that's what batch2 currently does), and some steps were requesting less memory than they required. If they landed on low-memory nodes, they would have failed. I will continue to increase as needed based on trial and error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7324#issuecomment-543208801
https://github.com/hail-is/hail/pull/7333#issuecomment-545621097:36,Deployability,deploy,deploy,36,Closing this while I debug with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7333#issuecomment-545621097
https://github.com/hail-is/hail/pull/7334#issuecomment-544655748:1704,Deployability,update,update,1704,">Can you help me understand why we can't set a very negative z-index on whatever element is creating the offset or use one of these shorter solutions?. Sure. These solutions don't work because they interfere with the layout of content. What you linked above works fine with 1 element, but not with adjacent elements. All of these solutions try to make up for the fact that the browser will position the element at the top of the browser (has no concept of non-0 offset). The javascript solution fixes this by introducing that non-0-offset. - I tried every permutation of these solutions, including every solution at the link you provided, in the original fix. They all interfere with layout in the presence of nested and adjacent modified elements. There is no z-index solution possible for all combinations, at least without JS (because you need adjacent elements to have descending ordered z-indices). MathJax was handling scrolling at page load. If you remove that line MathJax will scroll the page whenever it detects a hash in the URL. ""Since typesetting usually changes the vertical dimensions of the page, if the URL contains an anchor position, then after the page is typeset, you may no longer be positioned at the correct position on the page. MathJax can reposition to that location after it completes its initial typesetting of the page. This value controls whether MathJax will reposition the browser to the #hash location from the page URL after typesetting for the page"". * https://docs.mathjax.org/en/v2.7-latest/options/hub.html. . The reason we use history instead of say updating location.href is because there is no way to prevent the browser from scrolling to that location when you update that value. It's undefeatable, as mentioned in the comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748
https://github.com/hail-is/hail/pull/7334#issuecomment-544655748:943,Performance,load,load,943,">Can you help me understand why we can't set a very negative z-index on whatever element is creating the offset or use one of these shorter solutions?. Sure. These solutions don't work because they interfere with the layout of content. What you linked above works fine with 1 element, but not with adjacent elements. All of these solutions try to make up for the fact that the browser will position the element at the top of the browser (has no concept of non-0 offset). The javascript solution fixes this by introducing that non-0-offset. - I tried every permutation of these solutions, including every solution at the link you provided, in the original fix. They all interfere with layout in the presence of nested and adjacent modified elements. There is no z-index solution possible for all combinations, at least without JS (because you need adjacent elements to have descending ordered z-indices). MathJax was handling scrolling at page load. If you remove that line MathJax will scroll the page whenever it detects a hash in the URL. ""Since typesetting usually changes the vertical dimensions of the page, if the URL contains an anchor position, then after the page is typeset, you may no longer be positioned at the correct position on the page. MathJax can reposition to that location after it completes its initial typesetting of the page. This value controls whether MathJax will reposition the browser to the #hash location from the page URL after typesetting for the page"". * https://docs.mathjax.org/en/v2.7-latest/options/hub.html. . The reason we use history instead of say updating location.href is because there is no way to prevent the browser from scrolling to that location when you update that value. It's undefeatable, as mentioned in the comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748
https://github.com/hail-is/hail/pull/7334#issuecomment-544655748:1014,Safety,detect,detects,1014,">Can you help me understand why we can't set a very negative z-index on whatever element is creating the offset or use one of these shorter solutions?. Sure. These solutions don't work because they interfere with the layout of content. What you linked above works fine with 1 element, but not with adjacent elements. All of these solutions try to make up for the fact that the browser will position the element at the top of the browser (has no concept of non-0 offset). The javascript solution fixes this by introducing that non-0-offset. - I tried every permutation of these solutions, including every solution at the link you provided, in the original fix. They all interfere with layout in the presence of nested and adjacent modified elements. There is no z-index solution possible for all combinations, at least without JS (because you need adjacent elements to have descending ordered z-indices). MathJax was handling scrolling at page load. If you remove that line MathJax will scroll the page whenever it detects a hash in the URL. ""Since typesetting usually changes the vertical dimensions of the page, if the URL contains an anchor position, then after the page is typeset, you may no longer be positioned at the correct position on the page. MathJax can reposition to that location after it completes its initial typesetting of the page. This value controls whether MathJax will reposition the browser to the #hash location from the page URL after typesetting for the page"". * https://docs.mathjax.org/en/v2.7-latest/options/hub.html. . The reason we use history instead of say updating location.href is because there is no way to prevent the browser from scrolling to that location when you update that value. It's undefeatable, as mentioned in the comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748
https://github.com/hail-is/hail/pull/7334#issuecomment-544655748:1024,Security,hash,hash,1024,">Can you help me understand why we can't set a very negative z-index on whatever element is creating the offset or use one of these shorter solutions?. Sure. These solutions don't work because they interfere with the layout of content. What you linked above works fine with 1 element, but not with adjacent elements. All of these solutions try to make up for the fact that the browser will position the element at the top of the browser (has no concept of non-0 offset). The javascript solution fixes this by introducing that non-0-offset. - I tried every permutation of these solutions, including every solution at the link you provided, in the original fix. They all interfere with layout in the presence of nested and adjacent modified elements. There is no z-index solution possible for all combinations, at least without JS (because you need adjacent elements to have descending ordered z-indices). MathJax was handling scrolling at page load. If you remove that line MathJax will scroll the page whenever it detects a hash in the URL. ""Since typesetting usually changes the vertical dimensions of the page, if the URL contains an anchor position, then after the page is typeset, you may no longer be positioned at the correct position on the page. MathJax can reposition to that location after it completes its initial typesetting of the page. This value controls whether MathJax will reposition the browser to the #hash location from the page URL after typesetting for the page"". * https://docs.mathjax.org/en/v2.7-latest/options/hub.html. . The reason we use history instead of say updating location.href is because there is no way to prevent the browser from scrolling to that location when you update that value. It's undefeatable, as mentioned in the comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748
https://github.com/hail-is/hail/pull/7334#issuecomment-544655748:1422,Security,hash,hash,1422,">Can you help me understand why we can't set a very negative z-index on whatever element is creating the offset or use one of these shorter solutions?. Sure. These solutions don't work because they interfere with the layout of content. What you linked above works fine with 1 element, but not with adjacent elements. All of these solutions try to make up for the fact that the browser will position the element at the top of the browser (has no concept of non-0 offset). The javascript solution fixes this by introducing that non-0-offset. - I tried every permutation of these solutions, including every solution at the link you provided, in the original fix. They all interfere with layout in the presence of nested and adjacent modified elements. There is no z-index solution possible for all combinations, at least without JS (because you need adjacent elements to have descending ordered z-indices). MathJax was handling scrolling at page load. If you remove that line MathJax will scroll the page whenever it detects a hash in the URL. ""Since typesetting usually changes the vertical dimensions of the page, if the URL contains an anchor position, then after the page is typeset, you may no longer be positioned at the correct position on the page. MathJax can reposition to that location after it completes its initial typesetting of the page. This value controls whether MathJax will reposition the browser to the #hash location from the page URL after typesetting for the page"". * https://docs.mathjax.org/en/v2.7-latest/options/hub.html. . The reason we use history instead of say updating location.href is because there is no way to prevent the browser from scrolling to that location when you update that value. It's undefeatable, as mentioned in the comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748
https://github.com/hail-is/hail/pull/7334#issuecomment-544675789:249,Deployability,update,updated,249,"Regarding history and fake pages: I’m confused as to why fake pages would be used, since upon refresh that fake page wouldn’t correspond to a real page, but this shouldn’t interfere. The behavior without this solution should be the same: the url is updated with a hash. If you’ve noticed a concrete issue, please share it, because I may not understand the specific use (e.g. RTD). Haven’t seen any issues in testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544675789
https://github.com/hail-is/hail/pull/7334#issuecomment-544675789:264,Security,hash,hash,264,"Regarding history and fake pages: I’m confused as to why fake pages would be used, since upon refresh that fake page wouldn’t correspond to a real page, but this shouldn’t interfere. The behavior without this solution should be the same: the url is updated with a hash. If you’ve noticed a concrete issue, please share it, because I may not understand the specific use (e.g. RTD). Haven’t seen any issues in testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544675789
https://github.com/hail-is/hail/pull/7334#issuecomment-544675789:408,Testability,test,testing,408,"Regarding history and fake pages: I’m confused as to why fake pages would be used, since upon refresh that fake page wouldn’t correspond to a real page, but this shouldn’t interfere. The behavior without this solution should be the same: the url is updated with a hash. If you’ve noticed a concrete issue, please share it, because I may not understand the specific use (e.g. RTD). Haven’t seen any issues in testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544675789
https://github.com/hail-is/hail/pull/7334#issuecomment-544684545:341,Security,hash,hash,341,"OK, so the current solution addresses the problem, but:; - if MathJax changes the vertical layout of the page, we might have scrolled too far or not far enough to have the anchor located just below the header; - if we navigate to a new URL using any means other than clicking on an anchor tag, our history will have the URL with the special hash",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544684545
https://github.com/hail-is/hail/pull/7334#issuecomment-544692099:428,Security,hash,hash,428,"> if MathJax changes the vertical layout of the page, we might have scrolled too far or not far enough to have the anchor located just below the header. I haven't seen the issue in practice, but it would be better to listen to mathjaxj onload. Anchor tags / clicks of course wouldn't be affected. > if we navigate to a new URL using any means other than clicking on an anchor tag, our history will have the URL with the special hash. I don't think there's anything special about the hash; without this solution clicking on an anchor tag should produce exactly the same behavior (a #id appended).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544692099
https://github.com/hail-is/hail/pull/7334#issuecomment-544692099:483,Security,hash,hash,483,"> if MathJax changes the vertical layout of the page, we might have scrolled too far or not far enough to have the anchor located just below the header. I haven't seen the issue in practice, but it would be better to listen to mathjaxj onload. Anchor tags / clicks of course wouldn't be affected. > if we navigate to a new URL using any means other than clicking on an anchor tag, our history will have the URL with the special hash. I don't think there's anything special about the hash; without this solution clicking on an anchor tag should produce exactly the same behavior (a #id appended).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544692099
https://github.com/hail-is/hail/pull/7336#issuecomment-543974423:27,Performance,perform,performance,27,"The fix of the slow insert performance was to have `_close_batch` not submit N futures to try and `put_on_ready` queue. We were also using a ton of memory, but changing `get_jobs` in batches of 1000 helped the memory problem. I also split the create and delete into two separate pools so we don't block one operation with the other. My next goal is to figure out why the pods completing seems slow (maybe it's fine). I also think I haven't thought through the restart of batch2 carefully enough (do any pods get lost, performance, does it block?). . I think for now the performance of inserting 1 million jobs is acceptable. We're at approximately 20-30 seconds to build the DAG for 1 million jobs and then another 90-120 seconds to submit it. I'll try running it again over the weekend or on Monday 10-20 times to make sure it's consistent.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7336#issuecomment-543974423
https://github.com/hail-is/hail/pull/7336#issuecomment-543974423:113,Performance,queue,queue,113,"The fix of the slow insert performance was to have `_close_batch` not submit N futures to try and `put_on_ready` queue. We were also using a ton of memory, but changing `get_jobs` in batches of 1000 helped the memory problem. I also split the create and delete into two separate pools so we don't block one operation with the other. My next goal is to figure out why the pods completing seems slow (maybe it's fine). I also think I haven't thought through the restart of batch2 carefully enough (do any pods get lost, performance, does it block?). . I think for now the performance of inserting 1 million jobs is acceptable. We're at approximately 20-30 seconds to build the DAG for 1 million jobs and then another 90-120 seconds to submit it. I'll try running it again over the weekend or on Monday 10-20 times to make sure it's consistent.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7336#issuecomment-543974423
https://github.com/hail-is/hail/pull/7336#issuecomment-543974423:518,Performance,perform,performance,518,"The fix of the slow insert performance was to have `_close_batch` not submit N futures to try and `put_on_ready` queue. We were also using a ton of memory, but changing `get_jobs` in batches of 1000 helped the memory problem. I also split the create and delete into two separate pools so we don't block one operation with the other. My next goal is to figure out why the pods completing seems slow (maybe it's fine). I also think I haven't thought through the restart of batch2 carefully enough (do any pods get lost, performance, does it block?). . I think for now the performance of inserting 1 million jobs is acceptable. We're at approximately 20-30 seconds to build the DAG for 1 million jobs and then another 90-120 seconds to submit it. I'll try running it again over the weekend or on Monday 10-20 times to make sure it's consistent.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7336#issuecomment-543974423
https://github.com/hail-is/hail/pull/7336#issuecomment-543974423:570,Performance,perform,performance,570,"The fix of the slow insert performance was to have `_close_batch` not submit N futures to try and `put_on_ready` queue. We were also using a ton of memory, but changing `get_jobs` in batches of 1000 helped the memory problem. I also split the create and delete into two separate pools so we don't block one operation with the other. My next goal is to figure out why the pods completing seems slow (maybe it's fine). I also think I haven't thought through the restart of batch2 carefully enough (do any pods get lost, performance, does it block?). . I think for now the performance of inserting 1 million jobs is acceptable. We're at approximately 20-30 seconds to build the DAG for 1 million jobs and then another 90-120 seconds to submit it. I'll try running it again over the weekend or on Monday 10-20 times to make sure it's consistent.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7336#issuecomment-543974423
https://github.com/hail-is/hail/pull/7336#issuecomment-544184220:71,Performance,perform,performance,71,I'm closing this for now until I make sure that I haven't impacted the performance elsewhere in the system.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7336#issuecomment-544184220
https://github.com/hail-is/hail/pull/7342#issuecomment-547016313:11,Testability,test,test,11,Is there a test for this?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7342#issuecomment-547016313
https://github.com/hail-is/hail/pull/7347#issuecomment-544694599:69,Energy Efficiency,schedul,schedule,69,I also had to leave ensure_future(put_on_ready) in `deactivate` and `schedule`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7347#issuecomment-544694599
https://github.com/hail-is/hail/pull/7354#issuecomment-545104378:87,Integrability,interface,interface,87,"runtime in the container status is a little confusing. It's fine for now, but for user interface, runtime should be a top level state, and starting and running should be substates. But again, this is fine for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354#issuecomment-545104378
https://github.com/hail-is/hail/pull/7354#issuecomment-545167038:89,Integrability,interface,interface,89,"> runtime in the container status is a little confusing. It's fine for now, but for user interface, runtime should be a top level state, and starting and running should be substates. But again, this is fine for now. I'm a little confused by this. runtime isn't a state, it is rough the runtime of the container. Note, isn't the full runtime: it doesn't include image pull or log upload, for example. I included it in timing because it was something I timed. I included it because (1) it is comparable to `docker run` for comparison, and (2) I think it is what we should change the user for the runtime of the container ... I think? pull is also non trivial, as well as the storage of the image while it is running. Billing is going to be a challenge.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354#issuecomment-545167038
https://github.com/hail-is/hail/pull/7354#issuecomment-545167038:375,Testability,log,log,375,"> runtime in the container status is a little confusing. It's fine for now, but for user interface, runtime should be a top level state, and starting and running should be substates. But again, this is fine for now. I'm a little confused by this. runtime isn't a state, it is rough the runtime of the container. Note, isn't the full runtime: it doesn't include image pull or log upload, for example. I included it in timing because it was something I timed. I included it because (1) it is comparable to `docker run` for comparison, and (2) I think it is what we should change the user for the runtime of the container ... I think? pull is also non trivial, as well as the storage of the image while it is running. Billing is going to be a challenge.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7354#issuecomment-545167038
https://github.com/hail-is/hail/pull/7355#issuecomment-544988372:30,Testability,assert,assert,30,doesnt `NDArrayShape` need an assert?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7355#issuecomment-544988372
https://github.com/hail-is/hail/pull/7355#issuecomment-545073604:6,Testability,assert,asserts,6,added asserts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7355#issuecomment-545073604
https://github.com/hail-is/hail/issues/7358#issuecomment-555048507:121,Modifiability,evolve,evolved,121,I'm just going to close this and make separate tickets that aren't assigned to anyone in particular. The cheat sheet has evolved to a place where all of these are just nice to haves.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7358#issuecomment-555048507
https://github.com/hail-is/hail/pull/7359#issuecomment-545193658:604,Availability,echo,echo,604,"I'm not 100% sure, but I did this locally. I'd like to test it with aiodocker as well to make sure and confirm on the worker before merging. I couldn't find anything online that 100% confirmed, but my understanding of associated volumes was those created by the container create command based on this behavior. ```; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; (base) wmecc-475:ci jigold$ docker volume create foo; foo; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local foo; (base) wmecc-475:ci jigold$ docker create -v foo:/foo google/cloud-sdk:237.0.0-alpine echo hello; 1b10e2a6f6a2f7eb6a6fbe06ce5a6bcae85c00174aa3790267935f91714aa7f7; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local b4b0706c4dfd3ed1907c1fd3325303578f4805a626b88ecbc4935852440577aa; local foo; (base) wmecc-475:ci jigold$ curl --unix-socket /var/run/docker.sock -H ""Content-Type: application/json"" -X DELETE http:/v1.40/containers/1b10e2a6f6a2f7eb6a6fbe06ce5a6bcae85c00174aa3790267935f91714aa7f7?v=true; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local foo; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7359#issuecomment-545193658
https://github.com/hail-is/hail/pull/7359#issuecomment-545193658:55,Testability,test,test,55,"I'm not 100% sure, but I did this locally. I'd like to test it with aiodocker as well to make sure and confirm on the worker before merging. I couldn't find anything online that 100% confirmed, but my understanding of associated volumes was those created by the container create command based on this behavior. ```; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; (base) wmecc-475:ci jigold$ docker volume create foo; foo; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local foo; (base) wmecc-475:ci jigold$ docker create -v foo:/foo google/cloud-sdk:237.0.0-alpine echo hello; 1b10e2a6f6a2f7eb6a6fbe06ce5a6bcae85c00174aa3790267935f91714aa7f7; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local b4b0706c4dfd3ed1907c1fd3325303578f4805a626b88ecbc4935852440577aa; local foo; (base) wmecc-475:ci jigold$ curl --unix-socket /var/run/docker.sock -H ""Content-Type: application/json"" -X DELETE http:/v1.40/containers/1b10e2a6f6a2f7eb6a6fbe06ce5a6bcae85c00174aa3790267935f91714aa7f7?v=true; (base) wmecc-475:ci jigold$ docker volume ls; DRIVER VOLUME NAME; local foo; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7359#issuecomment-545193658
https://github.com/hail-is/hail/pull/7361#issuecomment-545585187:22,Deployability,update,update,22,Looks like we need to update the tests too.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7361#issuecomment-545585187
https://github.com/hail-is/hail/pull/7361#issuecomment-545585187:33,Testability,test,tests,33,Looks like we need to update the tests too.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7361#issuecomment-545585187
https://github.com/hail-is/hail/pull/7368#issuecomment-545942557:45,Deployability,release,release,45,Is this high prio enough to get in before we release 0.2.26?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7368#issuecomment-545942557
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:13,Availability,error,error,13,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:323,Availability,Error,Error,323,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:130,Deployability,rollout,rollout,130,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:158,Deployability,deploy,deployment,158,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:237,Deployability,rollout,rollout,237,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:272,Deployability,rollout,rollout,272,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:300,Deployability,deploy,deployment,300,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:353,Deployability,deploy,deployments,353,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:418,Deployability,rollout,rollout,418,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:522,Deployability,update,updated,522,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:261,Energy Efficiency,monitor,monitoring,261,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:407,Energy Efficiency,monitor,monitoring,407,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:147,Safety,timeout,timeout,147,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:289,Safety,timeout,timeout,289,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:435,Safety,timeout,timeout,435,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:546,Availability,alive,alive,546,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:788,Availability,avail,available,788,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:1113,Availability,alive,alive,1113,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:1520,Availability,avail,available,1520,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:118,Deployability,rollout,rollout,118,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:144,Deployability,deploy,deployment,144,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:260,Deployability,deploy,deployment,260,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:356,Deployability,deploy,deploy,356,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:416,Deployability,Deploy,Deployment,416,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:674,Deployability,rollout,rollout,674,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:702,Deployability,deploy,deployment,702,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:798,Deployability,deploy,deployment,798,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:1269,Deployability,deploy,deployment,1269,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:1401,Deployability,rollout,rollout,1401,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:295,Modifiability,config,config,295,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:364,Modifiability,config,config,364,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:1032,Modifiability,config,config,1032,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:590,Safety,timeout,timeout,590,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:607,Safety,timeout,timeout,607,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:691,Safety,timeout,timeout,691,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:761,Safety,timeout,timeout,761,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:843,Safety,timeout,timeout,843,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:1418,Safety,timeout,timeout,1418,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:1493,Safety,timeout,timeout,1493,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:1580,Safety,timeout,timeout,1580,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:526,Testability,assert,assert,526,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:930,Testability,log,logs,930,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:1667,Testability,log,logs,1667,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626
https://github.com/hail-is/hail/pull/7381#issuecomment-546443748:175,Performance,load,load,175,"Why does that seem sub-optimal? That seems right for readiness. If it can serve the root, it's probably ready to go. For liveness, ideally the page doesn't introduce too much load because it gets hit somewhat frequently. None of this really matters until we have traffic anyway. If we anticipate a surge, we can make sure these aren't overloading the system before the surge.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546443748
https://github.com/hail-is/hail/pull/7381#issuecomment-546465650:467,Modifiability,config,config,467,"edit: ah, I see, you removed the wait, great. Here is a possible alternative solution:. `kubectl -n blog wait --timeout=1h --for=condition=ready pods --selector=app=blog`. Tested with `pods --all`, `pods --selector=app=prometheus`. A difference that is worth being aware of: timeout will apply to each resource matched by the selector. So you'd need to check if the resource specified was a stateful set, and substitute a selector that they would need to pass in the config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546465650
https://github.com/hail-is/hail/pull/7381#issuecomment-546465650:112,Safety,timeout,timeout,112,"edit: ah, I see, you removed the wait, great. Here is a possible alternative solution:. `kubectl -n blog wait --timeout=1h --for=condition=ready pods --selector=app=blog`. Tested with `pods --all`, `pods --selector=app=prometheus`. A difference that is worth being aware of: timeout will apply to each resource matched by the selector. So you'd need to check if the resource specified was a stateful set, and substitute a selector that they would need to pass in the config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546465650
https://github.com/hail-is/hail/pull/7381#issuecomment-546465650:275,Safety,timeout,timeout,275,"edit: ah, I see, you removed the wait, great. Here is a possible alternative solution:. `kubectl -n blog wait --timeout=1h --for=condition=ready pods --selector=app=blog`. Tested with `pods --all`, `pods --selector=app=prometheus`. A difference that is worth being aware of: timeout will apply to each resource matched by the selector. So you'd need to check if the resource specified was a stateful set, and substitute a selector that they would need to pass in the config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546465650
https://github.com/hail-is/hail/pull/7381#issuecomment-546465650:172,Testability,Test,Tested,172,"edit: ah, I see, you removed the wait, great. Here is a possible alternative solution:. `kubectl -n blog wait --timeout=1h --for=condition=ready pods --selector=app=blog`. Tested with `pods --all`, `pods --selector=app=prometheus`. A difference that is worth being aware of: timeout will apply to each resource matched by the selector. So you'd need to check if the resource specified was a stateful set, and substitute a selector that they would need to pass in the config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546465650
https://github.com/hail-is/hail/pull/7381#issuecomment-548040720:908,Modifiability,config,config,908,"Regarding the X-Forwarded-Host. This is why the current setup works: ; (from express request lib); https://github.com/iconoeugen/express/blob/master/lib/request.js#L432. If X-Forwarded-Host is not set, the Host header is checked. So while not strictly necessary, until Ghost stops recommending X-Forwarded-Host, I would keep it, except with a slight change. Also, I think our use of Host is slightly wrong. From what I can tell, Host really should be the target/internal domain, and X-Forwarded-Host the host of the original recipient. By having $updated_host take either of these values (Host when X-Forwarded-Host isn't set, else X-Forwarded-Host) we slightly confuse the issue. This can also lead to bugs, when X-Forwarded-Host contains multiple values (which is in spec). Here is Kubernetes struggling with this issue https://github.com/kubernetes/ingress-nginx/issues/2463. So I would use the following config for Host:. ```; proxy_set_header Host $http_host; proxy_set_header X-Forwarded-Host $http_x_forwarded_host; ```. However, I'm also ok with you keeping it as you have it, because that's what is done elsewhere, which will increase the likelihood that we find that solution insufficient if it truly isn't.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548040720
https://github.com/hail-is/hail/pull/7381#issuecomment-548078429:266,Integrability,rout,router,266,"Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first. So if I understand correctly, for the different headers:; - X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I'm having trouble seeing when `$http_x_forwarded_proto` would ever be absent, although if it is, isn't $scheme always `http` since all traffic from gateway to router is via http? Or am I misunderstanding how this works?; - I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? I don't quite follow your comment about our use of `Host` being wrong, in this case; I *think* I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enough to know what happens if I change it to `proxy_set_header Host $http_host;` instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548078429
https://github.com/hail-is/hail/pull/7381#issuecomment-548078429:428,Integrability,protocol,protocol,428,"Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first. So if I understand correctly, for the different headers:; - X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I'm having trouble seeing when `$http_x_forwarded_proto` would ever be absent, although if it is, isn't $scheme always `http` since all traffic from gateway to router is via http? Or am I misunderstanding how this works?; - I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? I don't quite follow your comment about our use of `Host` being wrong, in this case; I *think* I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enough to know what happens if I change it to `proxy_set_header Host $http_host;` instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548078429
https://github.com/hail-is/hail/pull/7381#issuecomment-548078429:531,Integrability,rout,router,531,"Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first. So if I understand correctly, for the different headers:; - X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I'm having trouble seeing when `$http_x_forwarded_proto` would ever be absent, although if it is, isn't $scheme always `http` since all traffic from gateway to router is via http? Or am I misunderstanding how this works?; - I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? I don't quite follow your comment about our use of `Host` being wrong, in this case; I *think* I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enough to know what happens if I change it to `proxy_set_header Host $http_host;` instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548078429
https://github.com/hail-is/hail/pull/7381#issuecomment-548078429:817,Integrability,rout,router,817,"Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first. So if I understand correctly, for the different headers:; - X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I'm having trouble seeing when `$http_x_forwarded_proto` would ever be absent, although if it is, isn't $scheme always `http` since all traffic from gateway to router is via http? Or am I misunderstanding how this works?; - I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? I don't quite follow your comment about our use of `Host` being wrong, in this case; I *think* I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enough to know what happens if I change it to `proxy_set_header Host $http_host;` instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548078429
https://github.com/hail-is/hail/pull/7381#issuecomment-548078429:540,Modifiability,config,config,540,"Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first. So if I understand correctly, for the different headers:; - X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I'm having trouble seeing when `$http_x_forwarded_proto` would ever be absent, although if it is, isn't $scheme always `http` since all traffic from gateway to router is via http? Or am I misunderstanding how this works?; - I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? I don't quite follow your comment about our use of `Host` being wrong, in this case; I *think* I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enough to know what happens if I change it to `proxy_set_header Host $http_host;` instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548078429
https://github.com/hail-is/hail/pull/7381#issuecomment-548082569:281,Integrability,rout,router,281,"> Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first.; > ; > So if I understand correctly, for the different headers:; > ; > * X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I. Yep. In fact everything request to a Hail service (besides a lets-encrypt path) gets redirected to https. > Or am I misunderstanding how this works?. Nope, you have it correct. $http_x_forwarded_proto should never be absent, and would be fine to use instead of $updated_scheme (but I'd prefer one of those two, rather than https, because otherwise we're not relying on our upstream infrastructure). > * I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? . Yeah that's right. Host refers to the current server (or in the proxied case, what gateway set Host to). X-Forwarded-Host is set by gateway to be the $http_host at the time it proxies the request to router, which is going to be blog.hail.is. > I don't quite follow your comment about our use of `Host` being wrong, in this case; I _think_ I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enoug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569
https://github.com/hail-is/hail/pull/7381#issuecomment-548082569:443,Integrability,protocol,protocol,443,"> Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first.; > ; > So if I understand correctly, for the different headers:; > ; > * X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I. Yep. In fact everything request to a Hail service (besides a lets-encrypt path) gets redirected to https. > Or am I misunderstanding how this works?. Nope, you have it correct. $http_x_forwarded_proto should never be absent, and would be fine to use instead of $updated_scheme (but I'd prefer one of those two, rather than https, because otherwise we're not relying on our upstream infrastructure). > * I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? . Yeah that's right. Host refers to the current server (or in the proxied case, what gateway set Host to). X-Forwarded-Host is set by gateway to be the $http_host at the time it proxies the request to router, which is going to be blog.hail.is. > I don't quite follow your comment about our use of `Host` being wrong, in this case; I _think_ I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enoug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569
https://github.com/hail-is/hail/pull/7381#issuecomment-548082569:546,Integrability,rout,router,546,"> Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first.; > ; > So if I understand correctly, for the different headers:; > ; > * X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I. Yep. In fact everything request to a Hail service (besides a lets-encrypt path) gets redirected to https. > Or am I misunderstanding how this works?. Nope, you have it correct. $http_x_forwarded_proto should never be absent, and would be fine to use instead of $updated_scheme (but I'd prefer one of those two, rather than https, because otherwise we're not relying on our upstream infrastructure). > * I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? . Yeah that's right. Host refers to the current server (or in the proxied case, what gateway set Host to). X-Forwarded-Host is set by gateway to be the $http_host at the time it proxies the request to router, which is going to be blog.hail.is. > I don't quite follow your comment about our use of `Host` being wrong, in this case; I _think_ I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enoug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569
https://github.com/hail-is/hail/pull/7381#issuecomment-548082569:1688,Integrability,rout,router,1688," at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I. Yep. In fact everything request to a Hail service (besides a lets-encrypt path) gets redirected to https. > Or am I misunderstanding how this works?. Nope, you have it correct. $http_x_forwarded_proto should never be absent, and would be fine to use instead of $updated_scheme (but I'd prefer one of those two, rather than https, because otherwise we're not relying on our upstream infrastructure). > * I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? . Yeah that's right. Host refers to the current server (or in the proxied case, what gateway set Host to). X-Forwarded-Host is set by gateway to be the $http_host at the time it proxies the request to router, which is going to be blog.hail.is. > I don't quite follow your comment about our use of `Host` being wrong, in this case; I _think_ I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enough to know what happens if I change it to `proxy_set_header Host $http_host;` instead. I just mean that we're setting Host to the value that X-Forwarded-Host has, when it seems more appropriate for it to be the domain:port of the internal request. So we're relying on X-Forwarded-Host to set that value, but we could, and I think it may be cleaner / more informative, to rely on X-Forwarded-Host all the way through to the internal server that's receiving the request - in this case Ghost's ExpressJS server - instead of rewriting the Host field to be that of the external request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569
https://github.com/hail-is/hail/pull/7381#issuecomment-548082569:555,Modifiability,config,config,555,"> Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first.; > ; > So if I understand correctly, for the different headers:; > ; > * X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I. Yep. In fact everything request to a Hail service (besides a lets-encrypt path) gets redirected to https. > Or am I misunderstanding how this works?. Nope, you have it correct. $http_x_forwarded_proto should never be absent, and would be fine to use instead of $updated_scheme (but I'd prefer one of those two, rather than https, because otherwise we're not relying on our upstream infrastructure). > * I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? . Yeah that's right. Host refers to the current server (or in the proxied case, what gateway set Host to). X-Forwarded-Host is set by gateway to be the $http_host at the time it proxies the request to router, which is going to be blog.hail.is. > I don't quite follow your comment about our use of `Host` being wrong, in this case; I _think_ I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enoug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569
https://github.com/hail-is/hail/pull/7381#issuecomment-548082569:741,Security,encrypt,encrypt,741,"> Thanks for the explanation! I'm happy to make the change, I was just trying to understand the difference between Host and X-Forwarded-Host a little better before first.; > ; > So if I understand correctly, for the different headers:; > ; > * X-Forwarded-Proto gets passed to the router through the base https server in gateway, which sets X-Forwarded-Proto to `$scheme`, which is always going to be https since that's always going to be the protocol you're using for that server? And so when we use `$updated_scheme` for the blog server in the router's config, it's going to look at `$http_x_forwarded_proto` which will always have been set to `https` from the gateway? I. Yep. In fact everything request to a Hail service (besides a lets-encrypt path) gets redirected to https. > Or am I misunderstanding how this works?. Nope, you have it correct. $http_x_forwarded_proto should never be absent, and would be fine to use instead of $updated_scheme (but I'd prefer one of those two, rather than https, because otherwise we're not relying on our upstream infrastructure). > * I'm having trouble understanding the difference between `Host` and `X-Forwarded-Host`, still. As I understand it, `Host` is the name of the server that the current request is trying to reach, and `X-Forwarded-Host` is the name of the server that the original request was trying to reach? Which is why `Host` is set to `$service.internal` and `X-Forwarded-Host` is `$http_host` in the internal.hail.is server? . Yeah that's right. Host refers to the current server (or in the proxied case, what gateway set Host to). X-Forwarded-Host is set by gateway to be the $http_host at the time it proxies the request to router, which is going to be blog.hail.is. > I don't quite follow your comment about our use of `Host` being wrong, in this case; I _think_ I understand what you're saying? but I'm not sure why all of our stuff is setting `Host` to `$updated_host` if that's the case, and I don't understand what's happening enoug",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548082569
https://github.com/hail-is/hail/pull/7381#issuecomment-548089835:332,Availability,failure,failure,332,"Actually the endpoint does seem to be an issue. https://internal.hail.is/pr-7381-default-sx9ail9zkm77/blog/ works great. edit: Endpoint is / (`+ python3 wait-for.py 60 pr-7381-default-sx9ail9zkm77 Service -p 80 blog --endpoint /`), when I think it should be /blog? We could try setting the endpoint to /blog/ or /default/blog/. The failure also has a line about not being able to connect to hostname blog.pr-7381-default-sx9ail9zkm77. I don't know enough about CI to determine whether this is a problem, but my guess is that is normal. edit2: The wait command's port is 80, not 443. Do we need to force X-Forward-Proto to https to fix it? Although if this is going through gateway, I think the protocol should be https after the redirect from 80/http. edit3: Actually, wait-for.py allows a port to be set, so it seems appropriate to set `port: 443` in the wait command. edit4: Nevermind, 443 will not set protocol to https. It shouldn't matter, I don't think, as long as gateway is redirecting to https, but you could try setting X-Forwarded-Proto to https. I suspect the issue is in the url or domain.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548089835
https://github.com/hail-is/hail/pull/7381#issuecomment-548089835:694,Integrability,protocol,protocol,694,"Actually the endpoint does seem to be an issue. https://internal.hail.is/pr-7381-default-sx9ail9zkm77/blog/ works great. edit: Endpoint is / (`+ python3 wait-for.py 60 pr-7381-default-sx9ail9zkm77 Service -p 80 blog --endpoint /`), when I think it should be /blog? We could try setting the endpoint to /blog/ or /default/blog/. The failure also has a line about not being able to connect to hostname blog.pr-7381-default-sx9ail9zkm77. I don't know enough about CI to determine whether this is a problem, but my guess is that is normal. edit2: The wait command's port is 80, not 443. Do we need to force X-Forward-Proto to https to fix it? Although if this is going through gateway, I think the protocol should be https after the redirect from 80/http. edit3: Actually, wait-for.py allows a port to be set, so it seems appropriate to set `port: 443` in the wait command. edit4: Nevermind, 443 will not set protocol to https. It shouldn't matter, I don't think, as long as gateway is redirecting to https, but you could try setting X-Forwarded-Proto to https. I suspect the issue is in the url or domain.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548089835
https://github.com/hail-is/hail/pull/7381#issuecomment-548089835:905,Integrability,protocol,protocol,905,"Actually the endpoint does seem to be an issue. https://internal.hail.is/pr-7381-default-sx9ail9zkm77/blog/ works great. edit: Endpoint is / (`+ python3 wait-for.py 60 pr-7381-default-sx9ail9zkm77 Service -p 80 blog --endpoint /`), when I think it should be /blog? We could try setting the endpoint to /blog/ or /default/blog/. The failure also has a line about not being able to connect to hostname blog.pr-7381-default-sx9ail9zkm77. I don't know enough about CI to determine whether this is a problem, but my guess is that is normal. edit2: The wait command's port is 80, not 443. Do we need to force X-Forward-Proto to https to fix it? Although if this is going through gateway, I think the protocol should be https after the redirect from 80/http. edit3: Actually, wait-for.py allows a port to be set, so it seems appropriate to set `port: 443` in the wait command. edit4: Nevermind, 443 will not set protocol to https. It shouldn't matter, I don't think, as long as gateway is redirecting to https, but you could try setting X-Forwarded-Proto to https. I suspect the issue is in the url or domain.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548089835
https://github.com/hail-is/hail/pull/7381#issuecomment-548098055:91,Integrability,rout,route,91,"@danking should internal_base_url in wait-for.py be https? Do we have anything that should route through http?. As an alternative, I think it would be reasonable to always rewrite port: 443 to https:// in that function. I can PR if you're ok with that. edit: The comment states that the protocol should match hailtop.config. Not sure why that is, besides sharing gateway probably? In any case, this function doesn't match that. We're always in the `self._location == 'external'` space I think, which means https according to hailtop.config",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098055
https://github.com/hail-is/hail/pull/7381#issuecomment-548098055:287,Integrability,protocol,protocol,287,"@danking should internal_base_url in wait-for.py be https? Do we have anything that should route through http?. As an alternative, I think it would be reasonable to always rewrite port: 443 to https:// in that function. I can PR if you're ok with that. edit: The comment states that the protocol should match hailtop.config. Not sure why that is, besides sharing gateway probably? In any case, this function doesn't match that. We're always in the `self._location == 'external'` space I think, which means https according to hailtop.config",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098055
https://github.com/hail-is/hail/pull/7381#issuecomment-548098055:172,Modifiability,rewrite,rewrite,172,"@danking should internal_base_url in wait-for.py be https? Do we have anything that should route through http?. As an alternative, I think it would be reasonable to always rewrite port: 443 to https:// in that function. I can PR if you're ok with that. edit: The comment states that the protocol should match hailtop.config. Not sure why that is, besides sharing gateway probably? In any case, this function doesn't match that. We're always in the `self._location == 'external'` space I think, which means https according to hailtop.config",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098055
https://github.com/hail-is/hail/pull/7381#issuecomment-548098055:317,Modifiability,config,config,317,"@danking should internal_base_url in wait-for.py be https? Do we have anything that should route through http?. As an alternative, I think it would be reasonable to always rewrite port: 443 to https:// in that function. I can PR if you're ok with that. edit: The comment states that the protocol should match hailtop.config. Not sure why that is, besides sharing gateway probably? In any case, this function doesn't match that. We're always in the `self._location == 'external'` space I think, which means https according to hailtop.config",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098055
https://github.com/hail-is/hail/pull/7381#issuecomment-548098055:533,Modifiability,config,config,533,"@danking should internal_base_url in wait-for.py be https? Do we have anything that should route through http?. As an alternative, I think it would be reasonable to always rewrite port: 443 to https:// in that function. I can PR if you're ok with that. edit: The comment states that the protocol should match hailtop.config. Not sure why that is, besides sharing gateway probably? In any case, this function doesn't match that. We're always in the `self._location == 'external'` space I think, which means https according to hailtop.config",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098055
https://github.com/hail-is/hail/pull/7381#issuecomment-548098857:109,Availability,mainten,maintenance,109,"Also, strangely https://internal.hail.is/pr-7381-default-sx9ail9zkm77/blog/ now goes to 403 Forbidden, and a maintenance error is generated in Ghost. Shortly after the PR was built, that link worked. edit: The maintenance error potentially suggests we should wait longer to initiate our probes, although I can't tell until I know where the /blog GET at 31 minutes came from. That request happens 5 minutes before Ghost is actually up, so maybe a previous PR? Not certain. Separately, it takes ghost 6 seconds to actually boot, so if our readiness probe fires off 5s after the container is running, that may not be enough.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098857
https://github.com/hail-is/hail/pull/7381#issuecomment-548098857:121,Availability,error,error,121,"Also, strangely https://internal.hail.is/pr-7381-default-sx9ail9zkm77/blog/ now goes to 403 Forbidden, and a maintenance error is generated in Ghost. Shortly after the PR was built, that link worked. edit: The maintenance error potentially suggests we should wait longer to initiate our probes, although I can't tell until I know where the /blog GET at 31 minutes came from. That request happens 5 minutes before Ghost is actually up, so maybe a previous PR? Not certain. Separately, it takes ghost 6 seconds to actually boot, so if our readiness probe fires off 5s after the container is running, that may not be enough.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098857
https://github.com/hail-is/hail/pull/7381#issuecomment-548098857:210,Availability,mainten,maintenance,210,"Also, strangely https://internal.hail.is/pr-7381-default-sx9ail9zkm77/blog/ now goes to 403 Forbidden, and a maintenance error is generated in Ghost. Shortly after the PR was built, that link worked. edit: The maintenance error potentially suggests we should wait longer to initiate our probes, although I can't tell until I know where the /blog GET at 31 minutes came from. That request happens 5 minutes before Ghost is actually up, so maybe a previous PR? Not certain. Separately, it takes ghost 6 seconds to actually boot, so if our readiness probe fires off 5s after the container is running, that may not be enough.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098857
https://github.com/hail-is/hail/pull/7381#issuecomment-548098857:222,Availability,error,error,222,"Also, strangely https://internal.hail.is/pr-7381-default-sx9ail9zkm77/blog/ now goes to 403 Forbidden, and a maintenance error is generated in Ghost. Shortly after the PR was built, that link worked. edit: The maintenance error potentially suggests we should wait longer to initiate our probes, although I can't tell until I know where the /blog GET at 31 minutes came from. That request happens 5 minutes before Ghost is actually up, so maybe a previous PR? Not certain. Separately, it takes ghost 6 seconds to actually boot, so if our readiness probe fires off 5s after the container is running, that may not be enough.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548098857
https://github.com/hail-is/hail/pull/7381#issuecomment-548101042:38,Testability,test,tests,38,"PR namespaces are destroyed after the tests pass, so ghost should be gone.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548101042
https://github.com/hail-is/hail/pull/7381#issuecomment-548102338:23,Availability,mainten,maintenance,23,"What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace. The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which *should* do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing. The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102338
https://github.com/hail-is/hail/pull/7381#issuecomment-548102338:35,Availability,error,error,35,"What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace. The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which *should* do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing. The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102338
https://github.com/hail-is/hail/pull/7381#issuecomment-548102338:183,Availability,down,down,183,"What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace. The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which *should* do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing. The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102338
https://github.com/hail-is/hail/pull/7381#issuecomment-548102338:535,Availability,failure,failure,535,"What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace. The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which *should* do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing. The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102338
https://github.com/hail-is/hail/pull/7381#issuecomment-548102338:681,Integrability,rout,router,681,"What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace. The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which *should* do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing. The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102338
https://github.com/hail-is/hail/pull/7381#issuecomment-548102338:137,Testability,test,tests,137,"What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace. The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which *should* do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing. The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102338
https://github.com/hail-is/hail/pull/7381#issuecomment-548102338:530,Testability,test,test,530,"What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace. The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which *should* do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing. The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102338
https://github.com/hail-is/hail/pull/7381#issuecomment-548102772:25,Availability,mainten,maintenance,25,"> What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace.; > ; > The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which _should_ do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing.; > ; > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.; ```; [2019-10-30 20:03:15] [36mINFO[39m Ghost boot 5.169s; [2019-10-30 20:03:16] [31mERROR[39m ""GET /pr-7381-default-sx9ail9zkm77/blog/"" [31m503[39m 40ms; [31m; [31mSite is starting up, please wait a moment then retry.[39m. [1m[37mError ID:[39m[22m; [90m4f8b8590-fb50-11e9-ab7c-9dd7e9eff310[39m. [90m----------------------------------------[39m. [90mMaintenanceError: Site is starting up, please wait a moment then retry.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102772
https://github.com/hail-is/hail/pull/7381#issuecomment-548102772:37,Availability,error,error,37,"> What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace.; > ; > The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which _should_ do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing.; > ; > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.; ```; [2019-10-30 20:03:15] [36mINFO[39m Ghost boot 5.169s; [2019-10-30 20:03:16] [31mERROR[39m ""GET /pr-7381-default-sx9ail9zkm77/blog/"" [31m503[39m 40ms; [31m; [31mSite is starting up, please wait a moment then retry.[39m. [1m[37mError ID:[39m[22m; [90m4f8b8590-fb50-11e9-ab7c-9dd7e9eff310[39m. [90m----------------------------------------[39m. [90mMaintenanceError: Site is starting up, please wait a moment then retry.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102772
https://github.com/hail-is/hail/pull/7381#issuecomment-548102772:185,Availability,down,down,185,"> What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace.; > ; > The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which _should_ do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing.; > ; > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.; ```; [2019-10-30 20:03:15] [36mINFO[39m Ghost boot 5.169s; [2019-10-30 20:03:16] [31mERROR[39m ""GET /pr-7381-default-sx9ail9zkm77/blog/"" [31m503[39m 40ms; [31m; [31mSite is starting up, please wait a moment then retry.[39m. [1m[37mError ID:[39m[22m; [90m4f8b8590-fb50-11e9-ab7c-9dd7e9eff310[39m. [90m----------------------------------------[39m. [90mMaintenanceError: Site is starting up, please wait a moment then retry.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102772
https://github.com/hail-is/hail/pull/7381#issuecomment-548102772:551,Availability,failure,failure,551,"> What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace.; > ; > The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which _should_ do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing.; > ; > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.; ```; [2019-10-30 20:03:15] [36mINFO[39m Ghost boot 5.169s; [2019-10-30 20:03:16] [31mERROR[39m ""GET /pr-7381-default-sx9ail9zkm77/blog/"" [31m503[39m 40ms; [31m; [31mSite is starting up, please wait a moment then retry.[39m. [1m[37mError ID:[39m[22m; [90m4f8b8590-fb50-11e9-ab7c-9dd7e9eff310[39m. [90m----------------------------------------[39m. [90mMaintenanceError: Site is starting up, please wait a moment then retry.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102772
https://github.com/hail-is/hail/pull/7381#issuecomment-548102772:697,Integrability,rout,router,697,"> What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace.; > ; > The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which _should_ do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing.; > ; > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.; ```; [2019-10-30 20:03:15] [36mINFO[39m Ghost boot 5.169s; [2019-10-30 20:03:16] [31mERROR[39m ""GET /pr-7381-default-sx9ail9zkm77/blog/"" [31m503[39m 40ms; [31m; [31mSite is starting up, please wait a moment then retry.[39m. [1m[37mError ID:[39m[22m; [90m4f8b8590-fb50-11e9-ab7c-9dd7e9eff310[39m. [90m----------------------------------------[39m. [90mMaintenanceError: Site is starting up, please wait a moment then retry.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102772
https://github.com/hail-is/hail/pull/7381#issuecomment-548102772:139,Testability,test,tests,139,"> What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace.; > ; > The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which _should_ do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing.; > ; > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.; ```; [2019-10-30 20:03:15] [36mINFO[39m Ghost boot 5.169s; [2019-10-30 20:03:16] [31mERROR[39m ""GET /pr-7381-default-sx9ail9zkm77/blog/"" [31m503[39m 40ms; [31m; [31mSite is starting up, please wait a moment then retry.[39m. [1m[37mError ID:[39m[22m; [90m4f8b8590-fb50-11e9-ab7c-9dd7e9eff310[39m. [90m----------------------------------------[39m. [90mMaintenanceError: Site is starting up, please wait a moment then retry.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102772
https://github.com/hail-is/hail/pull/7381#issuecomment-548102772:546,Testability,test,test,546,"> What do you mean by ""a maintenance error is generated in Ghost""? I kind of assumed that the website wasn't working right now because the tests finished and `cleanup_deploy_blog` shut down the blog in the PR namespace.; > ; > The ""endpoint"" that I'm passing in to the `wait` command for the blog in build.yaml is `/`, which _should_ do the right thing interally because the URL that's actually being constructed in `wait-for.py` becomes `http://{service}.{namespace}/{namespace}/{service}{endpoint}`, which gives you the right thing.; > ; > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the `X-Forwarded-Proto` header isn't set. I'm not sure what the right fix is in this case.; ```; [2019-10-30 20:03:15] [36mINFO[39m Ghost boot 5.169s; [2019-10-30 20:03:16] [31mERROR[39m ""GET /pr-7381-default-sx9ail9zkm77/blog/"" [31m503[39m 40ms; [31m; [31mSite is starting up, please wait a moment then retry.[39m. [1m[37mError ID:[39m[22m; [90m4f8b8590-fb50-11e9-ab7c-9dd7e9eff310[39m. [90m----------------------------------------[39m. [90mMaintenanceError: Site is starting up, please wait a moment then retry.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548102772
https://github.com/hail-is/hail/pull/7381#issuecomment-548103041:23,Integrability,message,message,23,"ah---that's just a log message from the readinessProbe sending requests before the server is fully up, I believe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103041
https://github.com/hail-is/hail/pull/7381#issuecomment-548103041:19,Testability,log,log,19,"ah---that's just a log message from the readinessProbe sending requests before the server is fully up, I believe.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103041
https://github.com/hail-is/hail/pull/7381#issuecomment-548103287:53,Deployability,deploy,deployed,53,"(If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). ignore all the log messages that have to do with `/wang/blog/healthcheck/`---since it's logging to persistent storage, it's also printing all the old logs from a few days ago to the batch logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287
https://github.com/hail-is/hail/pull/7381#issuecomment-548103287:172,Integrability,message,messages,172,"(If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). ignore all the log messages that have to do with `/wang/blog/healthcheck/`---since it's logging to persistent storage, it's also printing all the old logs from a few days ago to the batch logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287
https://github.com/hail-is/hail/pull/7381#issuecomment-548103287:108,Testability,log,logs,108,"(If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). ignore all the log messages that have to do with `/wang/blog/healthcheck/`---since it's logging to persistent storage, it's also printing all the old logs from a few days ago to the batch logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287
https://github.com/hail-is/hail/pull/7381#issuecomment-548103287:168,Testability,log,log,168,"(If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). ignore all the log messages that have to do with `/wang/blog/healthcheck/`---since it's logging to persistent storage, it's also printing all the old logs from a few days ago to the batch logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287
https://github.com/hail-is/hail/pull/7381#issuecomment-548103287:241,Testability,log,logging,241,"(If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). ignore all the log messages that have to do with `/wang/blog/healthcheck/`---since it's logging to persistent storage, it's also printing all the old logs from a few days ago to the batch logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287
https://github.com/hail-is/hail/pull/7381#issuecomment-548103287:303,Testability,log,logs,303,"(If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). ignore all the log messages that have to do with `/wang/blog/healthcheck/`---since it's logging to persistent storage, it's also printing all the old logs from a few days ago to the batch logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287
https://github.com/hail-is/hail/pull/7381#issuecomment-548103287:341,Testability,log,logs,341,"(If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). ignore all the log messages that have to do with `/wang/blog/healthcheck/`---since it's logging to persistent storage, it's also printing all the old logs from a few days ago to the batch logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287
https://github.com/hail-is/hail/pull/7381#issuecomment-548103795:25,Integrability,message,message,25,"> ah---that's just a log message from the readinessProbe sending requests before the server is fully up, I believe. So partly it looks like that, especially with Ghost timing itself as >5s to actually listen on the port after the container starts up (or really after Ghost starts running which may be different). The strange this is the timing. I thought the timestamps were supposed to correspond to the time the request was received/log message was generated (the log messages themselves can be printed out of order, but I think the timestamps should be accurate).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103795
https://github.com/hail-is/hail/pull/7381#issuecomment-548103795:439,Integrability,message,message,439,"> ah---that's just a log message from the readinessProbe sending requests before the server is fully up, I believe. So partly it looks like that, especially with Ghost timing itself as >5s to actually listen on the port after the container starts up (or really after Ghost starts running which may be different). The strange this is the timing. I thought the timestamps were supposed to correspond to the time the request was received/log message was generated (the log messages themselves can be printed out of order, but I think the timestamps should be accurate).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103795
https://github.com/hail-is/hail/pull/7381#issuecomment-548103795:470,Integrability,message,messages,470,"> ah---that's just a log message from the readinessProbe sending requests before the server is fully up, I believe. So partly it looks like that, especially with Ghost timing itself as >5s to actually listen on the port after the container starts up (or really after Ghost starts running which may be different). The strange this is the timing. I thought the timestamps were supposed to correspond to the time the request was received/log message was generated (the log messages themselves can be printed out of order, but I think the timestamps should be accurate).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103795
https://github.com/hail-is/hail/pull/7381#issuecomment-548103795:21,Testability,log,log,21,"> ah---that's just a log message from the readinessProbe sending requests before the server is fully up, I believe. So partly it looks like that, especially with Ghost timing itself as >5s to actually listen on the port after the container starts up (or really after Ghost starts running which may be different). The strange this is the timing. I thought the timestamps were supposed to correspond to the time the request was received/log message was generated (the log messages themselves can be printed out of order, but I think the timestamps should be accurate).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103795
https://github.com/hail-is/hail/pull/7381#issuecomment-548103795:435,Testability,log,log,435,"> ah---that's just a log message from the readinessProbe sending requests before the server is fully up, I believe. So partly it looks like that, especially with Ghost timing itself as >5s to actually listen on the port after the container starts up (or really after Ghost starts running which may be different). The strange this is the timing. I thought the timestamps were supposed to correspond to the time the request was received/log message was generated (the log messages themselves can be printed out of order, but I think the timestamps should be accurate).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103795
https://github.com/hail-is/hail/pull/7381#issuecomment-548103795:466,Testability,log,log,466,"> ah---that's just a log message from the readinessProbe sending requests before the server is fully up, I believe. So partly it looks like that, especially with Ghost timing itself as >5s to actually listen on the port after the container starts up (or really after Ghost starts running which may be different). The strange this is the timing. I thought the timestamps were supposed to correspond to the time the request was received/log message was generated (the log messages themselves can be printed out of order, but I think the timestamps should be accurate).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103795
https://github.com/hail-is/hail/pull/7381#issuecomment-548104248:55,Deployability,deploy,deployed,55,"> (If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). looks great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548104248
https://github.com/hail-is/hail/pull/7381#issuecomment-548104248:110,Testability,log,logs,110,"> (If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). looks great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548104248
https://github.com/hail-is/hail/pull/7381#issuecomment-548104962:146,Availability,failure,failure,146,"> Not sure what to do about wait for and ghost. rather silly of ghost to refuse connections on http when the base url is set to https. > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the X-Forwarded-Proto header isn't set. I'm not sure what the right fix is in this case. I still don't quite understand why the wait command doesn't hit gateway. Isn't it just issuing an http request to `f'{base_url}/{endpoint)`? Why doesn't that hit gateway?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548104962
https://github.com/hail-is/hail/pull/7381#issuecomment-548104962:292,Integrability,rout,router,292,"> Not sure what to do about wait for and ghost. rather silly of ghost to refuse connections on http when the base url is set to https. > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the X-Forwarded-Proto header isn't set. I'm not sure what the right fix is in this case. I still don't quite understand why the wait command doesn't hit gateway. Isn't it just issuing an http request to `f'{base_url}/{endpoint)`? Why doesn't that hit gateway?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548104962
https://github.com/hail-is/hail/pull/7381#issuecomment-548104962:141,Testability,test,test,141,"> Not sure what to do about wait for and ghost. rather silly of ghost to refuse connections on http when the base url is set to https. > The test failure that I'm running into currently has to do with the fact that the wait command there queries the endpoint without going through either the router or the gateway, (since it's e.g. hitting http://blog.wang/wang/blog/ directly), so it's getting the 301 redirect to https because the X-Forwarded-Proto header isn't set. I'm not sure what the right fix is in this case. I still don't quite understand why the wait command doesn't hit gateway. Isn't it just issuing an http request to `f'{base_url}/{endpoint)`? Why doesn't that hit gateway?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548104962
https://github.com/hail-is/hail/pull/7381#issuecomment-548105738:366,Integrability,rout,routing,366,"> Only external requests go to the gateway. Why doesn't this do that (where baseurl is `http://{service}.namespace`. ```; base_url = internal_base_url(namespace, name, port); async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5.0)) as session:; while True:; try:; async with session.get(f'{base_url}/healthcheck') as resp:; ```. edit: Ah it's just routing through Kubernetes. I thought there would need to be something that specified the dns to use... Ok.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548105738
https://github.com/hail-is/hail/pull/7381#issuecomment-548105738:208,Safety,timeout,timeout,208,"> Only external requests go to the gateway. Why doesn't this do that (where baseurl is `http://{service}.namespace`. ```; base_url = internal_base_url(namespace, name, port); async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5.0)) as session:; while True:; try:; async with session.get(f'{base_url}/healthcheck') as resp:; ```. edit: Ah it's just routing through Kubernetes. I thought there would need to be something that specified the dns to use... Ok.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548105738
https://github.com/hail-is/hail/pull/7381#issuecomment-548106312:332,Integrability,rout,router,332,"http://blog.wang/wang/blog/, for instance, sends a request to the `blog` service in the `wang` namespace inside our k8s cluster, which is all handled by kubernetes. So the wait basically sends a GET request on the `/wang/blog/` endpoint directly to the `blog` service in that namespace, which never passes through either gateway or router.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548106312
https://github.com/hail-is/hail/pull/7381#issuecomment-548107052:334,Integrability,rout,router,334,"> http://blog.wang/wang/blog/, for instance, sends a request to the `blog` service in the `wang` namespace inside our k8s cluster, which is all handled by kubernetes. So the wait basically sends a GET request on the `/wang/blog/` endpoint directly to the `blog` service in that namespace, which never passes through either gateway or router. Yeah, I got it, thanks. I think we may need to set the X-Forwarded-Proto in the wait_for.py http command (session.get). https://docs.aiohttp.org/en/stable/client_advanced.html. And provide a `headers: `field within build.yml's wait property.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548107052
https://github.com/hail-is/hail/pull/7381#issuecomment-548133056:383,Performance,optimiz,optimizing,383,"> Just seems more complicated than using argparse to get repeated arguments. Actually looks like we can use nargs=2 to get: `--header 'key1' 'value1' --header 'key2' 'value2'`. Why introduce a JSON unparser and parser?. Up to you. JSON parser/unparser is stdlib, and it looks like the command line tool is little used (meaning, executed typically as part of a larger script). So I'm optimizing for the typical use case (it's easier to specify the YAML property using the dict method, and easier to write/maintain the parser, since no need to go from array to dict). Neat about args 2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548133056
https://github.com/hail-is/hail/issues/7384#issuecomment-546392520:161,Availability,error,error,161,"if the code snippet that I wrote above works, I think we should either cache the results of `classAsBytes` and reuse them in `result`, or honestly just throw an error if you try to call `result` multiple times on the same function builder. (what would be the use case? If we want to use the same function multiple times we generally call `val f = fb.result()` once and use `f` as many times as necessary to get instances of the desired function.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7384#issuecomment-546392520
https://github.com/hail-is/hail/issues/7384#issuecomment-546392520:71,Performance,cache,cache,71,"if the code snippet that I wrote above works, I think we should either cache the results of `classAsBytes` and reuse them in `result`, or honestly just throw an error if you try to call `result` multiple times on the same function builder. (what would be the use case? If we want to use the same function multiple times we generally call `val f = fb.result()` once and use `f` as many times as necessary to get instances of the desired function.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7384#issuecomment-546392520
https://github.com/hail-is/hail/issues/7384#issuecomment-546397181:44,Availability,error,error,44,"Gotcha. Yes, your test works. I'll throw an error for now, since this sounds like not a use case we want to support.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7384#issuecomment-546397181
https://github.com/hail-is/hail/issues/7384#issuecomment-546397181:18,Testability,test,test,18,"Gotcha. Yes, your test works. I'll throw an error for now, since this sounds like not a use case we want to support.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7384#issuecomment-546397181
https://github.com/hail-is/hail/pull/7385#issuecomment-546427387:252,Testability,test,tested,252,"I removed window.history.scrollRestoration = 'manual'. This has the effect of making Chrome and Firefox exhibit the same behavior (overshoot), and Safari show the ""correct"" behavior (no overshoot). 1 fewer change, the same amount of consistency across tested browsers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7385#issuecomment-546427387
https://github.com/hail-is/hail/pull/7386#issuecomment-546532261:21,Testability,test,test,21,"can you add a python test that does something to the effect of:; ```; t = hl.utils.range_table(100, 5); t = t.annotate(idx2 = hl.scan.count()); result = t.tail(10).collect(); expected = [hl.Struct(idx=i, idx2=i) for i in range(90, 100)]; self.assertEquals(result, expected); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7386#issuecomment-546532261
https://github.com/hail-is/hail/pull/7386#issuecomment-546532261:243,Testability,assert,assertEquals,243,"can you add a python test that does something to the effect of:; ```; t = hl.utils.range_table(100, 5); t = t.annotate(idx2 = hl.scan.count()); result = t.tail(10).collect(); expected = [hl.Struct(idx=i, idx2=i) for i in range(90, 100)]; self.assertEquals(result, expected); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7386#issuecomment-546532261
https://github.com/hail-is/hail/pull/7386#issuecomment-547133616:45,Performance,queue,queue,45,"FYI, the PR shows up in my scorecard.hail.is queue if I'm an assignee.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7386#issuecomment-547133616
https://github.com/hail-is/hail/issues/7393#issuecomment-547003344:36,Performance,perform,performing,36,"oh, whoa. splitting the code up and performing an action in between (`filter` followed by `collect` followed by `show`) fixes the problem",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7393#issuecomment-547003344
https://github.com/hail-is/hail/pull/7394#issuecomment-547637893:128,Performance,optimiz,optimization,128,"Can I ask you to add the same scan test here for rows and cols? This will mostly help protect against us trying to introduce an optimization that inadvertently breaks scans, which we've done in the past for e.g. filter intervals.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7394#issuecomment-547637893
https://github.com/hail-is/hail/pull/7394#issuecomment-547637893:35,Testability,test,test,35,"Can I ask you to add the same scan test here for rows and cols? This will mostly help protect against us trying to introduce an optimization that inadvertently breaks scans, which we've done in the past for e.g. filter intervals.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7394#issuecomment-547637893
https://github.com/hail-is/hail/issues/7396#issuecomment-547147774:30,Integrability,wrap,wrap,30,"yeah, encoders/decoders don't wrap within structs. That's not hard to fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7396#issuecomment-547147774
https://github.com/hail-is/hail/issues/7396#issuecomment-547153723:53,Availability,error,error,53,"to elaborate, I'm seeing this test case hit the same error:; ```; t = hl.utils.range_table(10).annotate(**{f'f{i}': 0 for i in range(1300)}); mt = hl.utils.range_matrix_table(10, 10). mt.annotate_cols(foo=t[mt.col_idx])._force_count_rows(); ```. with the following stack trace: ; ```; 2019-10-28 17:15:08 root: ERROR: Verify Output 1 for is/hail/codegen/generated/C_etypeDecode_9:; 2019-10-28 17:15:08 root: ERROR: RuntimeException: Method code too large!; From java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:333); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:372); 	at is.hail.expr.types.encoded.EType$.buildDecoder(EType.scala:199); 	at is.hail.io.TypedCodecSpec.buildDecoder(RowStore.scala:36); 	at is.hail.rvd.RVD.collect(RVD.scala:694); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:750); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:90); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:1125); 	at is.hail.expr.ir.Interpr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7396#issuecomment-547153723
https://github.com/hail-is/hail/issues/7396#issuecomment-547153723:311,Availability,ERROR,ERROR,311,"to elaborate, I'm seeing this test case hit the same error:; ```; t = hl.utils.range_table(10).annotate(**{f'f{i}': 0 for i in range(1300)}); mt = hl.utils.range_matrix_table(10, 10). mt.annotate_cols(foo=t[mt.col_idx])._force_count_rows(); ```. with the following stack trace: ; ```; 2019-10-28 17:15:08 root: ERROR: Verify Output 1 for is/hail/codegen/generated/C_etypeDecode_9:; 2019-10-28 17:15:08 root: ERROR: RuntimeException: Method code too large!; From java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:333); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:372); 	at is.hail.expr.types.encoded.EType$.buildDecoder(EType.scala:199); 	at is.hail.io.TypedCodecSpec.buildDecoder(RowStore.scala:36); 	at is.hail.rvd.RVD.collect(RVD.scala:694); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:750); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:90); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:1125); 	at is.hail.expr.ir.Interpr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7396#issuecomment-547153723
https://github.com/hail-is/hail/issues/7396#issuecomment-547153723:408,Availability,ERROR,ERROR,408,"to elaborate, I'm seeing this test case hit the same error:; ```; t = hl.utils.range_table(10).annotate(**{f'f{i}': 0 for i in range(1300)}); mt = hl.utils.range_matrix_table(10, 10). mt.annotate_cols(foo=t[mt.col_idx])._force_count_rows(); ```. with the following stack trace: ; ```; 2019-10-28 17:15:08 root: ERROR: Verify Output 1 for is/hail/codegen/generated/C_etypeDecode_9:; 2019-10-28 17:15:08 root: ERROR: RuntimeException: Method code too large!; From java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:333); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:372); 	at is.hail.expr.types.encoded.EType$.buildDecoder(EType.scala:199); 	at is.hail.io.TypedCodecSpec.buildDecoder(RowStore.scala:36); 	at is.hail.rvd.RVD.collect(RVD.scala:694); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:750); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:90); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:1125); 	at is.hail.expr.ir.Interpr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7396#issuecomment-547153723
https://github.com/hail-is/hail/issues/7396#issuecomment-547153723:30,Testability,test,test,30,"to elaborate, I'm seeing this test case hit the same error:; ```; t = hl.utils.range_table(10).annotate(**{f'f{i}': 0 for i in range(1300)}); mt = hl.utils.range_matrix_table(10, 10). mt.annotate_cols(foo=t[mt.col_idx])._force_count_rows(); ```. with the following stack trace: ; ```; 2019-10-28 17:15:08 root: ERROR: Verify Output 1 for is/hail/codegen/generated/C_etypeDecode_9:; 2019-10-28 17:15:08 root: ERROR: RuntimeException: Method code too large!; From java.lang.RuntimeException: Method code too large!; 	at is.hail.relocated.org.objectweb.asm.MethodWriter.a(Unknown Source); 	at is.hail.relocated.org.objectweb.asm.ClassWriter.toByteArray(Unknown Source); 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:333); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:372); 	at is.hail.expr.types.encoded.EType$.buildDecoder(EType.scala:199); 	at is.hail.io.TypedCodecSpec.buildDecoder(RowStore.scala:36); 	at is.hail.rvd.RVD.collect(RVD.scala:694); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:750); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:90); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$5.apply(InterpretNonCompilable.scala:16); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:16); 	at is.hail.expr.ir.TableMapGlobals.execute(TableIR.scala:1125); 	at is.hail.expr.ir.Interpr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7396#issuecomment-547153723
https://github.com/hail-is/hail/pull/7404#issuecomment-548046645:25,Availability,failure,failure,25,You're getting a doctest failure @tpoterba,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7404#issuecomment-548046645
https://github.com/hail-is/hail/issues/7410#issuecomment-585212012:70,Usability,learn,learning,70,"I imagine this is a low priority issue, but is there a workaround for learning what the structure of a grouped MT is in the meantime? Or has there been any progress on this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7410#issuecomment-585212012
https://github.com/hail-is/hail/issues/7410#issuecomment-585213036:188,Modifiability,variab,variable,188,"It's somewhat low-prio because we imagine that GroupedMatrixTable as pretty much just an intermediate between `group_{rows, cols}_by` and `aggregate` that probably shouldn't be bound to a variable and inspected. Its structure is the same as the parent MT, but just records group expressions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7410#issuecomment-585213036
https://github.com/hail-is/hail/issues/7417#issuecomment-548479314:222,Deployability,update,updated,222,"> Github doesn't have a way to say ""stacked on this PR"", so removal of the stacked tag is a manual gating. I'm surprised it doesn't have a way to do this, since when a PR is mentioned by a link or hash, that dependency is updated with its dependent. . Looks like this may be coming: https://twitter.com/natfriedman/status/1170804894241972224",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548479314
https://github.com/hail-is/hail/issues/7417#issuecomment-548479314:208,Integrability,depend,dependency,208,"> Github doesn't have a way to say ""stacked on this PR"", so removal of the stacked tag is a manual gating. I'm surprised it doesn't have a way to do this, since when a PR is mentioned by a link or hash, that dependency is updated with its dependent. . Looks like this may be coming: https://twitter.com/natfriedman/status/1170804894241972224",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548479314
https://github.com/hail-is/hail/issues/7417#issuecomment-548479314:239,Integrability,depend,dependent,239,"> Github doesn't have a way to say ""stacked on this PR"", so removal of the stacked tag is a manual gating. I'm surprised it doesn't have a way to do this, since when a PR is mentioned by a link or hash, that dependency is updated with its dependent. . Looks like this may be coming: https://twitter.com/natfriedman/status/1170804894241972224",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548479314
https://github.com/hail-is/hail/issues/7417#issuecomment-548479314:197,Security,hash,hash,197,"> Github doesn't have a way to say ""stacked on this PR"", so removal of the stacked tag is a manual gating. I'm surprised it doesn't have a way to do this, since when a PR is mentioned by a link or hash, that dependency is updated with its dependent. . Looks like this may be coming: https://twitter.com/natfriedman/status/1170804894241972224",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548479314
https://github.com/hail-is/hail/issues/7417#issuecomment-548480282:147,Integrability,depend,depends,147,"I wonder if, using a consistent template, we could have CI do this, either by looking at the first comment, or by looking at the commit. Look for ""depends on: #hash"" string, much like GitHub does with ""closes #hash"" in commits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548480282
https://github.com/hail-is/hail/issues/7417#issuecomment-548480282:160,Security,hash,hash,160,"I wonder if, using a consistent template, we could have CI do this, either by looking at the first comment, or by looking at the commit. Look for ""depends on: #hash"" string, much like GitHub does with ""closes #hash"" in commits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548480282
https://github.com/hail-is/hail/issues/7417#issuecomment-548480282:210,Security,hash,hash,210,"I wonder if, using a consistent template, we could have CI do this, either by looking at the first comment, or by looking at the commit. Look for ""depends on: #hash"" string, much like GitHub does with ""closes #hash"" in commits.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548480282
https://github.com/hail-is/hail/issues/7418#issuecomment-548483578:32,Availability,error,error,32,"I kinda think this should be an error, actually.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7418#issuecomment-548483578
https://github.com/hail-is/hail/issues/7418#issuecomment-548485790:101,Modifiability,variab,variable,101,"I think the clear default answer is referential transparency. Whether you bind something in a python variable, or you inline that definition, should be semantically equivalent. Unless we come up with a compelling reason to break that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7418#issuecomment-548485790
https://github.com/hail-is/hail/issues/7418#issuecomment-548485790:12,Usability,clear,clear,12,"I think the clear default answer is referential transparency. Whether you bind something in a python variable, or you inline that definition, should be semantically equivalent. Unless we come up with a compelling reason to break that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7418#issuecomment-548485790
https://github.com/hail-is/hail/issues/7433#issuecomment-548909738:0,Availability,Error,Error,0,"Error:; ```; Hail version: 0.2.26-5e79257ea31c; Error summary: GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7433#issuecomment-548909738
https://github.com/hail-is/hail/issues/7433#issuecomment-548909738:48,Availability,Error,Error,48,"Error:; ```; Hail version: 0.2.26-5e79257ea31c; Error summary: GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7433#issuecomment-548909738
https://github.com/hail-is/hail/issues/7433#issuecomment-548909738:128,Availability,error,errors,128,"Error:; ```; Hail version: 0.2.26-5e79257ea31c; Error summary: GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7433#issuecomment-548909738
https://github.com/hail-is/hail/issues/7433#issuecomment-548909738:166,Integrability,message,message,166,"Error:; ```; Hail version: 0.2.26-5e79257ea31c; Error summary: GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7433#issuecomment-548909738
https://github.com/hail-is/hail/issues/7433#issuecomment-548909738:273,Integrability,message,message,273,"Error:; ```; Hail version: 0.2.26-5e79257ea31c; Error summary: GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Bucket is requester pays bucket but no user project provided."",; ""reason"" : ""required""; } ],; ""message"" : ""Bucket is requester pays bucket but no user project provided.""; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7433#issuecomment-548909738
https://github.com/hail-is/hail/pull/7437#issuecomment-549065979:25,Availability,error,error,25,This is getting this SQL error in create_batch2_tables:. ERROR 1215 (HY000) at line 70: Cannot add foreign key constraint,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7437#issuecomment-549065979
https://github.com/hail-is/hail/pull/7437#issuecomment-549065979:57,Availability,ERROR,ERROR,57,This is getting this SQL error in create_batch2_tables:. ERROR 1215 (HY000) at line 70: Cannot add foreign key constraint,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7437#issuecomment-549065979
https://github.com/hail-is/hail/pull/7445#issuecomment-549184176:110,Availability,error,errors,110,"> First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. Nope. But I didn't run many tests with setup/cleanup containers actually doing anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445#issuecomment-549184176
https://github.com/hail-is/hail/pull/7445#issuecomment-549184176:230,Testability,test,tests,230,"> First, I'm seeing transient (but common, maybe 10% of the time?! Have you seen this before, Jackie?) gsutil errors in the setup/cleanup containers that look like: [Errno 2] No such file or directory. Nope. But I didn't run many tests with setup/cleanup containers actually doing anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7445#issuecomment-549184176
https://github.com/hail-is/hail/pull/7447#issuecomment-549387958:0,Deployability,Update,Update,0,Update: I got things to work if I reject all traffic to the metadata server except DNS.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7447#issuecomment-549387958
https://github.com/hail-is/hail/pull/7448#issuecomment-602325937:30,Integrability,wrap,wrapping,30,we've got another approach to wrapping now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7448#issuecomment-602325937
https://github.com/hail-is/hail/pull/7449#issuecomment-549440769:203,Availability,error,error,203,"@cseed Should the behavior of the logs be to not have a link if the job is ready or pending or to report None? Right now, we report None for `status` if it's ready or pending and have a web.HTTPNotFound error for logs when it's pending or ready. I think we should have it be consistent between logs and status and I think having no links in the ready and pending case is clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549440769
https://github.com/hail-is/hail/pull/7449#issuecomment-549440769:34,Testability,log,logs,34,"@cseed Should the behavior of the logs be to not have a link if the job is ready or pending or to report None? Right now, we report None for `status` if it's ready or pending and have a web.HTTPNotFound error for logs when it's pending or ready. I think we should have it be consistent between logs and status and I think having no links in the ready and pending case is clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549440769
https://github.com/hail-is/hail/pull/7449#issuecomment-549440769:213,Testability,log,logs,213,"@cseed Should the behavior of the logs be to not have a link if the job is ready or pending or to report None? Right now, we report None for `status` if it's ready or pending and have a web.HTTPNotFound error for logs when it's pending or ready. I think we should have it be consistent between logs and status and I think having no links in the ready and pending case is clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549440769
https://github.com/hail-is/hail/pull/7449#issuecomment-549440769:294,Testability,log,logs,294,"@cseed Should the behavior of the logs be to not have a link if the job is ready or pending or to report None? Right now, we report None for `status` if it's ready or pending and have a web.HTTPNotFound error for logs when it's pending or ready. I think we should have it be consistent between logs and status and I think having no links in the ready and pending case is clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549440769
https://github.com/hail-is/hail/pull/7449#issuecomment-549440769:371,Usability,clear,clearer,371,"@cseed Should the behavior of the logs be to not have a link if the job is ready or pending or to report None? Right now, we report None for `status` if it's ready or pending and have a web.HTTPNotFound error for logs when it's pending or ready. I think we should have it be consistent between logs and status and I think having no links in the ready and pending case is clearer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549440769
https://github.com/hail-is/hail/pull/7449#issuecomment-549809632:170,Deployability,configurat,configuration,170,"So I think we should fix this is in a slightly different way. First, I want to unify the log and status page. Second, I want the status to actually include the whole job configuration, of which status is just a sub-field, so you can look at detail to what you submitted. Then it would make sense for these links to always be present. That said, they obviously shouldn't be broken. Instead of 404, we should just say ""job is pending, no logs"" or wahtever. Let me make pass on the batch2 UI and then revisit this, OK?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549809632
https://github.com/hail-is/hail/pull/7449#issuecomment-549809632:170,Modifiability,config,configuration,170,"So I think we should fix this is in a slightly different way. First, I want to unify the log and status page. Second, I want the status to actually include the whole job configuration, of which status is just a sub-field, so you can look at detail to what you submitted. Then it would make sense for these links to always be present. That said, they obviously shouldn't be broken. Instead of 404, we should just say ""job is pending, no logs"" or wahtever. Let me make pass on the batch2 UI and then revisit this, OK?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549809632
https://github.com/hail-is/hail/pull/7449#issuecomment-549809632:89,Testability,log,log,89,"So I think we should fix this is in a slightly different way. First, I want to unify the log and status page. Second, I want the status to actually include the whole job configuration, of which status is just a sub-field, so you can look at detail to what you submitted. Then it would make sense for these links to always be present. That said, they obviously shouldn't be broken. Instead of 404, we should just say ""job is pending, no logs"" or wahtever. Let me make pass on the batch2 UI and then revisit this, OK?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549809632
https://github.com/hail-is/hail/pull/7449#issuecomment-549809632:436,Testability,log,logs,436,"So I think we should fix this is in a slightly different way. First, I want to unify the log and status page. Second, I want the status to actually include the whole job configuration, of which status is just a sub-field, so you can look at detail to what you submitted. Then it would make sense for these links to always be present. That said, they obviously shouldn't be broken. Instead of 404, we should just say ""job is pending, no logs"" or wahtever. Let me make pass on the batch2 UI and then revisit this, OK?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549809632
https://github.com/hail-is/hail/pull/7453#issuecomment-550407768:31,Availability,error,errors,31,"what the hell, bytecode verify errors from using locals? i saw another code-function creating locals so i assumed it was safe to do so, but apparently it isn't.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7453#issuecomment-550407768
https://github.com/hail-is/hail/pull/7453#issuecomment-550407768:121,Safety,safe,safe,121,"what the hell, bytecode verify errors from using locals? i saw another code-function creating locals so i assumed it was safe to do so, but apparently it isn't.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7453#issuecomment-550407768
https://github.com/hail-is/hail/pull/7454#issuecomment-550354045:58,Testability,log,logs,58,"Just for reference, this PR also moves copying the worker logs to the shutdown script and adds support for env in batch2. I also added a service account `ci-agent` in the default namespace for the test, dev scopes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7454#issuecomment-550354045
https://github.com/hail-is/hail/pull/7454#issuecomment-550354045:197,Testability,test,test,197,"Just for reference, this PR also moves copying the worker logs to the shutdown script and adds support for env in batch2. I also added a service account `ci-agent` in the default namespace for the test, dev scopes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7454#issuecomment-550354045
https://github.com/hail-is/hail/pull/7455#issuecomment-549889843:38,Deployability,pipeline,pipeline,38,"It's not the data, it's a complicated pipeline to replicate. I can try harder.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7455#issuecomment-549889843
https://github.com/hail-is/hail/pull/7455#issuecomment-549903705:18,Testability,test,test,18,"ok, added failing test!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7455#issuecomment-549903705
https://github.com/hail-is/hail/issues/7464#issuecomment-550295122:20,Availability,error,error,20,"this is not a great error message. The problem is you're missing the lz4 dependency, see here:; https://hail.is/docs/0.2/getting_started.html?highlight=lz4#requirements",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7464#issuecomment-550295122
https://github.com/hail-is/hail/issues/7464#issuecomment-550295122:26,Integrability,message,message,26,"this is not a great error message. The problem is you're missing the lz4 dependency, see here:; https://hail.is/docs/0.2/getting_started.html?highlight=lz4#requirements",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7464#issuecomment-550295122
https://github.com/hail-is/hail/issues/7464#issuecomment-550295122:73,Integrability,depend,dependency,73,"this is not a great error message. The problem is you're missing the lz4 dependency, see here:; https://hail.is/docs/0.2/getting_started.html?highlight=lz4#requirements",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7464#issuecomment-550295122
https://github.com/hail-is/hail/pull/7466#issuecomment-551108566:55,Security,access,access,55,"I think now that ci-agent lives in batch-pods, but has access to default, the ci-agent currently in batch-pods with cluster wide access is fine, and this should just be able to merged in as is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7466#issuecomment-551108566
https://github.com/hail-is/hail/pull/7466#issuecomment-551108566:129,Security,access,access,129,"I think now that ci-agent lives in batch-pods, but has access to default, the ci-agent currently in batch-pods with cluster wide access is fine, and this should just be able to merged in as is.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7466#issuecomment-551108566
https://github.com/hail-is/hail/pull/7466#issuecomment-551133834:57,Security,access,access,57,"> I think now that ci-agent lives in batch-pods, but has access to default, the ci-agent currently in batch-pods with cluster wide access is fine, and this should just be able to merged in as is. I agree. Looks like there is an issue, hello isn't showing up. I'll let you investigate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7466#issuecomment-551133834
https://github.com/hail-is/hail/pull/7466#issuecomment-551133834:131,Security,access,access,131,"> I think now that ci-agent lives in batch-pods, but has access to default, the ci-agent currently in batch-pods with cluster wide access is fine, and this should just be able to merged in as is. I agree. Looks like there is an issue, hello isn't showing up. I'll let you investigate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7466#issuecomment-551133834
https://github.com/hail-is/hail/pull/7470#issuecomment-550495837:25,Availability,error,error,25,"It's getting a forbidden error when trying to download the secret. I think I know why. The service account being used is the `batch2` service account from the default namespace. I think we need to do what I did with CI where the service account lives in the default namespace, but it can read secrets, service accounts in the batch pods namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7470#issuecomment-550495837
https://github.com/hail-is/hail/pull/7470#issuecomment-550495837:46,Availability,down,download,46,"It's getting a forbidden error when trying to download the secret. I think I know why. The service account being used is the `batch2` service account from the default namespace. I think we need to do what I did with CI where the service account lives in the default namespace, but it can read secrets, service accounts in the batch pods namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7470#issuecomment-550495837
https://github.com/hail-is/hail/pull/7470#issuecomment-551005475:80,Deployability,deploy,deploy,80,"OK, for the record, I changed ci-agent to:; - only exist in batch-pods,; - have deploy permissions in both default and batch-pods. In the future, I think we should:; - have a service account just for creating and destroying namespace (creator-and-destroyer-of-namespaces) used by create namespace step,; - the deploy step should use the admin account for the target namespace,; - and similarly for create database. This means we'll eventually need to include the namespace with the service account in batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7470#issuecomment-551005475
https://github.com/hail-is/hail/pull/7470#issuecomment-551005475:310,Deployability,deploy,deploy,310,"OK, for the record, I changed ci-agent to:; - only exist in batch-pods,; - have deploy permissions in both default and batch-pods. In the future, I think we should:; - have a service account just for creating and destroying namespace (creator-and-destroyer-of-namespaces) used by create namespace step,; - the deploy step should use the admin account for the target namespace,; - and similarly for create database. This means we'll eventually need to include the namespace with the service account in batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7470#issuecomment-551005475
https://github.com/hail-is/hail/pull/7474#issuecomment-551147727:54,Usability,clear,clear,54,I wanted to remove /batch in a different PR so it was clear what additional changes there were besides `rm /batch`. I can make it a separate commit though and that should fulfill the same purpose.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7474#issuecomment-551147727
https://github.com/hail-is/hail/pull/7474#issuecomment-551180556:56,Usability,clear,clear,56,> I wanted to remove /batch in a different PR so it was clear what additional changes there were besides rm /batch. Smart. Disregard my suggestion.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7474#issuecomment-551180556
https://github.com/hail-is/hail/pull/7478#issuecomment-551083244:8,Testability,test,test,8,Added a test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7478#issuecomment-551083244
https://github.com/hail-is/hail/pull/7479#issuecomment-553897323:67,Deployability,release,release,67,just a note that we're waiting for this PR to go in for the 0.2.27 release.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479#issuecomment-553897323
https://github.com/hail-is/hail/pull/7479#issuecomment-553991453:8,Testability,test,test,8,lots of test failues,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7479#issuecomment-553991453
https://github.com/hail-is/hail/pull/7484#issuecomment-551253066:40,Availability,error,error,40,"Ok. I was trying to hide the kubernetes error message from the users because I thought it might be confusing. But if you feel that's ok, then I'll make it just report the original error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7484#issuecomment-551253066
https://github.com/hail-is/hail/pull/7484#issuecomment-551253066:180,Availability,error,error,180,"Ok. I was trying to hide the kubernetes error message from the users because I thought it might be confusing. But if you feel that's ok, then I'll make it just report the original error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7484#issuecomment-551253066
https://github.com/hail-is/hail/pull/7484#issuecomment-551253066:46,Integrability,message,message,46,"Ok. I was trying to hide the kubernetes error message from the users because I thought it might be confusing. But if you feel that's ok, then I'll make it just report the original error.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7484#issuecomment-551253066
https://github.com/hail-is/hail/pull/7484#issuecomment-551259023:66,Security,sanitiz,sanitize,66,"Great; I was debating whether to suggest that. I think we'll need sanitize the docker and k8s issues eventually, but for now, the more information the better.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7484#issuecomment-551259023
https://github.com/hail-is/hail/pull/7498#issuecomment-556059479:124,Energy Efficiency,schedul,scheduling,124,I'm closing this in favor of keeping the fraction of cpus and memory fixed (and taking the max of the requests) until we're scheduling along multiple dimensions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7498#issuecomment-556059479
https://github.com/hail-is/hail/pull/7504#issuecomment-554178679:99,Testability,test,test,99,"So is there just no such thing as a ""distinct inner join"" in hail? My read of this (other than the test porting) is that we weren't using `innerJoinDistinct` anywhere anyway other than for tests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7504#issuecomment-554178679
https://github.com/hail-is/hail/pull/7504#issuecomment-554178679:189,Testability,test,tests,189,"So is there just no such thing as a ""distinct inner join"" in hail? My read of this (other than the test porting) is that we weren't using `innerJoinDistinct` anywhere anyway other than for tests",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7504#issuecomment-554178679
https://github.com/hail-is/hail/pull/7507#issuecomment-553959196:32,Deployability,deploy,deploy-config,32,"FYI, I had to add a default/gce-deploy-config for the deploy config for tasks running on batch2 workers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7507#issuecomment-553959196
https://github.com/hail-is/hail/pull/7507#issuecomment-553959196:54,Deployability,deploy,deploy,54,"FYI, I had to add a default/gce-deploy-config for the deploy config for tasks running on batch2 workers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7507#issuecomment-553959196
https://github.com/hail-is/hail/pull/7507#issuecomment-553959196:39,Modifiability,config,config,39,"FYI, I had to add a default/gce-deploy-config for the deploy config for tasks running on batch2 workers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7507#issuecomment-553959196
https://github.com/hail-is/hail/pull/7507#issuecomment-553959196:61,Modifiability,config,config,61,"FYI, I had to add a default/gce-deploy-config for the deploy config for tasks running on batch2 workers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7507#issuecomment-553959196
https://github.com/hail-is/hail/issues/7513#issuecomment-555075049:149,Deployability,update,updates,149,"Agh, so we won't be able to use Python 3.8 with hail at all until switching to Spark 3.0? And that will be locked up in waiting for dataproc / Terra updates as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7513#issuecomment-555075049
https://github.com/hail-is/hail/pull/7515#issuecomment-554446633:225,Performance,perform,perform,225,"We can make the virtual types line up with an explicit `SelectFields` to pick out the prefix used in the intervals from the key. We would just want to ensure that we can generate code (using the ""view"" struct ptype) that can perform the comparisons without copying the key.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7515#issuecomment-554446633
https://github.com/hail-is/hail/pull/7523#issuecomment-554461259:0,Testability,test,testing,0,testing,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523#issuecomment-554461259
https://github.com/hail-is/hail/pull/7523#issuecomment-557588150:54,Performance,load,load,54,recreating because the CI page is getting too slow to load.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7523#issuecomment-557588150
https://github.com/hail-is/hail/issues/7524#issuecomment-562856309:7,Modifiability,config,config,7,"Fixed, config and status now displayed in job page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7524#issuecomment-562856309
https://github.com/hail-is/hail/pull/7528#issuecomment-554517326:24,Performance,cache,cache,24,"We get a good number of cache hits, which definitely help, but we still compile so many classes. gs://cdv-hail/logs/hail-joint-caller-oops-20191115-1801.log.xz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7528#issuecomment-554517326
https://github.com/hail-is/hail/pull/7528#issuecomment-554517326:111,Testability,log,logs,111,"We get a good number of cache hits, which definitely help, but we still compile so many classes. gs://cdv-hail/logs/hail-joint-caller-oops-20191115-1801.log.xz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7528#issuecomment-554517326
https://github.com/hail-is/hail/pull/7528#issuecomment-554517326:153,Testability,log,log,153,"We get a good number of cache hits, which definitely help, but we still compile so many classes. gs://cdv-hail/logs/hail-joint-caller-oops-20191115-1801.log.xz",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7528#issuecomment-554517326
https://github.com/hail-is/hail/pull/7528#issuecomment-554563029:67,Performance,cache,cache,67,we're compiling like 20x fewer classes based on that. Every single cache hit is one fewer class compiled. The compilations there are value IRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7528#issuecomment-554563029
https://github.com/hail-is/hail/pull/7529#issuecomment-554389306:124,Integrability,interface,interfaces,124,"The specialization creates mangled class files, and having just one implementation makes it much easier to reason about the interfaces. I think we can add support for all the numeric types back once we've gotten rid of the old agg path, if anyone cares enough to do that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7529#issuecomment-554389306
https://github.com/hail-is/hail/pull/7530#issuecomment-555241754:100,Deployability,deploy,deploy,100,"OK, I think I addressed the comments and it is ready for another look. I tested the batch2 UI witih deploy dev and it seems good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7530#issuecomment-555241754
https://github.com/hail-is/hail/pull/7530#issuecomment-555241754:73,Testability,test,tested,73,"OK, I think I addressed the comments and it is ready for another look. I tested the batch2 UI witih deploy dev and it seems good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7530#issuecomment-555241754
https://github.com/hail-is/hail/pull/7534#issuecomment-556556045:112,Availability,down,down,112,"I also fixed a few buildImage's that had overly generous docker contexts, which, in my experience, really slows down image build due to loading the entire hail repository into the context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7534#issuecomment-556556045
https://github.com/hail-is/hail/pull/7534#issuecomment-556556045:136,Performance,load,loading,136,"I also fixed a few buildImage's that had overly generous docker contexts, which, in my experience, really slows down image build due to loading the entire hail repository into the context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7534#issuecomment-556556045
https://github.com/hail-is/hail/pull/7534#issuecomment-556556070:112,Availability,down,down,112,"I also fixed a few buildImage's that had overly generous docker contexts, which, in my experience, really slows down image build due to loading the entire hail repository into the context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7534#issuecomment-556556070
https://github.com/hail-is/hail/pull/7534#issuecomment-556556070:136,Performance,load,loading,136,"I also fixed a few buildImage's that had overly generous docker contexts, which, in my experience, really slows down image build due to loading the entire hail repository into the context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7534#issuecomment-556556070
https://github.com/hail-is/hail/pull/7534#issuecomment-557184077:258,Usability,simpl,simple,258,"OK, another idea to make this easier (maintaining this image separate from the build process is going to be painful): buildImage should have an optional script that runs before the docker build call. If you do this, you can just cat > Dockerfile with a very simple docker file to create the bootstrap image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7534#issuecomment-557184077
https://github.com/hail-is/hail/pull/7535#issuecomment-554525089:132,Availability,redundant,redundant,132,"Not really sure why this is coming up now (I don't see anything that changed in this PR, but the scala isn't compiling because of a redundant function definition in is.hail.expr.types and is.hail.expr.ir (coerce)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7535#issuecomment-554525089
https://github.com/hail-is/hail/pull/7535#issuecomment-554525089:132,Safety,redund,redundant,132,"Not really sure why this is coming up now (I don't see anything that changed in this PR, but the scala isn't compiling because of a redundant function definition in is.hail.expr.types and is.hail.expr.ir (coerce)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7535#issuecomment-554525089
https://github.com/hail-is/hail/pull/7536#issuecomment-555197402:10,Testability,test,tested,10,"I haven't tested it, but I think that should work",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7536#issuecomment-555197402
https://github.com/hail-is/hail/pull/7536#issuecomment-557703477:29,Deployability,release,release,29,"Ok, just used this script to release 0.2.28 without a problem so I think we should merge this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7536#issuecomment-557703477
https://github.com/hail-is/hail/pull/7540#issuecomment-555034067:103,Integrability,depend,dependent,103,reassigned since Arcturus has a lot of PRs right now and I had non-randomly assigned this based on the dependent PR's assignment anyway,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7540#issuecomment-555034067
https://github.com/hail-is/hail/pull/7543#issuecomment-554780465:63,Availability,down,down,63,This appears to have cut batch2 test time nearly in half (5-6m down from 11+m),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7543#issuecomment-554780465
https://github.com/hail-is/hail/pull/7543#issuecomment-554780465:32,Testability,test,test,32,This appears to have cut batch2 test time nearly in half (5-6m down from 11+m),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7543#issuecomment-554780465
https://github.com/hail-is/hail/pull/7553#issuecomment-558701848:51,Energy Efficiency,schedul,scheduler,51,I will reopen this once the billing and fair share scheduler go in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7553#issuecomment-558701848
https://github.com/hail-is/hail/pull/7557#issuecomment-555494694:195,Modifiability,refactor,refactor,195,"current issues: ; - `copyFrom` on this aggregator assumes that the seqOp has not been called on the data we're copying, just the initOp. This is somewhat annoying to fix because we would need to refactor `copyFrom` to take the AggregatorState rather than address, since the region is necessary to look up the java object.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7557#issuecomment-555494694
https://github.com/hail-is/hail/pull/7557#issuecomment-555627357:35,Testability,test,test,35,🤔 . I will investigate whether the test coverage is sufficient this afternoon.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7557#issuecomment-555627357
https://github.com/hail-is/hail/pull/7563#issuecomment-555743192:70,Testability,test,test,70,"That image includes rows 26 and 27 which I didn't bother fixing in my test, but these changes are templated so all rows will be fine.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7563#issuecomment-555743192
https://github.com/hail-is/hail/pull/7564#issuecomment-557584616:123,Deployability,rollout,rollout,123,"@cseed The CI tests rely on the service waiting functionality. In particular, they do not have permissions to use `kubectl rollout status` so I can't switch to waiting for a deployment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7564#issuecomment-557584616
https://github.com/hail-is/hail/pull/7564#issuecomment-557584616:174,Deployability,deploy,deployment,174,"@cseed The CI tests rely on the service waiting functionality. In particular, they do not have permissions to use `kubectl rollout status` so I can't switch to waiting for a deployment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7564#issuecomment-557584616
https://github.com/hail-is/hail/pull/7564#issuecomment-557584616:14,Testability,test,tests,14,"@cseed The CI tests rely on the service waiting functionality. In particular, they do not have permissions to use `kubectl rollout status` so I can't switch to waiting for a deployment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7564#issuecomment-557584616
https://github.com/hail-is/hail/pull/7564#issuecomment-558382580:79,Performance,race condition,race condition,79,"@cseed bump. This is blocking further testing on dbuf. Without this, there's a race condition where the image is deleted by CI before the entire StatefulSet starts.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7564#issuecomment-558382580
https://github.com/hail-is/hail/pull/7564#issuecomment-558382580:38,Testability,test,testing,38,"@cseed bump. This is blocking further testing on dbuf. Without this, there's a race condition where the image is deleted by CI before the entire StatefulSet starts.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7564#issuecomment-558382580
https://github.com/hail-is/hail/pull/7567#issuecomment-556266727:65,Availability,down,down,65,"looked into the failing tests concerning randomness, and tracked down the source of the failures:. - `ApplySeeded` is an `AbstractApplyNode`. this means the interpreter will try to ""memoize"" the function definition so as to generate it only once even if you call it in multiple places in the IR. ; - memoization is based on referential equality, so they need to be the exact same IR object in order for reuse to trigger; - the ""seeded function"" implementations used by the test suite will create a new randomness state per generated function; - the Interpret pipeline used to rewrite each ApplySeeded node to be different objects, but with your changes they are the same, so their functions get memoized, thus sharing the same state. im not entirely sure which of these was doing the wrong thing, but IMO your changes in this PR should not have introduced any problems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727
https://github.com/hail-is/hail/pull/7567#issuecomment-556266727:88,Availability,failure,failures,88,"looked into the failing tests concerning randomness, and tracked down the source of the failures:. - `ApplySeeded` is an `AbstractApplyNode`. this means the interpreter will try to ""memoize"" the function definition so as to generate it only once even if you call it in multiple places in the IR. ; - memoization is based on referential equality, so they need to be the exact same IR object in order for reuse to trigger; - the ""seeded function"" implementations used by the test suite will create a new randomness state per generated function; - the Interpret pipeline used to rewrite each ApplySeeded node to be different objects, but with your changes they are the same, so their functions get memoized, thus sharing the same state. im not entirely sure which of these was doing the wrong thing, but IMO your changes in this PR should not have introduced any problems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727
https://github.com/hail-is/hail/pull/7567#issuecomment-556266727:559,Deployability,pipeline,pipeline,559,"looked into the failing tests concerning randomness, and tracked down the source of the failures:. - `ApplySeeded` is an `AbstractApplyNode`. this means the interpreter will try to ""memoize"" the function definition so as to generate it only once even if you call it in multiple places in the IR. ; - memoization is based on referential equality, so they need to be the exact same IR object in order for reuse to trigger; - the ""seeded function"" implementations used by the test suite will create a new randomness state per generated function; - the Interpret pipeline used to rewrite each ApplySeeded node to be different objects, but with your changes they are the same, so their functions get memoized, thus sharing the same state. im not entirely sure which of these was doing the wrong thing, but IMO your changes in this PR should not have introduced any problems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727
https://github.com/hail-is/hail/pull/7567#issuecomment-556266727:576,Modifiability,rewrite,rewrite,576,"looked into the failing tests concerning randomness, and tracked down the source of the failures:. - `ApplySeeded` is an `AbstractApplyNode`. this means the interpreter will try to ""memoize"" the function definition so as to generate it only once even if you call it in multiple places in the IR. ; - memoization is based on referential equality, so they need to be the exact same IR object in order for reuse to trigger; - the ""seeded function"" implementations used by the test suite will create a new randomness state per generated function; - the Interpret pipeline used to rewrite each ApplySeeded node to be different objects, but with your changes they are the same, so their functions get memoized, thus sharing the same state. im not entirely sure which of these was doing the wrong thing, but IMO your changes in this PR should not have introduced any problems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727
https://github.com/hail-is/hail/pull/7567#issuecomment-556266727:24,Testability,test,tests,24,"looked into the failing tests concerning randomness, and tracked down the source of the failures:. - `ApplySeeded` is an `AbstractApplyNode`. this means the interpreter will try to ""memoize"" the function definition so as to generate it only once even if you call it in multiple places in the IR. ; - memoization is based on referential equality, so they need to be the exact same IR object in order for reuse to trigger; - the ""seeded function"" implementations used by the test suite will create a new randomness state per generated function; - the Interpret pipeline used to rewrite each ApplySeeded node to be different objects, but with your changes they are the same, so their functions get memoized, thus sharing the same state. im not entirely sure which of these was doing the wrong thing, but IMO your changes in this PR should not have introduced any problems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727
https://github.com/hail-is/hail/pull/7567#issuecomment-556266727:473,Testability,test,test,473,"looked into the failing tests concerning randomness, and tracked down the source of the failures:. - `ApplySeeded` is an `AbstractApplyNode`. this means the interpreter will try to ""memoize"" the function definition so as to generate it only once even if you call it in multiple places in the IR. ; - memoization is based on referential equality, so they need to be the exact same IR object in order for reuse to trigger; - the ""seeded function"" implementations used by the test suite will create a new randomness state per generated function; - the Interpret pipeline used to rewrite each ApplySeeded node to be different objects, but with your changes they are the same, so their functions get memoized, thus sharing the same state. im not entirely sure which of these was doing the wrong thing, but IMO your changes in this PR should not have introduced any problems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727
https://github.com/hail-is/hail/pull/7567#issuecomment-557024561:60,Availability,failure,failures,60,"Yeah, this PR really shouldn't have introduced the kinds of failures we're seeing. That seems to be the refrain of all of my lowering refactoring PRs though!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-557024561
https://github.com/hail-is/hail/pull/7567#issuecomment-557024561:134,Modifiability,refactor,refactoring,134,"Yeah, this PR really shouldn't have introduced the kinds of failures we're seeing. That seems to be the refrain of all of my lowering refactoring PRs though!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-557024561
https://github.com/hail-is/hail/pull/7570#issuecomment-557265229:201,Deployability,release,release,201,"John, one improvement to this that I think we should consider: add a build step that changes the url to a versioned link. This will be a bit involved since you won't be able to link inside of a GitHub release (which is a zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7570#issuecomment-557265229
https://github.com/hail-is/hail/issues/7572#issuecomment-557020739:21,Testability,log,logic,21,"I think I'd flip the logic. I'm not sure if this one is wrong:. ```; In [9]: hl.eval((p, hl.range(2).map(lambda x: p))); Out[9]: (0.46124206583236194, [0.46124206583236194, 0.46124206583236194]); ```. But if it's right, clearly this one is wrong:. ```; In [7]: p = 1 - r. In [8]: hl.eval(hl.range(2).map(lambda x: p)); Out[8]: [0.46124206583236194, 0.06052003544873086]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7572#issuecomment-557020739
https://github.com/hail-is/hail/issues/7572#issuecomment-557020739:220,Usability,clear,clearly,220,"I think I'd flip the logic. I'm not sure if this one is wrong:. ```; In [9]: hl.eval((p, hl.range(2).map(lambda x: p))); Out[9]: (0.46124206583236194, [0.46124206583236194, 0.46124206583236194]); ```. But if it's right, clearly this one is wrong:. ```; In [7]: p = 1 - r. In [8]: hl.eval(hl.range(2).map(lambda x: p)); Out[8]: [0.46124206583236194, 0.06052003544873086]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7572#issuecomment-557020739
https://github.com/hail-is/hail/issues/7574#issuecomment-613493348:21,Energy Efficiency,schedul,scheduled,21,moved to asana to be scheduled.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7574#issuecomment-613493348
https://github.com/hail-is/hail/pull/7578#issuecomment-557229025:13,Testability,test,tests,13,Failing Java tests,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7578#issuecomment-557229025
https://github.com/hail-is/hail/pull/7583#issuecomment-557243691:36,Deployability,deploy,deploy,36,Closing this while I debug with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7583#issuecomment-557243691
https://github.com/hail-is/hail/pull/7583#issuecomment-557524946:122,Availability,error,errors,122,Do I need to add resource requests to the build.yaml file and CI build.py? I'm worried about things getting out of memory errors now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7583#issuecomment-557524946
https://github.com/hail-is/hail/pull/7583#issuecomment-557686216:108,Availability,error,error,108,"I can't figure out why my out of memory test isn't working. It's reporting exit code 0 and no out of memory error even though when I do the same thing locally on my computer with docker run or on an instance in the cluster, I get exit code 137 and out of memory. I'm limiting the docker run command to the same amount of bytes that the docker command in the worker should be limiting it to (looked at the docker output in the worker logs). I think the next step is to try using curl to run docker containers rather than the docker cli.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7583#issuecomment-557686216
https://github.com/hail-is/hail/pull/7583#issuecomment-557686216:40,Testability,test,test,40,"I can't figure out why my out of memory test isn't working. It's reporting exit code 0 and no out of memory error even though when I do the same thing locally on my computer with docker run or on an instance in the cluster, I get exit code 137 and out of memory. I'm limiting the docker run command to the same amount of bytes that the docker command in the worker should be limiting it to (looked at the docker output in the worker logs). I think the next step is to try using curl to run docker containers rather than the docker cli.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7583#issuecomment-557686216
https://github.com/hail-is/hail/pull/7583#issuecomment-557686216:433,Testability,log,logs,433,"I can't figure out why my out of memory test isn't working. It's reporting exit code 0 and no out of memory error even though when I do the same thing locally on my computer with docker run or on an instance in the cluster, I get exit code 137 and out of memory. I'm limiting the docker run command to the same amount of bytes that the docker command in the worker should be limiting it to (looked at the docker output in the worker logs). I think the next step is to try using curl to run docker containers rather than the docker cli.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7583#issuecomment-557686216
https://github.com/hail-is/hail/pull/7587#issuecomment-557233459:12,Testability,test,test,12,"Also, do we test emitter stuff like this anywhere?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7587#issuecomment-557233459
https://github.com/hail-is/hail/pull/7587#issuecomment-557264052:8,Testability,test,tests,8,"We have tests that use whileLoop in StagedRegionValueSuite (in testing creation of staged arrays), but it would be useful to have this broken out and with deeper coverage of input paths (like your nested array case)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7587#issuecomment-557264052
https://github.com/hail-is/hail/pull/7587#issuecomment-557264052:63,Testability,test,testing,63,"We have tests that use whileLoop in StagedRegionValueSuite (in testing creation of staged arrays), but it would be useful to have this broken out and with deeper coverage of input paths (like your nested array case)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7587#issuecomment-557264052
https://github.com/hail-is/hail/pull/7587#issuecomment-560221403:39,Testability,test,test,39,"Is there an easy way for me to write a test that's just ""Run some `Code[_] block and check what it returns""?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7587#issuecomment-560221403
https://github.com/hail-is/hail/pull/7587#issuecomment-560541059:40,Testability,test,test,40,"Thanks, that was helpful. Now there's a test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7587#issuecomment-560541059
https://github.com/hail-is/hail/pull/7592#issuecomment-558807050:37,Safety,timeout,timeout,37,I suggest `asyncio.wait_for` for the timeout: https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7592#issuecomment-558807050
https://github.com/hail-is/hail/pull/7592#issuecomment-559056090:64,Testability,assert,assert,64,"You're missing an await:. ```; deploy_statuses = resp.json(); > assert len(deploy_statuses) == 1, deploy_statuses; E TypeError: object of type 'coroutine' has no len(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7592#issuecomment-559056090
https://github.com/hail-is/hail/pull/7593#issuecomment-558337832:4,Availability,failure,failure,4,The failure is due to the new memory requirements. Apparently the python script uses a lot more memory than I thought. Trying to find the magic number now. The tests were passing before the memory limits.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593#issuecomment-558337832
https://github.com/hail-is/hail/pull/7593#issuecomment-558337832:160,Testability,test,tests,160,The failure is due to the new memory requirements. Apparently the python script uses a lot more memory than I thought. Trying to find the magic number now. The tests were passing before the memory limits.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593#issuecomment-558337832
https://github.com/hail-is/hail/pull/7593#issuecomment-558340651:42,Performance,throughput,throughput,42,latest test yields:; ```; write aggregate-throughput: 0.263 GiB/s; read aggregate-throughput: 0.083 GiB/s; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593#issuecomment-558340651
https://github.com/hail-is/hail/pull/7593#issuecomment-558340651:82,Performance,throughput,throughput,82,latest test yields:; ```; write aggregate-throughput: 0.263 GiB/s; read aggregate-throughput: 0.083 GiB/s; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593#issuecomment-558340651
https://github.com/hail-is/hail/pull/7593#issuecomment-558340651:7,Testability,test,test,7,latest test yields:; ```; write aggregate-throughput: 0.263 GiB/s; read aggregate-throughput: 0.083 GiB/s; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7593#issuecomment-558340651
https://github.com/hail-is/hail/pull/7595#issuecomment-565176979:16,Testability,test,tests,16,"This is failing tests and isn't actually important, closing for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7595#issuecomment-565176979
https://github.com/hail-is/hail/pull/7596#issuecomment-560198758:69,Integrability,interface,interface,69,"For now, I'm going to add them by hand. Later there will be an admin interface for creating/deleting/editing billing projects.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7596#issuecomment-560198758
https://github.com/hail-is/hail/pull/7599#issuecomment-557715665:54,Usability,clear,clearly,54,"Sorry will fix this, trying to break up the larger PR clearly failed. One min.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7599#issuecomment-557715665
https://github.com/hail-is/hail/pull/7600#issuecomment-558726375:772,Deployability,update,update,772,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375
https://github.com/hail-is/hail/pull/7600#issuecomment-558726375:996,Deployability,update,update,996,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375
https://github.com/hail-is/hail/pull/7600#issuecomment-558726375:1273,Deployability,update,update,1273,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375
https://github.com/hail-is/hail/pull/7600#issuecomment-558726375:190,Energy Efficiency,charge,charge,190,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375
https://github.com/hail-is/hail/pull/7600#issuecomment-558726375:1202,Safety,avoid,avoid,1202,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375
https://github.com/hail-is/hail/pull/7600#issuecomment-558726375:1466,Usability,clear,clearer,1466,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375
https://github.com/hail-is/hail/pull/7605#issuecomment-557905128:45,Availability,error,error,45,FYI The last build failed due to a transient error (aiohttp.ServerDisconnectedError). I added it to the retry logic.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7605#issuecomment-557905128
https://github.com/hail-is/hail/pull/7605#issuecomment-557905128:110,Testability,log,logic,110,FYI The last build failed due to a transient error (aiohttp.ServerDisconnectedError). I added it to the retry logic.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7605#issuecomment-557905128
https://github.com/hail-is/hail/pull/7606#issuecomment-557902950:7,Testability,test,tested,7,I hand-tested the UI and everything looks good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557902950
https://github.com/hail-is/hail/pull/7606#issuecomment-557911497:131,Energy Efficiency,schedul,scheduled,131,"Rebased, this should be ready for review. For the moment, I made the worker type and cores not modifiable. We check if jobs can be scheduled on creation before insertion into the database, using memory/core, so changing the type or cores may make jobs in the database unable to be scheduled. The next step is having the scheduler and instance pool choose the right mix of instances for the workload (based on memory/cpu ratio and requested cores).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557911497
https://github.com/hail-is/hail/pull/7606#issuecomment-557911497:281,Energy Efficiency,schedul,scheduled,281,"Rebased, this should be ready for review. For the moment, I made the worker type and cores not modifiable. We check if jobs can be scheduled on creation before insertion into the database, using memory/core, so changing the type or cores may make jobs in the database unable to be scheduled. The next step is having the scheduler and instance pool choose the right mix of instances for the workload (based on memory/cpu ratio and requested cores).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557911497
https://github.com/hail-is/hail/pull/7606#issuecomment-557911497:320,Energy Efficiency,schedul,scheduler,320,"Rebased, this should be ready for review. For the moment, I made the worker type and cores not modifiable. We check if jobs can be scheduled on creation before insertion into the database, using memory/core, so changing the type or cores may make jobs in the database unable to be scheduled. The next step is having the scheduler and instance pool choose the right mix of instances for the workload (based on memory/cpu ratio and requested cores).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557911497
https://github.com/hail-is/hail/pull/7606#issuecomment-557911795:22,Deployability,deploy,deploy,22,Re-tested UI with dev deploy: looks good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557911795
https://github.com/hail-is/hail/pull/7606#issuecomment-557911795:3,Testability,test,tested,3,Re-tested UI with dev deploy: looks good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557911795
https://github.com/hail-is/hail/pull/7606#issuecomment-557913491:139,Availability,error,error,139,"Pushed one more fix: a batch test failed on job.wait() where /status threw 500. It was a running job, so batch hit the worker. The job was error, so it threw an exception and the container was being deleted. There was a race condition getting the container status:. ```; if self.container:; ... self.get_container_status() ...; ```. and deleting the container:. ```; if self.container:; ... call self.container.delete(); self.container = None; ```. If the delete happens between the check for self.container being defined and the call to self.container.show inside get_container_status, show throws 404. Thus, I modified get_container_status to return None on 404.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557913491
https://github.com/hail-is/hail/pull/7606#issuecomment-557913491:220,Performance,race condition,race condition,220,"Pushed one more fix: a batch test failed on job.wait() where /status threw 500. It was a running job, so batch hit the worker. The job was error, so it threw an exception and the container was being deleted. There was a race condition getting the container status:. ```; if self.container:; ... self.get_container_status() ...; ```. and deleting the container:. ```; if self.container:; ... call self.container.delete(); self.container = None; ```. If the delete happens between the check for self.container being defined and the call to self.container.show inside get_container_status, show throws 404. Thus, I modified get_container_status to return None on 404.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557913491
https://github.com/hail-is/hail/pull/7606#issuecomment-557913491:29,Testability,test,test,29,"Pushed one more fix: a batch test failed on job.wait() where /status threw 500. It was a running job, so batch hit the worker. The job was error, so it threw an exception and the container was being deleted. There was a race condition getting the container status:. ```; if self.container:; ... self.get_container_status() ...; ```. and deleting the container:. ```; if self.container:; ... call self.container.delete(); self.container = None; ```. If the delete happens between the check for self.container being defined and the call to self.container.show inside get_container_status, show throws 404. Thus, I modified get_container_status to return None on 404.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557913491
https://github.com/hail-is/hail/pull/7611#issuecomment-558444060:15,Testability,test,test,15,Worth adding a test?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558444060
https://github.com/hail-is/hail/pull/7611#issuecomment-558450046:17,Testability,test,tests,17,@konradjk Python tests exist. https://github.com/hail-is/hail/blob/master/hail/python/test/hail/utils/test_utils.py#L114. https://github.com/hail-is/hail/blob/d65959f65fe7c8018fcd83615e60ba8ef8233b96/hail/python/hail/fs/fs.py#L75. edit: GitHub is fancy,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558450046
https://github.com/hail-is/hail/pull/7611#issuecomment-558450046:86,Testability,test,test,86,@konradjk Python tests exist. https://github.com/hail-is/hail/blob/master/hail/python/test/hail/utils/test_utils.py#L114. https://github.com/hail-is/hail/blob/d65959f65fe7c8018fcd83615e60ba8ef8233b96/hail/python/hail/fs/fs.py#L75. edit: GitHub is fancy,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558450046
https://github.com/hail-is/hail/pull/7611#issuecomment-558457592:127,Testability,test,test,127,One that uses a glob though! I don’t see one at least. Maybe I missed (though I can’t imagine since it should have failed that test before this),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558457592
https://github.com/hail-is/hail/pull/7611#issuecomment-558616975:187,Integrability,interface,interface,187,"You’re right, it could be useful to have a scala-specific test, but any breaking changes away from old ls behavior will cause the hadoop_ls test to fail (since Python presents our public interface, the more important single test to have). edit: iPhone typos",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558616975
https://github.com/hail-is/hail/pull/7611#issuecomment-558616975:58,Testability,test,test,58,"You’re right, it could be useful to have a scala-specific test, but any breaking changes away from old ls behavior will cause the hadoop_ls test to fail (since Python presents our public interface, the more important single test to have). edit: iPhone typos",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558616975
https://github.com/hail-is/hail/pull/7611#issuecomment-558616975:140,Testability,test,test,140,"You’re right, it could be useful to have a scala-specific test, but any breaking changes away from old ls behavior will cause the hadoop_ls test to fail (since Python presents our public interface, the more important single test to have). edit: iPhone typos",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558616975
https://github.com/hail-is/hail/pull/7611#issuecomment-558616975:224,Testability,test,test,224,"You’re right, it could be useful to have a scala-specific test, but any breaking changes away from old ls behavior will cause the hadoop_ls test to fail (since Python presents our public interface, the more important single test to have). edit: iPhone typos",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558616975
https://github.com/hail-is/hail/pull/7611#issuecomment-558620422:17,Testability,test,test,17,"yeah, will add a test for the new functionality though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558620422
https://github.com/hail-is/hail/pull/7611#issuecomment-558865138:42,Testability,assert,assert,42,"Out of curiosity, what's different about `assert` and the `self.assertEqual`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558865138
https://github.com/hail-is/hail/pull/7611#issuecomment-558865138:64,Testability,assert,assertEqual,64,"Out of curiosity, what's different about `assert` and the `self.assertEqual`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-558865138
https://github.com/hail-is/hail/pull/7611#issuecomment-559186521:111,Availability,failure,failure,111,"self.assertEqual is unittest style. We use pytest now, which rewrites Python assert statements to provide good failure messages. New code should use `assert`, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521
https://github.com/hail-is/hail/pull/7611#issuecomment-559186521:119,Integrability,message,messages,119,"self.assertEqual is unittest style. We use pytest now, which rewrites Python assert statements to provide good failure messages. New code should use `assert`, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521
https://github.com/hail-is/hail/pull/7611#issuecomment-559186521:61,Modifiability,rewrite,rewrites,61,"self.assertEqual is unittest style. We use pytest now, which rewrites Python assert statements to provide good failure messages. New code should use `assert`, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521
https://github.com/hail-is/hail/pull/7611#issuecomment-559186521:5,Testability,assert,assertEqual,5,"self.assertEqual is unittest style. We use pytest now, which rewrites Python assert statements to provide good failure messages. New code should use `assert`, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521
https://github.com/hail-is/hail/pull/7611#issuecomment-559186521:77,Testability,assert,assert,77,"self.assertEqual is unittest style. We use pytest now, which rewrites Python assert statements to provide good failure messages. New code should use `assert`, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521
https://github.com/hail-is/hail/pull/7611#issuecomment-559186521:150,Testability,assert,assert,150,"self.assertEqual is unittest style. We use pytest now, which rewrites Python assert statements to provide good failure messages. New code should use `assert`, I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-559186521
https://github.com/hail-is/hail/pull/7611#issuecomment-560214050:21,Testability,test,tests,21,"Somehow block matrix tests seem to be failing on this PR, both for the doctests and for the regular python tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-560214050
https://github.com/hail-is/hail/pull/7611#issuecomment-560214050:107,Testability,test,tests,107,"Somehow block matrix tests seem to be failing on this PR, both for the doctests and for the regular python tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7611#issuecomment-560214050
https://github.com/hail-is/hail/pull/7614#issuecomment-558327378:72,Integrability,interface,interface,72,"Awesome! I will take a closer look, but here's my quick reaction to the interface. Was I was hoping to write was:. ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```. This is basically modeled after letrec in lisp/scheme/ml which would look something like:. ```; (letrec (f i x); (if (< i 10); (f (+ i 1) (+ x i)); x); (f 0 0)); ```. Another option is to get rid of `recur` and make the binding of the loop more explicit with something like:. ```; hl.loop(; lambda f, i, x:; hl.cond(i < 10, f(i + 1, x + i), x),; 0, 0); ```. where the first argument to the lambda is the loop itself. I think main problem with your proposals (except maybe 3) is that it assumes too much about the structure of the loop: namely, it embeds the exit condition into the structure of the loop. The loop may have several backwards calls or exit points (e.g. in a case or if tree) and there maybe shared and conditional work that happens inside that tree (and even nested loops!)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558327378
https://github.com/hail-is/hail/pull/7614#issuecomment-558330236:44,Integrability,interface,interface,44,"@cseed I think my point is that I *want* an interface that assumes the structure of the loop. I'm happy to implement other interfaces as well--the one that I've currently got in this PR is basically your second suggestion--but I think it would also be nice to have a while loop construct that looks syntactically more similar to what you'd expect a while loop to look like in python, if that makes sense?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558330236
https://github.com/hail-is/hail/pull/7614#issuecomment-558330236:123,Integrability,interface,interfaces,123,"@cseed I think my point is that I *want* an interface that assumes the structure of the loop. I'm happy to implement other interfaces as well--the one that I've currently got in this PR is basically your second suggestion--but I think it would also be nice to have a while loop construct that looks syntactically more similar to what you'd expect a while loop to look like in python, if that makes sense?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558330236
https://github.com/hail-is/hail/pull/7614#issuecomment-558331064:43,Integrability,interface,interfaces,43,"I do agree that I think all of my proposed interfaces are missing the ability to do things like nest loops, which is part of the reason I'm not sold on any of them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558331064
https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:100,Energy Efficiency,power,powerful,100,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125
https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:45,Integrability,interface,interface,45,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125
https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:118,Integrability,interface,interfaces,118,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125
https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:532,Integrability,wrap,wrapping,532,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125
https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:1676,Integrability,interface,interface,1676,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125
https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:1212,Modifiability,variab,variables,1212,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125
https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:1276,Modifiability,variab,variables,1276,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125
https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:1874,Performance,perform,performance,1874,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125
https://github.com/hail-is/hail/pull/7614#issuecomment-558699125:573,Security,threat,threat,573,"So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I don't have a strong feeling if you want to also add a Python-inspired while loop (although I personally would find the similarities misleading given the required differences, I understand others might feel differently). Your while loop should be naturally implementable in terms of mine, so I also suggest we focus on that first. Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Is Patrick's proposal for extra types written up anywhere? I don't like the idea of complicating the type hierarchy for internal bookkeeping like this. So I'm going to remark that in the code generator it is often natural to build data structures to aid the organization of the code generator, and those data structures need not need to be types/IRs. Given that Recur has to be in tail position, and you know exactly when you're existing the loop (branches that don't contain recur nodes). So the compilation looks like:. ```; set initial loop variables; # fall through into loop; Lloop:; ...; # recur; loop variables = new values; goto Lloop; Lan_exit_branch:; result = compile(branch); goto Lafter; Lanother_exit_branch:; result = compile(other_branch); goto Lafter; Lafter:; use result ...; ```. What I would do is ""peel"" off the ifs and lets (anything else?) that can sit in tail position and build a separate data structure for those nodes which I then traverse to emit the above code. Using the stream interface seems wrong to me also. What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. I'll comment more once I've looked over the code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-558699125
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:102,Energy Efficiency,power,powerful,102,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:47,Integrability,interface,interface,47,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:120,Integrability,interface,interfaces,120,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:180,Integrability,interface,interface,180,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:410,Integrability,wrap,wrapping,410,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:2259,Integrability,wrap,wrap,2259,"ecur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:2874,Integrability,interface,interface,2874,"h the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. As a side note, @iitalics stream emitter can handle streams of multiple values fine. Effectively, you can make a `Stream[(Code[A], Code[B], Code[C])]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:3032,Integrability,interface,interface,3032,"h the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. As a side note, @iitalics stream emitter can handle streams of multiple values fine. Effectively, you can make a `Stream[(Code[A], Code[B], Code[C])]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:2605,Modifiability,variab,variable,2605," add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. As a side note, @iitalics stream emitter can handle streams of multiple values",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:3443,Performance,perform,performance,3443,"h the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. As a side note, @iitalics stream emitter can handle streams of multiple values fine. Effectively, you can make a `Stream[(Code[A], Code[B], Code[C])]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:2244,Safety,safe,safe,2244,"ecur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:224,Security,expose,expose,224,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:451,Security,threat,threat,451,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:3141,Testability,log,logic,3141,"h the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop turns into? Since loops carry multiple values (by design), memory allocating these to create a tuple stream is going to be a performance non-starter. As a side note, @iitalics stream emitter can handle streams of multiple values fine. Effectively, you can make a `Stream[(Code[A], Code[B], Code[C])]`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:1265,Usability,simpl,simpler,1265,"ld implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:1989,Usability,simpl,simple,1989,"s explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:2293,Usability,simpl,simple,2293,"ambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an if statement either makes a value or it jumps away, there's no confusing mix. One nice property of this setup is that if an expression has a non-bottom type, then it is guaranteed not to jump away from itself (it may jump internally), so it is safe to method-wrap. This also make codegen very simple, and @iitalics has already implemented it! See `JoinPoint` and `JoinPoint.CallCC`. In the IR, I don't think this requires much change. If we're already adding a continuation context (as mentioned above), then `TailLoop` just needs to bind both a recur and a break continuation, where recur takes the loop variable types, and break takes the loop result type. Then `Recur` can be replaced by a `Jump` node which calls (jumps to) a continuation in context. There's also a middle ground where we make break continuations explicit in the IR, but we want to keep the scheme-like interface in python. Then the pass @cseed described for inferring where the loop exits are would just go in python instead of the emitter. > Using the stream interface seems wrong to me also. When I mentioned this, I was thinking we could reuse the region management logic from the stream emitter for loops, but I've since changed my mind. I think loops will have hard region management no matter what. > What's the type of the stream the loop t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407
https://github.com/hail-is/hail/pull/7614#issuecomment-559099522:261,Integrability,interface,interface,261,"This proposal mounts to programming with explicit continuations. It doesn't increase the expressiveness of the loop construct that I can see. Our users are reluctant enough to learn functional programming, I think continuations is one step too far for the user interface. Internally, I don't care as much, although personally I would prefer to code up my solution. @catoverdrive's doing the work, so I'll let them decide. > As a side note, @iitalics stream emitter. Ah, I thought @catoverdrive was referring to IR level streams.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559099522
https://github.com/hail-is/hail/pull/7614#issuecomment-559099522:176,Usability,learn,learn,176,"This proposal mounts to programming with explicit continuations. It doesn't increase the expressiveness of the loop construct that I can see. Our users are reluctant enough to learn functional programming, I think continuations is one step too far for the user interface. Internally, I don't care as much, although personally I would prefer to code up my solution. @catoverdrive's doing the work, so I'll let them decide. > As a side note, @iitalics stream emitter. Ah, I thought @catoverdrive was referring to IR level streams.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559099522
https://github.com/hail-is/hail/pull/7614#issuecomment-560028289:193,Testability,test,tests,193,"ok, great. @cseed I've essentially already implemented the thing that you've described (although I need to actually duplicate the IR nodes in python for the loop function to work, and add some tests), although I haven't peeled off the ifs and lets to emit separately (and I don't know if we need to? seems to work fine with the current code generator); we check that all recurs are in tail position when we typecheck the IR. I will clean up the python and give the loops names and then assign this to someone.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-560028289
https://github.com/hail-is/hail/issues/7623#issuecomment-559183070:70,Security,secur,security,70,"I think the answer is that we can't treat the separate namespace as a security boundary. i.e. all internal traffic should be authenticated, encrypted, and authorized by the receiver",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7623#issuecomment-559183070
https://github.com/hail-is/hail/issues/7623#issuecomment-559183070:125,Security,authenticat,authenticated,125,"I think the answer is that we can't treat the separate namespace as a security boundary. i.e. all internal traffic should be authenticated, encrypted, and authorized by the receiver",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7623#issuecomment-559183070
https://github.com/hail-is/hail/issues/7623#issuecomment-559183070:140,Security,encrypt,encrypted,140,"I think the answer is that we can't treat the separate namespace as a security boundary. i.e. all internal traffic should be authenticated, encrypted, and authorized by the receiver",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7623#issuecomment-559183070
https://github.com/hail-is/hail/issues/7623#issuecomment-559183070:155,Security,authoriz,authorized,155,"I think the answer is that we can't treat the separate namespace as a security boundary. i.e. all internal traffic should be authenticated, encrypted, and authorized by the receiver",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7623#issuecomment-559183070
https://github.com/hail-is/hail/pull/7626#issuecomment-559211542:99,Integrability,interface,interface,99,"At the least, I didn't want the Dockerfile to COPY a huge tgz in. This seems like a slightly nicer interface than something that just decompresses a whole build step input.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-559211542
https://github.com/hail-is/hail/pull/7626#issuecomment-560436226:58,Availability,down,download,58,"> My experiments show that clone+merge is ~20 seconds but download from GCS is ~3s. Where are you running clone+merge here? We time the clone and pull in the build:. > + git clone https://github.com/hail-is/hail.git .; > real	0m12.030s. > + git fetch -q akotlar/hail; > real	0m2.878s. We don't time the merge, so that should be fast. I spot-checked the base_image step from half a dozen builds and the spread was ~11-18s. How was the 3s measured?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560436226
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:319,Availability,down,download,319,"I conducted tests on my laptop and on the cluster. I made these comments at https://github.com/hail-is/hail/pull/7534 and on [Zulip](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Dev/topic/ci/near/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:460,Availability,down,download,460,"I conducted tests on my laptop and on the cluster. I made these comments at https://github.com/hail-is/hail/pull/7534 and on [Zulip](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Dev/topic/ci/near/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:589,Availability,down,download,589,"I conducted tests on my laptop and on the cluster. I made these comments at https://github.com/hail-is/hail/pull/7534 and on [Zulip](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Dev/topic/ci/near/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:699,Availability,down,download,699,"I conducted tests on my laptop and on the cluster. I made these comments at https://github.com/hail-is/hail/pull/7534 and on [Zulip](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Dev/topic/ci/near/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:1202,Availability,down,down,1202,"ar/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:1442,Availability,down,downstream,1442,"in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also clones the repo, so for my repo I wait at least 2 minutes before I even have a chance to get feedback; with this PR and #7534 I should wait like 45 seconds?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:1212,Integrability,rout,route,1212,"ar/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also c",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:1731,Performance,latency,latency,1731,"in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also clones the repo, so for my repo I wait at least 2 minutes before I even have a chance to get feedback; with this PR and #7534 I should wait like 45 seconds?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:1747,Performance,throughput,throughput,1747,"in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also clones the repo, so for my repo I wait at least 2 minutes before I even have a chance to get feedback; with this PR and #7534 I should wait like 45 seconds?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:1782,Performance,bottleneck,bottleneck,1782,"in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also clones the repo, so for my repo I wait at least 2 minutes before I even have a chance to get feedback; with this PR and #7534 I should wait like 45 seconds?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:12,Testability,test,tests,12,"I conducted tests on my laptop and on the cluster. I made these comments at https://github.com/hail-is/hail/pull/7534 and on [Zulip](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Dev/topic/ci/near/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:501,Testability,test,test,501,"I conducted tests on my laptop and on the cluster. I made these comments at https://github.com/hail-is/hail/pull/7534 and on [Zulip](https://hail.zulipchat.com/#narrow/stream/123011-Hail-Dev/topic/ci/near/180856479). From Zulip:; > By comparison, on my wired laptop (which should be strictly slower than in GCP), I can download and extract a tar -cvzf archive in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:2086,Testability,test,tests,2086,"in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also clones the repo, so for my repo I wait at least 2 minutes before I even have a chance to get feedback; with this PR and #7534 I should wait like 45 seconds?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:2151,Testability,test,test,2151,"in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also clones the repo, so for my repo I wait at least 2 minutes before I even have a chance to get feedback; with this PR and #7534 I should wait like 45 seconds?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560442927:2295,Usability,feedback,feedback,2295,"in 7.2 seconds; > ...; > The 20 seconds is: clone from github.com, git-merge; > The 7.2 seconds is: download from GCS, untar; > Just ran the test in the cloud using the google cloud sdk image started by k run, 3.7 seconds; > The download is super fast, like a second; > the untar is about the same in both contexts, 1.2 seconds; > But the download drops from 4.7 to ~1.5. Chris pointed out I should skip going to disk and pipe into tar, I have not timed that yet. I was seeing fetch being more like 8 minutes to my repository. My repository is significantly larger than Alex's. I could delete some old branches to address this. ---. > for inputs/outputs, I wonder if we should have a flag that indicates it is an archive and do the archive/extract automatically (like you've done here but more generally), and stop using cp -r. I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. > for downstream steps that only need a small part of the repo, is it better to copy out different pieces (archived or no) rather than copy the whole thing and extra the parts you need?. I haven't investigated this. I agree, there exists an inflection point where the size of data overcomes GCS latency and GCS-throughput / tar-decompress is the bottleneck. There's something to be said for tar'ing everything except for `.git`, but I didn't carefully check which steps need it and which steps do not. ---. In conclusion, I'd say this PR is necessary for #7534, and #7534 is a big quality of life improvement for those of us with large repos running tests on images that are deep on the critical path (the shuffler test is behind 3 images and build hail, which also clones the repo, so for my repo I wait at least 2 minutes before I even have a chance to get feedback; with this PR and #7534 I should wait like 45 seconds?).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560442927
https://github.com/hail-is/hail/pull/7626#issuecomment-560446660:240,Availability,down,download,240,"FYI, git clone can clone a single branch: https://stackoverflow.com/questions/1778088/how-do-i-clone-a-single-branch-in-git. > k run. Might be worth benchmarking in batch2, since that is where it will run. (k run! Shame on you!). > But the download drops from 4.7 to ~1.5. I don't understand this. Drops compared to what? Where'd the 4.7 come from?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560446660
https://github.com/hail-is/hail/pull/7626#issuecomment-560446660:149,Testability,benchmark,benchmarking,149,"FYI, git clone can clone a single branch: https://stackoverflow.com/questions/1778088/how-do-i-clone-a-single-branch-in-git. > k run. Might be worth benchmarking in batch2, since that is where it will run. (k run! Shame on you!). > But the download drops from 4.7 to ~1.5. I don't understand this. Drops compared to what? Where'd the 4.7 come from?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560446660
https://github.com/hail-is/hail/pull/7626#issuecomment-560450115:7,Availability,down,download,7,"4.7 to download on laptop, 1.5 to download on k8s. About 1.2 seconds to untar in either setting. That yields the ~7s on my laptop and 3s in k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560450115
https://github.com/hail-is/hail/pull/7626#issuecomment-560450115:34,Availability,down,download,34,"4.7 to download on laptop, 1.5 to download on k8s. About 1.2 seconds to untar in either setting. That yields the ~7s on my laptop and 3s in k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560450115
https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:104,Availability,down,download,104,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090
https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:166,Availability,down,download,166,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090
https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:389,Availability,down,downstream,389,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090
https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:78,Deployability,pipeline,pipeline,78,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090
https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:148,Deployability,pipeline,pipeline,148,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090
https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:185,Deployability,pipeline,pipeline,185,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090
https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:469,Security,hash,hash,469,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090
https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:87,Testability,test,test,87,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090
https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:157,Testability,test,test,157,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090
https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:194,Testability,test,test,194,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090
https://github.com/hail-is/hail/pull/7626#issuecomment-560459474:16,Availability,down,down,16,"> I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560459474
https://github.com/hail-is/hail/pull/7626#issuecomment-560459474:26,Integrability,rout,route,26,"> I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560459474
https://github.com/hail-is/hail/pull/7626#issuecomment-560459474:334,Performance,perform,performant,334,"> I almost went down this route. It would save a couple lines of tar/untar in runImage steps. I felt the savings wasn't worth the effort of implementing it. In the buildImage case (what this PR addressed), I think it's worth it to keep images small. The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560459474
https://github.com/hail-is/hail/pull/7626#issuecomment-560470857:679,Availability,down,download,679,"@cseed,. > The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future. I agree directory outputs with large file counts are untenable. Are you suggesting we do this at the batch level for any directory? That feels a bit opaque, because users may expect that if they specify their output is a directory that they can selectively download certain subdirectories. At the build.yaml level, I see a couple explicit options:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; codec: gzip; ```; and; ```; inputs:; - from: /resources.tgz; to: /io/resources; codec: gzip; ```; ?. There's also this, which irks me a bit because it's punny, but:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; ```; and. ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - .; ```; ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - foo; - bar; ```. A little bit my reaction to this is that we're creating a DSL that provides minor value relative to the user using `tar` in their `runImage` steps. E.g. build hail is explicit and not verbose:; ```; ...; tar czf test.tar.gz -C python test; tar czf resources.tar.gz -C src/test resources; tar czf data.tar.gz -C python/hail/docs data; tar czf www-src.tar.gz www; tar czf cluster-tests.tar.gz python/cluster-tests; ```; and `test_hail_java` is fine:; ```; ...; tar xzf resources.tar.gz -C src/test; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857
https://github.com/hail-is/hail/pull/7626#issuecomment-560470857:95,Performance,perform,performant,95,"@cseed,. > The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future. I agree directory outputs with large file counts are untenable. Are you suggesting we do this at the batch level for any directory? That feels a bit opaque, because users may expect that if they specify their output is a directory that they can selectively download certain subdirectories. At the build.yaml level, I see a couple explicit options:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; codec: gzip; ```; and; ```; inputs:; - from: /resources.tgz; to: /io/resources; codec: gzip; ```; ?. There's also this, which irks me a bit because it's punny, but:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; ```; and. ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - .; ```; ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - foo; - bar; ```. A little bit my reaction to this is that we're creating a DSL that provides minor value relative to the user using `tar` in their `runImage` steps. E.g. build hail is explicit and not verbose:; ```; ...; tar czf test.tar.gz -C python test; tar czf resources.tar.gz -C src/test resources; tar czf data.tar.gz -C python/hail/docs data; tar czf www-src.tar.gz www; tar czf cluster-tests.tar.gz python/cluster-tests; ```; and `test_hail_java` is fine:; ```; ...; tar xzf resources.tar.gz -C src/test; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857
https://github.com/hail-is/hail/pull/7626#issuecomment-560470857:1458,Testability,test,test,1458,"@cseed,. > The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future. I agree directory outputs with large file counts are untenable. Are you suggesting we do this at the batch level for any directory? That feels a bit opaque, because users may expect that if they specify their output is a directory that they can selectively download certain subdirectories. At the build.yaml level, I see a couple explicit options:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; codec: gzip; ```; and; ```; inputs:; - from: /resources.tgz; to: /io/resources; codec: gzip; ```; ?. There's also this, which irks me a bit because it's punny, but:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; ```; and. ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - .; ```; ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - foo; - bar; ```. A little bit my reaction to this is that we're creating a DSL that provides minor value relative to the user using `tar` in their `runImage` steps. E.g. build hail is explicit and not verbose:; ```; ...; tar czf test.tar.gz -C python test; tar czf resources.tar.gz -C src/test resources; tar czf data.tar.gz -C python/hail/docs data; tar czf www-src.tar.gz www; tar czf cluster-tests.tar.gz python/cluster-tests; ```; and `test_hail_java` is fine:; ```; ...; tar xzf resources.tar.gz -C src/test; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857
https://github.com/hail-is/hail/pull/7626#issuecomment-560470857:1480,Testability,test,test,1480,"@cseed,. > The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future. I agree directory outputs with large file counts are untenable. Are you suggesting we do this at the batch level for any directory? That feels a bit opaque, because users may expect that if they specify their output is a directory that they can selectively download certain subdirectories. At the build.yaml level, I see a couple explicit options:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; codec: gzip; ```; and; ```; inputs:; - from: /resources.tgz; to: /io/resources; codec: gzip; ```; ?. There's also this, which irks me a bit because it's punny, but:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; ```; and. ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - .; ```; ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - foo; - bar; ```. A little bit my reaction to this is that we're creating a DSL that provides minor value relative to the user using `tar` in their `runImage` steps. E.g. build hail is explicit and not verbose:; ```; ...; tar czf test.tar.gz -C python test; tar czf resources.tar.gz -C src/test resources; tar czf data.tar.gz -C python/hail/docs data; tar czf www-src.tar.gz www; tar czf cluster-tests.tar.gz python/cluster-tests; ```; and `test_hail_java` is fine:; ```; ...; tar xzf resources.tar.gz -C src/test; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857
https://github.com/hail-is/hail/pull/7626#issuecomment-560470857:1518,Testability,test,test,1518,"@cseed,. > The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future. I agree directory outputs with large file counts are untenable. Are you suggesting we do this at the batch level for any directory? That feels a bit opaque, because users may expect that if they specify their output is a directory that they can selectively download certain subdirectories. At the build.yaml level, I see a couple explicit options:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; codec: gzip; ```; and; ```; inputs:; - from: /resources.tgz; to: /io/resources; codec: gzip; ```; ?. There's also this, which irks me a bit because it's punny, but:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; ```; and. ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - .; ```; ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - foo; - bar; ```. A little bit my reaction to this is that we're creating a DSL that provides minor value relative to the user using `tar` in their `runImage` steps. E.g. build hail is explicit and not verbose:; ```; ...; tar czf test.tar.gz -C python test; tar czf resources.tar.gz -C src/test resources; tar czf data.tar.gz -C python/hail/docs data; tar czf www-src.tar.gz www; tar czf cluster-tests.tar.gz python/cluster-tests; ```; and `test_hail_java` is fine:; ```; ...; tar xzf resources.tar.gz -C src/test; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857
https://github.com/hail-is/hail/pull/7626#issuecomment-560470857:1624,Testability,test,tests,1624,"@cseed,. > The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future. I agree directory outputs with large file counts are untenable. Are you suggesting we do this at the batch level for any directory? That feels a bit opaque, because users may expect that if they specify their output is a directory that they can selectively download certain subdirectories. At the build.yaml level, I see a couple explicit options:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; codec: gzip; ```; and; ```; inputs:; - from: /resources.tgz; to: /io/resources; codec: gzip; ```; ?. There's also this, which irks me a bit because it's punny, but:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; ```; and. ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - .; ```; ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - foo; - bar; ```. A little bit my reaction to this is that we're creating a DSL that provides minor value relative to the user using `tar` in their `runImage` steps. E.g. build hail is explicit and not verbose:; ```; ...; tar czf test.tar.gz -C python test; tar czf resources.tar.gz -C src/test resources; tar czf data.tar.gz -C python/hail/docs data; tar czf www-src.tar.gz www; tar czf cluster-tests.tar.gz python/cluster-tests; ```; and `test_hail_java` is fine:; ```; ...; tar xzf resources.tar.gz -C src/test; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857
https://github.com/hail-is/hail/pull/7626#issuecomment-560470857:1652,Testability,test,tests,1652,"@cseed,. > The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future. I agree directory outputs with large file counts are untenable. Are you suggesting we do this at the batch level for any directory? That feels a bit opaque, because users may expect that if they specify their output is a directory that they can selectively download certain subdirectories. At the build.yaml level, I see a couple explicit options:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; codec: gzip; ```; and; ```; inputs:; - from: /resources.tgz; to: /io/resources; codec: gzip; ```; ?. There's also this, which irks me a bit because it's punny, but:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; ```; and. ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - .; ```; ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - foo; - bar; ```. A little bit my reaction to this is that we're creating a DSL that provides minor value relative to the user using `tar` in their `runImage` steps. E.g. build hail is explicit and not verbose:; ```; ...; tar czf test.tar.gz -C python test; tar czf resources.tar.gz -C src/test resources; tar czf data.tar.gz -C python/hail/docs data; tar czf www-src.tar.gz www; tar czf cluster-tests.tar.gz python/cluster-tests; ```; and `test_hail_java` is fine:; ```; ...; tar xzf resources.tar.gz -C src/test; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857
https://github.com/hail-is/hail/pull/7626#issuecomment-560470857:1737,Testability,test,test,1737,"@cseed,. > The point isn't to save the few untars, the point is to make the general facilities performant for everyone. Basically, I think directory outputs are untenable in the current design unless they are very small, which is why you're doing all this for your use case, and the tar is still probably as good (if not better) in that case. Fix it for everyone's use case so nobody has to go through this in the future. I agree directory outputs with large file counts are untenable. Are you suggesting we do this at the batch level for any directory? That feels a bit opaque, because users may expect that if they specify their output is a directory that they can selectively download certain subdirectories. At the build.yaml level, I see a couple explicit options:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; codec: gzip; ```; and; ```; inputs:; - from: /resources.tgz; to: /io/resources; codec: gzip; ```; ?. There's also this, which irks me a bit because it's punny, but:; ```; outputs:; - from: /io/repo/hail/resources; to: /resources.tgz; ```; and. ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - .; ```; ```; inputs:; - from: /resources.tgz; to: /io/resources; extract:; - foo; - bar; ```. A little bit my reaction to this is that we're creating a DSL that provides minor value relative to the user using `tar` in their `runImage` steps. E.g. build hail is explicit and not verbose:; ```; ...; tar czf test.tar.gz -C python test; tar czf resources.tar.gz -C src/test resources; tar czf data.tar.gz -C python/hail/docs data; tar czf www-src.tar.gz www; tar czf cluster-tests.tar.gz python/cluster-tests; ```; and `test_hail_java` is fine:; ```; ...; tar xzf resources.tar.gz -C src/test; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560470857
https://github.com/hail-is/hail/pull/7626#issuecomment-566260967:178,Testability,test,tests,178,"@jigold Cotton is on vacation and I need to get this merged to unblock work on dbuf & shuffler. I believe I've addressed Cotton's concerns. You can see my modification to the CI tests as evidence of that. I'd like your review. Don't approve just yet though, I need to ensure any build.yaml rule for directories has the new `directory: recursive` annotation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-566260967
https://github.com/hail-is/hail/pull/7626#issuecomment-566697070:1098,Performance,cache,cacheing,1098,@cseed single branch has a very limited effect:; ```; # for i in $(seq 0 2) ; do rm -rf hail && time git clone https://github.com/danking/hail.git --branch ci-decompress --single-branch >/dev/null 2>/dev/null; done; git clone https://github.com/danking/hail.git --branch ci-decompress > 2> 3.61s user 1.63s system 53% cpu 9.846 total; git clone https://github.com/danking/hail.git --branch ci-decompress > 2> 3.50s user 1.57s system 54% cpu 9.333 total; git clone https://github.com/danking/hail.git --branch ci-decompress > 2> 3.65s user 1.66s system 54% cpu 9.817 total; # for i in $(seq 0 2) ; do rm -rf hail && time git clone https://github.com/danking/hail.git >/dev/null 2>/dev/null; done ; git clone https://github.com/danking/hail.git > /dev/null 2> /dev/null 4.26s user 1.88s system 52% cpu 11.637 total; git clone https://github.com/danking/hail.git > /dev/null 2> /dev/null 4.24s user 1.87s system 51% cpu 11.917 total; git clone https://github.com/danking/hail.git > /dev/null 2> /dev/null 4.05s user 1.77s system 54% cpu 10.661 total; ```; I also noticed that GitHub does some limited cacheing. The fresh pull of the full repo was 14s. As you can see subsequent pulls are only 11s.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-566697070
https://github.com/hail-is/hail/pull/7631#issuecomment-559284193:104,Availability,error,errors,104,"FYI It failed due to transient connection reset by peer, I added ECONNRESET to the list of transient OS errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7631#issuecomment-559284193
https://github.com/hail-is/hail/pull/7636#issuecomment-560172025:17,Availability,toler,tolerations,17,I had to put the tolerations back to get it to pass. I'll remove them in a second pass after I remove the preemptible pool taint.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560172025
https://github.com/hail-is/hail/pull/7636#issuecomment-560586209:27,Availability,toler,tolerations,27,"Why prefer nodeSelector to tolerations/taints?. I thought we would have two taints: preemptible, non-preemptible. Pods must tolerate at least one (in practice, pods will tolerate no more than one). If a pod has no toleration, it is unscheduable. In this setting, Pods must set nodeSelector to exactly one of preemptible, non-preemptible. If they specify no nodeSelector, they'll be scheduled anywhere. It seems like the main difference is if we forget to specify the kind of pod this is. I feel like we should prefer the unscheduable case, rather than silently working. Are there other motivations for changing to nodeSelector? If someone accidentally tolerates two taints, that will jump out more in code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560586209
https://github.com/hail-is/hail/pull/7636#issuecomment-560586209:124,Availability,toler,tolerate,124,"Why prefer nodeSelector to tolerations/taints?. I thought we would have two taints: preemptible, non-preemptible. Pods must tolerate at least one (in practice, pods will tolerate no more than one). If a pod has no toleration, it is unscheduable. In this setting, Pods must set nodeSelector to exactly one of preemptible, non-preemptible. If they specify no nodeSelector, they'll be scheduled anywhere. It seems like the main difference is if we forget to specify the kind of pod this is. I feel like we should prefer the unscheduable case, rather than silently working. Are there other motivations for changing to nodeSelector? If someone accidentally tolerates two taints, that will jump out more in code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560586209
https://github.com/hail-is/hail/pull/7636#issuecomment-560586209:170,Availability,toler,tolerate,170,"Why prefer nodeSelector to tolerations/taints?. I thought we would have two taints: preemptible, non-preemptible. Pods must tolerate at least one (in practice, pods will tolerate no more than one). If a pod has no toleration, it is unscheduable. In this setting, Pods must set nodeSelector to exactly one of preemptible, non-preemptible. If they specify no nodeSelector, they'll be scheduled anywhere. It seems like the main difference is if we forget to specify the kind of pod this is. I feel like we should prefer the unscheduable case, rather than silently working. Are there other motivations for changing to nodeSelector? If someone accidentally tolerates two taints, that will jump out more in code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560586209
https://github.com/hail-is/hail/pull/7636#issuecomment-560586209:214,Availability,toler,toleration,214,"Why prefer nodeSelector to tolerations/taints?. I thought we would have two taints: preemptible, non-preemptible. Pods must tolerate at least one (in practice, pods will tolerate no more than one). If a pod has no toleration, it is unscheduable. In this setting, Pods must set nodeSelector to exactly one of preemptible, non-preemptible. If they specify no nodeSelector, they'll be scheduled anywhere. It seems like the main difference is if we forget to specify the kind of pod this is. I feel like we should prefer the unscheduable case, rather than silently working. Are there other motivations for changing to nodeSelector? If someone accidentally tolerates two taints, that will jump out more in code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560586209
https://github.com/hail-is/hail/pull/7636#issuecomment-560586209:652,Availability,toler,tolerates,652,"Why prefer nodeSelector to tolerations/taints?. I thought we would have two taints: preemptible, non-preemptible. Pods must tolerate at least one (in practice, pods will tolerate no more than one). If a pod has no toleration, it is unscheduable. In this setting, Pods must set nodeSelector to exactly one of preemptible, non-preemptible. If they specify no nodeSelector, they'll be scheduled anywhere. It seems like the main difference is if we forget to specify the kind of pod this is. I feel like we should prefer the unscheduable case, rather than silently working. Are there other motivations for changing to nodeSelector? If someone accidentally tolerates two taints, that will jump out more in code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560586209
https://github.com/hail-is/hail/pull/7636#issuecomment-560586209:382,Energy Efficiency,schedul,scheduled,382,"Why prefer nodeSelector to tolerations/taints?. I thought we would have two taints: preemptible, non-preemptible. Pods must tolerate at least one (in practice, pods will tolerate no more than one). If a pod has no toleration, it is unscheduable. In this setting, Pods must set nodeSelector to exactly one of preemptible, non-preemptible. If they specify no nodeSelector, they'll be scheduled anywhere. It seems like the main difference is if we forget to specify the kind of pod this is. I feel like we should prefer the unscheduable case, rather than silently working. Are there other motivations for changing to nodeSelector? If someone accidentally tolerates two taints, that will jump out more in code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560586209
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:274,Availability,toler,toleration,274,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:428,Availability,toler,toleration,428,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:607,Availability,down,downsizing,607,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:1001,Availability,downtime,downtime,1001,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:78,Deployability,configurat,configuration,78,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:404,Deployability,configurat,configuration,404,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:569,Energy Efficiency,schedul,scheduler,569,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:935,Energy Efficiency,monitor,monitoring,935,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:1063,Energy Efficiency,monitor,monitoring,1063,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:78,Modifiability,config,configuration,78,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:404,Modifiability,config,configuration,404,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:206,Usability,simpl,simplest,206,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:1139,Usability,simpl,simplify,1139,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683
https://github.com/hail-is/hail/pull/7636#issuecomment-562079366:74,Availability,toler,tolerations,74,"OK, I overcame my irrational fear of taits, and changed everything to use tolerations. If I can't get things to work like this, I will try node selectors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-562079366
https://github.com/hail-is/hail/pull/7636#issuecomment-565690039:232,Availability,toler,tolerations,232,"> I think we should change the taints on the node pools before we merge this PR. The GCP UI has changed and I don't see where we can change the taints. I think that's backwards. This PR shouldn't change the scheduling (it just adds tolerations to non-existent taints), so it should go in first, then we should add the taint and let the preemptible workload on non-preemptible nodes get rescheduled to preemptible nodes. However, I think what we should do is create a new tainted non-preemptible pool, merge this PR, and then delete the old pool. Yeah, looks like you can't edit labels or taints from the UI. Maybe you can from the command line? ; Anyway, with the above strategy which seems upgrade safe, it doesn't matter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039
https://github.com/hail-is/hail/pull/7636#issuecomment-565690039:691,Deployability,upgrade,upgrade,691,"> I think we should change the taints on the node pools before we merge this PR. The GCP UI has changed and I don't see where we can change the taints. I think that's backwards. This PR shouldn't change the scheduling (it just adds tolerations to non-existent taints), so it should go in first, then we should add the taint and let the preemptible workload on non-preemptible nodes get rescheduled to preemptible nodes. However, I think what we should do is create a new tainted non-preemptible pool, merge this PR, and then delete the old pool. Yeah, looks like you can't edit labels or taints from the UI. Maybe you can from the command line? ; Anyway, with the above strategy which seems upgrade safe, it doesn't matter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039
https://github.com/hail-is/hail/pull/7636#issuecomment-565690039:207,Energy Efficiency,schedul,scheduling,207,"> I think we should change the taints on the node pools before we merge this PR. The GCP UI has changed and I don't see where we can change the taints. I think that's backwards. This PR shouldn't change the scheduling (it just adds tolerations to non-existent taints), so it should go in first, then we should add the taint and let the preemptible workload on non-preemptible nodes get rescheduled to preemptible nodes. However, I think what we should do is create a new tainted non-preemptible pool, merge this PR, and then delete the old pool. Yeah, looks like you can't edit labels or taints from the UI. Maybe you can from the command line? ; Anyway, with the above strategy which seems upgrade safe, it doesn't matter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039
https://github.com/hail-is/hail/pull/7636#issuecomment-565690039:699,Safety,safe,safe,699,"> I think we should change the taints on the node pools before we merge this PR. The GCP UI has changed and I don't see where we can change the taints. I think that's backwards. This PR shouldn't change the scheduling (it just adds tolerations to non-existent taints), so it should go in first, then we should add the taint and let the preemptible workload on non-preemptible nodes get rescheduled to preemptible nodes. However, I think what we should do is create a new tainted non-preemptible pool, merge this PR, and then delete the old pool. Yeah, looks like you can't edit labels or taints from the UI. Maybe you can from the command line? ; Anyway, with the above strategy which seems upgrade safe, it doesn't matter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039
https://github.com/hail-is/hail/issues/7638#issuecomment-560197965:15,Availability,error,error,15,Stack overflow error inside of NormalizeNames.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7638#issuecomment-560197965
https://github.com/hail-is/hail/issues/7638#issuecomment-561430746:161,Availability,down,down,161,presumably related to: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/mysterious.20behavior/near/182390817 (log posted next message down),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7638#issuecomment-561430746
https://github.com/hail-is/hail/issues/7638#issuecomment-561430746:153,Integrability,message,message,153,presumably related to: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/mysterious.20behavior/near/182390817 (log posted next message down),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7638#issuecomment-561430746
https://github.com/hail-is/hail/issues/7638#issuecomment-561430746:137,Testability,log,log,137,presumably related to: https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/mysterious.20behavior/near/182390817 (log posted next message down),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7638#issuecomment-561430746
https://github.com/hail-is/hail/pull/7639#issuecomment-561962333:69,Integrability,interface,interface,69,after discussion with @tpoterba pausing on this until the PContainer interface is cleaned up. Aiming to finish this up next week.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7639#issuecomment-561962333
https://github.com/hail-is/hail/pull/7639#issuecomment-568135072:442,Testability,test,tests,442,"This is at a place where people could look. Currently plan is to implement storeShallow on every PType separately (could keep all in PType for convenience initially). Majority of functionality: https://github.com/hail-is/hail/pull/7639/files#diff-2cba834adc6803ff8b274f8634bb46c0R394; ; # TODO:. - [x] Implement deep copy; - [ ] Implement for things that are not PCanonicalArray or PArrayBackedContainer; - [ ] Non-staged version; - [ ] More tests. cc @patrick-schultz, @catoverdrive, @danking for feedback",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7639#issuecomment-568135072
https://github.com/hail-is/hail/pull/7639#issuecomment-568135072:498,Usability,feedback,feedback,498,"This is at a place where people could look. Currently plan is to implement storeShallow on every PType separately (could keep all in PType for convenience initially). Majority of functionality: https://github.com/hail-is/hail/pull/7639/files#diff-2cba834adc6803ff8b274f8634bb46c0R394; ; # TODO:. - [x] Implement deep copy; - [ ] Implement for things that are not PCanonicalArray or PArrayBackedContainer; - [ ] Non-staged version; - [ ] More tests. cc @patrick-schultz, @catoverdrive, @danking for feedback",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7639#issuecomment-568135072
https://github.com/hail-is/hail/issues/7644#issuecomment-560944851:19,Availability,down,down,19,it's a bit further down but definitely there,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644#issuecomment-560944851
https://github.com/hail-is/hail/issues/7644#issuecomment-561201747:82,Availability,down,down,82,Funny I actually did this a few weeks ago and then realized it was logged further down. It's around line 76 in a random log I just looked at. I could move it to log from python early on?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644#issuecomment-561201747
https://github.com/hail-is/hail/issues/7644#issuecomment-561201747:67,Testability,log,logged,67,Funny I actually did this a few weeks ago and then realized it was logged further down. It's around line 76 in a random log I just looked at. I could move it to log from python early on?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644#issuecomment-561201747
https://github.com/hail-is/hail/issues/7644#issuecomment-561201747:120,Testability,log,log,120,Funny I actually did this a few weeks ago and then realized it was logged further down. It's around line 76 in a random log I just looked at. I could move it to log from python early on?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644#issuecomment-561201747
https://github.com/hail-is/hail/issues/7644#issuecomment-561201747:161,Testability,log,log,161,Funny I actually did this a few weeks ago and then realized it was logged further down. It's around line 76 in a random log I just looked at. I could move it to log from python early on?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644#issuecomment-561201747
https://github.com/hail-is/hail/issues/7644#issuecomment-561262685:48,Testability,log,log,48,"ah, yeah, it was below the fold when I opened a log. now I know what to search for to find it. thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7644#issuecomment-561262685
https://github.com/hail-is/hail/pull/7646#issuecomment-561448054:44,Performance,optimiz,optimized,44,"Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments. Cotton also had the suggestion of writing this function unstaged using two utility functions:; ```scala; def findFirstNonZeroByte(addr: Long, n: Long): Long; def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448054
https://github.com/hail-is/hail/pull/7646#issuecomment-561448054:103,Usability,simpl,simplified,103,"Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments. Cotton also had the suggestion of writing this function unstaged using two utility functions:; ```scala; def findFirstNonZeroByte(addr: Long, n: Long): Long; def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448054
https://github.com/hail-is/hail/pull/7646#issuecomment-561448218:6,Modifiability,rewrite,rewrite,6,"Let's rewrite to use this style. I think that will be both simple and performant, to make us all happy!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448218
https://github.com/hail-is/hail/pull/7646#issuecomment-561448218:70,Performance,perform,performant,70,"Let's rewrite to use this style. I think that will be both simple and performant, to make us all happy!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448218
https://github.com/hail-is/hail/pull/7646#issuecomment-561448218:59,Usability,simpl,simple,59,"Let's rewrite to use this style. I think that will be both simple and performant, to make us all happy!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561448218
https://github.com/hail-is/hail/pull/7646#issuecomment-561475978:853,Energy Efficiency,power,power,853,"> Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments.; > ; > Cotton also had the suggestion of writing this function unstaged using two utility functions:; > ; > ```scala; > def findFirstNonZeroByte(addr: Long, n: Long): Long; > def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; > ```. How do we convert from address: Code[Long] to Long without going through emit? I suppose you mean creating an unstated version, and then calling it through invokeScalaObject?. I'm happy to write this. edit: In this version, if we don't want to assume alignment, what is an easier way other than checking the alignment of the addr? I think we're assuming int alignment in much of our codebase, but this is also not guaranteed (since we only aim to guarantee power of 2, as discussed last week).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561475978
https://github.com/hail-is/hail/pull/7646#issuecomment-561475978:46,Performance,optimiz,optimized,46,"> Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments.; > ; > Cotton also had the suggestion of writing this function unstaged using two utility functions:; > ; > ```scala; > def findFirstNonZeroByte(addr: Long, n: Long): Long; > def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; > ```. How do we convert from address: Code[Long] to Long without going through emit? I suppose you mean creating an unstated version, and then calling it through invokeScalaObject?. I'm happy to write this. edit: In this version, if we don't want to assume alignment, what is an easier way other than checking the alignment of the addr? I think we're assuming int alignment in much of our codebase, but this is also not guaranteed (since we only aim to guarantee power of 2, as discussed last week).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561475978
https://github.com/hail-is/hail/pull/7646#issuecomment-561475978:105,Usability,simpl,simplified,105,"> Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments.; > ; > Cotton also had the suggestion of writing this function unstaged using two utility functions:; > ; > ```scala; > def findFirstNonZeroByte(addr: Long, n: Long): Long; > def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; > ```. How do we convert from address: Code[Long] to Long without going through emit? I suppose you mean creating an unstated version, and then calling it through invokeScalaObject?. I'm happy to write this. edit: In this version, if we don't want to assume alignment, what is an easier way other than checking the alignment of the addr? I think we're assuming int alignment in much of our codebase, but this is also not guaranteed (since we only aim to guarantee power of 2, as discussed last week).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561475978
https://github.com/hail-is/hail/pull/7646#issuecomment-561492434:138,Testability,test,tests,138,"I now see an ""anyMissing"" method, which was merged yesterday, written by @patrick-schultz. It looks similar to your proposal, although my tests do not pass with it (using `sourceType.anyMissing(mb, sourceOffset).cne(const(false)).orEmpty(Code._fatal(msg))`). As an aside, I do not find it easier to read, although it is terser. There is the question of whether we want embed 4's in place of a class attribute (or instance attribute) representing that value. If that is made to work I'm ok with replacing the ensure function, although I would still prefer that align the address to 8 byte (reading bytes until that point) and check 8 byte sections for as long as possible",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561492434
https://github.com/hail-is/hail/pull/7646#issuecomment-561510056:182,Modifiability,config,config,182,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056
https://github.com/hail-is/hail/pull/7646#issuecomment-561510056:536,Performance,perform,performance,536,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056
https://github.com/hail-is/hail/pull/7646#issuecomment-561510056:49,Testability,test,test,49,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056
https://github.com/hail-is/hail/pull/7646#issuecomment-561510056:108,Testability,test,test,108,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056
https://github.com/hail-is/hail/pull/7646#issuecomment-561510056:303,Testability,benchmark,benchmarks,303,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056
https://github.com/hail-is/hail/pull/7646#issuecomment-561510056:4,Usability,simpl,simpler,4,"The simpler version is slower, but in the python test, not by a large amount (previous version was, in this test 23.8s or so, although Scala benches may show a larger difference). {""config"": {""cores"": 1, ""version"": ""0.2.28-7888aeb97570"", ""timestamp"": ""2019-12-04 02:07:13.182303"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [28.613776744999996, 28.361242108, 28.481231283]}]}. So 20% slower. I would prefer to use longs, because it doesn't feel right to me to leave performance on the table, however I'm ok with this tradeoff if you find it aligns with your goals better. ; - Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. edit: I propose to put the unstaged version for a later time, but can do now as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561510056
https://github.com/hail-is/hail/pull/7646#issuecomment-561605053:469,Testability,log,logic,469,"> Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. since the array is 4-byte-aligned, you can't read 4 bytes and switch to longs -- it's possible that the missing bits begin on an address that would produce aligned long reads. You'll need to compute whether you need to read an integer or not. This logic is somewhat complex, and I find the staged version harder to reason about (it took 5 minutes or so to understand the control flow). I would prefer the unstaged version, so that we can reason about correctness more easily, test the function more easily, and maintain it better in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561605053
https://github.com/hail-is/hail/pull/7646#issuecomment-561605053:697,Testability,test,test,697,"> Regarding longs, to deal with alignment: right now we assume we're int aligned. To read longs, could we read the first 4 bytes as an int, then switch to longs, then do bits for the remaining length. Should be as terse. since the array is 4-byte-aligned, you can't read 4 bytes and switch to longs -- it's possible that the missing bits begin on an address that would produce aligned long reads. You'll need to compute whether you need to read an integer or not. This logic is somewhat complex, and I find the staged version harder to reason about (it took 5 minutes or so to understand the control flow). I would prefer the unstaged version, so that we can reason about correctness more easily, test the function more easily, and maintain it better in the future.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561605053
https://github.com/hail-is/hail/pull/7646#issuecomment-561694360:445,Testability,log,logic,445,"> since the array is 4-byte-aligned, you can't read 4 bytes and switch to longs -- it's possible that the missing bits begin on an address that would produce aligned long reads. You'll need to compute whether you need to read an integer or not. Right, my proposal would be to first read an int (provided the length has at least 32 bits left), at which point I think I'm 8-byte aligned (address xxx4, to address xxx8). Then the rest of the loops logic is effectively the same as you had, except reading longs instead of bytes in the middle loop. So this version would have 2 loops and 3 length checks, as opposed to 3 loops and 3 length checks, and be 2x as fast in the limit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561694360
https://github.com/hail-is/hail/pull/7646#issuecomment-561757839:645,Energy Efficiency,allocate,allocate,645,"@tpoterba Done. A few differences from your suggestion: ; 1) No real need to check at the byte level, because modulus will be at most 3 for non-4-divisible lengths (at the byte level...and we only need to check (nBits - (m1*32)) / 8 bytes anyway), and so we would test a max of 3 bytes, and usually less than that in practice.; 2) Cotton had suggested the unstated function live on Memory...but since Memory appears to only call into unsafe, Region calls its own functions, and this code relied on Region, I put the unstaged function in Region rather than Memory. . The tests live in PContainerTest. I can move them to Region, but as we need to allocate some memory, and the easiest way to do that is through ScalaToRegionValue, which requires a ptype, and we're interested in the missing bytes at the moment, the easiest translation would have the Region test calling into PContainer, so it didn't seem to matter much. Let me know if you see it differently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561757839
https://github.com/hail-is/hail/pull/7646#issuecomment-561757839:434,Safety,unsafe,unsafe,434,"@tpoterba Done. A few differences from your suggestion: ; 1) No real need to check at the byte level, because modulus will be at most 3 for non-4-divisible lengths (at the byte level...and we only need to check (nBits - (m1*32)) / 8 bytes anyway), and so we would test a max of 3 bytes, and usually less than that in practice.; 2) Cotton had suggested the unstated function live on Memory...but since Memory appears to only call into unsafe, Region calls its own functions, and this code relied on Region, I put the unstaged function in Region rather than Memory. . The tests live in PContainerTest. I can move them to Region, but as we need to allocate some memory, and the easiest way to do that is through ScalaToRegionValue, which requires a ptype, and we're interested in the missing bytes at the moment, the easiest translation would have the Region test calling into PContainer, so it didn't seem to matter much. Let me know if you see it differently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561757839
https://github.com/hail-is/hail/pull/7646#issuecomment-561757839:264,Testability,test,test,264,"@tpoterba Done. A few differences from your suggestion: ; 1) No real need to check at the byte level, because modulus will be at most 3 for non-4-divisible lengths (at the byte level...and we only need to check (nBits - (m1*32)) / 8 bytes anyway), and so we would test a max of 3 bytes, and usually less than that in practice.; 2) Cotton had suggested the unstated function live on Memory...but since Memory appears to only call into unsafe, Region calls its own functions, and this code relied on Region, I put the unstaged function in Region rather than Memory. . The tests live in PContainerTest. I can move them to Region, but as we need to allocate some memory, and the easiest way to do that is through ScalaToRegionValue, which requires a ptype, and we're interested in the missing bytes at the moment, the easiest translation would have the Region test calling into PContainer, so it didn't seem to matter much. Let me know if you see it differently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561757839
https://github.com/hail-is/hail/pull/7646#issuecomment-561757839:570,Testability,test,tests,570,"@tpoterba Done. A few differences from your suggestion: ; 1) No real need to check at the byte level, because modulus will be at most 3 for non-4-divisible lengths (at the byte level...and we only need to check (nBits - (m1*32)) / 8 bytes anyway), and so we would test a max of 3 bytes, and usually less than that in practice.; 2) Cotton had suggested the unstated function live on Memory...but since Memory appears to only call into unsafe, Region calls its own functions, and this code relied on Region, I put the unstaged function in Region rather than Memory. . The tests live in PContainerTest. I can move them to Region, but as we need to allocate some memory, and the easiest way to do that is through ScalaToRegionValue, which requires a ptype, and we're interested in the missing bytes at the moment, the easiest translation would have the Region test calling into PContainer, so it didn't seem to matter much. Let me know if you see it differently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561757839
https://github.com/hail-is/hail/pull/7646#issuecomment-561757839:856,Testability,test,test,856,"@tpoterba Done. A few differences from your suggestion: ; 1) No real need to check at the byte level, because modulus will be at most 3 for non-4-divisible lengths (at the byte level...and we only need to check (nBits - (m1*32)) / 8 bytes anyway), and so we would test a max of 3 bytes, and usually less than that in practice.; 2) Cotton had suggested the unstated function live on Memory...but since Memory appears to only call into unsafe, Region calls its own functions, and this code relied on Region, I put the unstaged function in Region rather than Memory. . The tests live in PContainerTest. I can move them to Region, but as we need to allocate some memory, and the easiest way to do that is through ScalaToRegionValue, which requires a ptype, and we're interested in the missing bytes at the moment, the easiest translation would have the Region test calling into PContainer, so it didn't seem to matter much. Let me know if you see it differently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561757839
https://github.com/hail-is/hail/pull/7646#issuecomment-561908098:117,Modifiability,config,config,117,"Performance is close, if slightly worse. Could be laptop load differences. Insignificant, this is a great balance. {""config"": {""cores"": 1, ""version"": ""0.2.28-42f5ab7d9617"", ""timestamp"": ""2019-12-04 19:20:11.757847"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [25.609369775000005, 25.694102771999994, 26.285334770000006]}]}",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561908098
https://github.com/hail-is/hail/pull/7646#issuecomment-561908098:0,Performance,Perform,Performance,0,"Performance is close, if slightly worse. Could be laptop load differences. Insignificant, this is a great balance. {""config"": {""cores"": 1, ""version"": ""0.2.28-42f5ab7d9617"", ""timestamp"": ""2019-12-04 19:20:11.757847"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [25.609369775000005, 25.694102771999994, 26.285334770000006]}]}",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561908098
https://github.com/hail-is/hail/pull/7646#issuecomment-561908098:57,Performance,load,load,57,"Performance is close, if slightly worse. Could be laptop load differences. Insignificant, this is a great balance. {""config"": {""cores"": 1, ""version"": ""0.2.28-42f5ab7d9617"", ""timestamp"": ""2019-12-04 19:20:11.757847"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [25.609369775000005, 25.694102771999994, 26.285334770000006]}]}",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561908098
https://github.com/hail-is/hail/pull/7646#issuecomment-561908098:238,Testability,benchmark,benchmarks,238,"Performance is close, if slightly worse. Could be laptop load differences. Insignificant, this is a great balance. {""config"": {""cores"": 1, ""version"": ""0.2.28-42f5ab7d9617"", ""timestamp"": ""2019-12-04 19:20:11.757847"", ""system"": ""darwin""}, ""benchmarks"": [{""name"": ""make_ndarray_bench"", ""failed"": false, ""timed_out"": false, ""times"": [25.609369775000005, 25.694102771999994, 26.285334770000006]}]}",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561908098
https://github.com/hail-is/hail/pull/7653#issuecomment-564041054:108,Availability,redundant,redundant,108,"I’ll look for closely once I get to the retreat, but first impression is that centering and normalizing are redundant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7653#issuecomment-564041054
https://github.com/hail-is/hail/pull/7653#issuecomment-564041054:108,Safety,redund,redundant,108,"I’ll look for closely once I get to the retreat, but first impression is that centering and normalizing are redundant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7653#issuecomment-564041054
https://github.com/hail-is/hail/pull/7653#issuecomment-564274326:394,Availability,redundant,redundant,394,"Maybe no longer relevant, but zeroing missings *after* centering is; equivalent to using non-missing terms only rather than mean imputing,; provided you then use N_nonmissing for the final normalization. On Tue, Dec 10, 2019 at 8:50 AM Jon Bloom <notifications@github.com> wrote:. > I’ll look for closely once I get to the retreat, but first impression is; > that centering and normalizing are redundant.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/7653?email_source=notifications&email_token=ACC577VJUORGGYMDZUE72IDQX6NDNA5CNFSM4JVAFXT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGPJKXQ#issuecomment-564041054>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC577UWWGRAMQHAOV7TGADQX6NDNANCNFSM4JVAFXTQ>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7653#issuecomment-564274326
https://github.com/hail-is/hail/pull/7653#issuecomment-564274326:394,Safety,redund,redundant,394,"Maybe no longer relevant, but zeroing missings *after* centering is; equivalent to using non-missing terms only rather than mean imputing,; provided you then use N_nonmissing for the final normalization. On Tue, Dec 10, 2019 at 8:50 AM Jon Bloom <notifications@github.com> wrote:. > I’ll look for closely once I get to the retreat, but first impression is; > that centering and normalizing are redundant.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/7653?email_source=notifications&email_token=ACC577VJUORGGYMDZUE72IDQX6NDNA5CNFSM4JVAFXT2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEGPJKXQ#issuecomment-564041054>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ACC577UWWGRAMQHAOV7TGADQX6NDNANCNFSM4JVAFXTQ>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7653#issuecomment-564274326
https://github.com/hail-is/hail/pull/7654#issuecomment-562222241:32,Testability,log,logic,32,minor nitpick---can we move the logic from TableMapIRNew back into TableMapRows.execute and delete TableMapIRNew? I'd prefer to keep the locations of the execute logic consistent.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7654#issuecomment-562222241
https://github.com/hail-is/hail/pull/7654#issuecomment-562222241:162,Testability,log,logic,162,minor nitpick---can we move the logic from TableMapIRNew back into TableMapRows.execute and delete TableMapIRNew? I'd prefer to keep the locations of the execute logic consistent.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7654#issuecomment-562222241
https://github.com/hail-is/hail/pull/7663#issuecomment-562152990:37,Testability,test,test,37,"yes, good idea. I added a hard-coded test of both the arrays and n_discordant.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7663#issuecomment-562152990
https://github.com/hail-is/hail/pull/7674#issuecomment-562891346:220,Deployability,patch,patches,220,"I think this is ready. Adds a createDatabase2 step that adds database migrations. What's a database migration? The idea is that a database starts as an empty database and is built up or modified over time by a series of patches or migrations. The database has a version which is the number of migrations applied (starting from 1). This is stored in the table `{database_name}_migration_version`. Each migration involves running a `.sql` or `.py` script. The logic that applies migrations computes a checksum of these scripts and stores them in the database in table `{database_name}_migrations`. When applying migrations again in the future, these checksums are verified. A create database step now looks like (from the CI tests):. ```; - kind: createDatabase2; name: hello2_database; databaseName: hello2; migrations:; - name: create-tables; script: /io/sql/create-hello2-tables.sql; - name: insert; script: /io/sql/insert.py; inputs:; - from: /repo/ci/test/resources/sql; to: /io/; namespace:; valueFrom: default_ns.name; dependsOn:; - default_ns; - copy_files; ```. migrations is a the list of migrations that need to be applied to get the current version. So the idea is, if you want to change the schema of the database, you just add another migration at the end to make the changes you want. After this goes in, I'll make a separate PR to switch everything to this new createDatabase2 step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346
https://github.com/hail-is/hail/pull/7674#issuecomment-562891346:1024,Integrability,depend,dependsOn,1024,"I think this is ready. Adds a createDatabase2 step that adds database migrations. What's a database migration? The idea is that a database starts as an empty database and is built up or modified over time by a series of patches or migrations. The database has a version which is the number of migrations applied (starting from 1). This is stored in the table `{database_name}_migration_version`. Each migration involves running a `.sql` or `.py` script. The logic that applies migrations computes a checksum of these scripts and stores them in the database in table `{database_name}_migrations`. When applying migrations again in the future, these checksums are verified. A create database step now looks like (from the CI tests):. ```; - kind: createDatabase2; name: hello2_database; databaseName: hello2; migrations:; - name: create-tables; script: /io/sql/create-hello2-tables.sql; - name: insert; script: /io/sql/insert.py; inputs:; - from: /repo/ci/test/resources/sql; to: /io/; namespace:; valueFrom: default_ns.name; dependsOn:; - default_ns; - copy_files; ```. migrations is a the list of migrations that need to be applied to get the current version. So the idea is, if you want to change the schema of the database, you just add another migration at the end to make the changes you want. After this goes in, I'll make a separate PR to switch everything to this new createDatabase2 step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346
https://github.com/hail-is/hail/pull/7674#issuecomment-562891346:499,Security,checksum,checksum,499,"I think this is ready. Adds a createDatabase2 step that adds database migrations. What's a database migration? The idea is that a database starts as an empty database and is built up or modified over time by a series of patches or migrations. The database has a version which is the number of migrations applied (starting from 1). This is stored in the table `{database_name}_migration_version`. Each migration involves running a `.sql` or `.py` script. The logic that applies migrations computes a checksum of these scripts and stores them in the database in table `{database_name}_migrations`. When applying migrations again in the future, these checksums are verified. A create database step now looks like (from the CI tests):. ```; - kind: createDatabase2; name: hello2_database; databaseName: hello2; migrations:; - name: create-tables; script: /io/sql/create-hello2-tables.sql; - name: insert; script: /io/sql/insert.py; inputs:; - from: /repo/ci/test/resources/sql; to: /io/; namespace:; valueFrom: default_ns.name; dependsOn:; - default_ns; - copy_files; ```. migrations is a the list of migrations that need to be applied to get the current version. So the idea is, if you want to change the schema of the database, you just add another migration at the end to make the changes you want. After this goes in, I'll make a separate PR to switch everything to this new createDatabase2 step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346
https://github.com/hail-is/hail/pull/7674#issuecomment-562891346:648,Security,checksum,checksums,648,"I think this is ready. Adds a createDatabase2 step that adds database migrations. What's a database migration? The idea is that a database starts as an empty database and is built up or modified over time by a series of patches or migrations. The database has a version which is the number of migrations applied (starting from 1). This is stored in the table `{database_name}_migration_version`. Each migration involves running a `.sql` or `.py` script. The logic that applies migrations computes a checksum of these scripts and stores them in the database in table `{database_name}_migrations`. When applying migrations again in the future, these checksums are verified. A create database step now looks like (from the CI tests):. ```; - kind: createDatabase2; name: hello2_database; databaseName: hello2; migrations:; - name: create-tables; script: /io/sql/create-hello2-tables.sql; - name: insert; script: /io/sql/insert.py; inputs:; - from: /repo/ci/test/resources/sql; to: /io/; namespace:; valueFrom: default_ns.name; dependsOn:; - default_ns; - copy_files; ```. migrations is a the list of migrations that need to be applied to get the current version. So the idea is, if you want to change the schema of the database, you just add another migration at the end to make the changes you want. After this goes in, I'll make a separate PR to switch everything to this new createDatabase2 step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346
https://github.com/hail-is/hail/pull/7674#issuecomment-562891346:458,Testability,log,logic,458,"I think this is ready. Adds a createDatabase2 step that adds database migrations. What's a database migration? The idea is that a database starts as an empty database and is built up or modified over time by a series of patches or migrations. The database has a version which is the number of migrations applied (starting from 1). This is stored in the table `{database_name}_migration_version`. Each migration involves running a `.sql` or `.py` script. The logic that applies migrations computes a checksum of these scripts and stores them in the database in table `{database_name}_migrations`. When applying migrations again in the future, these checksums are verified. A create database step now looks like (from the CI tests):. ```; - kind: createDatabase2; name: hello2_database; databaseName: hello2; migrations:; - name: create-tables; script: /io/sql/create-hello2-tables.sql; - name: insert; script: /io/sql/insert.py; inputs:; - from: /repo/ci/test/resources/sql; to: /io/; namespace:; valueFrom: default_ns.name; dependsOn:; - default_ns; - copy_files; ```. migrations is a the list of migrations that need to be applied to get the current version. So the idea is, if you want to change the schema of the database, you just add another migration at the end to make the changes you want. After this goes in, I'll make a separate PR to switch everything to this new createDatabase2 step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346
https://github.com/hail-is/hail/pull/7674#issuecomment-562891346:723,Testability,test,tests,723,"I think this is ready. Adds a createDatabase2 step that adds database migrations. What's a database migration? The idea is that a database starts as an empty database and is built up or modified over time by a series of patches or migrations. The database has a version which is the number of migrations applied (starting from 1). This is stored in the table `{database_name}_migration_version`. Each migration involves running a `.sql` or `.py` script. The logic that applies migrations computes a checksum of these scripts and stores them in the database in table `{database_name}_migrations`. When applying migrations again in the future, these checksums are verified. A create database step now looks like (from the CI tests):. ```; - kind: createDatabase2; name: hello2_database; databaseName: hello2; migrations:; - name: create-tables; script: /io/sql/create-hello2-tables.sql; - name: insert; script: /io/sql/insert.py; inputs:; - from: /repo/ci/test/resources/sql; to: /io/; namespace:; valueFrom: default_ns.name; dependsOn:; - default_ns; - copy_files; ```. migrations is a the list of migrations that need to be applied to get the current version. So the idea is, if you want to change the schema of the database, you just add another migration at the end to make the changes you want. After this goes in, I'll make a separate PR to switch everything to this new createDatabase2 step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346
https://github.com/hail-is/hail/pull/7674#issuecomment-562891346:954,Testability,test,test,954,"I think this is ready. Adds a createDatabase2 step that adds database migrations. What's a database migration? The idea is that a database starts as an empty database and is built up or modified over time by a series of patches or migrations. The database has a version which is the number of migrations applied (starting from 1). This is stored in the table `{database_name}_migration_version`. Each migration involves running a `.sql` or `.py` script. The logic that applies migrations computes a checksum of these scripts and stores them in the database in table `{database_name}_migrations`. When applying migrations again in the future, these checksums are verified. A create database step now looks like (from the CI tests):. ```; - kind: createDatabase2; name: hello2_database; databaseName: hello2; migrations:; - name: create-tables; script: /io/sql/create-hello2-tables.sql; - name: insert; script: /io/sql/insert.py; inputs:; - from: /repo/ci/test/resources/sql; to: /io/; namespace:; valueFrom: default_ns.name; dependsOn:; - default_ns; - copy_files; ```. migrations is a the list of migrations that need to be applied to get the current version. So the idea is, if you want to change the schema of the database, you just add another migration at the end to make the changes you want. After this goes in, I'll make a separate PR to switch everything to this new createDatabase2 step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346
https://github.com/hail-is/hail/pull/7682#issuecomment-562812256:117,Testability,test,test,117,"Yeah, I imagine we do. `InferPType` isn't used right now, correct? I'll add the new case, just checking if there's a test that would have caught this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7682#issuecomment-562812256
https://github.com/hail-is/hail/pull/7682#issuecomment-562812319:307,Modifiability,enhance,enhancement,307,"> Yeah, I imagine we do. `InferPType` isn't used right now, correct? I'll add the new case, just checking if there's a test that would have caught this. Yeah, I edited my comment. Totally not used right now, which is why you didn't notice. I think making an issue is also fine, like you said it would be an enhancement, not something necessary for what you wrote to work. Looks really cool btw.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7682#issuecomment-562812319
https://github.com/hail-is/hail/pull/7682#issuecomment-562812319:119,Testability,test,test,119,"> Yeah, I imagine we do. `InferPType` isn't used right now, correct? I'll add the new case, just checking if there's a test that would have caught this. Yeah, I edited my comment. Totally not used right now, which is why you didn't notice. I think making an issue is also fine, like you said it would be an enhancement, not something necessary for what you wrote to work. Looks really cool btw.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7682#issuecomment-562812319
https://github.com/hail-is/hail/pull/7682#issuecomment-566206151:37,Testability,Assert,AssertionError,37,"Alright, replaced it with `throw new AssertionError(...)` because Scala can't infer that `assert(false)` always fails.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7682#issuecomment-566206151
https://github.com/hail-is/hail/pull/7682#issuecomment-566206151:90,Testability,assert,assert,90,"Alright, replaced it with `throw new AssertionError(...)` because Scala can't infer that `assert(false)` always fails.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7682#issuecomment-566206151
https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:529,Deployability,deploy,deploy,529,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906
https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:325,Integrability,depend,depends,325,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906
https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:268,Modifiability,config,config,268,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906
https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:918,Modifiability,config,config,918,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906
https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:972,Security,password,password,972,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906
https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:181,Testability,test,test,181,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906
https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:227,Testability,test,test,227,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906
https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:450,Testability,test,test,450,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906
https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:654,Testability,test,tests,654,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906
https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:131,Energy Efficiency,power,power,131,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944
https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:170,Modifiability,config,config,170,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944
https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:365,Modifiability,config,config,365,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944
https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:586,Modifiability,config,config,586,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944
https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:312,Testability,test,tests,312,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944
https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:420,Testability,test,tests,420,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944
https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:534,Testability,test,test,534,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944
https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:751,Testability,test,test,751,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944
https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:940,Testability,test,test,940,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944
https://github.com/hail-is/hail/pull/7683#issuecomment-562890518:309,Testability,test,test,309,"> I'm confused even more. I thought test_instance was a database within db-gh0um the same as all of the other databases within db-gh0um. Is this a separate cloud sql instance?. Correct. No. test_instance is a database that conceptually an instance, in that it is used to satisfy ""create database"" requests in test CI, but just gives out test_instance as the ""created"" database without any isolation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562890518
https://github.com/hail-is/hail/pull/7687#issuecomment-563878450:27,Availability,failure,failure,27,"Can't quite figure out the failure in LDPruneSuite, will follow up tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7687#issuecomment-563878450
https://github.com/hail-is/hail/pull/7687#issuecomment-563899553:62,Testability,test,tests,62,"Ok solved that and a related issue. Should be good to review, tests pass.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7687#issuecomment-563899553
https://github.com/hail-is/hail/pull/7687#issuecomment-565554122:41,Testability,test,tests,41,"@tpoterba the changes you proposed cause tests to fail (OrderingSuite.testBinarySearchOnDict) . Also, I'm confused by the proposal, because arrayRep has no fundamental type, and while we could add one, the fundamental types in PSet and PDict were constructed differently in master (PSet's elementType is not necessarily a PStruct in master). edit: Ah, I think I misunderstood, you wanted `override val fundamentalType: PArray = arrayRep.fundamentalType` I think. I could place a lazy fundamentalType on arrayRep that used whatever elementType was defined, and completely remove fundamentalType from PSet and PDict",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7687#issuecomment-565554122
https://github.com/hail-is/hail/pull/7687#issuecomment-565554122:70,Testability,test,testBinarySearchOnDict,70,"@tpoterba the changes you proposed cause tests to fail (OrderingSuite.testBinarySearchOnDict) . Also, I'm confused by the proposal, because arrayRep has no fundamental type, and while we could add one, the fundamental types in PSet and PDict were constructed differently in master (PSet's elementType is not necessarily a PStruct in master). edit: Ah, I think I misunderstood, you wanted `override val fundamentalType: PArray = arrayRep.fundamentalType` I think. I could place a lazy fundamentalType on arrayRep that used whatever elementType was defined, and completely remove fundamentalType from PSet and PDict",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7687#issuecomment-565554122
https://github.com/hail-is/hail/pull/7687#issuecomment-565585194:43,Testability,test,tests,43,"> @tpoterba the changes you proposed cause tests to fail (OrderingSuite.testBinarySearchOnDict) . Also, I'm confused by the proposal, because arrayRep has no fundamental type, and while we could add one, the fundamental types in PSet and PDict were constructed differently in master (PSet's elementType is not necessarily a PStruct in master). > edit: Ah, I think I misunderstood, you wanted override val fundamentalType: PArray = arrayRep.fundamentalType I think. I could place a lazy fundamentalType on arrayRep that used whatever elementType was defined, and completely remove fundamentalType from PSet and PDict. Actually, you can set the fundamentalType on PCanonicalArrayBackedCollection. That seems best.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7687#issuecomment-565585194
https://github.com/hail-is/hail/pull/7687#issuecomment-565585194:72,Testability,test,testBinarySearchOnDict,72,"> @tpoterba the changes you proposed cause tests to fail (OrderingSuite.testBinarySearchOnDict) . Also, I'm confused by the proposal, because arrayRep has no fundamental type, and while we could add one, the fundamental types in PSet and PDict were constructed differently in master (PSet's elementType is not necessarily a PStruct in master). > edit: Ah, I think I misunderstood, you wanted override val fundamentalType: PArray = arrayRep.fundamentalType I think. I could place a lazy fundamentalType on arrayRep that used whatever elementType was defined, and completely remove fundamentalType from PSet and PDict. Actually, you can set the fundamentalType on PCanonicalArrayBackedCollection. That seems best.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7687#issuecomment-565585194
https://github.com/hail-is/hail/pull/7691#issuecomment-563439003:36,Testability,log,log,36,"I thought about changing this to a `log.info`, but I'm quite worried about what happens when the IR is large (possibly quadratic logging).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7691#issuecomment-563439003
https://github.com/hail-is/hail/pull/7691#issuecomment-563439003:129,Testability,log,logging,129,"I thought about changing this to a `log.info`, but I'm quite worried about what happens when the IR is large (possibly quadratic logging).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7691#issuecomment-563439003
https://github.com/hail-is/hail/pull/7693#issuecomment-563585896:10,Availability,error,error,10,Assertion error in RVB,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7693#issuecomment-563585896
https://github.com/hail-is/hail/pull/7693#issuecomment-563585896:0,Testability,Assert,Assertion,0,Assertion error in RVB,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7693#issuecomment-563585896
https://github.com/hail-is/hail/pull/7694#issuecomment-565600586:13,Testability,test,tests,13,"Some failing tests, related to unexpected `config_File` arg. ```; database.py:134:26: E1123: Unexpected keyword argument 'config_File' in function call (unexpected-keyword-arg)`; ```. ```; File ""/usr/local/lib/python3.6/dist-packages/gear/database.py"", line 134, in async_init; self.pool = await create_database_pool(config_File=config_file, autocommit=False, maxsize=maxsize); TypeError: create_database_pool() got an unexpected keyword argument 'config_File'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694#issuecomment-565600586
https://github.com/hail-is/hail/pull/7694#issuecomment-565620376:18,Testability,test,test,18,> bump. Failing a test,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7694#issuecomment-565620376
https://github.com/hail-is/hail/issues/7701#issuecomment-564222699:40,Testability,log,logic,40,"but yes, you're totally right about the logic on EBaseStruct needing to move into PTypes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7701#issuecomment-564222699
https://github.com/hail-is/hail/issues/7701#issuecomment-564225592:686,Energy Efficiency,allocate,allocate,686,"> buildSkip does not need a ptype. This method is used to skip encoded data, which is never getting decoded into a physical representation. So in EArray:. ```scala; def _buildSkip(mb: EmitMethodBuilder, r: Code[Region], in: Code[InputBuffer]): Code[Unit] = {; val len = mb.newLocal[Int](""len""); val i = mb.newLocal[Int](""i""); val skip = elementType.buildSkip(mb). if (elementType.required) {; Code(; len := in.readInt(),; i := 0,; Code.whileLoop(i < len,; Code(; skip(r, in),; i := i + const(1)))); } else {; val mbytes = mb.newLocal[Long](""mbytes""); val nMissing = mb.newLocal[Int](""nMissing""); Code(; len := in.readInt(),; nMissing := PCanonicalArray.nMissingBytes(len),; mbytes := r.allocate(const(1), nMissing.toL),; in.readBytes(r, mbytes, nMissing),; i := 0,; Code.whileLoop(i < len,; Region.loadBit(mbytes, i.toL).mux(; Code._empty,; skip(r, in)),; i := i + const(1))); }; }; ```. Do you want to just code (len + 7) >>> 3 in EArray (say `val nMissingBytes = (len+7) >>> 3` in the constructor). This would be a fast way to delete `PCanonicalArray.nMissingBytes`. The actual missing ness encoding scheme doesn't seem tied to PArrays (notable the allocation is using a different alignment altogether).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7701#issuecomment-564225592
https://github.com/hail-is/hail/issues/7701#issuecomment-564225592:798,Performance,load,loadBit,798,"> buildSkip does not need a ptype. This method is used to skip encoded data, which is never getting decoded into a physical representation. So in EArray:. ```scala; def _buildSkip(mb: EmitMethodBuilder, r: Code[Region], in: Code[InputBuffer]): Code[Unit] = {; val len = mb.newLocal[Int](""len""); val i = mb.newLocal[Int](""i""); val skip = elementType.buildSkip(mb). if (elementType.required) {; Code(; len := in.readInt(),; i := 0,; Code.whileLoop(i < len,; Code(; skip(r, in),; i := i + const(1)))); } else {; val mbytes = mb.newLocal[Long](""mbytes""); val nMissing = mb.newLocal[Int](""nMissing""); Code(; len := in.readInt(),; nMissing := PCanonicalArray.nMissingBytes(len),; mbytes := r.allocate(const(1), nMissing.toL),; in.readBytes(r, mbytes, nMissing),; i := 0,; Code.whileLoop(i < len,; Region.loadBit(mbytes, i.toL).mux(; Code._empty,; skip(r, in)),; i := i + const(1))); }; }; ```. Do you want to just code (len + 7) >>> 3 in EArray (say `val nMissingBytes = (len+7) >>> 3` in the constructor). This would be a fast way to delete `PCanonicalArray.nMissingBytes`. The actual missing ness encoding scheme doesn't seem tied to PArrays (notable the allocation is using a different alignment altogether).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7701#issuecomment-564225592
https://github.com/hail-is/hail/issues/7703#issuecomment-567074778:78,Performance,Load,LoadPlink,78,"edit:. Never mind about the proposal. I think copy is useful, for instance in LoadPlink:. ```scala; val (sampleInfo, signature) = LoadPlink.parseFam(fam, ffConfig, hc.sFS). val nameMap = Map(""id"" -> ""s""); val saSignature = signature.copy(fields = signature.fields.map(f => f.copy(name = nameMap.getOrElse(f.name, f.name)))); ```. Here if I return a non-canonical PStruct from LoadPlink, I may want to have a single method that would allow me to construct a new instance, using the same class as that used to construct signature.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7703#issuecomment-567074778
https://github.com/hail-is/hail/issues/7703#issuecomment-567074778:130,Performance,Load,LoadPlink,130,"edit:. Never mind about the proposal. I think copy is useful, for instance in LoadPlink:. ```scala; val (sampleInfo, signature) = LoadPlink.parseFam(fam, ffConfig, hc.sFS). val nameMap = Map(""id"" -> ""s""); val saSignature = signature.copy(fields = signature.fields.map(f => f.copy(name = nameMap.getOrElse(f.name, f.name)))); ```. Here if I return a non-canonical PStruct from LoadPlink, I may want to have a single method that would allow me to construct a new instance, using the same class as that used to construct signature.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7703#issuecomment-567074778
https://github.com/hail-is/hail/issues/7703#issuecomment-567074778:376,Performance,Load,LoadPlink,376,"edit:. Never mind about the proposal. I think copy is useful, for instance in LoadPlink:. ```scala; val (sampleInfo, signature) = LoadPlink.parseFam(fam, ffConfig, hc.sFS). val nameMap = Map(""id"" -> ""s""); val saSignature = signature.copy(fields = signature.fields.map(f => f.copy(name = nameMap.getOrElse(f.name, f.name)))); ```. Here if I return a non-canonical PStruct from LoadPlink, I may want to have a single method that would allow me to construct a new instance, using the same class as that used to construct signature.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7703#issuecomment-567074778
https://github.com/hail-is/hail/pull/7705#issuecomment-564753932:223,Integrability,depend,dependency,223,"On, one thing, I removed delete_batch_database. I don't quite know how to write it with the current setup. I think we should add a new kind of step which is dev-only and only run if you ask for it explicitly (not just as a dependency).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7705#issuecomment-564753932
https://github.com/hail-is/hail/pull/7705#issuecomment-565009061:367,Deployability,deploy,deploy,367,"I'm not sure what the best/easiest thing to do is. I think we can accomplish the same thing by copying and pasting the delete tables step (runImage) into a dev branch while testing. . The other easiest thing I can think of is to add a `run_if_requested=True` option to each build step config and modify ci to skip over steps that aren't specifically requested in dev deploy. I don't think a new step is a good idea because what if I want an optional runImage step or an optional Deploy step, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7705#issuecomment-565009061
https://github.com/hail-is/hail/pull/7705#issuecomment-565009061:479,Deployability,Deploy,Deploy,479,"I'm not sure what the best/easiest thing to do is. I think we can accomplish the same thing by copying and pasting the delete tables step (runImage) into a dev branch while testing. . The other easiest thing I can think of is to add a `run_if_requested=True` option to each build step config and modify ci to skip over steps that aren't specifically requested in dev deploy. I don't think a new step is a good idea because what if I want an optional runImage step or an optional Deploy step, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7705#issuecomment-565009061
https://github.com/hail-is/hail/pull/7705#issuecomment-565009061:285,Modifiability,config,config,285,"I'm not sure what the best/easiest thing to do is. I think we can accomplish the same thing by copying and pasting the delete tables step (runImage) into a dev branch while testing. . The other easiest thing I can think of is to add a `run_if_requested=True` option to each build step config and modify ci to skip over steps that aren't specifically requested in dev deploy. I don't think a new step is a good idea because what if I want an optional runImage step or an optional Deploy step, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7705#issuecomment-565009061
https://github.com/hail-is/hail/pull/7705#issuecomment-565009061:173,Testability,test,testing,173,"I'm not sure what the best/easiest thing to do is. I think we can accomplish the same thing by copying and pasting the delete tables step (runImage) into a dev branch while testing. . The other easiest thing I can think of is to add a `run_if_requested=True` option to each build step config and modify ci to skip over steps that aren't specifically requested in dev deploy. I don't think a new step is a good idea because what if I want an optional runImage step or an optional Deploy step, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7705#issuecomment-565009061
https://github.com/hail-is/hail/pull/7712#issuecomment-566671854:18,Modifiability,refactor,refactor,18,"> This is a small refactor, right?. Yes, no problem",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7712#issuecomment-566671854
https://github.com/hail-is/hail/pull/7712#issuecomment-566681403:300,Integrability,interface,interfaces,300,"> I realize this is a flip-flop and probably a bit frustrating, but I think leaving the ComplexPTypes on the abstract PLocus, PCall, PInterval seems preferable to needing to define a `representation` on those classes. We can move the ComplexPType inheritance to the concrete implementations of those interfaces when we get rid of the usages of `representation` outside the ptypes package.; > ; > This is a small refactor, right?. The issue is that without declaring the representation type on those classes we will need more casts (for instance line 17 of BinarySearch now needs one), and this is inherently the same thing as assuming PCanonical* rather than the abstract class (so we should not cast, either use the abstract class and define representation's type, or match on the specific implementation).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7712#issuecomment-566681403
https://github.com/hail-is/hail/pull/7712#issuecomment-566681403:247,Modifiability,inherit,inheritance,247,"> I realize this is a flip-flop and probably a bit frustrating, but I think leaving the ComplexPTypes on the abstract PLocus, PCall, PInterval seems preferable to needing to define a `representation` on those classes. We can move the ComplexPType inheritance to the concrete implementations of those interfaces when we get rid of the usages of `representation` outside the ptypes package.; > ; > This is a small refactor, right?. The issue is that without declaring the representation type on those classes we will need more casts (for instance line 17 of BinarySearch now needs one), and this is inherently the same thing as assuming PCanonical* rather than the abstract class (so we should not cast, either use the abstract class and define representation's type, or match on the specific implementation).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7712#issuecomment-566681403
https://github.com/hail-is/hail/pull/7712#issuecomment-566681403:412,Modifiability,refactor,refactor,412,"> I realize this is a flip-flop and probably a bit frustrating, but I think leaving the ComplexPTypes on the abstract PLocus, PCall, PInterval seems preferable to needing to define a `representation` on those classes. We can move the ComplexPType inheritance to the concrete implementations of those interfaces when we get rid of the usages of `representation` outside the ptypes package.; > ; > This is a small refactor, right?. The issue is that without declaring the representation type on those classes we will need more casts (for instance line 17 of BinarySearch now needs one), and this is inherently the same thing as assuming PCanonical* rather than the abstract class (so we should not cast, either use the abstract class and define representation's type, or match on the specific implementation).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7712#issuecomment-566681403
https://github.com/hail-is/hail/pull/7713#issuecomment-565371834:49,Testability,log,logic,49,Rebased and now use the new database transaction logic.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7713#issuecomment-565371834
https://github.com/hail-is/hail/pull/7714#issuecomment-565101526:140,Deployability,update,update,140,"Got it. In spite of my claim ""I was trying to avoid bulk operations"" I see that close_batch scans over all ready jobs (a bulk operation) to update ready_cores. I'm not quite sure what to do here. I'm not actually sure if a long-running query on close_batch is going to cause problems and it is not trivial to make it incremental.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7714#issuecomment-565101526
https://github.com/hail-is/hail/pull/7714#issuecomment-565101526:46,Safety,avoid,avoid,46,"Got it. In spite of my claim ""I was trying to avoid bulk operations"" I see that close_batch scans over all ready jobs (a bulk operation) to update ready_cores. I'm not quite sure what to do here. I'm not actually sure if a long-running query on close_batch is going to cause problems and it is not trivial to make it incremental.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7714#issuecomment-565101526
https://github.com/hail-is/hail/pull/7722#issuecomment-568536375:123,Deployability,deploy,deploy,123,"I thought self.steps was still used. For example in `build`:. ```; def build(self, batch, code, scope):; assert scope in ('deploy', 'test', 'dev'). for step in self.steps:; if step.scopes is None or scope in step.scopes:; step.build(batch, code, scope). if scope == 'dev':; return. step_to_parent_steps = defaultdict(set); for step in self.steps:; for dep in step.all_deps():; step_to_parent_steps[dep].add(step). for step in self.steps:; parent_jobs = flatten([parent_step.wrapped_job() for parent_step in step_to_parent_steps[step]]). log.info(f""Cleanup {step.name} after running {[parent_step.name for parent_step in step_to_parent_steps[step]]}""). if step.scopes is None or scope in step.scopes:; step.cleanup(batch, scope, parent_jobs); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7722#issuecomment-568536375
https://github.com/hail-is/hail/pull/7722#issuecomment-568536375:105,Testability,assert,assert,105,"I thought self.steps was still used. For example in `build`:. ```; def build(self, batch, code, scope):; assert scope in ('deploy', 'test', 'dev'). for step in self.steps:; if step.scopes is None or scope in step.scopes:; step.build(batch, code, scope). if scope == 'dev':; return. step_to_parent_steps = defaultdict(set); for step in self.steps:; for dep in step.all_deps():; step_to_parent_steps[dep].add(step). for step in self.steps:; parent_jobs = flatten([parent_step.wrapped_job() for parent_step in step_to_parent_steps[step]]). log.info(f""Cleanup {step.name} after running {[parent_step.name for parent_step in step_to_parent_steps[step]]}""). if step.scopes is None or scope in step.scopes:; step.cleanup(batch, scope, parent_jobs); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7722#issuecomment-568536375
https://github.com/hail-is/hail/pull/7722#issuecomment-568536375:133,Testability,test,test,133,"I thought self.steps was still used. For example in `build`:. ```; def build(self, batch, code, scope):; assert scope in ('deploy', 'test', 'dev'). for step in self.steps:; if step.scopes is None or scope in step.scopes:; step.build(batch, code, scope). if scope == 'dev':; return. step_to_parent_steps = defaultdict(set); for step in self.steps:; for dep in step.all_deps():; step_to_parent_steps[dep].add(step). for step in self.steps:; parent_jobs = flatten([parent_step.wrapped_job() for parent_step in step_to_parent_steps[step]]). log.info(f""Cleanup {step.name} after running {[parent_step.name for parent_step in step_to_parent_steps[step]]}""). if step.scopes is None or scope in step.scopes:; step.cleanup(batch, scope, parent_jobs); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7722#issuecomment-568536375
https://github.com/hail-is/hail/pull/7722#issuecomment-568536375:537,Testability,log,log,537,"I thought self.steps was still used. For example in `build`:. ```; def build(self, batch, code, scope):; assert scope in ('deploy', 'test', 'dev'). for step in self.steps:; if step.scopes is None or scope in step.scopes:; step.build(batch, code, scope). if scope == 'dev':; return. step_to_parent_steps = defaultdict(set); for step in self.steps:; for dep in step.all_deps():; step_to_parent_steps[dep].add(step). for step in self.steps:; parent_jobs = flatten([parent_step.wrapped_job() for parent_step in step_to_parent_steps[step]]). log.info(f""Cleanup {step.name} after running {[parent_step.name for parent_step in step_to_parent_steps[step]]}""). if step.scopes is None or scope in step.scopes:; step.cleanup(batch, scope, parent_jobs); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7722#issuecomment-568536375
https://github.com/hail-is/hail/pull/7728#issuecomment-565619961:177,Deployability,deploy,deployments,177,This will probably behave better with this: https://github.com/hail-is/hail/pull/7636. The four was roughly chosen to match the k8s maximum pool size so there is space for test deployments. One problem we're seeing now is preemptible workloads get scheduled on non-preemptible nodes meaning there isn't space for non-preemptible test workloads.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7728#issuecomment-565619961
https://github.com/hail-is/hail/pull/7728#issuecomment-565619961:248,Energy Efficiency,schedul,scheduled,248,This will probably behave better with this: https://github.com/hail-is/hail/pull/7636. The four was roughly chosen to match the k8s maximum pool size so there is space for test deployments. One problem we're seeing now is preemptible workloads get scheduled on non-preemptible nodes meaning there isn't space for non-preemptible test workloads.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7728#issuecomment-565619961
https://github.com/hail-is/hail/pull/7728#issuecomment-565619961:172,Testability,test,test,172,This will probably behave better with this: https://github.com/hail-is/hail/pull/7636. The four was roughly chosen to match the k8s maximum pool size so there is space for test deployments. One problem we're seeing now is preemptible workloads get scheduled on non-preemptible nodes meaning there isn't space for non-preemptible test workloads.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7728#issuecomment-565619961
https://github.com/hail-is/hail/pull/7728#issuecomment-565619961:329,Testability,test,test,329,This will probably behave better with this: https://github.com/hail-is/hail/pull/7636. The four was roughly chosen to match the k8s maximum pool size so there is space for test deployments. One problem we're seeing now is preemptible workloads get scheduled on non-preemptible nodes meaning there isn't space for non-preemptible test workloads.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7728#issuecomment-565619961
https://github.com/hail-is/hail/pull/7729#issuecomment-565757263:35,Availability,error,error,35,Some kind of batch service account error on this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7729#issuecomment-565757263
https://github.com/hail-is/hail/pull/7733#issuecomment-566223349:21,Availability,error,error,21,Can't figure out the error. Some segmentation fault. Can trigger on regressionTestUnifyBug. Looks to be triggered in Compile(MakeTuple.ordered),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7733#issuecomment-566223349
https://github.com/hail-is/hail/pull/7733#issuecomment-566223349:46,Availability,fault,fault,46,Can't figure out the error. Some segmentation fault. Can trigger on regressionTestUnifyBug. Looks to be triggered in Compile(MakeTuple.ordered),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7733#issuecomment-566223349
https://github.com/hail-is/hail/pull/7733#issuecomment-566778522:209,Modifiability,inherit,inheritance,209,"For clarity: I removed PCanonicalBaseStruct because most method implementations were moved back to PBaseStruct, and I found its now-limited benefit to not be worth the drawback of a significantly more complex inheritance structure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7733#issuecomment-566778522
https://github.com/hail-is/hail/pull/7734#issuecomment-567051493:85,Integrability,interface,interface,85,"John, I merged in your changes. Let me know if you had something else in mind for an interface. I moved all implementations, besides codeOrdering to PCanonicalNDArray because in my mind it's harder to track implementations across multiple loci, but let me know if you feel PNDArray needs some default methods (for instance, I could imagine numElements to be this way, but decided not to, because its signature really depends on the shape signature, which lives on PCanonicalNDArray)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7734#issuecomment-567051493
https://github.com/hail-is/hail/pull/7734#issuecomment-567051493:417,Integrability,depend,depends,417,"John, I merged in your changes. Let me know if you had something else in mind for an interface. I moved all implementations, besides codeOrdering to PCanonicalNDArray because in my mind it's harder to track implementations across multiple loci, but let me know if you feel PNDArray needs some default methods (for instance, I could imagine numElements to be this way, but decided not to, because its signature really depends on the shape signature, which lives on PCanonicalNDArray)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7734#issuecomment-567051493
https://github.com/hail-is/hail/pull/7734#issuecomment-567100923:128,Usability,clear,clear,128,"Looks good to me. PNDArray is a little unstable at the moment and probably going to get shaken up in January, so I don't have a clear sense of any must have default methods.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7734#issuecomment-567100923
https://github.com/hail-is/hail/pull/7734#issuecomment-567166972:130,Usability,clear,clear,130,"> Looks good to me. PNDArray is a little unstable at the moment and probably going to get shaken up in January, so I don't have a clear sense of any must have default methods. cool, we'll stay on top of it then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7734#issuecomment-567166972
https://github.com/hail-is/hail/pull/7736#issuecomment-574196142:22,Testability,benchmark,benchmarks,22,I'll reopen this when benchmarks are done,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7736#issuecomment-574196142
https://github.com/hail-is/hail/pull/7738#issuecomment-566591869:29,Integrability,wrap,wrapping,29,Also added Arcturus's method wrapping PR to changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7738#issuecomment-566591869
https://github.com/hail-is/hail/pull/7742#issuecomment-566308517:5,Availability,failure,failure,5,Same failure on #7742,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7742#issuecomment-566308517
https://github.com/hail-is/hail/pull/7750#issuecomment-567082926:105,Energy Efficiency,allocate,allocated,105,You should leave the optional/required classes -- those are easy ways to intern a ptype so it never gets allocated more than once. > by adding the final class modifier to PCanonicalString. You can remove the `final` modifier here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7750#issuecomment-567082926
https://github.com/hail-is/hail/pull/7750#issuecomment-567091607:107,Energy Efficiency,allocate,allocated,107,> You should leave the optional/required classes -- those are easy ways to intern a ptype so it never gets allocated more than once.; > ; > > by adding the final class modifier to PCanonicalString; > ; > You can remove the `final` modifier here. done,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7750#issuecomment-567091607
https://github.com/hail-is/hail/pull/7751#issuecomment-567170120:10,Testability,test,tests,10,man these tests take a while in ci,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7751#issuecomment-567170120
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:126,Modifiability,extend,extends,126,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1178,Modifiability,extend,extends,1178,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:443,Performance,cache,cachedVarid,443,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:480,Performance,cache,cachedRsid,480,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:714,Performance,load,loadField,714,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:777,Performance,load,loadField,777,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:814,Performance,cache,cachedVarid,814,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:834,Performance,cache,cachedRsid,834,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:885,Performance,cache,cachedVarid,885,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:907,Performance,cache,cachedVarid,907,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:929,Performance,load,loadString,929,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:962,Performance,cache,cachedVarid,962,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1006,Performance,cache,cachedRsid,1006,"This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1027,Performance,cache,cachedRsid,1027,"This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1048,Performance,load,loadString,1048,"``scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1080,Performance,cache,cachedRsid,1080,"``scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1469,Performance,cache,cachedVarid,1469,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1491,Performance,cache,cachedVarid,1491,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1520,Performance,load,loadString,1520,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1553,Performance,cache,cachedVarid,1553,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1597,Performance,cache,cachedRsid,1597,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1618,Performance,cache,cachedRsid,1618,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1645,Performance,load,loadString,1645,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:1677,Performance,cache,cachedRsid,1677,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:2037,Performance,load,loadString,2037,"ield = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the constructor. Because if not, we can have one loadString function. It is confusing (to me), to have multiple versions of a function that differ so greatly in semantics.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:573,Testability,assert,assert,573,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/issues/7754#issuecomment-567164437:631,Testability,assert,assert,631,"@tpoterba . This would some uses a bit more difficult:. (Ex: ExportBGen); ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val varidField = rowType.fieldByName(""varid""). private val rsidIdx = rsidField.index; private val varidIdx = varidField.index. private var region: Region = _; private var rsidOffset: Long = _; private var varidOffset: Long = _. private var cachedVarid: String = _; private var cachedRsid: String = _. def setRegion(region: Region, offset: Long) {; this.region = region. assert(rowType.isFieldDefined(region, offset, varidIdx)); assert(rowType.isFieldDefined(region, offset, rsidIdx)); this.rsidOffset = rowType.loadField(region, offset, rsidIdx); this.varidOffset = rowType.loadField(region, offset, varidIdx). cachedVarid = null; cachedRsid = null; }. def varid(): String = {; if (cachedVarid == null); cachedVarid = PString.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = PString.loadString(region, rsidOffset); cachedRsid; }; }; ``` . I could fix this by:. ```scala; class GenAnnotationView(rowType: PStruct) extends View {; private val rsidField = rowType.fieldByName(""rsid""); private val rsidFieldType = rsidField.typ.asInstanceOf[PString]; private val varidField = rowType.fieldByName(""varid""); private val varidFieldType = varidField.typ.asInstanceOf[PString]. # ... def varid(): String = {; if (cachedVarid == null); cachedVarid = varidFieldType.loadString(region, varidOffset); cachedVarid; }. def rsid(): String = {; if (cachedRsid == null); cachedRsid = rsidFieldType.loadString(region, rsidOffset); cachedRsid; }; }; ```. However, it's a bit clunkier than the utility method, and will cost a bit more memory. What do you think about keeping the method as a static method? Would you prefer it be moved off PString to some other location?. Also, this is probably a good time to discuss whether we want region in the construct",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7754#issuecomment-567164437
https://github.com/hail-is/hail/pull/7768#issuecomment-570711004:36,Testability,test,test,36,"Still have a couple more letters to test, but right now everything seems to be more or less working, with timezone passing now supported.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7768#issuecomment-570711004
https://github.com/hail-is/hail/pull/7780#issuecomment-568618504:14,Security,Expose,ExposedPorts,14,Closing until ExposedPorts goes in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7780#issuecomment-568618504
https://github.com/hail-is/hail/pull/7782#issuecomment-568575288:56,Availability,error,errors,56,"I tested this on my branch that had a bunch of deadlock errors and those were replaced with CallError in schedule job because the job was running, cancelled, or instance not active.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568575288
https://github.com/hail-is/hail/pull/7782#issuecomment-568575288:105,Energy Efficiency,schedul,schedule,105,"I tested this on my branch that had a bunch of deadlock errors and those were replaced with CallError in schedule job because the job was running, cancelled, or instance not active.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568575288
https://github.com/hail-is/hail/pull/7782#issuecomment-568575288:2,Testability,test,tested,2,"I tested this on my branch that had a bunch of deadlock errors and those were replaced with CallError in schedule job because the job was running, cancelled, or instance not active.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568575288
https://github.com/hail-is/hail/pull/7782#issuecomment-568579582:240,Availability,resilien,resilient,240,"Hmm, so I've been using a branch to run some 10k and 100k scale tests of /bin/true https://github.com/hail-is/hail/pull/7783 and I've found deadlocks to be rather rare?. In that PR, I only changed the known deadlocking calls to be deadlock resilient. However, deadlock errors seem to be a feature of mysql and it seems were always intended to retry them, so I think this PR (7782) is the right solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568579582
https://github.com/hail-is/hail/pull/7782#issuecomment-568579582:269,Availability,error,errors,269,"Hmm, so I've been using a branch to run some 10k and 100k scale tests of /bin/true https://github.com/hail-is/hail/pull/7783 and I've found deadlocks to be rather rare?. In that PR, I only changed the known deadlocking calls to be deadlock resilient. However, deadlock errors seem to be a feature of mysql and it seems were always intended to retry them, so I think this PR (7782) is the right solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568579582
https://github.com/hail-is/hail/pull/7782#issuecomment-568579582:64,Testability,test,tests,64,"Hmm, so I've been using a branch to run some 10k and 100k scale tests of /bin/true https://github.com/hail-is/hail/pull/7783 and I've found deadlocks to be rather rare?. In that PR, I only changed the known deadlocking calls to be deadlock resilient. However, deadlock errors seem to be a feature of mysql and it seems were always intended to retry them, so I think this PR (7782) is the right solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568579582
https://github.com/hail-is/hail/pull/7782#issuecomment-568581842:78,Availability,error,errors,78,"FWIW, in my current run I've successfully run 14440 jobs and seen 36 deadlock errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7782#issuecomment-568581842
https://github.com/hail-is/hail/pull/7783#issuecomment-568580624:178,Availability,error,error,178,"> retry every deadlock in two deadlock prone SQL operations. I prefer @jigold's change which is almost ready: https://github.com/hail-is/hail/pull/7782. > retry every docker 500 error, it's 500, not our fault, just retry, right?. 500 is meaningless, docker returns 500 for everything. I'm quite concerned about infinite retry loops here, and I'd rather have documentation on the errors we're getting from docker if possible. Looking over the other changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783#issuecomment-568580624
https://github.com/hail-is/hail/pull/7783#issuecomment-568580624:203,Availability,fault,fault,203,"> retry every deadlock in two deadlock prone SQL operations. I prefer @jigold's change which is almost ready: https://github.com/hail-is/hail/pull/7782. > retry every docker 500 error, it's 500, not our fault, just retry, right?. 500 is meaningless, docker returns 500 for everything. I'm quite concerned about infinite retry loops here, and I'd rather have documentation on the errors we're getting from docker if possible. Looking over the other changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783#issuecomment-568580624
https://github.com/hail-is/hail/pull/7783#issuecomment-568580624:379,Availability,error,errors,379,"> retry every deadlock in two deadlock prone SQL operations. I prefer @jigold's change which is almost ready: https://github.com/hail-is/hail/pull/7782. > retry every docker 500 error, it's 500, not our fault, just retry, right?. 500 is meaningless, docker returns 500 for everything. I'm quite concerned about infinite retry loops here, and I'd rather have documentation on the errors we're getting from docker if possible. Looking over the other changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783#issuecomment-568580624
https://github.com/hail-is/hail/pull/7783#issuecomment-568581120:151,Availability,error,errors,151,"> 500 is meaningless, docker returns 500 for everything. I'm quite concerned about infinite retry loops here, and I'd rather have documentation on the errors we're getting from docker if possible. OK, I can add the two other failures I found, but it's infuriating to get most of the way through 100k jobs and then have one fail for some new 500. It feels like there's a long tail of rare docker errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783#issuecomment-568581120
https://github.com/hail-is/hail/pull/7783#issuecomment-568581120:225,Availability,failure,failures,225,"> 500 is meaningless, docker returns 500 for everything. I'm quite concerned about infinite retry loops here, and I'd rather have documentation on the errors we're getting from docker if possible. OK, I can add the two other failures I found, but it's infuriating to get most of the way through 100k jobs and then have one fail for some new 500. It feels like there's a long tail of rare docker errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783#issuecomment-568581120
https://github.com/hail-is/hail/pull/7783#issuecomment-568581120:395,Availability,error,errors,395,"> 500 is meaningless, docker returns 500 for everything. I'm quite concerned about infinite retry loops here, and I'd rather have documentation on the errors we're getting from docker if possible. OK, I can add the two other failures I found, but it's infuriating to get most of the way through 100k jobs and then have one fail for some new 500. It feels like there's a long tail of rare docker errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783#issuecomment-568581120
https://github.com/hail-is/hail/pull/7783#issuecomment-568585850:114,Availability,error,errors,114,"I agree. And parsing strings to figure out what's going on is insanity. Here's a middle-ground: retry unknown 500 errors once. If they really are rare, they won't reoccur (p^2 very small).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7783#issuecomment-568585850
https://github.com/hail-is/hail/pull/7784#issuecomment-570278981:162,Availability,toler,tolerate,162,"heh, wow, the situation wrt kube-dns is pretty bad eh? https://github.com/kubernetes/kubernetes/issues/57659. My understanding is that the other kube-system pods tolerate every taint (by their special kube-system nature), so I think this argument only applies to kube-dns, is that your understanding?. We should stay alert to DNS issues, even with this change, because all our DNS queries will be routed to non-preemptible nodes (see: https://github.com/kubernetes/kubernetes/issues/57659#issuecomment-477009356).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7784#issuecomment-570278981
https://github.com/hail-is/hail/pull/7784#issuecomment-570278981:397,Integrability,rout,routed,397,"heh, wow, the situation wrt kube-dns is pretty bad eh? https://github.com/kubernetes/kubernetes/issues/57659. My understanding is that the other kube-system pods tolerate every taint (by their special kube-system nature), so I think this argument only applies to kube-dns, is that your understanding?. We should stay alert to DNS issues, even with this change, because all our DNS queries will be routed to non-preemptible nodes (see: https://github.com/kubernetes/kubernetes/issues/57659#issuecomment-477009356).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7784#issuecomment-570278981
https://github.com/hail-is/hail/pull/7784#issuecomment-571213114:93,Energy Efficiency,schedul,scheduling,93,"I only saw kube-dns failing, although it is possible there were additional issues. Is k8s OK scheduling system pods on preemptibles? I hadn't considered it before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7784#issuecomment-571213114
https://github.com/hail-is/hail/pull/7784#issuecomment-571218268:39,Availability,toler,tolerate,39,"I read somewhere that kube-system pods tolerate any taint because, e.g. the kube-proxy, is necessary to even participate in k8s. These all exist:; ```; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-0msn 1/1 Running 0 13d; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-fqkt 1/1 Running 0 13d; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-g3z7 1/1 Running 0 60m; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-pdlv 1/1 Running 0 14h; kube-proxy-gke-vdc-non-preemptible-pool-5-80798769-td40 1/1 Running 0 3d20h; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-4fkb 1/1 Running 0 12m; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-51jf 1/1 Running 0 100m; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-cr11 1/1 Running 0 2d22h; kube-proxy-gke-vdc-preemptible-pool-9c7148b2-x6cl 1/1 Running 0 18h; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7784#issuecomment-571218268
https://github.com/hail-is/hail/pull/7786#issuecomment-580065507:49,Usability,clear,clear,49,"Hey, thanks for all the pictures, this is really clear. Looking at this, tho, I have question: what problem is it solving? Maybe asked another way, what does it look like without this that's an issue?. For a narrow window, you get a double scrollbar: one on the table and one on the window. That has always seemed like bad design to me.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7786#issuecomment-580065507
https://github.com/hail-is/hail/pull/7786#issuecomment-580067553:528,Usability,UX,UX,528,"The issue it's solving: allows the dropdown be the requested 75% width of the table. In order to have this you need to set width on the parent of the table, and allow the table to stretch. Hard part is not having it stretch too much when it's empty. Easy alternative is to have the dropdown fixed width. For the double scroll: for very narrow views, I don't think you should see that anymore, unless the table is wider than 1024px. ; * For very wide views, if we set max-width: 100% on the parent, you can have a slightly nicer UX than would have had previously, in that the table is scrollable (so 1 scroll bar), but the search bar is no longer off screen (ex below). But now parent is full width. To solve this can add a media query for narrow views. I found another issue, this time in Safari, will PR. Or can revert, up to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7786#issuecomment-580067553
https://github.com/hail-is/hail/pull/7792#issuecomment-570291808:14,Testability,test,tests,14,Failing scala tests in PruneSuite,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7792#issuecomment-570291808
https://github.com/hail-is/hail/pull/7795#issuecomment-570321116:19,Testability,test,tests,19,"I have to add some tests, but where I got hung up on this a few months ago is that it created a ton of code duplication that I wasn't sure how to abstract away",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7795#issuecomment-570321116
https://github.com/hail-is/hail/issues/7797#issuecomment-570617965:31,Availability,error,error,31,maybe we should throw a better error here?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7797#issuecomment-570617965
https://github.com/hail-is/hail/pull/7803#issuecomment-571280175:61,Availability,error,errors,61,This fails currently. I want CI and eyes on this. Validation errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571280175
https://github.com/hail-is/hail/pull/7803#issuecomment-571280175:50,Security,Validat,Validation,50,This fails currently. I want CI and eyes on this. Validation errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571280175
https://github.com/hail-is/hail/pull/7803#issuecomment-571283839:123,Availability,error,errors,123,"Also, I'm forging ahead for the rest of the day at least. I can't seem to figure out where the issue is for the validation errors that I'm seeing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571283839
https://github.com/hail-is/hail/pull/7803#issuecomment-571283839:112,Security,validat,validation,112,"Also, I'm forging ahead for the rest of the day at least. I can't seem to figure out where the issue is for the validation errors that I'm seeing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571283839
https://github.com/hail-is/hail/pull/7803#issuecomment-571308864:30,Performance,perform,performance,30,I'd really like to see size + performance benchmarks here -- I think the `matrix_table_decode_and_count` and `matrix_table_decode_and_count_just_gt` ones will be interesting.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571308864
https://github.com/hail-is/hail/pull/7803#issuecomment-571308864:42,Testability,benchmark,benchmarks,42,I'd really like to see size + performance benchmarks here -- I think the `matrix_table_decode_and_count` and `matrix_table_decode_and_count_just_gt` ones will be interesting.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-571308864
https://github.com/hail-is/hail/pull/7803#issuecomment-600366372:68,Testability,test,test,68,"With my latest commits, we finally have a change that passes gradle test and pytest with the flag enabled.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7803#issuecomment-600366372
https://github.com/hail-is/hail/pull/7808#issuecomment-573321397:37,Energy Efficiency,monitor,monitoring,37,bump. This would be nice to have for monitoring batch tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7808#issuecomment-573321397
https://github.com/hail-is/hail/pull/7808#issuecomment-573321397:54,Testability,test,tests,54,bump. This would be nice to have for monitoring batch tests.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7808#issuecomment-573321397
https://github.com/hail-is/hail/pull/7814#issuecomment-571721872:87,Deployability,deploy,deploy,87,"I'm not sure I follow. Can you elaborate more on ""We aren't including the test repo on deploy""? I thought we were always testing everything on deploy (ie running Hail, batch, pipeline tests etc).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7814#issuecomment-571721872
https://github.com/hail-is/hail/pull/7814#issuecomment-571721872:143,Deployability,deploy,deploy,143,"I'm not sure I follow. Can you elaborate more on ""We aren't including the test repo on deploy""? I thought we were always testing everything on deploy (ie running Hail, batch, pipeline tests etc).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7814#issuecomment-571721872
https://github.com/hail-is/hail/pull/7814#issuecomment-571721872:175,Deployability,pipeline,pipeline,175,"I'm not sure I follow. Can you elaborate more on ""We aren't including the test repo on deploy""? I thought we were always testing everything on deploy (ie running Hail, batch, pipeline tests etc).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7814#issuecomment-571721872
https://github.com/hail-is/hail/pull/7814#issuecomment-571721872:74,Testability,test,test,74,"I'm not sure I follow. Can you elaborate more on ""We aren't including the test repo on deploy""? I thought we were always testing everything on deploy (ie running Hail, batch, pipeline tests etc).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7814#issuecomment-571721872
https://github.com/hail-is/hail/pull/7814#issuecomment-571721872:121,Testability,test,testing,121,"I'm not sure I follow. Can you elaborate more on ""We aren't including the test repo on deploy""? I thought we were always testing everything on deploy (ie running Hail, batch, pipeline tests etc).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7814#issuecomment-571721872
https://github.com/hail-is/hail/pull/7814#issuecomment-571721872:184,Testability,test,tests,184,"I'm not sure I follow. Can you elaborate more on ""We aren't including the test repo on deploy""? I thought we were always testing everything on deploy (ie running Hail, batch, pipeline tests etc).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7814#issuecomment-571721872
https://github.com/hail-is/hail/pull/7821#issuecomment-574201074:91,Performance,perform,performance,91,"Let's chat today to make a plan for getting these reviewed. I certainly want to see size + performance benchmarks -- I'm more than a little concerned that this change by itself will be *slower* for the 1kg matrix tables -- you'll do several java object allocations per matrix entry to decode with these (for AD and PL arrays), where previously we were doing none.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7821#issuecomment-574201074
https://github.com/hail-is/hail/pull/7821#issuecomment-574201074:103,Testability,benchmark,benchmarks,103,"Let's chat today to make a plan for getting these reviewed. I certainly want to see size + performance benchmarks -- I'm more than a little concerned that this change by itself will be *slower* for the 1kg matrix tables -- you'll do several java object allocations per matrix entry to decode with these (for AD and PL arrays), where previously we were doing none.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7821#issuecomment-574201074
https://github.com/hail-is/hail/pull/7821#issuecomment-595913767:51,Modifiability,variab,variable,51,"Can confirm that at least locally, the environment variable caused a table to be written with packed integers, and the tests passed with the flag on (I assume).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7821#issuecomment-595913767
https://github.com/hail-is/hail/pull/7821#issuecomment-595913767:119,Testability,test,tests,119,"Can confirm that at least locally, the environment variable caused a table to be written with packed integers, and the tests passed with the flag on (I assume).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7821#issuecomment-595913767
https://github.com/hail-is/hail/pull/7821#issuecomment-595928559:139,Performance,optimiz,optimize,139,"once this is in, am I right that the plan is:; - get transposed array<struct> etype in similarly; - make transposed array<struct> ptype; - optimize these three in tandem to beat current performance / file size",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7821#issuecomment-595928559
https://github.com/hail-is/hail/pull/7821#issuecomment-595928559:186,Performance,perform,performance,186,"once this is in, am I right that the plan is:; - get transposed array<struct> etype in similarly; - make transposed array<struct> ptype; - optimize these three in tandem to beat current performance / file size",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7821#issuecomment-595928559
https://github.com/hail-is/hail/issues/7824#issuecomment-597216721:75,Availability,error,error,75,Hey Tim! Just curious what the status on this bug is. I also just got this error (I did update to the newest version incase it was fixed).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7824#issuecomment-597216721
https://github.com/hail-is/hail/issues/7824#issuecomment-597216721:88,Deployability,update,update,88,Hey Tim! Just curious what the status on this bug is. I also just got this error (I did update to the newest version incase it was fixed).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7824#issuecomment-597216721
https://github.com/hail-is/hail/issues/7826#issuecomment-575727189:757,Integrability,interface,interface,757,"I think you're slightly misinterpreting this bit:. > Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. > Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. > Alex Kotlar:; In which case I don't think the region argument to loadElement would be needed; agreed. > Tim Poterba: ok, I think I'm convinced. In order to do a lazy bgen ptype, we need the *value*, not the type, to hold a region handle. PType doesn't need to be associated with a region -- that's a concept that doesn't really make sense given that PType is really a memory layout spec and codegen interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575727189
https://github.com/hail-is/hail/issues/7826#issuecomment-575727189:327,Performance,load,loadElement,327,"I think you're slightly misinterpreting this bit:. > Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. > Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. > Alex Kotlar:; In which case I don't think the region argument to loadElement would be needed; agreed. > Tim Poterba: ok, I think I'm convinced. In order to do a lazy bgen ptype, we need the *value*, not the type, to hold a region handle. PType doesn't need to be associated with a region -- that's a concept that doesn't really make sense given that PType is really a memory layout spec and codegen interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575727189
https://github.com/hail-is/hail/issues/7826#issuecomment-575727189:423,Performance,load,loadElement,423,"I think you're slightly misinterpreting this bit:. > Patrick Schultz: I think we have most of the infrastructure needed to have a hail type hold a region. Then a lazily decoding PType can hold a (uniquely owned) region to decode into, invisibly to callers. > Patrick Schultz: In which case I don't think the region argument to loadElement would be needed. > Alex Kotlar:; In which case I don't think the region argument to loadElement would be needed; agreed. > Tim Poterba: ok, I think I'm convinced. In order to do a lazy bgen ptype, we need the *value*, not the type, to hold a region handle. PType doesn't need to be associated with a region -- that's a concept that doesn't really make sense given that PType is really a memory layout spec and codegen interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575727189
https://github.com/hail-is/hail/issues/7826#issuecomment-575729333:9,Performance,load,loadLength,9,"So then, loadLength would take instead of a Long, a RegionOwnedAddress, which would say be a (region, Int) tuple correct? Because I'm not exactly sure what Hail values we are referring to in this context, unless we mean ""a Scala value that points to some data that the Hail program needs to access"" (since currently we pass around memory addresses that are Scala primitives, and call methods on PTypes...so I thought the proposal was to give the PType management of regions, so that the caller would not need to think about this, which is closer to what I had envisioned).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575729333
https://github.com/hail-is/hail/issues/7826#issuecomment-575729333:291,Security,access,access,291,"So then, loadLength would take instead of a Long, a RegionOwnedAddress, which would say be a (region, Int) tuple correct? Because I'm not exactly sure what Hail values we are referring to in this context, unless we mean ""a Scala value that points to some data that the Hail program needs to access"" (since currently we pass around memory addresses that are Scala primitives, and call methods on PTypes...so I thought the proposal was to give the PType management of regions, so that the caller would not need to think about this, which is closer to what I had envisioned).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575729333
https://github.com/hail-is/hail/issues/7826#issuecomment-575730831:99,Performance,load,loadLength,99,A Hail value is data in a region (you don't know which region). we don't need to pass a region in `loadLength` because the region that the lazily decoded value might decode into isn't going to be that region anyway -- it must be the same region where the lazily decoded value is living.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575730831
https://github.com/hail-is/hail/issues/7826#issuecomment-575731116:68,Security,access,access,68,"Right I understand, so Hail value refers to something Hail needs to access, with the modifier ""off-heap"". If a PType always places its items into one region, that doesn't seem problematic.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575731116
https://github.com/hail-is/hail/issues/7826#issuecomment-575731997:39,Performance,load,loadLength,39,"So in this case, does the PType method loadLength do anything with a region? If so, is that region stored in the value (Scala object) passed to loadLength? Such as `loadLength(address: RegionOwnedAddress)` where RegionOwnedAddress contains (region, Int).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575731997
https://github.com/hail-is/hail/issues/7826#issuecomment-575731997:144,Performance,load,loadLength,144,"So in this case, does the PType method loadLength do anything with a region? If so, is that region stored in the value (Scala object) passed to loadLength? Such as `loadLength(address: RegionOwnedAddress)` where RegionOwnedAddress contains (region, Int).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575731997
https://github.com/hail-is/hail/issues/7826#issuecomment-575731997:165,Performance,load,loadLength,165,"So in this case, does the PType method loadLength do anything with a region? If so, is that region stored in the value (Scala object) passed to loadLength? Such as `loadLength(address: RegionOwnedAddress)` where RegionOwnedAddress contains (region, Int).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575731997
https://github.com/hail-is/hail/issues/7826#issuecomment-575732118:197,Usability,simpl,simpler,197,"> Right I understand. If a PType always places its items into one region, that doesn't seem problematic. I'm having trouble explaining why I think this is wrong. I think it basically makes nothing simpler, and makes many things more complicated (InferPTypes as written will not work, since you have no idea about regions at that point).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575732118
https://github.com/hail-is/hail/issues/7826#issuecomment-575732251:41,Performance,load,loadLength,41,"> So in this case, does the PType method loadLength do anything with a region?. Nope! That's why we should remove it. That discussion on Zulip resulted in me being convinced that we never need a region on loadLength.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575732251
https://github.com/hail-is/hail/issues/7826#issuecomment-575732251:205,Performance,load,loadLength,205,"> So in this case, does the PType method loadLength do anything with a region?. Nope! That's why we should remove it. That discussion on Zulip resulted in me being convinced that we never need a region on loadLength.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575732251
https://github.com/hail-is/hail/issues/7826#issuecomment-575732722:30,Performance,load,loadLength,30,"Sure, so the proposal is that loadLength takes some region-containing object, and this object has a loadAddress method on it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575732722
https://github.com/hail-is/hail/issues/7826#issuecomment-575732722:100,Performance,load,loadAddress,100,"Sure, so the proposal is that loadLength takes some region-containing object, and this object has a loadAddress method on it?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575732722
https://github.com/hail-is/hail/issues/7826#issuecomment-575733539:145,Integrability,interface,interfaces,145,"> Sure, so the proposal is that loadLength takes some region-containing object, and this object has a loadAddress method on it?. No, I think the interfaces you've created in your remove-region-from-load PR are correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575733539
https://github.com/hail-is/hail/issues/7826#issuecomment-575733539:32,Performance,load,loadLength,32,"> Sure, so the proposal is that loadLength takes some region-containing object, and this object has a loadAddress method on it?. No, I think the interfaces you've created in your remove-region-from-load PR are correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575733539
https://github.com/hail-is/hail/issues/7826#issuecomment-575733539:102,Performance,load,loadAddress,102,"> Sure, so the proposal is that loadLength takes some region-containing object, and this object has a loadAddress method on it?. No, I think the interfaces you've created in your remove-region-from-load PR are correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575733539
https://github.com/hail-is/hail/issues/7826#issuecomment-575733539:198,Performance,load,load,198,"> Sure, so the proposal is that loadLength takes some region-containing object, and this object has a loadAddress method on it?. No, I think the interfaces you've created in your remove-region-from-load PR are correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575733539
https://github.com/hail-is/hail/issues/7826#issuecomment-575745357:30,Usability,learn,learn,30,"Thanks guys, something new to learn!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575745357
https://github.com/hail-is/hail/issues/7826#issuecomment-575791483:195,Usability,clear,clearly,195,"Tim, if you're willing would like to keep this open (or make another issue), to track progress on Patrick/your proposal. Patrick walked me through it, and I like it. The proposal I had, although clearly not explained well, is similar in nature, and for educational reasons, would like to talk to you about it, to see pro's/cons (maybe next week?). Patrick's proposal diagram attached. I would like to implement this once this becomes a priority. ![IMG_6021](https://user-images.githubusercontent.com/5543229/72645429-a610d580-3941-11ea-8086-85fe1f8618a4.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7826#issuecomment-575791483
https://github.com/hail-is/hail/issues/7828#issuecomment-572726743:76,Energy Efficiency,schedul,scheduling,76,"Would it be useful to give people repr() feedback on lazy operations? Like ""scheduling x"" and ""executing x""? cc @tpoterba. Would be a separate PR, but related issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7828#issuecomment-572726743
https://github.com/hail-is/hail/issues/7828#issuecomment-572726743:41,Usability,feedback,feedback,41,"Would it be useful to give people repr() feedback on lazy operations? Like ""scheduling x"" and ""executing x""? cc @tpoterba. Would be a separate PR, but related issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7828#issuecomment-572726743
https://github.com/hail-is/hail/issues/7828#issuecomment-572727108:32,Usability,feedback,feedback,32,I think the best way to provide feedback is with things like progress bars.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7828#issuecomment-572727108
https://github.com/hail-is/hail/issues/7828#issuecomment-572727108:61,Usability,progress bar,progress bars,61,I think the best way to provide feedback is with things like progress bars.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7828#issuecomment-572727108
https://github.com/hail-is/hail/issues/7828#issuecomment-572729552:87,Usability,progress bar,progress bar,87,"For a very fast operation that is expected by the user to be immediately executed, the progress bar would suggest that operation completed very quickly, but wouldn't tell them that the operation that was actually completed was generating an IR with that operation's node present.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7828#issuecomment-572729552
https://github.com/hail-is/hail/pull/7830#issuecomment-574725641:39,Availability,error,error,39,"I got a bad local variable compilation error without the fields for the bindings. I suspect this is a locals/fields problem elsewhere but don't want to debug right now, so reverted.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7830#issuecomment-574725641
https://github.com/hail-is/hail/pull/7830#issuecomment-574725641:18,Modifiability,variab,variable,18,"I got a bad local variable compilation error without the fields for the bindings. I suspect this is a locals/fields problem elsewhere but don't want to debug right now, so reverted.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7830#issuecomment-574725641
https://github.com/hail-is/hail/pull/7831#issuecomment-572767655:28,Usability,clear,clearMissingBits,28,"one sec, need to remove the clearMissingBits function",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7831#issuecomment-572767655
https://github.com/hail-is/hail/pull/7833#issuecomment-572811460:64,Energy Efficiency,schedul,schedule,64,This should be working. I'll add the optimization to not double schedule jobs in another PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7833#issuecomment-572811460
https://github.com/hail-is/hail/pull/7833#issuecomment-572811460:37,Performance,optimiz,optimization,37,This should be working. I'll add the optimization to not double schedule jobs in another PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7833#issuecomment-572811460
https://github.com/hail-is/hail/pull/7833#issuecomment-573028759:40,Deployability,update,updates,40,I didnt quite fix all of the select for updates when getting the instance state. Don’t merge until I get a chance to fix it (computer ran out of battery),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7833#issuecomment-573028759
https://github.com/hail-is/hail/pull/7833#issuecomment-573028759:145,Energy Efficiency,battery,battery,145,I didnt quite fix all of the select for updates when getting the instance state. Don’t merge until I get a chance to fix it (computer ran out of battery),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7833#issuecomment-573028759
https://github.com/hail-is/hail/pull/7838#issuecomment-573179477:56,Testability,test,tested,56,I made a new PR from #7833 because I know this has been tested except for the three extra commits to remove testing infrastructure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7838#issuecomment-573179477
https://github.com/hail-is/hail/pull/7838#issuecomment-573179477:108,Testability,test,testing,108,I made a new PR from #7833 because I know this has been tested except for the three extra commits to remove testing infrastructure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7838#issuecomment-573179477
https://github.com/hail-is/hail/pull/7842#issuecomment-574208348:42,Testability,assert,assertion,42,"> oops, forgot to request changes.; > ; > assertion should move into getNestedElementPTypes as something like the following:; > ; > ```scala; > val vTypes = values.iterator.map(_.virtualType).toSet; > if (vTypes.size != 1); > throw new AssertionError(s""invalid call to getNestedElementPTypes: $vTypes""); > ```. Should we check isOfType instead of virtual type, for case when we say try to coalesce canonical and non-canonical implementations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7842#issuecomment-574208348
https://github.com/hail-is/hail/pull/7842#issuecomment-574208348:236,Testability,Assert,AssertionError,236,"> oops, forgot to request changes.; > ; > assertion should move into getNestedElementPTypes as something like the following:; > ; > ```scala; > val vTypes = values.iterator.map(_.virtualType).toSet; > if (vTypes.size != 1); > throw new AssertionError(s""invalid call to getNestedElementPTypes: $vTypes""); > ```. Should we check isOfType instead of virtual type, for case when we say try to coalesce canonical and non-canonical implementations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7842#issuecomment-574208348
https://github.com/hail-is/hail/pull/7850#issuecomment-573430599:182,Integrability,message,message,182,I tested this by hand by replacing the actual function call with throwing a TimeoutError and making sure both the client and the batch-driver gave the appropriate warning in the log message and didn't proceed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7850#issuecomment-573430599
https://github.com/hail-is/hail/pull/7850#issuecomment-573430599:76,Safety,Timeout,TimeoutError,76,I tested this by hand by replacing the actual function call with throwing a TimeoutError and making sure both the client and the batch-driver gave the appropriate warning in the log message and didn't proceed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7850#issuecomment-573430599
https://github.com/hail-is/hail/pull/7850#issuecomment-573430599:2,Testability,test,tested,2,I tested this by hand by replacing the actual function call with throwing a TimeoutError and making sure both the client and the batch-driver gave the appropriate warning in the log message and didn't proceed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7850#issuecomment-573430599
https://github.com/hail-is/hail/pull/7850#issuecomment-573430599:178,Testability,log,log,178,I tested this by hand by replacing the actual function call with throwing a TimeoutError and making sure both the client and the batch-driver gave the appropriate warning in the log message and didn't proceed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7850#issuecomment-573430599
https://github.com/hail-is/hail/pull/7851#issuecomment-574442792:183,Deployability,deploy,deploy,183,"I think I got everything, but I still want to test it again by hand in the morning once we're sure there's no other changes to make. I was testing the before migration by running dev deploy from master and cancelling a batch while it was submitting along with some completed ones and then tried to deploy the new version and made sure the ready cores etc. were now correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7851#issuecomment-574442792
https://github.com/hail-is/hail/pull/7851#issuecomment-574442792:298,Deployability,deploy,deploy,298,"I think I got everything, but I still want to test it again by hand in the morning once we're sure there's no other changes to make. I was testing the before migration by running dev deploy from master and cancelling a batch while it was submitting along with some completed ones and then tried to deploy the new version and made sure the ready cores etc. were now correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7851#issuecomment-574442792
https://github.com/hail-is/hail/pull/7851#issuecomment-574442792:46,Testability,test,test,46,"I think I got everything, but I still want to test it again by hand in the morning once we're sure there's no other changes to make. I was testing the before migration by running dev deploy from master and cancelling a batch while it was submitting along with some completed ones and then tried to deploy the new version and made sure the ready cores etc. were now correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7851#issuecomment-574442792
https://github.com/hail-is/hail/pull/7851#issuecomment-574442792:139,Testability,test,testing,139,"I think I got everything, but I still want to test it again by hand in the morning once we're sure there's no other changes to make. I was testing the before migration by running dev deploy from master and cancelling a batch while it was submitting along with some completed ones and then tried to deploy the new version and made sure the ready cores etc. were now correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7851#issuecomment-574442792
https://github.com/hail-is/hail/pull/7853#issuecomment-581597915:75,Availability,error,errors,75,"I'm going to close for now. If we realize this solves some of the deadlock errors, then we can reopen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7853#issuecomment-581597915
https://github.com/hail-is/hail/pull/7855#issuecomment-573659765:54,Availability,down,down,54,"Note, this means migrations for batch will shut batch down, but batch won't be running to restart itself (!) so this will have to be done manually. The UI will also be down, so we'll have to watch the database to verify the migration has been applied to know when to restart batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573659765
https://github.com/hail-is/hail/pull/7855#issuecomment-573659765:168,Availability,down,down,168,"Note, this means migrations for batch will shut batch down, but batch won't be running to restart itself (!) so this will have to be done manually. The UI will also be down, so we'll have to watch the database to verify the migration has been applied to know when to restart batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573659765
https://github.com/hail-is/hail/pull/7855#issuecomment-573683956:102,Deployability,deploy,deployments,102,Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded? Maybe have k8s get the deployment yaml configuration before deleting each deployment and then reapply the yamls for each shutdown step in order. I'm worried this PR will break all dev deploys with migrations in them as you can't just do `make -c batch deploy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956
https://github.com/hail-is/hail/pull/7855#issuecomment-573683956:187,Deployability,deploy,deployment,187,Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded? Maybe have k8s get the deployment yaml configuration before deleting each deployment and then reapply the yamls for each shutdown step in order. I'm worried this PR will break all dev deploys with migrations in them as you can't just do `make -c batch deploy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956
https://github.com/hail-is/hail/pull/7855#issuecomment-573683956:203,Deployability,configurat,configuration,203,Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded? Maybe have k8s get the deployment yaml configuration before deleting each deployment and then reapply the yamls for each shutdown step in order. I'm worried this PR will break all dev deploys with migrations in them as you can't just do `make -c batch deploy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956
https://github.com/hail-is/hail/pull/7855#issuecomment-573683956:238,Deployability,deploy,deployment,238,Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded? Maybe have k8s get the deployment yaml configuration before deleting each deployment and then reapply the yamls for each shutdown step in order. I'm worried this PR will break all dev deploys with migrations in them as you can't just do `make -c batch deploy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956
https://github.com/hail-is/hail/pull/7855#issuecomment-573683956:348,Deployability,deploy,deploys,348,Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded? Maybe have k8s get the deployment yaml configuration before deleting each deployment and then reapply the yamls for each shutdown step in order. I'm worried this PR will break all dev deploys with migrations in them as you can't just do `make -c batch deploy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956
https://github.com/hail-is/hail/pull/7855#issuecomment-573683956:416,Deployability,deploy,deploy,416,Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded? Maybe have k8s get the deployment yaml configuration before deleting each deployment and then reapply the yamls for each shutdown step in order. I'm worried this PR will break all dev deploys with migrations in them as you can't just do `make -c batch deploy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956
https://github.com/hail-is/hail/pull/7855#issuecomment-573683956:203,Modifiability,config,configuration,203,Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded? Maybe have k8s get the deployment yaml configuration before deleting each deployment and then reapply the yamls for each shutdown step in order. I'm worried this PR will break all dev deploys with migrations in them as you can't just do `make -c batch deploy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956
https://github.com/hail-is/hail/pull/7855#issuecomment-573731718:75,Availability,down,down,75,"> I'm worried this PR will break all dev deploys. The migration will bring down the service (batch, etc.) inside the dev namespace, but the dev deploy itself is running in the production batch, so there should be no issue. The deploys will still happen as normal after the migrations, e.g. deploy_batch will run after batch_database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573731718
https://github.com/hail-is/hail/pull/7855#issuecomment-573731718:41,Deployability,deploy,deploys,41,"> I'm worried this PR will break all dev deploys. The migration will bring down the service (batch, etc.) inside the dev namespace, but the dev deploy itself is running in the production batch, so there should be no issue. The deploys will still happen as normal after the migrations, e.g. deploy_batch will run after batch_database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573731718
https://github.com/hail-is/hail/pull/7855#issuecomment-573731718:144,Deployability,deploy,deploy,144,"> I'm worried this PR will break all dev deploys. The migration will bring down the service (batch, etc.) inside the dev namespace, but the dev deploy itself is running in the production batch, so there should be no issue. The deploys will still happen as normal after the migrations, e.g. deploy_batch will run after batch_database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573731718
https://github.com/hail-is/hail/pull/7855#issuecomment-573731718:227,Deployability,deploy,deploys,227,"> I'm worried this PR will break all dev deploys. The migration will bring down the service (batch, etc.) inside the dev namespace, but the dev deploy itself is running in the production batch, so there should be no issue. The deploys will still happen as normal after the migrations, e.g. deploy_batch will run after batch_database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573731718
https://github.com/hail-is/hail/pull/7855#issuecomment-573739953:104,Deployability,deploy,deployments,104,> Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded?. This is an interesting idea. I think it would effectively require merging the database and the deploy steps. We can't re-deploy the existing config because that's the wrong (old) one.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573739953
https://github.com/hail-is/hail/pull/7855#issuecomment-573739953:262,Deployability,deploy,deploy,262,> Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded?. This is an interesting idea. I think it would effectively require merging the database and the deploy steps. We can't re-deploy the existing config because that's the wrong (old) one.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573739953
https://github.com/hail-is/hail/pull/7855#issuecomment-573739953:288,Deployability,deploy,deploy,288,> Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded?. This is an interesting idea. I think it would effectively require merging the database and the deploy steps. We can't re-deploy the existing config because that's the wrong (old) one.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573739953
https://github.com/hail-is/hail/pull/7855#issuecomment-573739953:308,Modifiability,config,config,308,> Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded?. This is an interesting idea. I think it would effectively require merging the database and the deploy steps. We can't re-deploy the existing config because that's the wrong (old) one.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573739953
https://github.com/hail-is/hail/pull/7861#issuecomment-573827957:10,Testability,Test,Tested,10,@konradjk Tested,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7861#issuecomment-573827957
https://github.com/hail-is/hail/pull/7862#issuecomment-574275404:38,Testability,test,tests,38,"This works now, just need to add some tests though instead of just terminal experiments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7862#issuecomment-574275404
https://github.com/hail-is/hail/pull/7862#issuecomment-574322586:13,Testability,test,tested,13,"Alright, all tested, should be good :+1:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7862#issuecomment-574322586
https://github.com/hail-is/hail/pull/7863#issuecomment-573896406:23,Availability,failure,failure,23,"Yeah, that was a weird failure. I wonder if a node got preempted and createDatabase2 is not idempotent.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7863#issuecomment-573896406
https://github.com/hail-is/hail/pull/7864#issuecomment-580346915:8,Testability,test,test,8,"Can you test how much time this adds to `test_pipeline`? The original problem should be fixed now that this would have caught (spec in database too big). I'm a little worried about every ci build adding 500 spec, status, and log files, but those are going into a 1 day bucket now and it's probably not very expensive for the API calls. Maybe this should be part of a separate batch benchmark suite that's run before major changes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-580346915
https://github.com/hail-is/hail/pull/7864#issuecomment-580346915:225,Testability,log,log,225,"Can you test how much time this adds to `test_pipeline`? The original problem should be fixed now that this would have caught (spec in database too big). I'm a little worried about every ci build adding 500 spec, status, and log files, but those are going into a 1 day bucket now and it's probably not very expensive for the API calls. Maybe this should be part of a separate batch benchmark suite that's run before major changes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-580346915
https://github.com/hail-is/hail/pull/7864#issuecomment-580346915:382,Testability,benchmark,benchmark,382,"Can you test how much time this adds to `test_pipeline`? The original problem should be fixed now that this would have caught (spec in database too big). I'm a little worried about every ci build adding 500 spec, status, and log files, but those are going into a 1 day bucket now and it's probably not very expensive for the API calls. Maybe this should be part of a separate batch benchmark suite that's run before major changes?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-580346915
https://github.com/hail-is/hail/pull/7864#issuecomment-580362794:75,Deployability,pipeline,pipeline,75,tests took 8m for this PR and 10m for another recent PR that didn't change pipeline,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-580362794
https://github.com/hail-is/hail/pull/7864#issuecomment-580362794:0,Testability,test,tests,0,tests took 8m for this PR and 10m for another recent PR that didn't change pipeline,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-580362794
https://github.com/hail-is/hail/pull/7864#issuecomment-583466260:193,Deployability,pipeline,pipeline,193,"can we please get this in? If there's a reason I'm not seeing to be cautious, I want to hear it. But for my own sanity, I'd like another line of defense against needing to rewrite benchmark-on-pipeline again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-583466260
https://github.com/hail-is/hail/pull/7864#issuecomment-583466260:172,Modifiability,rewrite,rewrite,172,"can we please get this in? If there's a reason I'm not seeing to be cautious, I want to hear it. But for my own sanity, I'd like another line of defense against needing to rewrite benchmark-on-pipeline again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-583466260
https://github.com/hail-is/hail/pull/7864#issuecomment-583466260:180,Testability,benchmark,benchmark-on-pipeline,180,"can we please get this in? If there's a reason I'm not seeing to be cautious, I want to hear it. But for my own sanity, I'd like another line of defense against needing to rewrite benchmark-on-pipeline again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-583466260
https://github.com/hail-is/hail/pull/7864#issuecomment-583542097:28,Testability,test,tests,28,"takes 10m, similar to other tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-583542097
https://github.com/hail-is/hail/pull/7868#issuecomment-574469050:12,Availability,failure,failures,12,"Some parser failures:. for example:; `Rule 'type' didn't match at 'PCStruct{id:PCString' (line 1, column 1).`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7868#issuecomment-574469050
https://github.com/hail-is/hail/pull/7868#issuecomment-574486661:0,Testability,test,tests,0,tests still failing,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7868#issuecomment-574486661
https://github.com/hail-is/hail/pull/7868#issuecomment-574487828:315,Integrability,interface,interface,315,"> > Tim could you help me understand why we do not need the IR node to be supported in Python?; > ; > the In node is unused in Python, and that's good -- python doesn't know about physical types. Ah, I thought the In node was used as then to represent data inflow (from some outside source), and thought the Python interface may want to generale that node. Ok, so what you said makes sense. In that case, why do we also have a `In(_, Type)` constructor (`def apply(i: Int, typ: Type): In = In(i, PType.canonical(typ))`)?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7868#issuecomment-574487828
https://github.com/hail-is/hail/pull/7868#issuecomment-574658178:211,Testability,test,tests,211,"> Ok, so what you said makes sense. In that case, why do we also have a In(_, Type) constructor (def apply(i: Int, typ: Type): In = In(i, PType.canonical(typ)))?. My guess is that there are hundreds of lines of tests that used the old `In(i, virtual type)` and we didn't want to change those all at once. As long as the ptype is constructed using `canonical` both in the `In` and the call to `Compile`, this should be fine.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7868#issuecomment-574658178
https://github.com/hail-is/hail/pull/7875#issuecomment-574377227:85,Availability,failure,failures,85,"> submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; > if any request fails, raise an exception, which is caught by outer submit, which retries a configurable number of times, logging a configurable number of errors. I haven't dug into the PR yet, but will just remark I'm going to argue pretty strenuously to maintain our current model here: infinitely retry transient errors with exponential backoff and no retry of non-transient errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227
https://github.com/hail-is/hail/pull/7875#issuecomment-574377227:251,Availability,error,errors,251,"> submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; > if any request fails, raise an exception, which is caught by outer submit, which retries a configurable number of times, logging a configurable number of errors. I haven't dug into the PR yet, but will just remark I'm going to argue pretty strenuously to maintain our current model here: infinitely retry transient errors with exponential backoff and no retry of non-transient errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227
https://github.com/hail-is/hail/pull/7875#issuecomment-574377227:412,Availability,error,errors,412,"> submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; > if any request fails, raise an exception, which is caught by outer submit, which retries a configurable number of times, logging a configurable number of errors. I haven't dug into the PR yet, but will just remark I'm going to argue pretty strenuously to maintain our current model here: infinitely retry transient errors with exponential backoff and no retry of non-transient errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227
https://github.com/hail-is/hail/pull/7875#issuecomment-574377227:474,Availability,error,errors,474,"> submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; > if any request fails, raise an exception, which is caught by outer submit, which retries a configurable number of times, logging a configurable number of errors. I haven't dug into the PR yet, but will just remark I'm going to argue pretty strenuously to maintain our current model here: infinitely retry transient errors with exponential backoff and no retry of non-transient errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227
https://github.com/hail-is/hail/pull/7875#issuecomment-574377227:188,Modifiability,config,configurable,188,"> submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; > if any request fails, raise an exception, which is caught by outer submit, which retries a configurable number of times, logging a configurable number of errors. I haven't dug into the PR yet, but will just remark I'm going to argue pretty strenuously to maintain our current model here: infinitely retry transient errors with exponential backoff and no retry of non-transient errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227
https://github.com/hail-is/hail/pull/7875#issuecomment-574377227:228,Modifiability,config,configurable,228,"> submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; > if any request fails, raise an exception, which is caught by outer submit, which retries a configurable number of times, logging a configurable number of errors. I haven't dug into the PR yet, but will just remark I'm going to argue pretty strenuously to maintain our current model here: infinitely retry transient errors with exponential backoff and no retry of non-transient errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227
https://github.com/hail-is/hail/pull/7875#issuecomment-574377227:218,Testability,log,logging,218,"> submit 50-way parallel bunch, with a maximum of (by default) 10 individual request failures; > if any request fails, raise an exception, which is caught by outer submit, which retries a configurable number of times, logging a configurable number of errors. I haven't dug into the PR yet, but will just remark I'm going to argue pretty strenuously to maintain our current model here: infinitely retry transient errors with exponential backoff and no retry of non-transient errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574377227
https://github.com/hail-is/hail/pull/7875#issuecomment-574380438:29,Availability,error,errors,29,> infinitely retry transient errors with exponential backoff and no retry of non-transient errors. In other words: Do or do not. There is no try.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574380438
https://github.com/hail-is/hail/pull/7875#issuecomment-574380438:91,Availability,error,errors,91,> infinitely retry transient errors with exponential backoff and no retry of non-transient errors. In other words: Do or do not. There is no try.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574380438
https://github.com/hail-is/hail/pull/7875#issuecomment-574394825:241,Availability,error,errors,241,"I can't make create idempotent, it returns a fresh batch id. I did make `jobs/create` idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I only retry transient errors, but I allow the user to say they don't want that. A missing internet connection is apparently considered ""transient"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574394825
https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:782,Availability,error,error,782,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367
https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:903,Availability,error,errors,903,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367
https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:957,Availability,error,errors,957,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367
https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:997,Availability,error,errors,997,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367
https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:1089,Availability,error,errors,1089,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367
https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:1161,Availability,error,error,1161,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367
https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:1188,Availability,down,down,1188,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367
https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:1275,Availability,down,down,1275,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367
https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:1601,Availability,failure,failure,1601,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367
https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:1481,Testability,log,logging,1481,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367
https://github.com/hail-is/hail/pull/7875#issuecomment-574400367:1576,Testability,log,log,1576,"> I can't make create idempotent, it returns a fresh batch id. I'm saying we can (and should, for all such requests) make /batches/create idempotent by having the client generate a random token that it includes in the POST request to create a batch. The create batch code then first checks a batch exists with the random string and returns that id if it exists before deciding to create a fresh batch. > I did make jobs/create idempotent, but if you have 10M jobs, I don't want the client to regenerate a 10M job DAG (which takes longer than the submits do). I'm not sure I understand the alternative you're considering in the second half of this statement. > A missing internet connection is apparently considered ""transient"". Yes. Transient just means the expectation is that the error will eventually go away. Most computers usually get hooked up to the internet again. My mental model for transient errors is like this: there are two types of transient errors, short and long. Short transient errors occur sporadically with some very low probability p (so that two such such transient errors will almost occur twice in a row). The other is a long transient error like a service going down and waiting for a liveness check to notice and restart, internet connection going down, or laptop being losing wifi connection, being moved, etc. For the first type, you want to retry immediately. For the second, you want to retry with exponential backoff. I think the PR that introduced logging for retry went in when I was gone. I think it should follow the same strategy: I would log always on the second failure, and then with exponential backoff. Same with the batch client. > I allow the user to say they don't want that. Fine. I'm saying I want the default to be infinite. I doubt anyone will ever change it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-574400367
https://github.com/hail-is/hail/pull/7875#issuecomment-575711955:103,Availability,failure,failures,103,"This is ready for final review. Since Konrad is blocked by it, I suggest, unless there are correctness failures or major performance issues, we merge it and I will address comments in follow up PRs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575711955
https://github.com/hail-is/hail/pull/7875#issuecomment-575711955:121,Performance,perform,performance,121,"This is ready for final review. Since Konrad is blocked by it, I suggest, unless there are correctness failures or major performance issues, we merge it and I will address comments in follow up PRs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575711955
https://github.com/hail-is/hail/pull/7875#issuecomment-575720943:123,Deployability,UPDATE,UPDATE,123,Cotton had an old comment that's now disappeared saying some of the SELECTs in the Python code should have been SELECT FOR UPDATE. I think it's okay to do that in a separate PR. Can we get a concise list of things that still need to be addressed later from this PR?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575720943
https://github.com/hail-is/hail/pull/7875#issuecomment-575750888:181,Availability,error,error,181,"AFAIK, everything has been addressed. Are you referring to [this](https://github.com/hail-is/hail/pull/7875#discussion_r367430472)? Now we user `insert` and catch the duplicate key error instead of the previous `select ... for update` followed by an `insert`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575750888
https://github.com/hail-is/hail/pull/7875#issuecomment-575750888:227,Deployability,update,update,227,"AFAIK, everything has been addressed. Are you referring to [this](https://github.com/hail-is/hail/pull/7875#discussion_r367430472)? Now we user `insert` and catch the duplicate key error instead of the previous `select ... for update` followed by an `insert`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575750888
https://github.com/hail-is/hail/pull/7875#issuecomment-575754308:86,Deployability,UPDATE,UPDATE,86,"OK, I think this PR is OK wrt that issue. I introduce one SQL command and it uses FOR UPDATE because we later insert that record.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754308
https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:14,Availability,error,errors,14,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393
https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:414,Safety,timeout,timeout,414,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393
https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:9,Testability,test,test,9,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393
https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:31,Testability,test,test,31,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393
https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:51,Testability,Test,Test,51,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393
https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:115,Testability,Test,Test,115,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393
https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:179,Testability,test,test,179,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393
https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:195,Testability,Test,Test,195,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393
https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:200,Testability,test,testMethod,200,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393
https://github.com/hail-is/hail/pull/7875#issuecomment-575754393:476,Testability,test,test,476,"You have test errors:. ```; io/test/test_batch.py::Test::test_restartable_insert FAILED; _________________________ Test.test_restartable_insert _________________________. self = <test.test_batch.Test testMethod=test_restartable_insert>. def test_restartable_insert(self):; def every_third_time():; nonlocal i; i += 1; if i % 3 == 0:; return True; return False; with aiohttp.ClientSession(raise_for_status=True,; > timeout=aiohttp.ClientTimeout(total=60)) as real_session:. io/test/test_batch.py:441: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ . self = <aiohttp.client.ClientSession object at 0x7f96a2a4d550>. def __enter__(self) -> None:; > raise TypeError(""Use async with instead""); E TypeError: Use async with instead. usr/local/lib/python3.6/dist-packages/aiohttp/client.py:964: TypeError; ```. ```; self._tasks = ordered_tasks; try:; self._backend._run(self, dry_run, verbose, delete_scratch_on_exit, **backend_kwargs); finally:; > self._backend.close(); E AttributeError: 'LocalBackend' object has no attribute 'close'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754393
https://github.com/hail-is/hail/pull/7886#issuecomment-574780144:20,Testability,test,test,20,"I also verified the test fails against current batch, both in dev namespace and in default.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7886#issuecomment-574780144
https://github.com/hail-is/hail/pull/7886#issuecomment-574813098:106,Testability,log,logs,106,"Looks like something went wrong with an always_run cancel step, but there's not enough information in the logs to figure out why.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7886#issuecomment-574813098
https://github.com/hail-is/hail/pull/7886#issuecomment-574813815:39,Availability,ERROR,ERROR,39,"Ah, figured out what's going on:; ```; ERROR	2020-01-15 18:17:49,022	batch.py	schedule_job:385	error while scheduling job (11, 3) on instance batch-worker-pr-7886-default-npqddriu0gh7-z20pv	Traceback (most recent call last):\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 375, in schedule_job\n raise e\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 366, in schedule_job\n await session.post(url, json=body)\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 589, in _request\n resp.raise_for_status()\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 947, in raise_for_status\n headers=self.headers)\naiohttp.client_exceptions.ClientResponseError: 413, message='Request Entity Too Large', url='http://10.128.0.25:5000/api/v1alpha/batches/jobs/create; ```. This is causing an instance to be marked unhealthy. Somehow that's causing an always_run job to not run before a batch is considered finished.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7886#issuecomment-574813815
https://github.com/hail-is/hail/pull/7886#issuecomment-574813815:95,Availability,error,error,95,"Ah, figured out what's going on:; ```; ERROR	2020-01-15 18:17:49,022	batch.py	schedule_job:385	error while scheduling job (11, 3) on instance batch-worker-pr-7886-default-npqddriu0gh7-z20pv	Traceback (most recent call last):\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 375, in schedule_job\n raise e\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 366, in schedule_job\n await session.post(url, json=body)\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 589, in _request\n resp.raise_for_status()\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 947, in raise_for_status\n headers=self.headers)\naiohttp.client_exceptions.ClientResponseError: 413, message='Request Entity Too Large', url='http://10.128.0.25:5000/api/v1alpha/batches/jobs/create; ```. This is causing an instance to be marked unhealthy. Somehow that's causing an always_run job to not run before a batch is considered finished.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7886#issuecomment-574813815
https://github.com/hail-is/hail/pull/7886#issuecomment-574813815:107,Energy Efficiency,schedul,scheduling,107,"Ah, figured out what's going on:; ```; ERROR	2020-01-15 18:17:49,022	batch.py	schedule_job:385	error while scheduling job (11, 3) on instance batch-worker-pr-7886-default-npqddriu0gh7-z20pv	Traceback (most recent call last):\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 375, in schedule_job\n raise e\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 366, in schedule_job\n await session.post(url, json=body)\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 589, in _request\n resp.raise_for_status()\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 947, in raise_for_status\n headers=self.headers)\naiohttp.client_exceptions.ClientResponseError: 413, message='Request Entity Too Large', url='http://10.128.0.25:5000/api/v1alpha/batches/jobs/create; ```. This is causing an instance to be marked unhealthy. Somehow that's causing an always_run job to not run before a batch is considered finished.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7886#issuecomment-574813815
https://github.com/hail-is/hail/pull/7886#issuecomment-574813815:748,Integrability,message,message,748,"Ah, figured out what's going on:; ```; ERROR	2020-01-15 18:17:49,022	batch.py	schedule_job:385	error while scheduling job (11, 3) on instance batch-worker-pr-7886-default-npqddriu0gh7-z20pv	Traceback (most recent call last):\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 375, in schedule_job\n raise e\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 366, in schedule_job\n await session.post(url, json=body)\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 589, in _request\n resp.raise_for_status()\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 947, in raise_for_status\n headers=self.headers)\naiohttp.client_exceptions.ClientResponseError: 413, message='Request Entity Too Large', url='http://10.128.0.25:5000/api/v1alpha/batches/jobs/create; ```. This is causing an instance to be marked unhealthy. Somehow that's causing an always_run job to not run before a batch is considered finished.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7886#issuecomment-574813815
https://github.com/hail-is/hail/pull/7886#issuecomment-574836135:19,Availability,error,error,19,You have an import error,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7886#issuecomment-574836135
https://github.com/hail-is/hail/pull/7889#issuecomment-574823386:31,Integrability,message,message,31,https://zulipchat.com/api/send-message,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7889#issuecomment-574823386
https://github.com/hail-is/hail/issues/7893#issuecomment-613679220:56,Deployability,update,update,56,"I'm not sure what to do about this. I don't think I can update this to use Python semantics instead of java semantics (n = number of times to split, instead of number of split results) without breaking pipelines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7893#issuecomment-613679220
https://github.com/hail-is/hail/issues/7893#issuecomment-613679220:202,Deployability,pipeline,pipelines,202,"I'm not sure what to do about this. I don't think I can update this to use Python semantics instead of java semantics (n = number of times to split, instead of number of split results) without breaking pipelines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7893#issuecomment-613679220
https://github.com/hail-is/hail/pull/7899#issuecomment-575399003:134,Integrability,message,message,134,"> danking pushed 1 commit to branch master. Commits by cseed (1).; > duh (#7899) (6b7f8f6). using the intended PR title as the commit message requires no more typing and saves us from ""duh""!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7899#issuecomment-575399003
https://github.com/hail-is/hail/pull/7903#issuecomment-575779083:46,Performance,load,loadEleme,46,">> FIXME. ```scala; joinF.invoke(region, rtyp.loadEleme; ``` . I was referring to this... I wanted to check that joinF.invoke still needed region, didn't quite follow the invocation here",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7903#issuecomment-575779083
https://github.com/hail-is/hail/pull/7904#issuecomment-575915606:79,Testability,test,tests,79,"Tim, once the remove-region pr/branch is merged, this is ready to look at, all tests pass (there will be a rebase needed, likely)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-575915606
https://github.com/hail-is/hail/pull/7904#issuecomment-575935878:4,Testability,test,tests,4,All tests pass,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-575935878
https://github.com/hail-is/hail/pull/7904#issuecomment-576358168:164,Availability,error,error,164,">> the store methods on PString should take a Code[String], not a Code[Array[Byte]], I think. Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an `allocateAndStoreString` method, which takes care of both steps, potentially more efficiently, but also more ergonomically. I left the allocate(region: Region, length: Int) and store(addr: Long, str: String) methods, though we have no current use for them so I worry will wind up as bloat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358168
https://github.com/hail-is/hail/pull/7904#issuecomment-576358168:207,Energy Efficiency,allocate,allocate,207,">> the store methods on PString should take a Code[String], not a Code[Array[Byte]], I think. Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an `allocateAndStoreString` method, which takes care of both steps, potentially more efficiently, but also more ergonomically. I left the allocate(region: Region, length: Int) and store(addr: Long, str: String) methods, though we have no current use for them so I worry will wind up as bloat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358168
https://github.com/hail-is/hail/pull/7904#issuecomment-576358168:305,Energy Efficiency,allocate,allocate,305,">> the store methods on PString should take a Code[String], not a Code[Array[Byte]], I think. Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an `allocateAndStoreString` method, which takes care of both steps, potentially more efficiently, but also more ergonomically. I left the allocate(region: Region, length: Int) and store(addr: Long, str: String) methods, though we have no current use for them so I worry will wind up as bloat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358168
https://github.com/hail-is/hail/pull/7904#issuecomment-576358168:421,Energy Efficiency,allocate,allocate,421,">> the store methods on PString should take a Code[String], not a Code[Array[Byte]], I think. Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an `allocateAndStoreString` method, which takes care of both steps, potentially more efficiently, but also more ergonomically. I left the allocate(region: Region, length: Int) and store(addr: Long, str: String) methods, though we have no current use for them so I worry will wind up as bloat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358168
https://github.com/hail-is/hail/pull/7904#issuecomment-576358168:555,Energy Efficiency,allocate,allocateAndStoreString,555,">> the store methods on PString should take a Code[String], not a Code[Array[Byte]], I think. Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an `allocateAndStoreString` method, which takes care of both steps, potentially more efficiently, but also more ergonomically. I left the allocate(region: Region, length: Int) and store(addr: Long, str: String) methods, though we have no current use for them so I worry will wind up as bloat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358168
https://github.com/hail-is/hail/pull/7904#issuecomment-576358168:636,Energy Efficiency,efficient,efficiently,636,">> the store methods on PString should take a Code[String], not a Code[Array[Byte]], I think. Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an `allocateAndStoreString` method, which takes care of both steps, potentially more efficiently, but also more ergonomically. I left the allocate(region: Region, length: Int) and store(addr: Long, str: String) methods, though we have no current use for them so I worry will wind up as bloat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358168
https://github.com/hail-is/hail/pull/7904#issuecomment-576358168:689,Energy Efficiency,allocate,allocate,689,">> the store methods on PString should take a Code[String], not a Code[Array[Byte]], I think. Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an `allocateAndStoreString` method, which takes care of both steps, potentially more efficiently, but also more ergonomically. I left the allocate(region: Region, length: Int) and store(addr: Long, str: String) methods, though we have no current use for them so I worry will wind up as bloat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358168
https://github.com/hail-is/hail/pull/7904#issuecomment-576358719:72,Availability,error,error,72,"> Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an allocateAndStoreString method, which takes care of both steps, potentially more efficiently, but also more ergonomically. PString.allocate probably shouldn't exist, then! The public interfaces to PString probably need to include the ability to copy value=>value (address) and to put a string in a region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719
https://github.com/hail-is/hail/pull/7904#issuecomment-576358719:115,Energy Efficiency,allocate,allocate,115,"> Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an allocateAndStoreString method, which takes care of both steps, potentially more efficiently, but also more ergonomically. PString.allocate probably shouldn't exist, then! The public interfaces to PString probably need to include the ability to copy value=>value (address) and to put a string in a region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719
https://github.com/hail-is/hail/pull/7904#issuecomment-576358719:213,Energy Efficiency,allocate,allocate,213,"> Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an allocateAndStoreString method, which takes care of both steps, potentially more efficiently, but also more ergonomically. PString.allocate probably shouldn't exist, then! The public interfaces to PString probably need to include the ability to copy value=>value (address) and to put a string in a region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719
https://github.com/hail-is/hail/pull/7904#issuecomment-576358719:329,Energy Efficiency,allocate,allocate,329,"> Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an allocateAndStoreString method, which takes care of both steps, potentially more efficiently, but also more ergonomically. PString.allocate probably shouldn't exist, then! The public interfaces to PString probably need to include the ability to copy value=>value (address) and to put a string in a region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719
https://github.com/hail-is/hail/pull/7904#issuecomment-576358719:462,Energy Efficiency,allocate,allocateAndStoreString,462,"> Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an allocateAndStoreString method, which takes care of both steps, potentially more efficiently, but also more ergonomically. PString.allocate probably shouldn't exist, then! The public interfaces to PString probably need to include the ability to copy value=>value (address) and to put a string in a region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719
https://github.com/hail-is/hail/pull/7904#issuecomment-576358719:542,Energy Efficiency,efficient,efficiently,542,"> Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an allocateAndStoreString method, which takes care of both steps, potentially more efficiently, but also more ergonomically. PString.allocate probably shouldn't exist, then! The public interfaces to PString probably need to include the ability to copy value=>value (address) and to put a string in a region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719
https://github.com/hail-is/hail/pull/7904#issuecomment-576358719:592,Energy Efficiency,allocate,allocate,592,"> Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an allocateAndStoreString method, which takes care of both steps, potentially more efficiently, but also more ergonomically. PString.allocate probably shouldn't exist, then! The public interfaces to PString probably need to include the ability to copy value=>value (address) and to put a string in a region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719
https://github.com/hail-is/hail/pull/7904#issuecomment-576358719:644,Integrability,interface,interfaces,644,"> Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an allocateAndStoreString method, which takes care of both steps, potentially more efficiently, but also more ergonomically. PString.allocate probably shouldn't exist, then! The public interfaces to PString probably need to include the ability to copy value=>value (address) and to put a string in a region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719
https://github.com/hail-is/hail/pull/7904#issuecomment-576360896:48,Energy Efficiency,allocate,allocateAndStoreString,48,"Tim, ok, to remove in this PR (in favor of the `allocateAndStoreString` method alone, which is parametrized on `Region` and `String`)? Don't want to surprise you during your review. ```scala; def allocate(region: Region, byteLength: Int): Long. def allocate(region: Code[Region], byteLength: Code[Int]): Code[Long]. def store(addr: Long, str: String). def store(addr: Code[Long], str: Code[String]): Code[Unit]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576360896
https://github.com/hail-is/hail/pull/7904#issuecomment-576360896:196,Energy Efficiency,allocate,allocate,196,"Tim, ok, to remove in this PR (in favor of the `allocateAndStoreString` method alone, which is parametrized on `Region` and `String`)? Don't want to surprise you during your review. ```scala; def allocate(region: Region, byteLength: Int): Long. def allocate(region: Code[Region], byteLength: Code[Int]): Code[Long]. def store(addr: Long, str: String). def store(addr: Code[Long], str: Code[String]): Code[Unit]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576360896
https://github.com/hail-is/hail/pull/7904#issuecomment-576360896:249,Energy Efficiency,allocate,allocate,249,"Tim, ok, to remove in this PR (in favor of the `allocateAndStoreString` method alone, which is parametrized on `Region` and `String`)? Don't want to surprise you during your review. ```scala; def allocate(region: Region, byteLength: Int): Long. def allocate(region: Code[Region], byteLength: Code[Int]): Code[Long]. def store(addr: Long, str: String). def store(addr: Code[Long], str: Code[String]): Code[Unit]; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576360896
https://github.com/hail-is/hail/pull/7904#issuecomment-576362658:132,Energy Efficiency,allocate,allocateAndStoreString,132,"> that seems fine. These aren't used outside of PString, right?. No longer. TakeByAggregatorSuite.scala and Functions.scala now use allocateAndStoreString",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576362658
https://github.com/hail-is/hail/pull/7904#issuecomment-576364613:10,Testability,assert,assertion,10,"> another assertion to remove. Could have missed others, this is a big diff. Will double check for others in all modified files, one min (I don't think I added any new ones however)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576364613
https://github.com/hail-is/hail/pull/7904#issuecomment-576366860:148,Testability,assert,assertions,148,"Tim, the only ones I've found are on copyFromType; I made a note in a previous PR (the remove region PR I believe) about potentially removing those assertions (where cast follows). Since this is already a large PR, I propose I PR their removal separately. What do you think",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576366860
https://github.com/hail-is/hail/pull/7909#issuecomment-575790241:32,Deployability,deploy,deploy,32,I'm going to test this with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7909#issuecomment-575790241
https://github.com/hail-is/hail/pull/7909#issuecomment-575790241:13,Testability,test,test,13,I'm going to test this with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7909#issuecomment-575790241
https://github.com/hail-is/hail/pull/7909#issuecomment-575794175:15,Deployability,deploy,deploy,15,Tests with dev deploy worked.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7909#issuecomment-575794175
https://github.com/hail-is/hail/pull/7909#issuecomment-575794175:0,Testability,Test,Tests,0,Tests with dev deploy worked.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7909#issuecomment-575794175
https://github.com/hail-is/hail/pull/7910#issuecomment-576348264:25,Deployability,deploy,deploy,25,Closing this while I dev deploy test it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7910#issuecomment-576348264
https://github.com/hail-is/hail/pull/7910#issuecomment-576348264:32,Testability,test,test,32,Closing this while I dev deploy test it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7910#issuecomment-576348264
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:1161,Modifiability,config,config,1161,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:1213,Modifiability,config,config,1213,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:1293,Modifiability,config,config,1293,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:8,Testability,test,test,8,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:28,Testability,Test,Test,28,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:94,Testability,Test,Test,94,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:161,Testability,test,test,161,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:177,Testability,Test,Test,177,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:182,Testability,test,testMethod,182,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:1263,Testability,assert,assert,1263,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:1307,Testability,Assert,AssertionError,1307,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:1429,Testability,assert,assert,1429,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:1467,Testability,test,test,1467,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575931099:1491,Testability,Assert,AssertionError,1491,"```; io/test/test_batch.py::Test::test_batch_create_validation FAILED; ______________________ Test.test_batch_create_validation _______________________. self = <test.test_batch.Test testMethod=test_batch_create_validation>. def test_batch_create_validation(self):; bad_configs = [; # unexpected field fleep; {'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz', 'fleep': 'quam'},; # billing project None/missing; {'billing_project': None, 'n_jobs': 5, 'token': 'baz'},; {'n_jobs': 5, 'token': 'baz'},; # n_jobs None/missing; {'billing_project': 'foo', 'n_jobs': None, 'token': 'baz'},; {'billing_project': 'foo', 'token': 'baz'},; # n_jobs wrong type; {'billing_project': 'foo', 'n_jobs': '5', 'token': 'baz'},; # token None/missing; {'billing_project': 'foo', 'n_jobs': 5, 'token': None},; {'billing_project': 'foo', 'n_jobs': 5},; # attribute key/value None; {'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; {'attributes': {'k': None}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'},; ]; url = deploy_config.url('batch', '/api/v1alpha/batches/create'); headers = service_auth_headers(deploy_config, 'batch'); for config in bad_configs:; r = requests.post(url, json=config, allow_redirects=True, headers=headers); > assert r.status_code == 400, (config, r); E AssertionError: ({'attributes': {None: 'v'}, 'billing_project': 'foo', 'n_jobs': 5, 'token': 'baz'}, <Response [403]>); E assert 403 == 400; E -403; E +400. io/test/test_batch.py:487: AssertionError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575931099
https://github.com/hail-is/hail/pull/7915#issuecomment-575941769:86,Modifiability,rewrite,rewrite,86,"Yeah, so it looks like nullable=False isn't working in a keyschema. I'm just going to rewrite this by hand like I did for jobs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575941769
https://github.com/hail-is/hail/pull/7915#issuecomment-575944836:362,Performance,load,loads,362,"OK, I figured out what was happening. The problem wasn't with cerberus (although I'm happy to with my change), it is that json.dump always converts a dictionary key into a string. I had with a key None, and it got turned into the string 'null' in json, because json object values must string keys:. ```; >>> import json; >>> d = {None: 5, 'foo': None}; >>> json.loads(json.dumps(d)); {'null': 5, 'foo': None}; ```. I remove the broken test. Note, I pushed two more changes that probably need a proper review:; - moved jobs validation to batch (from batch_client), I'd been meaning to do that,; - and I wrote the batch validator explicit in the style of the jobs validator (I'd be meaning to do that, too).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575944836
https://github.com/hail-is/hail/pull/7915#issuecomment-575944836:523,Security,validat,validation,523,"OK, I figured out what was happening. The problem wasn't with cerberus (although I'm happy to with my change), it is that json.dump always converts a dictionary key into a string. I had with a key None, and it got turned into the string 'null' in json, because json object values must string keys:. ```; >>> import json; >>> d = {None: 5, 'foo': None}; >>> json.loads(json.dumps(d)); {'null': 5, 'foo': None}; ```. I remove the broken test. Note, I pushed two more changes that probably need a proper review:; - moved jobs validation to batch (from batch_client), I'd been meaning to do that,; - and I wrote the batch validator explicit in the style of the jobs validator (I'd be meaning to do that, too).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575944836
https://github.com/hail-is/hail/pull/7915#issuecomment-575944836:618,Security,validat,validator,618,"OK, I figured out what was happening. The problem wasn't with cerberus (although I'm happy to with my change), it is that json.dump always converts a dictionary key into a string. I had with a key None, and it got turned into the string 'null' in json, because json object values must string keys:. ```; >>> import json; >>> d = {None: 5, 'foo': None}; >>> json.loads(json.dumps(d)); {'null': 5, 'foo': None}; ```. I remove the broken test. Note, I pushed two more changes that probably need a proper review:; - moved jobs validation to batch (from batch_client), I'd been meaning to do that,; - and I wrote the batch validator explicit in the style of the jobs validator (I'd be meaning to do that, too).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575944836
https://github.com/hail-is/hail/pull/7915#issuecomment-575944836:662,Security,validat,validator,662,"OK, I figured out what was happening. The problem wasn't with cerberus (although I'm happy to with my change), it is that json.dump always converts a dictionary key into a string. I had with a key None, and it got turned into the string 'null' in json, because json object values must string keys:. ```; >>> import json; >>> d = {None: 5, 'foo': None}; >>> json.loads(json.dumps(d)); {'null': 5, 'foo': None}; ```. I remove the broken test. Note, I pushed two more changes that probably need a proper review:; - moved jobs validation to batch (from batch_client), I'd been meaning to do that,; - and I wrote the batch validator explicit in the style of the jobs validator (I'd be meaning to do that, too).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575944836
https://github.com/hail-is/hail/pull/7915#issuecomment-575944836:435,Testability,test,test,435,"OK, I figured out what was happening. The problem wasn't with cerberus (although I'm happy to with my change), it is that json.dump always converts a dictionary key into a string. I had with a key None, and it got turned into the string 'null' in json, because json object values must string keys:. ```; >>> import json; >>> d = {None: 5, 'foo': None}; >>> json.loads(json.dumps(d)); {'null': 5, 'foo': None}; ```. I remove the broken test. Note, I pushed two more changes that probably need a proper review:; - moved jobs validation to batch (from batch_client), I'd been meaning to do that,; - and I wrote the batch validator explicit in the style of the jobs validator (I'd be meaning to do that, too).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7915#issuecomment-575944836
https://github.com/hail-is/hail/pull/7916#issuecomment-575914242:23,Availability,error,error,23,You have a file naming error in build.yaml.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7916#issuecomment-575914242
https://github.com/hail-is/hail/pull/7927#issuecomment-576372543:827,Integrability,interface,interface,827,"sorry, I didn't quite understand your last comment. Suppose we have a `PFixedLengthArray(length: Int, element: PType, required: Boolean)`. We need an algorithm that will have the following properties:; ```; unify(; PCanonicalArray(PInt32Required, true),; PFixedLengthArray(1, PInt32Required, false)); returns PCanonicalArray(PInt32Required, false); ```. ```; unify(; PFixedLengthArray(1, PInt32Required, false)); PCanonicalArray(PInt32Required, true),; returns PCanonicalArray(PInt32Required, false); ```. ```; unify(; PFixedLengthArray(1, PInt32Required, false),; PFixedLengthArray(1, PInt32Required, true)); returns PFixedLengthArray(1, PInt32Required, false); ```. calling something on the head is wrong, as is matching on the head alone. This is going to be a complicated thing, and I want all the logic (at least for each interface like PArray) in the same place.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7927#issuecomment-576372543
https://github.com/hail-is/hail/pull/7927#issuecomment-576372543:802,Testability,log,logic,802,"sorry, I didn't quite understand your last comment. Suppose we have a `PFixedLengthArray(length: Int, element: PType, required: Boolean)`. We need an algorithm that will have the following properties:; ```; unify(; PCanonicalArray(PInt32Required, true),; PFixedLengthArray(1, PInt32Required, false)); returns PCanonicalArray(PInt32Required, false); ```. ```; unify(; PFixedLengthArray(1, PInt32Required, false)); PCanonicalArray(PInt32Required, true),; returns PCanonicalArray(PInt32Required, false); ```. ```; unify(; PFixedLengthArray(1, PInt32Required, false),; PFixedLengthArray(1, PInt32Required, true)); returns PFixedLengthArray(1, PInt32Required, false); ```. calling something on the head is wrong, as is matching on the head alone. This is going to be a complicated thing, and I want all the logic (at least for each interface like PArray) in the same place.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7927#issuecomment-576372543
https://github.com/hail-is/hail/pull/7933#issuecomment-576842320:0,Availability,Failure,Failure,0,"Failure in image_fetcher_image:. ```; Traceback (most recent call last):; File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 279, in run; await docker_call_retry(self.container.start); File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 86, in docker_call_retry; return await f(*args, **kwargs); File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start; data=kwargs; File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'read unix @->@/containerd-shim/moby/7f911041e4fcc5ea78b6b3979d1232d0954193bced0e1e85acd2133b80cc463e/shim.sock: read: connection reset by peer: unknown'); ```. I will add this to the retry list. Another reason to drop docker ASAP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933#issuecomment-576842320
https://github.com/hail-is/hail/pull/7933#issuecomment-576842320:534,Performance,load,loads,534,"Failure in image_fetcher_image:. ```; Traceback (most recent call last):; File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 279, in run; await docker_call_retry(self.container.start); File \""/usr/local/lib/python3.6/site-packages/batch/worker.py\"", line 86, in docker_call_retry; return await f(*args, **kwargs); File \""/usr/local/lib/python3.6/site-packages/aiodocker/containers.py\"", line 188, in start; data=kwargs; File \""/usr/local/lib/python3.6/site-packages/aiodocker/docker.py\"", line 166, in _query; json.loads(what.decode('utf8'))); aiodocker.exceptions.DockerError: DockerError(500, 'read unix @->@/containerd-shim/moby/7f911041e4fcc5ea78b6b3979d1232d0954193bced0e1e85acd2133b80cc463e/shim.sock: read: connection reset by peer: unknown'); ```. I will add this to the retry list. Another reason to drop docker ASAP.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933#issuecomment-576842320
https://github.com/hail-is/hail/pull/7933#issuecomment-577704626:167,Deployability,update,updated,167,"OK, I think this is ready to go.; - Migration tested with passing colors (!); - I disabled the check incremental loop (but left the code in for future debugging); - I updated estimated-current.txt with the latest from improve-cancel.sql",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933#issuecomment-577704626
https://github.com/hail-is/hail/pull/7933#issuecomment-577704626:46,Testability,test,tested,46,"OK, I think this is ready to go.; - Migration tested with passing colors (!); - I disabled the check incremental loop (but left the code in for future debugging); - I updated estimated-current.txt with the latest from improve-cancel.sql",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7933#issuecomment-577704626
https://github.com/hail-is/hail/pull/7934#issuecomment-576899438:21,Testability,benchmark,benchmark,21,We spend 77% of this benchmark in LZ4 compress HC. Big oof.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7934#issuecomment-576899438
https://github.com/hail-is/hail/pull/7949#issuecomment-578182332:74,Energy Efficiency,schedul,scheduling,74,"> I don't see where the instance version gets checked. I assume if you're scheduling a v2 batch, you should check it is a v2 instance, no? Just to confirm the migration plan: we'll merge this, then once all the instances have flipped to v2, then we make another PR that changes the format to v2?. I thought we discussed this and decided we'll merge, wait for instances to flip to 2, and then merge the second pr to flip the batch format to 2. > Want to confirm the testing plan. You've tested this with batch format v1 (this PR), batch format v2, and tested the migration (that is, switching from master to this PR with batch format v1). I tested it from v1 -> v2. Forgot to test switching from master to v1. Will do once you're happy with the code, and will repeat the other tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-578182332
https://github.com/hail-is/hail/pull/7949#issuecomment-578182332:465,Testability,test,testing,465,"> I don't see where the instance version gets checked. I assume if you're scheduling a v2 batch, you should check it is a v2 instance, no? Just to confirm the migration plan: we'll merge this, then once all the instances have flipped to v2, then we make another PR that changes the format to v2?. I thought we discussed this and decided we'll merge, wait for instances to flip to 2, and then merge the second pr to flip the batch format to 2. > Want to confirm the testing plan. You've tested this with batch format v1 (this PR), batch format v2, and tested the migration (that is, switching from master to this PR with batch format v1). I tested it from v1 -> v2. Forgot to test switching from master to v1. Will do once you're happy with the code, and will repeat the other tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-578182332
https://github.com/hail-is/hail/pull/7949#issuecomment-578182332:486,Testability,test,tested,486,"> I don't see where the instance version gets checked. I assume if you're scheduling a v2 batch, you should check it is a v2 instance, no? Just to confirm the migration plan: we'll merge this, then once all the instances have flipped to v2, then we make another PR that changes the format to v2?. I thought we discussed this and decided we'll merge, wait for instances to flip to 2, and then merge the second pr to flip the batch format to 2. > Want to confirm the testing plan. You've tested this with batch format v1 (this PR), batch format v2, and tested the migration (that is, switching from master to this PR with batch format v1). I tested it from v1 -> v2. Forgot to test switching from master to v1. Will do once you're happy with the code, and will repeat the other tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-578182332
https://github.com/hail-is/hail/pull/7949#issuecomment-578182332:551,Testability,test,tested,551,"> I don't see where the instance version gets checked. I assume if you're scheduling a v2 batch, you should check it is a v2 instance, no? Just to confirm the migration plan: we'll merge this, then once all the instances have flipped to v2, then we make another PR that changes the format to v2?. I thought we discussed this and decided we'll merge, wait for instances to flip to 2, and then merge the second pr to flip the batch format to 2. > Want to confirm the testing plan. You've tested this with batch format v1 (this PR), batch format v2, and tested the migration (that is, switching from master to this PR with batch format v1). I tested it from v1 -> v2. Forgot to test switching from master to v1. Will do once you're happy with the code, and will repeat the other tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-578182332
https://github.com/hail-is/hail/pull/7949#issuecomment-578182332:640,Testability,test,tested,640,"> I don't see where the instance version gets checked. I assume if you're scheduling a v2 batch, you should check it is a v2 instance, no? Just to confirm the migration plan: we'll merge this, then once all the instances have flipped to v2, then we make another PR that changes the format to v2?. I thought we discussed this and decided we'll merge, wait for instances to flip to 2, and then merge the second pr to flip the batch format to 2. > Want to confirm the testing plan. You've tested this with batch format v1 (this PR), batch format v2, and tested the migration (that is, switching from master to this PR with batch format v1). I tested it from v1 -> v2. Forgot to test switching from master to v1. Will do once you're happy with the code, and will repeat the other tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-578182332
https://github.com/hail-is/hail/pull/7949#issuecomment-578182332:675,Testability,test,test,675,"> I don't see where the instance version gets checked. I assume if you're scheduling a v2 batch, you should check it is a v2 instance, no? Just to confirm the migration plan: we'll merge this, then once all the instances have flipped to v2, then we make another PR that changes the format to v2?. I thought we discussed this and decided we'll merge, wait for instances to flip to 2, and then merge the second pr to flip the batch format to 2. > Want to confirm the testing plan. You've tested this with batch format v1 (this PR), batch format v2, and tested the migration (that is, switching from master to this PR with batch format v1). I tested it from v1 -> v2. Forgot to test switching from master to v1. Will do once you're happy with the code, and will repeat the other tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-578182332
https://github.com/hail-is/hail/pull/7949#issuecomment-578182332:776,Testability,test,tests,776,"> I don't see where the instance version gets checked. I assume if you're scheduling a v2 batch, you should check it is a v2 instance, no? Just to confirm the migration plan: we'll merge this, then once all the instances have flipped to v2, then we make another PR that changes the format to v2?. I thought we discussed this and decided we'll merge, wait for instances to flip to 2, and then merge the second pr to flip the batch format to 2. > Want to confirm the testing plan. You've tested this with batch format v1 (this PR), batch format v2, and tested the migration (that is, switching from master to this PR with batch format v1). I tested it from v1 -> v2. Forgot to test switching from master to v1. Will do once you're happy with the code, and will repeat the other tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-578182332
https://github.com/hail-is/hail/pull/7949#issuecomment-578296538:14,Integrability,interface,interface,14,"I changed the interface for `Batch.jobs()` to return an iterator of Job objects rather than statuses that were partial. I think this makes more sense to me. To do that, I got rid of parent_ids on Job because they weren't used anywhere and were misleading because we never return the parent_ids when creating Job objects.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-578296538
https://github.com/hail-is/hail/pull/7949#issuecomment-579013590:42,Deployability,deploy,deploy,42,I'm closing this while debugging with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-579013590
https://github.com/hail-is/hail/pull/7949#issuecomment-579919520:96,Availability,error,error,96,"I updated the ""before attempts"" trigger because there was a bug where the start and end time on error (i.e. create fails) are both None and then when the instance gets deactivated, the reason is overwritten to deactivated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-579919520
https://github.com/hail-is/hail/pull/7949#issuecomment-579919520:2,Deployability,update,updated,2,"I updated the ""before attempts"" trigger because there was a bug where the start and end time on error (i.e. create fails) are both None and then when the instance gets deactivated, the reason is overwritten to deactivated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-579919520
https://github.com/hail-is/hail/pull/7950#issuecomment-577814309:159,Availability,down,download,159,"Related: if you host the pdf within docs, we could keep users within the docs url, while also giving them the use of their native PDF viewer (which will allow download). I think that's neater. Actually, this should be possible with the raw link, not sure why my browser prompts me to immediately download. edit: Ah, this is why: https://webapps.stackexchange.com/questions/48061/can-i-trick-github-into-displaying-the-pdf-in-the-browser-instead-of-downloading. Github places a header to prevent this (content-disposition: attachment). We should just host this file",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7950#issuecomment-577814309
https://github.com/hail-is/hail/pull/7950#issuecomment-577814309:296,Availability,down,download,296,"Related: if you host the pdf within docs, we could keep users within the docs url, while also giving them the use of their native PDF viewer (which will allow download). I think that's neater. Actually, this should be possible with the raw link, not sure why my browser prompts me to immediately download. edit: Ah, this is why: https://webapps.stackexchange.com/questions/48061/can-i-trick-github-into-displaying-the-pdf-in-the-browser-instead-of-downloading. Github places a header to prevent this (content-disposition: attachment). We should just host this file",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7950#issuecomment-577814309
https://github.com/hail-is/hail/pull/7950#issuecomment-577814309:448,Availability,down,downloading,448,"Related: if you host the pdf within docs, we could keep users within the docs url, while also giving them the use of their native PDF viewer (which will allow download). I think that's neater. Actually, this should be possible with the raw link, not sure why my browser prompts me to immediately download. edit: Ah, this is why: https://webapps.stackexchange.com/questions/48061/can-i-trick-github-into-displaying-the-pdf-in-the-browser-instead-of-downloading. Github places a header to prevent this (content-disposition: attachment). We should just host this file",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7950#issuecomment-577814309
https://github.com/hail-is/hail/pull/7950#issuecomment-577856190:223,Deployability,release,release,223,"Fixed, thanks for the push, that was easy to do and is so much better than the old thing. I can't delete the powerpoints from their old location on github yet since the current website still links to them, but after 0.2.32 release I'll delete the old ones.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7950#issuecomment-577856190
https://github.com/hail-is/hail/pull/7950#issuecomment-577856190:109,Energy Efficiency,power,powerpoints,109,"Fixed, thanks for the push, that was easy to do and is so much better than the old thing. I can't delete the powerpoints from their old location on github yet since the current website still links to them, but after 0.2.32 release I'll delete the old ones.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7950#issuecomment-577856190
https://github.com/hail-is/hail/pull/7951#issuecomment-580336498:99,Testability,test,testing,99,@chrisvittal I won't have time to do any work on this PR for the next two weeks. Can you take over testing + benchmarking?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7951#issuecomment-580336498
https://github.com/hail-is/hail/pull/7951#issuecomment-580336498:109,Testability,benchmark,benchmarking,109,@chrisvittal I won't have time to do any work on this PR for the next two weeks. Can you take over testing + benchmarking?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7951#issuecomment-580336498
https://github.com/hail-is/hail/pull/7952#issuecomment-577853956:333,Availability,checkpoint,checkpointed,333,"> Ideally, I'd release the references to the regions of my producer once I finished constructing the new RegionValue. I think references from a region to other regions should be treated the same as data directly contained in the region. In particular, in the current model it all gets freed at the same time. If we move to a stacked/checkpointed model like Cotton suggested, a checkpoint would remember both where in the region to move back to, and where in the list of region dependencies to move back to, releasing those regions to the right of that point.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-577853956
https://github.com/hail-is/hail/pull/7952#issuecomment-577853956:377,Availability,checkpoint,checkpoint,377,"> Ideally, I'd release the references to the regions of my producer once I finished constructing the new RegionValue. I think references from a region to other regions should be treated the same as data directly contained in the region. In particular, in the current model it all gets freed at the same time. If we move to a stacked/checkpointed model like Cotton suggested, a checkpoint would remember both where in the region to move back to, and where in the list of region dependencies to move back to, releasing those regions to the right of that point.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-577853956
https://github.com/hail-is/hail/pull/7952#issuecomment-577853956:15,Deployability,release,release,15,"> Ideally, I'd release the references to the regions of my producer once I finished constructing the new RegionValue. I think references from a region to other regions should be treated the same as data directly contained in the region. In particular, in the current model it all gets freed at the same time. If we move to a stacked/checkpointed model like Cotton suggested, a checkpoint would remember both where in the region to move back to, and where in the list of region dependencies to move back to, releasing those regions to the right of that point.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-577853956
https://github.com/hail-is/hail/pull/7952#issuecomment-577853956:477,Integrability,depend,dependencies,477,"> Ideally, I'd release the references to the regions of my producer once I finished constructing the new RegionValue. I think references from a region to other regions should be treated the same as data directly contained in the region. In particular, in the current model it all gets freed at the same time. If we move to a stacked/checkpointed model like Cotton suggested, a checkpoint would remember both where in the region to move back to, and where in the list of region dependencies to move back to, releasing those regions to the right of that point.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-577853956
https://github.com/hail-is/hail/pull/7952#issuecomment-578798366:386,Deployability,release,release,386,"I don't think this keeps too much garbage in memory. Your next method extracts exactly the data it needs from its producer. No garbage there, you asked for only data you absolutely need. You stated (via `addReferenceTo`) that your region references these child regions, so that memory must be accessible at least as long as your region is accessible. Whoever is consuming your data can release all this memory by clearing the region you're using. The only nodes which should be clearing are folks who call `next` multiple times *and don't need that data to have the same lifetime*. This is true for filter, only surviving values must live, other values' lifetimes may end when we discover they fail the filter condition. It's also true for `write` because after one value is dumped into a file, it is no longer needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366
https://github.com/hail-is/hail/pull/7952#issuecomment-578798366:293,Security,access,accessible,293,"I don't think this keeps too much garbage in memory. Your next method extracts exactly the data it needs from its producer. No garbage there, you asked for only data you absolutely need. You stated (via `addReferenceTo`) that your region references these child regions, so that memory must be accessible at least as long as your region is accessible. Whoever is consuming your data can release all this memory by clearing the region you're using. The only nodes which should be clearing are folks who call `next` multiple times *and don't need that data to have the same lifetime*. This is true for filter, only surviving values must live, other values' lifetimes may end when we discover they fail the filter condition. It's also true for `write` because after one value is dumped into a file, it is no longer needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366
https://github.com/hail-is/hail/pull/7952#issuecomment-578798366:339,Security,access,accessible,339,"I don't think this keeps too much garbage in memory. Your next method extracts exactly the data it needs from its producer. No garbage there, you asked for only data you absolutely need. You stated (via `addReferenceTo`) that your region references these child regions, so that memory must be accessible at least as long as your region is accessible. Whoever is consuming your data can release all this memory by clearing the region you're using. The only nodes which should be clearing are folks who call `next` multiple times *and don't need that data to have the same lifetime*. This is true for filter, only surviving values must live, other values' lifetimes may end when we discover they fail the filter condition. It's also true for `write` because after one value is dumped into a file, it is no longer needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366
https://github.com/hail-is/hail/pull/7952#issuecomment-578798366:413,Usability,clear,clearing,413,"I don't think this keeps too much garbage in memory. Your next method extracts exactly the data it needs from its producer. No garbage there, you asked for only data you absolutely need. You stated (via `addReferenceTo`) that your region references these child regions, so that memory must be accessible at least as long as your region is accessible. Whoever is consuming your data can release all this memory by clearing the region you're using. The only nodes which should be clearing are folks who call `next` multiple times *and don't need that data to have the same lifetime*. This is true for filter, only surviving values must live, other values' lifetimes may end when we discover they fail the filter condition. It's also true for `write` because after one value is dumped into a file, it is no longer needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366
https://github.com/hail-is/hail/pull/7952#issuecomment-578798366:478,Usability,clear,clearing,478,"I don't think this keeps too much garbage in memory. Your next method extracts exactly the data it needs from its producer. No garbage there, you asked for only data you absolutely need. You stated (via `addReferenceTo`) that your region references these child regions, so that memory must be accessible at least as long as your region is accessible. Whoever is consuming your data can release all this memory by clearing the region you're using. The only nodes which should be clearing are folks who call `next` multiple times *and don't need that data to have the same lifetime*. This is true for filter, only surviving values must live, other values' lifetimes may end when we discover they fail the filter condition. It's also true for `write` because after one value is dumped into a file, it is no longer needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366
https://github.com/hail-is/hail/pull/7958#issuecomment-578199551:0,Testability,Test,Test,0,"Test fails on the inference difference of the inner PArray:. This fails:; ```scala; destType = PStruct(false, ""foo"" -> PStruct(""bar"" -> PArray(PInt32(false), true))); ```; This makes it pass:; ```scala; destType = PStruct(false, ""foo"" -> PStruct(""bar"" -> PArray(PInt32(false), true))); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958#issuecomment-578199551
https://github.com/hail-is/hail/pull/7958#issuecomment-578204878:16,Testability,test,test,16,"Tim, bug in the test fixed...but I'm not entirely clear why it should have caused an issue yet. In PBaseStruct.copyFromType, I was calling srcFieldType.storeShallowAtOffset instead of dstFieldType.storeShallowAtOffset, in a case where srcFieldType was +PCArray and dstFieldType was PCArray, aka:. ```scala; srcFieldType: +PCArray[+PInt32], dstFieldType: PCArray[PInt32]; ```. Where the invocation is:. ```scala; srcFieldType.storeShallowAtOffset(; this.fieldOffset(dstStructAddress, dstField.index),; dstFieldType.copyFromType(...); ```. The storeShallowAtOffset function on PCArray is stateless and identical between required and non-required PCArray instantiations:. ```scala; def storeShallowAtOffset(dstAddress: Code[Long], valueAddress: Code[Long]): Code[Unit] =; Region.storeAddress(dstAddress, valueAddress); ```. I don't have a clear idea why this issue occurred. Also, clearly not easily triggered, required PStruct(""bar"" -> PArray(PInt32(true),false) dest and PStruct(""bar"" -> PArray(PInt32(true),true) source, having the ""bar"" field be a primitive wouldn't do it (we had those tests)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958#issuecomment-578204878
https://github.com/hail-is/hail/pull/7958#issuecomment-578204878:1088,Testability,test,tests,1088,"Tim, bug in the test fixed...but I'm not entirely clear why it should have caused an issue yet. In PBaseStruct.copyFromType, I was calling srcFieldType.storeShallowAtOffset instead of dstFieldType.storeShallowAtOffset, in a case where srcFieldType was +PCArray and dstFieldType was PCArray, aka:. ```scala; srcFieldType: +PCArray[+PInt32], dstFieldType: PCArray[PInt32]; ```. Where the invocation is:. ```scala; srcFieldType.storeShallowAtOffset(; this.fieldOffset(dstStructAddress, dstField.index),; dstFieldType.copyFromType(...); ```. The storeShallowAtOffset function on PCArray is stateless and identical between required and non-required PCArray instantiations:. ```scala; def storeShallowAtOffset(dstAddress: Code[Long], valueAddress: Code[Long]): Code[Unit] =; Region.storeAddress(dstAddress, valueAddress); ```. I don't have a clear idea why this issue occurred. Also, clearly not easily triggered, required PStruct(""bar"" -> PArray(PInt32(true),false) dest and PStruct(""bar"" -> PArray(PInt32(true),true) source, having the ""bar"" field be a primitive wouldn't do it (we had those tests)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958#issuecomment-578204878
https://github.com/hail-is/hail/pull/7958#issuecomment-578204878:50,Usability,clear,clear,50,"Tim, bug in the test fixed...but I'm not entirely clear why it should have caused an issue yet. In PBaseStruct.copyFromType, I was calling srcFieldType.storeShallowAtOffset instead of dstFieldType.storeShallowAtOffset, in a case where srcFieldType was +PCArray and dstFieldType was PCArray, aka:. ```scala; srcFieldType: +PCArray[+PInt32], dstFieldType: PCArray[PInt32]; ```. Where the invocation is:. ```scala; srcFieldType.storeShallowAtOffset(; this.fieldOffset(dstStructAddress, dstField.index),; dstFieldType.copyFromType(...); ```. The storeShallowAtOffset function on PCArray is stateless and identical between required and non-required PCArray instantiations:. ```scala; def storeShallowAtOffset(dstAddress: Code[Long], valueAddress: Code[Long]): Code[Unit] =; Region.storeAddress(dstAddress, valueAddress); ```. I don't have a clear idea why this issue occurred. Also, clearly not easily triggered, required PStruct(""bar"" -> PArray(PInt32(true),false) dest and PStruct(""bar"" -> PArray(PInt32(true),true) source, having the ""bar"" field be a primitive wouldn't do it (we had those tests)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958#issuecomment-578204878
https://github.com/hail-is/hail/pull/7958#issuecomment-578204878:836,Usability,clear,clear,836,"Tim, bug in the test fixed...but I'm not entirely clear why it should have caused an issue yet. In PBaseStruct.copyFromType, I was calling srcFieldType.storeShallowAtOffset instead of dstFieldType.storeShallowAtOffset, in a case where srcFieldType was +PCArray and dstFieldType was PCArray, aka:. ```scala; srcFieldType: +PCArray[+PInt32], dstFieldType: PCArray[PInt32]; ```. Where the invocation is:. ```scala; srcFieldType.storeShallowAtOffset(; this.fieldOffset(dstStructAddress, dstField.index),; dstFieldType.copyFromType(...); ```. The storeShallowAtOffset function on PCArray is stateless and identical between required and non-required PCArray instantiations:. ```scala; def storeShallowAtOffset(dstAddress: Code[Long], valueAddress: Code[Long]): Code[Unit] =; Region.storeAddress(dstAddress, valueAddress); ```. I don't have a clear idea why this issue occurred. Also, clearly not easily triggered, required PStruct(""bar"" -> PArray(PInt32(true),false) dest and PStruct(""bar"" -> PArray(PInt32(true),true) source, having the ""bar"" field be a primitive wouldn't do it (we had those tests)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958#issuecomment-578204878
https://github.com/hail-is/hail/pull/7958#issuecomment-578204878:878,Usability,clear,clearly,878,"Tim, bug in the test fixed...but I'm not entirely clear why it should have caused an issue yet. In PBaseStruct.copyFromType, I was calling srcFieldType.storeShallowAtOffset instead of dstFieldType.storeShallowAtOffset, in a case where srcFieldType was +PCArray and dstFieldType was PCArray, aka:. ```scala; srcFieldType: +PCArray[+PInt32], dstFieldType: PCArray[PInt32]; ```. Where the invocation is:. ```scala; srcFieldType.storeShallowAtOffset(; this.fieldOffset(dstStructAddress, dstField.index),; dstFieldType.copyFromType(...); ```. The storeShallowAtOffset function on PCArray is stateless and identical between required and non-required PCArray instantiations:. ```scala; def storeShallowAtOffset(dstAddress: Code[Long], valueAddress: Code[Long]): Code[Unit] =; Region.storeAddress(dstAddress, valueAddress); ```. I don't have a clear idea why this issue occurred. Also, clearly not easily triggered, required PStruct(""bar"" -> PArray(PInt32(true),false) dest and PStruct(""bar"" -> PArray(PInt32(true),true) source, having the ""bar"" field be a primitive wouldn't do it (we had those tests)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958#issuecomment-578204878
https://github.com/hail-is/hail/pull/7958#issuecomment-578213912:22,Testability,test,tests,22,"ready to look at, all tests pass. open to suggestions for the name, not happy with it. would prefer to just rename the new function, smaller change, limited time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958#issuecomment-578213912
https://github.com/hail-is/hail/pull/7958#issuecomment-578252764:20,Testability,test,tests,20,"Failing some python tests, I had only run Scala tests, which all passed, will look Tuesday",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958#issuecomment-578252764
https://github.com/hail-is/hail/pull/7958#issuecomment-578252764:48,Testability,test,tests,48,"Failing some python tests, I had only run Scala tests, which all passed, will look Tuesday",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7958#issuecomment-578252764
https://github.com/hail-is/hail/pull/7959#issuecomment-578509253:37,Availability,error,errors,37,"This comes up with a number of match errors on Agg IR. Wanted to check that these should be implemented in InferPType, because my impression was that relation IR would not be.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-578509253
https://github.com/hail-is/hail/pull/7959#issuecomment-578544609:522,Availability,error,errors,522,"Ah, I thought that besides MatrixIR, TableIR, and BlockMatrixIR, aggregations were included (since I saw these as being operations over rows/columns. What is the definition of a relational ir? Brief search didn't yield much, just the concept of relational operators, which appear to be too general to apply here (==, >= are relational operator). Besides various Agg* irs, we also have CollectDistributedArray, and a bunch of others. I will add the missing ones (that are present in InferType), until I no longer get match errors, and we can review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-578544609
https://github.com/hail-is/hail/pull/7959#issuecomment-579992599:275,Availability,failure,failures,275,"Tim, . This actually isn't bad. I took a stab at the aggregators, and in the case of those that need InitOps and SeqOps, used the existing AggSignature `toPhysical` instance method, which seems to do about the right thing, with minor modification. Also caught some Inference failures! Still have one more to fix, then check over any missing nodes (RunAgg is missing, have a placeholder comment for that, which I'll fill in now.). Only 1 test failing in IRSuite! That is ArrayFold2, which is just a bug we didn't catch before, and has nothing to do with new nodes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-579992599
https://github.com/hail-is/hail/pull/7959#issuecomment-579992599:437,Testability,test,test,437,"Tim, . This actually isn't bad. I took a stab at the aggregators, and in the case of those that need InitOps and SeqOps, used the existing AggSignature `toPhysical` instance method, which seems to do about the right thing, with minor modification. Also caught some Inference failures! Still have one more to fix, then check over any missing nodes (RunAgg is missing, have a placeholder comment for that, which I'll fill in now.). Only 1 test failing in IRSuite! That is ArrayFold2, which is just a bug we didn't catch before, and has nothing to do with new nodes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-579992599
https://github.com/hail-is/hail/pull/7959#issuecomment-580018794:4,Testability,test,tests,4,JVM tests pass,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-580018794
https://github.com/hail-is/hail/pull/7959#issuecomment-580019710:7,Testability,test,tests,7,python tests are fucked. will look tomorrow.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-580019710
https://github.com/hail-is/hail/pull/7959#issuecomment-580063888:0,Testability,test,tests,0,"tests all passing (python and jvm), ready to look.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-580063888
https://github.com/hail-is/hail/pull/7959#issuecomment-580073608:27,Integrability,depend,depend,27,"> For ArrayMap2, should it depend l || r requiredeness?. yep!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7959#issuecomment-580073608
https://github.com/hail-is/hail/pull/7962#issuecomment-578311507:18,Testability,benchmark,benchmark,18,Definitely need a benchmark. Will write one. How long is a benchmark allowed to take?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578311507
https://github.com/hail-is/hail/pull/7962#issuecomment-578311507:59,Testability,benchmark,benchmark,59,Definitely need a benchmark. Will write one. How long is a benchmark allowed to take?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578311507
https://github.com/hail-is/hail/pull/7962#issuecomment-578324936:14,Safety,timeout,timeout,14,"There's a 30m timeout. Also things are a bit slower on batch than on your laptop, I'd shoot for a couple minutes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578324936
https://github.com/hail-is/hail/pull/7962#issuecomment-578331901:19,Testability,benchmark,benchmark,19,"OK, I added a 1min benchmark",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578331901
https://github.com/hail-is/hail/pull/7962#issuecomment-578338479:33,Testability,benchmark,benchmark,33,I should think more about how to benchmark this properly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578338479
https://github.com/hail-is/hail/pull/7962#issuecomment-578344916:259,Deployability,install,install,259,"I reran the benchmarks with nothing else running on my laptop. I think this should work right?. ```; git checkout pc-relate && \; git status && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big && \; git checkout master && \; (cd ../../hail && make install) && \; git status && \; git checkout pc-relate && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big; ```; Benchmark should import the installed hail. pc-relate branch:; ```; 2020-01-24 18:38:08,147: INFO: burn in: 30.09s; 2020-01-24 18:38:35,904: INFO: run 1: 27.75s; 2020-01-24 18:39:03,001: INFO: run 2: 27.09s; 2020-01-24 18:39:29,144: INFO: run 3: 26.14s; ```; master:; ```; 2020-01-24 18:41:08,254: INFO: burn in: 32.71s; 2020-01-24 18:41:37,239: INFO: run 1: 28.98s; 2020-01-24 18:42:05,393: INFO: run 2: 28.15s; 2020-01-24 18:42:33,411: INFO: run 3: 28.01s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578344916
https://github.com/hail-is/hail/pull/7962#issuecomment-578344916:411,Deployability,install,installed,411,"I reran the benchmarks with nothing else running on my laptop. I think this should work right?. ```; git checkout pc-relate && \; git status && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big && \; git checkout master && \; (cd ../../hail && make install) && \; git status && \; git checkout pc-relate && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big; ```; Benchmark should import the installed hail. pc-relate branch:; ```; 2020-01-24 18:38:08,147: INFO: burn in: 30.09s; 2020-01-24 18:38:35,904: INFO: run 1: 27.75s; 2020-01-24 18:39:03,001: INFO: run 2: 27.09s; 2020-01-24 18:39:29,144: INFO: run 3: 26.14s; ```; master:; ```; 2020-01-24 18:41:08,254: INFO: burn in: 32.71s; 2020-01-24 18:41:37,239: INFO: run 1: 28.98s; 2020-01-24 18:42:05,393: INFO: run 2: 28.15s; 2020-01-24 18:42:33,411: INFO: run 3: 28.01s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578344916
https://github.com/hail-is/hail/pull/7962#issuecomment-578344916:12,Testability,benchmark,benchmarks,12,"I reran the benchmarks with nothing else running on my laptop. I think this should work right?. ```; git checkout pc-relate && \; git status && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big && \; git checkout master && \; (cd ../../hail && make install) && \; git status && \; git checkout pc-relate && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big; ```; Benchmark should import the installed hail. pc-relate branch:; ```; 2020-01-24 18:38:08,147: INFO: burn in: 30.09s; 2020-01-24 18:38:35,904: INFO: run 1: 27.75s; 2020-01-24 18:39:03,001: INFO: run 2: 27.09s; 2020-01-24 18:39:29,144: INFO: run 3: 26.14s; ```; master:; ```; 2020-01-24 18:41:08,254: INFO: burn in: 32.71s; 2020-01-24 18:41:37,239: INFO: run 1: 28.98s; 2020-01-24 18:42:05,393: INFO: run 2: 28.15s; 2020-01-24 18:42:33,411: INFO: run 3: 28.01s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578344916
https://github.com/hail-is/hail/pull/7962#issuecomment-578344916:383,Testability,Benchmark,Benchmark,383,"I reran the benchmarks with nothing else running on my laptop. I think this should work right?. ```; git checkout pc-relate && \; git status && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big && \; git checkout master && \; (cd ../../hail && make install) && \; git status && \; git checkout pc-relate && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big; ```; Benchmark should import the installed hail. pc-relate branch:; ```; 2020-01-24 18:38:08,147: INFO: burn in: 30.09s; 2020-01-24 18:38:35,904: INFO: run 1: 27.75s; 2020-01-24 18:39:03,001: INFO: run 2: 27.09s; 2020-01-24 18:39:29,144: INFO: run 3: 26.14s; ```; master:; ```; 2020-01-24 18:41:08,254: INFO: burn in: 32.71s; 2020-01-24 18:41:37,239: INFO: run 1: 28.98s; 2020-01-24 18:42:05,393: INFO: run 2: 28.15s; 2020-01-24 18:42:33,411: INFO: run 3: 28.01s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578344916
https://github.com/hail-is/hail/pull/7964#issuecomment-578799305:51,Usability,undo,undocumented,51,"I think Tim is right and that seems better than an undocumented hidden option, though I admit it's about as undocumented and hidden.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7964#issuecomment-578799305
https://github.com/hail-is/hail/pull/7964#issuecomment-578799305:108,Usability,undo,undocumented,108,"I think Tim is right and that seems better than an undocumented hidden option, though I admit it's about as undocumented and hidden.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7964#issuecomment-578799305
https://github.com/hail-is/hail/pull/7969#issuecomment-578475115:4,Testability,test,tests,4,all tests pass (including pytest),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-578475115
https://github.com/hail-is/hail/pull/7969#issuecomment-578549818:88,Testability,test,tests,88,"Ready to look at. TypeCheck modified more extensively than strictly needed for existing tests (only needed MakeArray modification here), because we shouldn't be relying on requiredeness for any virtual type, so this acts as 1) a more correct implementation 2) a helpful check than we aren't abusing requiredeness, in a relatively small PR. When we remove requiredeness from Type, I will either alias isOfType to leftTyp == rightTyp, or find/replace these back to ==.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-578549818
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:2646,Availability,failure,failure,2646,"in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in parse; ir_map); /miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('xro1961', <py4j.java_gateway.GatewayClient object at 0x7ffa62e9e390>, 'z:is.hail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])'; tpl = JavaObject id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])\n\tat is.hail.expr.ir.IR$class.typ(IR....a:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: NoSuchElementException: next on empty iterator; E ; E Java stack trace:; E java.lang.RuntimeException: typ: inference failure: ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:2822,Availability,failure,failure,2822,"v.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in parse; ir_map); /miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('xro1961', <py4j.java_gateway.GatewayClient object at 0x7ffa62e9e390>, 'z:is.hail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])'; tpl = JavaObject id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])\n\tat is.hail.expr.ir.IR$class.typ(IR....a:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: NoSuchElementException: next on empty iterator; E ; E Java stack trace:; E java.lang.RuntimeException: typ: inference failure: ; E (MakeArray Array[Int32]); E 	at is.hail.expr.ir.IR$class.typ(IR.scala:34); E 	at is.hail.expr.ir.MakeArray.typ(IR.scala:135);",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:3466,Availability,Error,Error,3466,"ail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])'; tpl = JavaObject id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])\n\tat is.hail.expr.ir.IR$class.typ(IR....a:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: NoSuchElementException: next on empty iterator; E ; E Java stack trace:; E java.lang.RuntimeException: typ: inference failure: ; E (MakeArray Array[Int32]); E 	at is.hail.expr.ir.IR$class.typ(IR.scala:34); E 	at is.hail.expr.ir.MakeArray.typ(IR.scala:135); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:889); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:301); E 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:1084); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:3690,Availability,failure,failure,3690,"ect id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])\n\tat is.hail.expr.ir.IR$class.typ(IR....a:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: NoSuchElementException: next on empty iterator; E ; E Java stack trace:; E java.lang.RuntimeException: typ: inference failure: ; E (MakeArray Array[Int32]); E 	at is.hail.expr.ir.IR$class.typ(IR.scala:34); E 	at is.hail.expr.ir.MakeArray.typ(IR.scala:135); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:889); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.repUntil(Parser.scala:301); E 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:1084); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1591); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1597); E 	at is.hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:1329,Integrability,wrap,wrapper,1329,"estMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in parse; ir_map); /miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:1599,Integrability,wrap,wrapper,1599,"[1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in parse; ir_map); /miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('xro1961', <py4j.java_gateway.GatewayClient object at 0x7ffa62e9e390>, 'z:is.hail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inf",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:3109,Integrability,protocol,protocol,3109,"way_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('xro1961', <py4j.java_gateway.GatewayClient object at 0x7ffa62e9e390>, 'z:is.hail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])'; tpl = JavaObject id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])\n\tat is.hail.expr.ir.IR$class.typ(IR....a:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n'. def deco(*args, **kwargs):; import pyspark; try:; return f(*args, **kwargs); except py4j.protocol.Py4JJavaError as e:; s = e.java_exception.toString(); ; # py4j catches NoSuchElementExceptions to stop array iteration; if s.startswith('java.util.NoSuchElementException'):; raise; ; tpl = Env.jutils().handleForPython(e.java_exception); deepest, full = tpl._1(), tpl._2(); raise FatalError('%s\n\nJava stack trace:\n%s\n'; 'Hail version: %s\n'; > 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; E hail.utils.java.FatalError: NoSuchElementException: next on empty iterator; E ; E Java stack trace:; E java.lang.RuntimeException: typ: inference failure: ; E (MakeArray Array[Int32]); E 	at is.hail.expr.ir.IR$class.typ(IR.scala:34); E 	at is.hail.expr.ir.MakeArray.typ(IR.scala:135); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:889); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$$anonfun$ir_value_children$1.apply(Pa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:5835,Integrability,Wrap,WrappedArray,5835,"epUntil(Parser.scala:301); E 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:1084); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1591); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1596); E 	at is.hail.expr.ir.IRParser.parse_value_ir(Parser.scala); E 	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E 	at py4j.Gateway.invoke(Gateway.java:282); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.NoSuchElementException: next on empty iterator; E 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); E 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); E 	at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63); E 	at scala.collection.IterableLike$class.head(IterableLike.scala:107); E 	at scala.collection.mutable.WrappedArray.scala$collection$IndexedSeqOptimized$$super$head(WrappedArray.scala:35); E 	at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126); E 	at scala.collection.mutable.WrappedArray.head(WrappedArray.scala:35); E 	at is.hail.expr.ir.InferType$.apply(InferType.scala:30). """""". will check tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:5897,Integrability,Wrap,WrappedArray,5897,"epUntil(Parser.scala:301); E 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:1084); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1591); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1596); E 	at is.hail.expr.ir.IRParser.parse_value_ir(Parser.scala); E 	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E 	at py4j.Gateway.invoke(Gateway.java:282); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.NoSuchElementException: next on empty iterator; E 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); E 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); E 	at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63); E 	at scala.collection.IterableLike$class.head(IterableLike.scala:107); E 	at scala.collection.mutable.WrappedArray.scala$collection$IndexedSeqOptimized$$super$head(WrappedArray.scala:35); E 	at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126); E 	at scala.collection.mutable.WrappedArray.head(WrappedArray.scala:35); E 	at is.hail.expr.ir.InferType$.apply(InferType.scala:30). """""". will check tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:6038,Integrability,Wrap,WrappedArray,6038,"epUntil(Parser.scala:301); E 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:1084); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1591); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1596); E 	at is.hail.expr.ir.IRParser.parse_value_ir(Parser.scala); E 	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E 	at py4j.Gateway.invoke(Gateway.java:282); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.NoSuchElementException: next on empty iterator; E 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); E 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); E 	at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63); E 	at scala.collection.IterableLike$class.head(IterableLike.scala:107); E 	at scala.collection.mutable.WrappedArray.scala$collection$IndexedSeqOptimized$$super$head(WrappedArray.scala:35); E 	at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126); E 	at scala.collection.mutable.WrappedArray.head(WrappedArray.scala:35); E 	at is.hail.expr.ir.InferType$.apply(InferType.scala:30). """""". will check tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:6056,Integrability,Wrap,WrappedArray,6056,"epUntil(Parser.scala:301); E 	at is.hail.expr.ir.IRParser$.ir_value_children(Parser.scala:676); E 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:1084); E 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:680); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1591); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1597); E 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1596); E 	at is.hail.expr.ir.IRParser.parse_value_ir(Parser.scala); E 	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source); E 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); E 	at java.lang.reflect.Method.invoke(Method.java:498); E 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); E 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); E 	at py4j.Gateway.invoke(Gateway.java:282); E 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); E 	at py4j.commands.CallCommand.execute(CallCommand.java:79); E 	at py4j.GatewayConnection.run(GatewayConnection.java:238); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.NoSuchElementException: next on empty iterator; E 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); E 	at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); E 	at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63); E 	at scala.collection.IterableLike$class.head(IterableLike.scala:107); E 	at scala.collection.mutable.WrappedArray.scala$collection$IndexedSeqOptimized$$super$head(WrappedArray.scala:35); E 	at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126); E 	at scala.collection.mutable.WrappedArray.head(WrappedArray.scala:35); E 	at is.hail.expr.ir.InferType$.apply(InferType.scala:30). """""". will check tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:1820,Performance,load,loads,1820," True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in parse; ir_map); /miniconda3/lib/python3.7/site-packages/py4j/java_gateway.py:1257: in __call__; answer, self.gateway_client, self.target_id, self.name); _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _. args = ('xro1961', <py4j.java_gateway.GatewayClient object at 0x7ffa62e9e390>, 'z:is.hail.expr.ir.IRParser', 'parse_value_ir'), kwargs = {}; pyspark = <module 'pyspark' from '/miniconda3/lib/python3.7/site-packages/pyspark/__init__.py'>, s = 'java.lang.RuntimeException: typ: inference failure: \n(MakeArray Array[Int32])'; tpl = JavaObject id=o1962, deepest = 'NoSuchElementException: next on empty iterator'; full = 'java.lang.RuntimeException: typ: inference fai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:52,Testability,test,test,52,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:81,Testability,Test,Tests,81,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:212,Testability,Test,Tests,212,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:337,Testability,test,test,337,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:362,Testability,Test,Tests,362,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:368,Testability,test,testMethod,368,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:436,Testability,assert,assertEqual,436,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:511,Testability,assert,assertEqual,511,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:590,Testability,assert,assertEqual,590,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:668,Testability,assert,assertEqual,668,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:741,Testability,assert,assertEqual,741,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:841,Testability,assert,assertEqual,841,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:897,Testability,assert,assertTrue,897,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7969#issuecomment-579035930:964,Testability,test,test,964,"darn, the latest few changes break everything. """"""; test/hail/expr/test_expr.py::Tests::test_array_methods FAILED [ 11%]; _________________________________________________________________________________________ Tests.test_array_methods __________________________________________________________________________________________. self = <test.hail.expr.test_expr.Tests testMethod=test_array_methods>. def test_array_methods(self):; self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5])), False); self.assertEqual(hl.eval(hl.any(lambda x: x % 2 == 0, [1, 3, 5, 6])), True); ; self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [1, 3, 5, 6])), False); self.assertEqual(hl.eval(hl.all(lambda x: x % 2 == 0, [2, 6])), True); ; self.assertEqual(hl.eval(hl.map(lambda x: x % 2 == 0, [0, 1, 4, 6])), [True, False, True, True]); ; self.assertEqual(hl.eval(hl.len([0, 1, 4, 6])), 4); ; > self.assertTrue(math.isnan(hl.eval(hl.mean(hl.empty_array(hl.tint))))). test/hail/expr/test_expr.py:2240: ; _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-540>:2: in eval; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:189: in eval; return eval_timed(expression)[0]; </miniconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-538>:2: in eval_timed; ???; hail/typecheck/check.py:585: in wrapper; return __original_func(*args_, **kwargs_); hail/expr/expressions/expression_utils.py:155: in eval_timed; return Env.backend().execute(expression._ir, True); hail/backend/backend.py:109: in execute; result = json.loads(Env.hc()._jhc.backend().executeJSON(self._to_java_ir(ir))); hail/backend/backend.py:105: in _to_java_ir; ir._jir = ir.parse(r(ir), ir_map=r.jirs); hail/ir/base_ir.py:244: in p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7969#issuecomment-579035930
https://github.com/hail-is/hail/pull/7971#issuecomment-578807275:703,Integrability,message,message,703,"> This seems fine, but it seems like you really want parallelism to match bandwidth. When you're on a gigabit ethernet connection (like me at my desk) you're leaving a lot on the table. No. More concurrency isn't going to increase the bandwidth from a single process if the network stack is full. Increasing the value of parallelism here has two benefits that I can see: (1) amortizing request overhead, which is already very small for 1MB requests. 2-way parallelism would probably be sufficient for that. (2) Increasing the parallelism on the server if the batch or database processes are single-process limited. For the latter, we can increase the parallelism but should correspondingly decrease the message size. My bandwidth numbers were the minimum required for things to work without back-off. There's no relationship between that and the maximum bandwidth this code can support.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7971#issuecomment-578807275
https://github.com/hail-is/hail/pull/7971#issuecomment-578807275:195,Performance,concurren,concurrency,195,"> This seems fine, but it seems like you really want parallelism to match bandwidth. When you're on a gigabit ethernet connection (like me at my desk) you're leaving a lot on the table. No. More concurrency isn't going to increase the bandwidth from a single process if the network stack is full. Increasing the value of parallelism here has two benefits that I can see: (1) amortizing request overhead, which is already very small for 1MB requests. 2-way parallelism would probably be sufficient for that. (2) Increasing the parallelism on the server if the batch or database processes are single-process limited. For the latter, we can increase the parallelism but should correspondingly decrease the message size. My bandwidth numbers were the minimum required for things to work without back-off. There's no relationship between that and the maximum bandwidth this code can support.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7971#issuecomment-578807275
https://github.com/hail-is/hail/pull/7978#issuecomment-578955557:22,Testability,log,logging,22,How does this prevent logging in to the workers?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7978#issuecomment-578955557
https://github.com/hail-is/hail/pull/7978#issuecomment-578967196:42,Security,firewall,firewall-rules,42,"See the separate document in `team` about firewall-rules. In particular, the default network will block all connections.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7978#issuecomment-578967196
https://github.com/hail-is/hail/pull/7978#issuecomment-578973470:168,Security,access,access,168,"Dataproc does have an option to tag all nodes (`--tags`), but we purposely intend to prevent SSH connections from the outside world to the worker nodes. If you need to access the worker machines, you can still connect to them from the master node (because, due to Spark/Dataproc, we must permit all TCP/IP connections between all Dataproc nodes). I realize this is annoying, but I have only needed to do this once in my three years of working on Hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7978#issuecomment-578973470
https://github.com/hail-is/hail/pull/7978#issuecomment-578973978:43,Security,Secur,Security,43,"In particular, the design of the Broad GCP Security Best Practices is to disable everything by default and then selectively enable. The master tag enables SSH access for the master node.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7978#issuecomment-578973978
https://github.com/hail-is/hail/pull/7978#issuecomment-578973978:159,Security,access,access,159,"In particular, the design of the Broad GCP Security Best Practices is to disable everything by default and then selectively enable. The master tag enables SSH access for the master node.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7978#issuecomment-578973978
https://github.com/hail-is/hail/issues/7979#issuecomment-579782403:212,Energy Efficiency,efficient,efficient,212,Dan pointed out that a flat version of this could also be achieved with `hl.range(nd.shape[0]).map(lambda i: nd[i])`. That could of course also be generalized to higher dimensional ndarrays as well. I wonder how efficient it would be though. Would be nice if deforesting could naturally cross boundary between arrays and ndarrays.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/7979#issuecomment-579782403
https://github.com/hail-is/hail/pull/7990#issuecomment-579967208:124,Integrability,message,messages,124,"I should have removed it when I was done. I used it to debug a bunch of stuff. I do not understand why, but, by default, no messages are printed. Setting the level to WARNING at least got my `log.warn` messages to print, of which I temporarily added many during debugging of insert. Anyway, I agree with the latter statement.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579967208
https://github.com/hail-is/hail/pull/7990#issuecomment-579967208:202,Integrability,message,messages,202,"I should have removed it when I was done. I used it to debug a bunch of stuff. I do not understand why, but, by default, no messages are printed. Setting the level to WARNING at least got my `log.warn` messages to print, of which I temporarily added many during debugging of insert. Anyway, I agree with the latter statement.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579967208
https://github.com/hail-is/hail/pull/7990#issuecomment-579967208:192,Testability,log,log,192,"I should have removed it when I was done. I used it to debug a bunch of stuff. I do not understand why, but, by default, no messages are printed. Setting the level to WARNING at least got my `log.warn` messages to print, of which I temporarily added many during debugging of insert. Anyway, I agree with the latter statement.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579967208
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:0,Deployability,Update,Updated,0,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:1223,Integrability,message,messages,1223,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:170,Testability,test,tests,170,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:205,Testability,test,test,205,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:225,Testability,Test,Test,225,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:282,Testability,log,log,282,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:351,Testability,test,test,351,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:405,Testability,test,test,405,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:449,Testability,log,log,449,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:718,Testability,log,log,718,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:789,Testability,test,test,789,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:842,Testability,test,test,842,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:903,Testability,log,log,903,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:982,Testability,log,log,982,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:1028,Testability,log,logs,1028,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:1056,Testability,log,logger,1056,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:1162,Testability,log,logger,1162,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:1219,Testability,log,log,1219,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:290,Availability,error,error,290,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:297,Availability,error,error,297,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:333,Availability,error,error,333,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:47,Integrability,message,messages,47,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:143,Integrability,message,message,143,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:202,Testability,log,logging,202,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:211,Testability,log,log,211,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:217,Testability,log,logging,217,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:236,Testability,test,test,236,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:244,Testability,log,log,244,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:262,Testability,log,log,262,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7990#issuecomment-579976077:286,Testability,log,log,286,"> I do not understand why, but, by default, no messages are printed. Hmm, it looks to me like the default is to print (for WARN and above) the message with no further format:. ```; $ cat foo.py; import logging. log = logging.getLogger('test'). log.info('info'); log.warning('warning'); log.error('error'); $ python3 foo.py; warning; error; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579976077
https://github.com/hail-is/hail/pull/7996#issuecomment-580044196:33,Deployability,deploy,deploy,33,"OK, everything checks out in dev deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/7996#issuecomment-580044196
https://github.com/hail-is/hail/pull/8002#issuecomment-580248649:18,Deployability,deploy,deploy,18,"I tested with dev deploy, this looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8002#issuecomment-580248649
https://github.com/hail-is/hail/pull/8002#issuecomment-580248649:2,Testability,test,tested,2,"I tested with dev deploy, this looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8002#issuecomment-580248649
https://github.com/hail-is/hail/issues/8004#issuecomment-580298708:19,Energy Efficiency,efficient,efficient,19,Tim is just really efficient! :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8004#issuecomment-580298708
https://github.com/hail-is/hail/issues/8004#issuecomment-580325987:21,Deployability,deploy,deploy,21,Will be live when we deploy the docs for the next version!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/8004#issuecomment-580325987
https://github.com/hail-is/hail/pull/8008#issuecomment-580537133:16,Testability,test,tests,16,"interesting, no tests fail as is",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580537133
https://github.com/hail-is/hail/pull/8008#issuecomment-580538063:14,Testability,test,tests,14,"need some new tests then! Let's add two tables to the src/test/resources dir, one of which has some schema that is all optional, one of which is all required. We can union them for segfaults!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580538063
https://github.com/hail-is/hail/pull/8008#issuecomment-580538063:58,Testability,test,test,58,"need some new tests then! Let's add two tables to the src/test/resources dir, one of which has some schema that is all optional, one of which is all required. We can union them for segfaults!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580538063
https://github.com/hail-is/hail/pull/8008#issuecomment-580774813:190,Testability,test,test,190,"Hm:. <img width=""484"" alt=""Screen Shot 2020-01-31 at 10 17 03 AM"" src=""https://user-images.githubusercontent.com/5543229/73550580-d614aa00-4412-11ea-82b9-4dd11825cc59.png"">. (for the pushed test). specifying the type as hl.null(hl.tint32), neither did hl.null('int32') (which doesn't feel as ergonomic, but is advertised under the [Missingness section](https://hail.is/docs/0.2/overview/expressions.html#missingness)), doesn't work sadly. As a dev using Hail I miss being able to specify data types exactly. I may want to specify that data is dense, or sparse.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580774813
https://github.com/hail-is/hail/pull/8008#issuecomment-580777705:141,Modifiability,extend,extends,141,"Ah there is some upcast that happens before we get to case class TableUnion:. ```scala; case class TableUnion(children: IndexedSeq[TableIR]) extends TableIR {; assert(children.forall(c => c.typ.rowType == children.head.typ.rowType)); println(""ALL SAME""); ```. If this is added, ""ALL SAME"" gets printed 16 times after ""RESULT"". Not sure why that isn't 8 (maybe due to a lowering pass causing a copy?) or 24, but that's less important. edit: Nope, these tables must be interpreted as being of both non-missing types in the ""maybeNull"" column by the import function, or I'm specifying the type wrong:. ```python; def union(...):; left_key = self.key.dtype; for i, ht, in enumerate(tables):; if left_key != ht.key.dtype:; raise ValueError(...); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580777705
https://github.com/hail-is/hail/pull/8008#issuecomment-580777705:160,Testability,assert,assert,160,"Ah there is some upcast that happens before we get to case class TableUnion:. ```scala; case class TableUnion(children: IndexedSeq[TableIR]) extends TableIR {; assert(children.forall(c => c.typ.rowType == children.head.typ.rowType)); println(""ALL SAME""); ```. If this is added, ""ALL SAME"" gets printed 16 times after ""RESULT"". Not sure why that isn't 8 (maybe due to a lowering pass causing a copy?) or 24, but that's less important. edit: Nope, these tables must be interpreted as being of both non-missing types in the ""maybeNull"" column by the import function, or I'm specifying the type wrong:. ```python; def union(...):; left_key = self.key.dtype; for i, ht, in enumerate(tables):; if left_key != ht.key.dtype:; raise ValueError(...); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580777705
https://github.com/hail-is/hail/pull/8008#issuecomment-580834992:566,Deployability,update,updated,566,"Does not work (passes) with PL present in all rows, which surprised me since type should be virtual type should be taken as stated, I believe. Haven't investigated further (stats day) to see what the IR generated is. Also does not work if I edit the VCF file and insert a bogus PL of .,.,. for each sample. An upcast seems to be happening in the mt1 child, because PL is clearly missing in mt2:. <img width=""705"" alt=""Screenshot 2020-01-31 12 40 13"" src=""https://user-images.githubusercontent.com/5543229/73561429-f9e1eb00-4426-11ea-9bb8-0cec77398d92.png"">. code in updated, pushed test. edit, to show that mt1 does have expected entries (though this shouldn't matter unless array_elements_required doesn't loosen requiredeness over the imputed type):. MT1:; <img width=""170"" alt=""Screenshot 2020-01-31 12 47 00"" src=""https://user-images.githubusercontent.com/5543229/73561943-07e43b80-4428-11ea-847e-65f2f3771af8.png"">; MT2:; <img width=""208"" alt=""Screenshot 2020-01-31 12 47 05"" src=""https://user-images.githubusercontent.com/5543229/73561944-087cd200-4428-11ea-8968-6daf53291d83.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580834992
https://github.com/hail-is/hail/pull/8008#issuecomment-580834992:582,Testability,test,test,582,"Does not work (passes) with PL present in all rows, which surprised me since type should be virtual type should be taken as stated, I believe. Haven't investigated further (stats day) to see what the IR generated is. Also does not work if I edit the VCF file and insert a bogus PL of .,.,. for each sample. An upcast seems to be happening in the mt1 child, because PL is clearly missing in mt2:. <img width=""705"" alt=""Screenshot 2020-01-31 12 40 13"" src=""https://user-images.githubusercontent.com/5543229/73561429-f9e1eb00-4426-11ea-9bb8-0cec77398d92.png"">. code in updated, pushed test. edit, to show that mt1 does have expected entries (though this shouldn't matter unless array_elements_required doesn't loosen requiredeness over the imputed type):. MT1:; <img width=""170"" alt=""Screenshot 2020-01-31 12 47 00"" src=""https://user-images.githubusercontent.com/5543229/73561943-07e43b80-4428-11ea-847e-65f2f3771af8.png"">; MT2:; <img width=""208"" alt=""Screenshot 2020-01-31 12 47 05"" src=""https://user-images.githubusercontent.com/5543229/73561944-087cd200-4428-11ea-8968-6daf53291d83.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580834992
https://github.com/hail-is/hail/pull/8008#issuecomment-580834992:371,Usability,clear,clearly,371,"Does not work (passes) with PL present in all rows, which surprised me since type should be virtual type should be taken as stated, I believe. Haven't investigated further (stats day) to see what the IR generated is. Also does not work if I edit the VCF file and insert a bogus PL of .,.,. for each sample. An upcast seems to be happening in the mt1 child, because PL is clearly missing in mt2:. <img width=""705"" alt=""Screenshot 2020-01-31 12 40 13"" src=""https://user-images.githubusercontent.com/5543229/73561429-f9e1eb00-4426-11ea-9bb8-0cec77398d92.png"">. code in updated, pushed test. edit, to show that mt1 does have expected entries (though this shouldn't matter unless array_elements_required doesn't loosen requiredeness over the imputed type):. MT1:; <img width=""170"" alt=""Screenshot 2020-01-31 12 47 00"" src=""https://user-images.githubusercontent.com/5543229/73561943-07e43b80-4428-11ea-847e-65f2f3771af8.png"">; MT2:; <img width=""208"" alt=""Screenshot 2020-01-31 12 47 05"" src=""https://user-images.githubusercontent.com/5543229/73561944-087cd200-4428-11ea-8968-6daf53291d83.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580834992
https://github.com/hail-is/hail/pull/8008#issuecomment-581048722:2447,Availability,error,error,2447,"ld 1: PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],rsid:PCString,qual:PFloat64,filters:PCSet[PCString],info:PCStruct{NEGATIVE_TRAIN_SITE:PBoolean,HWP:PFloat64,AC:PCArray[PInt32],culprit:PCString,MQ0:PInt32,ReadPosRankSum:PFloat64,AN:PInt32,InbreedingCoeff:PFloat64,AF:PCArray[PFloat64],GQ_STDDEV:PFloat64,FS:PFloat64,DP:PInt32,GQ_MEAN:PFloat64,POSITIVE_TRAIN_SITE:PBoolean,VQSLOD:PFloat64,ClippingRankSum:PFloat64,BaseQRankSum:PFloat64,MLEAF:PCArray[PFloat64],MLEAC:PCArray[PInt32],MQ:PFloat64,QD:PFloat64,END:PInt32,DB:PBoolean,HaplotypeScore:PFloat64,MQRankSum:PFloat64,CCC:PInt32,NCC:PInt32,DS:PBoolean},s:PCString,GT:PCCall,AD:PCArray[+PInt32],DP:PInt32,GQ:PInt32,PL:PCArray[+PInt32]}. Child 2: PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],rsid:PCString,qual:PFloat64,filters:PCSet[PCString],info:PCStruct{NEGATIVE_TRAIN_SITE:PBoolean,HWP:PFloat64,AC:PCArray[PInt32],culprit:PCString,MQ0:PInt32,ReadPosRankSum:PFloat64,AN:PInt32,InbreedingCoeff:PFloat64,AF:PCArray[PFloat64],GQ_STDDEV:PFloat64,FS:PFloat64,DP:PInt32,GQ_MEAN:PFloat64,POSITIVE_TRAIN_SITE:PBoolean,VQSLOD:PFloat64,ClippingRankSum:PFloat64,BaseQRankSum:PFloat64,MLEAF:PCArray[PFloat64],MLEAC:PCArray[PInt32],MQ:PFloat64,QD:PFloat64,END:PInt32,DB:PBoolean,HaplotypeScore:PFloat64,MQRankSum:PFloat64,CCC:PInt32,NCC:PInt32,DS:PBoolean},s:PCString,GT:PCCall,AD:PCArray[PInt32],DP:PInt32,GQ:PInt32,PL:PCArray[PInt32]}. No problems doing:. ```python; mt1 = hl.import_vcf(resource('sample.vcf'), array_elements_required=True); mt2 = hl.import_vcf(resource('sample_missing_pl2.vcf'), array_elements_required=False); mt1.entries().union(mt2.entries()).count(); ```. Also shows me that array_elements_required=True really does force requiredeness to be passed on the virtual type; by setting the array_elements_required=True on the vcf file with missing PL values, I can also trigger an error LoweringPipeline, suggesting that requiredeness (and other type information) is not overridden by reading over the data first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581048722
https://github.com/hail-is/hail/pull/8008#issuecomment-581048722:351,Testability,assert,assert,351,"Ok, have the right branch being triggered (types not the same), but causes no issues. For:; ```scala; val typ: TableType = {; if (children.forall(_.typ == children.head.typ)) {; println(""SAME""); children.head.typ; } else {; println(""NOT THE SAME""); var i = 0; children.map(c => {; i+= 1; println(s""Child ${i}: ${PType.canonical(c.typ.rowType)}""). }); assert(children.forall(c => c.typ.rowType.isOfType(children.head.typ.rowType))); TableType(children.head.typ.rowType.deepOptional().asInstanceOf[TStruct], children.head.typ.key, children.head.typ.globalType); }; }; ```. Get. Child 1: PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],rsid:PCString,qual:PFloat64,filters:PCSet[PCString],info:PCStruct{NEGATIVE_TRAIN_SITE:PBoolean,HWP:PFloat64,AC:PCArray[PInt32],culprit:PCString,MQ0:PInt32,ReadPosRankSum:PFloat64,AN:PInt32,InbreedingCoeff:PFloat64,AF:PCArray[PFloat64],GQ_STDDEV:PFloat64,FS:PFloat64,DP:PInt32,GQ_MEAN:PFloat64,POSITIVE_TRAIN_SITE:PBoolean,VQSLOD:PFloat64,ClippingRankSum:PFloat64,BaseQRankSum:PFloat64,MLEAF:PCArray[PFloat64],MLEAC:PCArray[PInt32],MQ:PFloat64,QD:PFloat64,END:PInt32,DB:PBoolean,HaplotypeScore:PFloat64,MQRankSum:PFloat64,CCC:PInt32,NCC:PInt32,DS:PBoolean},s:PCString,GT:PCCall,AD:PCArray[+PInt32],DP:PInt32,GQ:PInt32,PL:PCArray[+PInt32]}. Child 2: PCStruct{locus:PCLocus(GRCh37),alleles:PCArray[PCString],rsid:PCString,qual:PFloat64,filters:PCSet[PCString],info:PCStruct{NEGATIVE_TRAIN_SITE:PBoolean,HWP:PFloat64,AC:PCArray[PInt32],culprit:PCString,MQ0:PInt32,ReadPosRankSum:PFloat64,AN:PInt32,InbreedingCoeff:PFloat64,AF:PCArray[PFloat64],GQ_STDDEV:PFloat64,FS:PFloat64,DP:PInt32,GQ_MEAN:PFloat64,POSITIVE_TRAIN_SITE:PBoolean,VQSLOD:PFloat64,ClippingRankSum:PFloat64,BaseQRankSum:PFloat64,MLEAF:PCArray[PFloat64],MLEAC:PCArray[PInt32],MQ:PFloat64,QD:PFloat64,END:PInt32,DB:PBoolean,HaplotypeScore:PFloat64,MQRankSum:PFloat64,CCC:PInt32,NCC:PInt32,DS:PBoolean},s:PCString,GT:PCCall,AD:PCArray[PInt32],DP:PInt32,GQ:PInt32,PL:PCArray[PInt32]}. No problems doing",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581048722
https://github.com/hail-is/hail/pull/8008#issuecomment-581049805:216,Deployability,update,updated,216,"> you also don't need to add another resource VCF to make this fail. Yeah, I wasn't sure if the array_elements_required would be overridden if no missingness was found in the data. I wanted to test that possibility (updated my comment with the results of that test). > ah, crap, there's a simplify rule that turns a TableCount(TableUnion(...)) into the sum of the TableCounts for each child. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581049805
https://github.com/hail-is/hail/pull/8008#issuecomment-581049805:193,Testability,test,test,193,"> you also don't need to add another resource VCF to make this fail. Yeah, I wasn't sure if the array_elements_required would be overridden if no missingness was found in the data. I wanted to test that possibility (updated my comment with the results of that test). > ah, crap, there's a simplify rule that turns a TableCount(TableUnion(...)) into the sum of the TableCounts for each child. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581049805
https://github.com/hail-is/hail/pull/8008#issuecomment-581049805:260,Testability,test,test,260,"> you also don't need to add another resource VCF to make this fail. Yeah, I wasn't sure if the array_elements_required would be overridden if no missingness was found in the data. I wanted to test that possibility (updated my comment with the results of that test). > ah, crap, there's a simplify rule that turns a TableCount(TableUnion(...)) into the sum of the TableCounts for each child. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581049805
https://github.com/hail-is/hail/pull/8008#issuecomment-581049805:289,Usability,simpl,simplify,289,"> you also don't need to add another resource VCF to make this fail. Yeah, I wasn't sure if the array_elements_required would be overridden if no missingness was found in the data. I wanted to test that possibility (updated my comment with the results of that test). > ah, crap, there's a simplify rule that turns a TableCount(TableUnion(...)) into the sum of the TableCounts for each child. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581049805
https://github.com/hail-is/hail/pull/8008#issuecomment-581053876:21,Testability,test,test,21,"actually, here's the test I think I want -- it probably won't segfault as written, but will write bad data:. ```python; mt1 = hl.import_vcf(resource('sample.vcf'), array_elements_required=True); mt2 = hl.import_vcf(resource('sample.vcf'), array_elements_required=False). mt3 = mt1.entries().union(mt2.entries()); mt4 = mt1.entries().union(mt2.entries()); assert mt3._same(mt3); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581053876
