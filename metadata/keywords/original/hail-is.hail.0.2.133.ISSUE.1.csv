id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/hail-is/hail/issues/1806:3929,Energy Efficiency,schedul,scheduler,3929,.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:21); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:17); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:498); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMetho,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:6442,Energy Efficiency,schedul,scheduler,6442,pl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-ebabd77; Error summary: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. ​; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:6514,Energy Efficiency,schedul,scheduler,6514,pl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-ebabd77; Error summary: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. ​; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:802,Integrability,protocol,protocol,802,"```; from hail import *; hc = HailContext(); input_vcf = ""gs://seqr-hail/reference_data/GRCh38/1kg/ALL.GRCh38_sites.20170504.vcf.gz""; vds = hc.import_vcf(input_vcf, npartitions=1000, force=True); ```. causes. ```; FatalErrorTraceback (most recent call last); <ipython-input-4-5e86630fbae5> in <module>(); ----> 1 vds = hc.import_vcf(input_vcf, npartitions=1000, force=True). <decorator-gen-291> in import_vcf(self, path, force, force_bgz, header_file, npartitions, sites_only, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields). /home/hail/pyhail-hail-is-master-ebabd77.zip/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, seqr-pipeline-cluster-grch38-w-0.c.seqr-project.internal): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:2413,Performance,concurren,concurrent,2413,; 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:2498,Performance,concurren,concurrent,2498,); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:4625,Performance,Load,LoadVCF,4625,uler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:21); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:17); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:498); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskSto,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:4640,Performance,Load,LoadVCF,4640,667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1065); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1059); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:21); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:17); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:498); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:10,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:6638,Performance,concurren,concurrent,6638,pl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-ebabd77; Error summary: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. ​; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:6723,Performance,concurren,concurrent,6723,pl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-ebabd77; Error summary: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. ​; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:1019,Safety,abort,aborted,1019,"lContext(); input_vcf = ""gs://seqr-hail/reference_data/GRCh38/1kg/ALL.GRCh38_sites.20170504.vcf.gz""; vds = hc.import_vcf(input_vcf, npartitions=1000, force=True); ```. causes. ```; FatalErrorTraceback (most recent call last); <ipython-input-4-5e86630fbae5> in <module>(); ----> 1 vds = hc.import_vcf(input_vcf, npartitions=1000, force=True). <decorator-gen-291> in import_vcf(self, path, force, force_bgz, header_file, npartitions, sites_only, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields). /home/hail/pyhail-hail-is-master-ebabd77.zip/hail/java.pyc in handle_py4j(func, *args, **kwargs); 111 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 112 'Hail version: %s\n'; --> 113 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 114 except py4j.protocol.Py4JError as e:; 115 if e.args[0].startswith('An error occurred while calling'):. FatalError: IllegalArgumentException: Size exceeds Integer.MAX_VALUE. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 20 times, most recent failure: Lost task 0.19 in stage 0.0 (TID 19, seqr-pipeline-cluster-grch38-w-0.c.seqr-project.internal): java.lang.IllegalArgumentException: Size exceeds Integer.MAX_VALUE; 	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:869); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:103); 	at org.apache.spark.storage.DiskStore$$anonfun$getBytes$2.apply(DiskStore.scala:91); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1310); 	at org.apache.spark.storage.DiskStore.getBytes(DiskStore.scala:105); 	at org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:438); 	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:606); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitions",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:2824,Safety,abort,abortStage,2824,k.storage.BlockManager.getOrElseUpdate(BlockManager.scala:663); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:2922,Safety,abort,abortStage,2922,Compute(RDD.scala:330); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1806:3167,Safety,abort,abortStage,3167,(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1806
https://github.com/hail-is/hail/issues/1807:23,Availability,error,error,23,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1807
https://github.com/hail-is/hail/issues/1807:528,Deployability,deploy,deploy,528,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1807
https://github.com/hail-is/hail/issues/1807:565,Deployability,deploy,deploy,565,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1807
https://github.com/hail-is/hail/issues/1807:638,Deployability,deploy,deploy,638,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1807
https://github.com/hail-is/hail/issues/1807:715,Deployability,deploy,deploy,715,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1807
https://github.com/hail-is/hail/issues/1807:787,Deployability,deploy,deploy,787,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1807
https://github.com/hail-is/hail/issues/1807:857,Deployability,deploy,deploy,857,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1807
https://github.com/hail-is/hail/issues/1807:261,Performance,load,loadClass,261,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1807
https://github.com/hail-is/hail/issues/1807:320,Performance,load,loadClass,320,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1807
https://github.com/hail-is/hail/issues/1807:90,Testability,test,test,90,"I am getting following error while using spark submit with --class ""is.hail.driver.Main"" /test/spark/hail15may.jar. java.lang.ClassNotFoundException: is.hail.driver.Main; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.spark.util.Utils$.classForName(Utils.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:693); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1807
https://github.com/hail-is/hail/pull/1811:38,Integrability,interface,interface,38,@cseed I'd like your thoughts on this interface and implementation. I added this to make implementing PCRelate a little bit easier because many of the operations I wanted on matrices didn't exist on Spark's matrices.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1811
https://github.com/hail-is/hail/issues/1818:248,Availability,error,error,248,"Hi:; I come form DCH in China. Now I try to use the following commands to import table in tutorial;; ```; from hail import *; hs= HailContext(); table=hc.import_table('1000Genomes.sample_annotations',impute=True).key_by('Sample'); ```. However, an error appeared like this :; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'import_table' is not defined. Should I import other modules to use import_table command? Thanks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1818
https://github.com/hail-is/hail/issues/1820:256,Availability,error,error,256,"- Nested arrays appear to be supported in the current code, and I don't think this is intended.; - Why do exportFormat and exportInfo differ?; - Number is accessed from the field attrs without being checked. There could be something silly in there.; - Bad error messages (don't say which field was the problem)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1820
https://github.com/hail-is/hail/issues/1820:262,Integrability,message,messages,262,"- Nested arrays appear to be supported in the current code, and I don't think this is intended.; - Why do exportFormat and exportInfo differ?; - Number is accessed from the field attrs without being checked. There could be something silly in there.; - Bad error messages (don't say which field was the problem)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1820
https://github.com/hail-is/hail/issues/1820:155,Security,access,accessed,155,"- Nested arrays appear to be supported in the current code, and I don't think this is intended.; - Why do exportFormat and exportInfo differ?; - Number is accessed from the field attrs without being checked. There could be something silly in there.; - Bad error messages (don't say which field was the problem)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1820
https://github.com/hail-is/hail/pull/1821:9,Availability,error,error,9,- better error message for invalid export type; - don't support nested collections (bugfix),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1821
https://github.com/hail-is/hail/pull/1821:15,Integrability,message,message,15,- better error message for invalid export type; - don't support nested collections (bugfix),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1821
https://github.com/hail-is/hail/issues/1822:3499,Availability,failure,failure,3499,"t org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:488); 	at is.hail.variant.VariantDatasetFunctions$.write$extension(VariantDataset.scala:941); 	at is.hail.variant.VariantDatasetFunctions.write(VariantDataset.scala:911); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurren",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:3558,Availability,failure,failure,3558,"r.scala:488); 	at is.hail.variant.VariantDatasetFunctions$.write$extension(VariantDataset.scala:941); 	at is.hail.variant.VariantDatasetFunctions.write(VariantDataset.scala:911); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:3611,Deployability,pipeline,pipeline-cluster-,3611,"e$extension(VariantDataset.scala:941); 	at is.hail.variant.VariantDatasetFunctions.write(VariantDataset.scala:911); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lan",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:4203,Energy Efficiency,schedul,scheduler,4203,"t py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:4275,Energy Efficiency,schedul,scheduler,4275,"	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:5694,Energy Efficiency,schedul,scheduler,5694,t is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:5734,Energy Efficiency,schedul,scheduler,5734,:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:5833,Energy Efficiency,schedul,scheduler,5833,edRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:5931,Energy Efficiency,schedul,scheduler,5931,llection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:6185,Energy Efficiency,schedul,scheduler,6185,; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:6266,Energy Efficiency,schedul,scheduler,6266,writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasource,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:6372,Energy Efficiency,schedul,scheduler,6372,Container$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:14,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:6522,Energy Efficiency,schedul,scheduler,6522,48); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:6611,Energy Efficiency,schedul,scheduler,6611,erContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonf,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:6709,Energy Efficiency,schedul,scheduler,6709,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.SQ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:6805,Energy Efficiency,schedul,scheduler,6805,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); 	at org.apache.spark.sql.execution.dataso,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:6970,Energy Efficiency,schedul,scheduler,6970,.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectRe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:10623,Energy Efficiency,schedul,scheduler,10623,.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.exec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:10695,Energy Efficiency,schedul,scheduler,10695,er.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(Writ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:12412,Energy Efficiency,schedul,scheduler,12412,oolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-1908254; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:12484,Energy Efficiency,schedul,scheduler,12484,oolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-1908254; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:4399,Performance,concurren,concurrent,4399,"java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:4484,Performance,concurren,concurrent,4484," to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFina",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:10819,Performance,concurren,concurrent,10819,invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(Writ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:10904,Performance,concurren,concurrent,10904,and.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailure,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:12608,Performance,concurren,concurrent,12608,oolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-1908254; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:12693,Performance,concurren,concurrent,12693,oolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-1908254; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:572,Safety,abort,aborted,572,"```; Write out vds: gs://seqr-hail/datasets/GRCh38/1kg/1kg.liftover.vep.vds; [Stage 5:======================================================> (22 + 1) / 23]Traceback (most recent call last):; File ""/tmp/956b6743-f2ac-4bf6-a82e-20a431c52c1c/do_vep.py"", line 22, in <module>; vds.write(output_vds, overwrite=True); File ""<decorator-gen-90>"", line 2, in write; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:13",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:3478,Safety,abort,aborted,3478,"t org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:488); 	at is.hail.variant.VariantDatasetFunctions$.write$extension(VariantDataset.scala:941); 	at is.hail.variant.VariantDatasetFunctions.write(VariantDataset.scala:911); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Job aborted due to stage failure: Task 22 in stage 5.0 failed 20 times, most recent failure: Lost task 22.19 in stage 5.0 (TID 133, seqr-pipeline-cluster-grch38-w-1.c.seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurren",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:5865,Safety,abort,abortStage,5865,scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:5963,Safety,abort,abortStage,5963,n$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:6208,Safety,abort,abortStage,6208,cution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:482,Testability,Assert,AssertionError,482,"```; Write out vds: gs://seqr-hail/datasets/GRCh38/1kg/1kg.liftover.vep.vds; [Stage 5:======================================================> (22 + 1) / 23]Traceback (most recent call last):; File ""/tmp/956b6743-f2ac-4bf6-a82e-20a431c52c1c/do_vep.py"", line 22, in <module>; vds.write(output_vds, overwrite=True); File ""<decorator-gen-90>"", line 2, in write; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:13",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:498,Testability,assert,assertion,498,"```; Write out vds: gs://seqr-hail/datasets/GRCh38/1kg/1kg.liftover.vep.vds; [Stage 5:======================================================> (22 + 1) / 23]Traceback (most recent call last):; File ""/tmp/956b6743-f2ac-4bf6-a82e-20a431c52c1c/do_vep.py"", line 22, in <module>; vds.write(output_vds, overwrite=True); File ""<decorator-gen-90>"", line 2, in write; File ""/home/ec2-user/BuildAgent/work/c38e75e72b769a7c/python/hail/java.py"", line 113, in handle_py4j; hail.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted.; 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:149); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115); 	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:13",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:4619,Testability,Assert,AssertionError,4619,seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:4635,Testability,assert,assertion,4635,seqr-project.internal): org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:4671,Testability,assert,assert,4671,ark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	... 8 more. Driver stacktrace:; 	at o,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:11026,Testability,Assert,AssertionError,11026,java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:11042,Testability,assert,assertion,11042,java.lang.Thread.run(Thread.java:745)org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/issues/1822:11078,Testability,assert,assert,11078,org.apache.spark.SparkException: Task failed while writing rows; 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:261); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.methods.VEP$$anonfun$16$$anon$1.hasNext(VEP.scala:398); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:211); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply$mcV$sp(WriterContainer.scala:253); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer$$anonfun$writeRows$1.apply(WriterContainer.scala:252); 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1348); 	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:258); 	at org.apache.spark.sql.execution.dat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1822
https://github.com/hail-is/hail/pull/1825:86,Testability,test,tested,86,This produces a python script `_build/doctest/test_hail_docs.py` with all of the code tested by the Sphinx doctest module.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1825
https://github.com/hail-is/hail/issues/1847:27,Testability,Assert,AssertionError,27,```; hail.java.FatalError: AssertionError: assertion failed: when invoking; is.hail.variant.Genotype.unboxedDosage(): double: wrong return type int; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1847
https://github.com/hail-is/hail/issues/1847:43,Testability,assert,assertion,43,```; hail.java.FatalError: AssertionError: assertion failed: when invoking; is.hail.variant.Genotype.unboxedDosage(): double: wrong return type int; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1847
https://github.com/hail-is/hail/pull/1851:5,Modifiability,refactor,refactoring,5,"Just refactoring. VSM is now parameterized by a `MatrixT` that has the row, column and cell types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1851
https://github.com/hail-is/hail/pull/1851:29,Modifiability,parameteriz,parameterized,29,"Just refactoring. VSM is now parameterized by a `MatrixT` that has the row, column and cell types.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1851
https://github.com/hail-is/hail/issues/1852:18,Performance,cache,cache,18,"We have persist / cache, but no unpersist",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1852
https://github.com/hail-is/hail/pull/1857:275,Testability,test,tests,275,"... and some infrastructure for generic row (variant) key. Some VSM methods require sample key is String, basically those that use pedigrees. I think it is feature complete but there is currently no way to create a non-string column key and there is no non-string column key tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1857
https://github.com/hail-is/hail/pull/1863:297,Availability,mask,mask,297,"- removed repeated imports in dataset; - fixed typos and clarified docs; - collapsed RegUtils to provide one ingest function each for hardCalls, dosages, and keyedRows. These optionally impute missing and produce sparse, dense, and dense vectors respectively. The first two also optionally take a mask. hardCalls no longer checks for constant vectors.; - killed the two RegUtils mutate matrix functions entirely; - simplified code in regression methods taking Vector[Double] input; - temp added constantHardCalls to RegUtils and in regressions to retrofit changes to preserve 0.1 behavior of missing for constant vectors. In 0.2 constant vectors will be treated the same as others, resulting in lack of convergence, Double.NaN, etc.; - temp added hardCallsAndAC to RegUtils and in linreg to retrofit changes to preserve 0.1 behavior of calculating AC pre-imputation for filtering. In 0.2, AC will be calculated post-imputation for hardCalls and dosages using sum(x); - combined empirical and HWE normalized arrays into one function; - removed various default parameters in tests. About 75 lines are due to retrofit, so for 0.2 it's about 200 added and 420 deleted.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1863
https://github.com/hail-is/hail/pull/1863:1073,Testability,test,tests,1073,"- removed repeated imports in dataset; - fixed typos and clarified docs; - collapsed RegUtils to provide one ingest function each for hardCalls, dosages, and keyedRows. These optionally impute missing and produce sparse, dense, and dense vectors respectively. The first two also optionally take a mask. hardCalls no longer checks for constant vectors.; - killed the two RegUtils mutate matrix functions entirely; - simplified code in regression methods taking Vector[Double] input; - temp added constantHardCalls to RegUtils and in regressions to retrofit changes to preserve 0.1 behavior of missing for constant vectors. In 0.2 constant vectors will be treated the same as others, resulting in lack of convergence, Double.NaN, etc.; - temp added hardCallsAndAC to RegUtils and in linreg to retrofit changes to preserve 0.1 behavior of calculating AC pre-imputation for filtering. In 0.2, AC will be calculated post-imputation for hardCalls and dosages using sum(x); - combined empirical and HWE normalized arrays into one function; - removed various default parameters in tests. About 75 lines are due to retrofit, so for 0.2 it's about 200 added and 420 deleted.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1863
https://github.com/hail-is/hail/pull/1863:415,Usability,simpl,simplified,415,"- removed repeated imports in dataset; - fixed typos and clarified docs; - collapsed RegUtils to provide one ingest function each for hardCalls, dosages, and keyedRows. These optionally impute missing and produce sparse, dense, and dense vectors respectively. The first two also optionally take a mask. hardCalls no longer checks for constant vectors.; - killed the two RegUtils mutate matrix functions entirely; - simplified code in regression methods taking Vector[Double] input; - temp added constantHardCalls to RegUtils and in regressions to retrofit changes to preserve 0.1 behavior of missing for constant vectors. In 0.2 constant vectors will be treated the same as others, resulting in lack of convergence, Double.NaN, etc.; - temp added hardCallsAndAC to RegUtils and in linreg to retrofit changes to preserve 0.1 behavior of calculating AC pre-imputation for filtering. In 0.2, AC will be calculated post-imputation for hardCalls and dosages using sum(x); - combined empirical and HWE normalized arrays into one function; - removed various default parameters in tests. About 75 lines are due to retrofit, so for 0.2 it's about 200 added and 420 deleted.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1863
https://github.com/hail-is/hail/pull/1864:191,Testability,assert,assert,191,Just because an RDD is OrderedPartitioner partitioned doesn't mean it; is an OrderedRDD (might not be sorted within partitions). Users who; want this behavior should call OrderedRDD.apply to assert it is an; OrderedRDD.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1864
https://github.com/hail-is/hail/pull/1871:485,Testability,test,tests,485,"This task creates `hail.zip` and places it in `build/distributions`. This zip contains a top-level folder `hail` which contains:. - `bin`: three scripts for running python, ipython, and jupyter with the hail environment set correctly. These scripts also check that `SPARK_HOME` is set to something sensible.; - `python`: the python files from the hail repository; - `docs`: the contents of `build/www/hail`, i.e. `hail.is/hail`, i.e the docs; - `hail-all-spark.jar`: the hail jar sans tests. I also checked in the darwin/OSX identity-by-state library. When `shadowJar` is run on any system other than OS X, the resulting jar will contain both that architecture's library and the OS X library. In particular, the build machines will generate jars with both the OS X library and linux x86-64 library. Resolves #1810",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1871
https://github.com/hail-is/hail/pull/1884:8,Deployability,update,updated,8,"Haven't updated python docs yet, but the Scala code should be fine to start reviewing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1884
https://github.com/hail-is/hail/pull/1900:92,Availability,error,error,92,"Add 2 new tutorials, document distributions as preferred way to run locally, small fixes to error messages and printouts",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1900
https://github.com/hail-is/hail/pull/1900:98,Integrability,message,messages,98,"Add 2 new tutorials, document distributions as preferred way to run locally, small fixes to error messages and printouts",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1900
https://github.com/hail-is/hail/pull/1902:1133,Integrability,Wrap,WrappedArray,1133," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1152,Integrability,Wrap,WrappedArray,1152," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1174,Integrability,Wrap,WrappedArray,1174," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1192,Integrability,Wrap,WrappedArray,1192," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1221,Integrability,Wrap,WrappedArray,1221," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1261,Integrability,Wrap,WrappedArray,1261," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1300,Integrability,Wrap,WrappedArray,1300," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1334,Integrability,Wrap,WrappedArray,1334," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1382,Integrability,Wrap,WrappedArray,1382," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1405,Integrability,Wrap,WrappedArray,1405," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1437,Integrability,Wrap,WrappedArray,1437," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1498,Integrability,Wrap,WrappedArray,1498," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1521,Integrability,Wrap,WrappedArray,1521," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1557,Integrability,Wrap,WrappedArray,1557," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1592,Integrability,Wrap,WrappedArray,1592," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1610,Integrability,Wrap,WrappedArray,1610," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1645,Integrability,Wrap,WrappedArray,1645," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1689,Integrability,Wrap,WrappedArray,1689," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1718,Integrability,Wrap,WrappedArray,1718," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1747,Integrability,Wrap,WrappedArray,1747," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1765,Integrability,Wrap,WrappedArray,1765," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1801,Integrability,Wrap,WrappedArray,1801," ## Example Draw 1; ```scala; t Struct {; aj0iFnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1850,Integrability,Wrap,WrappedArray,1850,"Fnxi: Array[Set[Array[Dict[Empty, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( Wrap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1881,Integrability,Wrap,WrappedArray,1881,"y, Dict[Array[Call], Call]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1905,Integrability,Wrap,WrappedArray,1905,"l]]]]],; dA0IS78I: Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1923,Integrability,Wrap,WrappedArray,1923," Set[Empty],; VeGLA3v: Array[Array[Struct {; C: Empty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); ,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:1974,Integrability,Wrap,WrappedArray,1974,"ty,; sHjMXnj: Boolean,; G: Call; }]],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2009,Integrability,Wrap,WrappedArray,2009,"],; Ni: Struct {; HCrSI: Empty,; nJt7: Boolean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2051,Integrability,Wrap,WrappedArray,2051,"lean; },; nxb8CkLI: Int,; y4mYv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2080,Integrability,Wrap,WrappedArray,2080,"Yv_DvH: Dict[Array[Genotype], Variant]; }; v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); ,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2123,Integrability,Wrap,WrappedArray,2123,"v [ WrappedArray( Set( WrappedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(nu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2150,Integrability,Wrap,WrappedArray,2150,"pedArray(null); , WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2196,Integrability,Wrap,WrappedArray,2196,"ay(Map(null -> Map(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map())))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2214,Integrability,Wrap,WrappedArray,2214,"(WrappedArray(2) -> 2)), Map(null -> Map(WrappedArray() -> 1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedAr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2273,Integrability,Wrap,WrappedArray,2273,"1)), Map(null -> Map(WrappedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2297,Integrability,Wrap,WrappedArray,2297,"ppedArray(1) -> 2))); ); , Set(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2326,Integrability,Wrap,WrappedArray,2326,"t(WrappedArray(Map(null -> null), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(Wra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2354,Integrability,Wrap,WrappedArray,2354,"ull), Map(null -> Map(WrappedArray(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); );",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2383,Integrability,Wrap,WrappedArray,2383,"Array(2) -> 31, WrappedArray(0) -> 0)))); , Set(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2430,Integrability,Wrap,WrappedArray,2430,"(WrappedArray(Map(null -> Map()), Map(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2466,Integrability,Wrap,WrappedArray,2466,"p(null -> null))); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2495,Integrability,Wrap,WrappedArray,2495,"ppedArray(Map()); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2538,Integrability,Wrap,WrappedArray,2538,"> Map())); , WrappedArray(Map(null -> null)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2578,Integrability,Wrap,WrappedArray,2578,"ll)); , WrappedArray(); , WrappedArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2609,Integrability,Wrap,WrappedArray,2609,"edArray(null, Map(null -> Map(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArra",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2638,Integrability,Wrap,WrappedArray,2638,"(WrappedArray() -> null)))); , Set(); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,fal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2681,Integrability,Wrap,WrappedArray,2681,"( WrappedArray(Map(null -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2703,Integrability,Wrap,WrappedArray,2703,"l -> Map(WrappedArray() -> null))); , WrappedArray(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2751,Integrability,Wrap,WrappedArray,2751,"ay(); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2820,Integrability,Wrap,WrappedArray,2820,"Map()), Map(null -> Map(WrappedArray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([nul",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2853,Integrability,Wrap,WrappedArray,2853,"ray() -> 2)))); , Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2871,Integrability,Wrap,WrappedArray,2871,"Set(WrappedArray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( Wrappe",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2900,Integrability,Wrap,WrappedArray,2900,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2935,Integrability,Wrap,WrappedArray,2935,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2964,Integrability,Wrap,WrappedArray,2964,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:2987,Integrability,Wrap,WrappedArray,2987,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3016,Integrability,Wrap,WrappedArray,3016,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3042,Integrability,Wrap,WrappedArray,3042,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3093,Integrability,Wrap,WrappedArray,3093,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3111,Integrability,Wrap,WrappedArray,3111,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3140,Integrability,Wrap,WrappedArray,3140,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3170,Integrability,Wrap,WrappedArray,3170,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3212,Integrability,Wrap,WrappedArray,3212,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3241,Integrability,Wrap,WrappedArray,3241,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3267,Integrability,Wrap,WrappedArray,3267,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3330,Integrability,Wrap,WrappedArray,3330,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3377,Integrability,Wrap,WrappedArray,3377,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3391,Integrability,Wrap,WrappedArray,3391,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3444,Integrability,Wrap,WrappedArray,3444,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3462,Integrability,Wrap,WrappedArray,3462,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3509,Integrability,Wrap,WrappedArray,3509,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3535,Integrability,Wrap,WrappedArray,3535,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3561,Integrability,Wrap,WrappedArray,3561,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3605,Integrability,Wrap,WrappedArray,3605,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3658,Integrability,Wrap,WrappedArray,3658,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3698,Integrability,Wrap,WrappedArray,3698,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3732,Integrability,Wrap,WrappedArray,3732,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3763,Integrability,Wrap,WrappedArray,3763,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3810,Integrability,Wrap,WrappedArray,3810,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3872,Integrability,Wrap,WrappedArray,3872,"rray()); , Set( WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(Map(null -> null)); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(5) -> 2))); , WrappedArray(Map())); , Set(); , Set(); , Set(WrappedArray(); , WrappedArray(Map(null -> Map()))); , Set(); , Set( null; , WrappedArray()); , Set( WrappedArray(Map(null -> Map(WrappedArray(1) -> 19))); , WrappedArray(Map(null -> Map(WrappedArray(0) -> 2)), Map(null -> Map())); , WrappedArray(Map(null -> Map())); , WrappedArray(Map(null -> Map(WrappedArray(2) -> null)), Map(null -> Map(WrappedArray() -> 17)), Map(null -> Map(WrappedArray() -> 1)))); , Set(WrappedArray(Map(null -> Map(WrappedArray(null) -> 1)), Map(null -> Map(WrappedArray(2) -> 0, WrappedArray() -> null)), Map(), Map())); , Set(WrappedArray(Map(null -> Map()), Map(null -> Map()), Map(null -> Map(WrappedArray(0) -> 0)))); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(0) -> null)))); , Set(WrappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3935,Integrability,Wrap,WrappedArray,3935,"rappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.:.:3:PL=4,328,0) -> Xtfuo:983235984:G:ACT; , WrappedArray() -> null; , WrappedArray( 0/0:.:.:.:GP=0.835968017578125,0.00244140625,0.161590576171875; , 0/0:401640437,122557639:1151958249:2098:PL=0,29,43; , 1/1:30,8:65:128:PL=419,997,0; ) -> zEeTaoQ:931965768:AGA:*; , WrappedArray(1/1:.:357602351:3000:PL=53,81,0) -> bu8qWa3:440101875:C:CAACG,G,TAAAT,GT,AC,GC,A,TTGCGA; ); ]; ```; ## Example Draw 2; ```scala; t Struct{; W: Empty,; VH6KA0: Array[Empty],; Y6b: Array[Dict[Interval, Set[Interval]]],; gvU1NyKqvR: Set[Dict[Set[Double], Locus]],; wY: Struct{; nT: Variant,; L_9BUfa0z8D: Empty; },; eiN: Array[Variant],; bj36QLPq: Struct{; EPMpm: Long,; ohcAn: Interval; },; y26WlwsMcu: Dict[Locus, Double],; LpE: Array[String],; UroYb: String,; dk: Struct{; VONVG8X9R: Boolean,; eo6fz1YMhtD: Empty; },; wi: Interval,; JK0osYJHRz: Array[Dict[Long, Long]]; }; v [ null; , WrappedArray(null, null, null); , WrappedArray( Map(21:474864761-21:2142353440 -> Set(12:291332689-21:2122081729)); , Map(6:64658535-18:852482744 -> Set(4:920440399-20:99044206), 1:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:3961,Integrability,Wrap,WrappedArray,3961,"rappedArray(Map())); , Set( WrappedArray(Map()); , WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(null -> Map()))); , Set(); , Set( WrappedArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.:.:3:PL=4,328,0) -> Xtfuo:983235984:G:ACT; , WrappedArray() -> null; , WrappedArray( 0/0:.:.:.:GP=0.835968017578125,0.00244140625,0.161590576171875; , 0/0:401640437,122557639:1151958249:2098:PL=0,29,43; , 1/1:30,8:65:128:PL=419,997,0; ) -> zEeTaoQ:931965768:AGA:*; , WrappedArray(1/1:.:357602351:3000:PL=53,81,0) -> bu8qWa3:440101875:C:CAACG,G,TAAAT,GT,AC,GC,A,TTGCGA; ); ]; ```; ## Example Draw 2; ```scala; t Struct{; W: Empty,; VH6KA0: Array[Empty],; Y6b: Array[Dict[Interval, Set[Interval]]],; gvU1NyKqvR: Set[Dict[Set[Double], Locus]],; wY: Struct{; nT: Variant,; L_9BUfa0z8D: Empty; },; eiN: Array[Variant],; bj36QLPq: Struct{; EPMpm: Long,; ohcAn: Interval; },; y26WlwsMcu: Dict[Locus, Double],; LpE: Array[String],; UroYb: String,; dk: Struct{; VONVG8X9R: Boolean,; eo6fz1YMhtD: Empty; },; wi: Interval,; JK0osYJHRz: Array[Dict[Long, Long]]; }; v [ null; , WrappedArray(null, null, null); , WrappedArray( Map(21:474864761-21:2142353440 -> Set(12:291332689-21:2122081729)); , Map(6:64658535-18:852482744 -> Set(4:920440399-20:99044206), 1:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:4157,Integrability,Wrap,WrappedArray,4157,"edArray(); , WrappedArray(Map(null -> Map(WrappedArray(null) -> 2))); , WrappedArray(Map(null -> Map()))); , Set( WrappedArray(Map(null -> Map(WrappedArray() -> 0))); , WrappedArray(Map(), Map(null -> Map()), Map(), Map(null -> Map(WrappedArray(0) -> null)))); ); , Set(null); , WrappedArray( WrappedArray(null, [null,false,1], [null,true,6]); , WrappedArray(); , WrappedArray([null,false,0], [null,true,1]); , WrappedArray(); , null; , WrappedArray(); , null; , WrappedArray(null, [null,false,0], null); , WrappedArray(null, null, null, [null,false,null]); , WrappedArray(null, [null,true,null]); , WrappedArray([null,true,null]); , WrappedArray([null,true,1]); , WrappedArray([null,false,null], null, null); , WrappedArray([null,null,null])); , null; , 2147483647; , Map( WrappedArray(1/1:.:.:3:PL=4,328,0) -> Xtfuo:983235984:G:ACT; , WrappedArray() -> null; , WrappedArray( 0/0:.:.:.:GP=0.835968017578125,0.00244140625,0.161590576171875; , 0/0:401640437,122557639:1151958249:2098:PL=0,29,43; , 1/1:30,8:65:128:PL=419,997,0; ) -> zEeTaoQ:931965768:AGA:*; , WrappedArray(1/1:.:357602351:3000:PL=53,81,0) -> bu8qWa3:440101875:C:CAACG,G,TAAAT,GT,AC,GC,A,TTGCGA; ); ]; ```; ## Example Draw 2; ```scala; t Struct{; W: Empty,; VH6KA0: Array[Empty],; Y6b: Array[Dict[Interval, Set[Interval]]],; gvU1NyKqvR: Set[Dict[Set[Double], Locus]],; wY: Struct{; nT: Variant,; L_9BUfa0z8D: Empty; },; eiN: Array[Variant],; bj36QLPq: Struct{; EPMpm: Long,; ohcAn: Interval; },; y26WlwsMcu: Dict[Locus, Double],; LpE: Array[String],; UroYb: String,; dk: Struct{; VONVG8X9R: Boolean,; eo6fz1YMhtD: Empty; },; wi: Interval,; JK0osYJHRz: Array[Dict[Long, Long]]; }; v [ null; , WrappedArray(null, null, null); , WrappedArray( Map(21:474864761-21:2142353440 -> Set(12:291332689-21:2122081729)); , Map(6:64658535-18:852482744 -> Set(4:920440399-20:99044206), 1:1996448901-Y:2076322341 -> Set(21:1147405636-Y:1021924735)); , Map(); , Map(14:1359868957-19:1770937039 -> Set(8:2031139633-X:1928544108, 4:384201545-12:1909645017",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:4755,Integrability,Wrap,WrappedArray,4755,,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:4789,Integrability,Wrap,WrappedArray,4789,,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:6910,Integrability,Wrap,WrappedArray,6910,"6538111)); , Map(2:1576674560-19:2051426361 -> Set(3:1729279166-4:1468182755)); ); , Set( Map(Set(37.286923408013536) -> 14:236896985, Set(-15.021445198381315) -> 3:2086181046, Set(18.144202437411707) -> 11:43701721, Set(4.9E-324) -> 15:152457683); , Map(null -> 1:930696534); , Map(Set(4.9E-324) -> 19:1442338618); , Map(Set(-36.763366959632954) -> 22:672685435, Set(-1.7976931348623157E308) -> 9:1705107082); , Map(Set() -> 1:186365173, Set(-6.398293809835415E307) -> 11:26598828); , Map(); , Map(Set() -> 19:759113415, Set(0.0, 1.9948878010656322E307, 22.324353631389556) -> 7:1349934030); , Map(Set() -> 6:10152252, Set(1.7976931348623157E308) -> 8:1499191709, Set(4.607043291623889E307) -> 5:316220126, Set(1.0) -> 20:1314749173, Set(4.9E-324) -> 10:1106413461); ); , [D3YSSo:1677841928:TA:*,null]; , WrappedArray( AuXYPeE9g:1581545128:G:GG; , VPnCju:2025837886:CAG:TGG; , GZoiw:11124631:TG:A; , C5c6HNsTe:1539490834:TA:AAC,C,AGC,GAA,GTCA,ACGC,CCT,GG; , z7jhYG8STJ:523877649:A:T,GGACA,G,*,TC,AAAC,AA,GG,AGA; , y0eEFwipq98:835986978:GT:TGTGT; , E4al5h:482756775:C:CGAC; , HAJ1jrSuaoq:1724809203:GA:TC; , w:1098499131:GT:GGT; , dHCyUjxqhr:1258701198:TA:AGC,CC,GTT,TGA,ACATGGA,GC,A,TGAGA,TAAC; ); , [-16,14:878485900-MT:1655450544]; , Map(4:1001834270 -> -1.4590773156850653E308, 10:2024397357 -> 41.20125040141795); , WrappedArray(-E, x', , mZW}Xaz, '(<vn:R, wNtR{u'T, h, , 0)C, PJ), :Q-#J7)N39j, g, , null); , k; , [true,null]; , 11:422731466-12:1318927466; , WrappedArray( Map(-4362806367897070770 -> -1); , Map(-64 -> 37, 8932649469340641424 -> -9223372036854775808, -2581551729758561812 -> 4866402986430542844, -21 -> -53); , null; , Map(48 -> -1, -9203674311477531463 -> -20)); ]; ```. # Example Draw 3; ```scala; t Struct{; YiG6N: Set[Set[Struct{; hAu0wTu: Empty; }]],; tGFebQm0H: Array[Array[Array[Empty]]],; _t24kfMLN: Long,; eSPJnjhs3: Array[Dict[Boolean, Dict[Interval, Empty]]],; B: Float,; pXm: Set[Genotype],; JRsi9GV: Array[Struct{; k0sLX9N: Empty; }],; pMq: Set[Dict[Set[Call], Empty",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:7425,Integrability,Wrap,WrappedArray,7425,"p(Set() -> 19:759113415, Set(0.0, 1.9948878010656322E307, 22.324353631389556) -> 7:1349934030); , Map(Set() -> 6:10152252, Set(1.7976931348623157E308) -> 8:1499191709, Set(4.607043291623889E307) -> 5:316220126, Set(1.0) -> 20:1314749173, Set(4.9E-324) -> 10:1106413461); ); , [D3YSSo:1677841928:TA:*,null]; , WrappedArray( AuXYPeE9g:1581545128:G:GG; , VPnCju:2025837886:CAG:TGG; , GZoiw:11124631:TG:A; , C5c6HNsTe:1539490834:TA:AAC,C,AGC,GAA,GTCA,ACGC,CCT,GG; , z7jhYG8STJ:523877649:A:T,GGACA,G,*,TC,AAAC,AA,GG,AGA; , y0eEFwipq98:835986978:GT:TGTGT; , E4al5h:482756775:C:CGAC; , HAJ1jrSuaoq:1724809203:GA:TC; , w:1098499131:GT:GGT; , dHCyUjxqhr:1258701198:TA:AGC,CC,GTT,TGA,ACATGGA,GC,A,TGAGA,TAAC; ); , [-16,14:878485900-MT:1655450544]; , Map(4:1001834270 -> -1.4590773156850653E308, 10:2024397357 -> 41.20125040141795); , WrappedArray(-E, x', , mZW}Xaz, '(<vn:R, wNtR{u'T, h, , 0)C, PJ), :Q-#J7)N39j, g, , null); , k; , [true,null]; , 11:422731466-12:1318927466; , WrappedArray( Map(-4362806367897070770 -> -1); , Map(-64 -> 37, 8932649469340641424 -> -9223372036854775808, -2581551729758561812 -> 4866402986430542844, -21 -> -53); , null; , Map(48 -> -1, -9203674311477531463 -> -20)); ]; ```. # Example Draw 3; ```scala; t Struct{; YiG6N: Set[Set[Struct{; hAu0wTu: Empty; }]],; tGFebQm0H: Array[Array[Array[Empty]]],; _t24kfMLN: Long,; eSPJnjhs3: Array[Dict[Boolean, Dict[Interval, Empty]]],; B: Float,; pXm: Set[Genotype],; JRsi9GV: Array[Struct{; k0sLX9N: Empty; }],; pMq: Set[Dict[Set[Call], Empty]],; _: Empty,; J: AltAllele,; zo3Fqqd: Variant,; gxw1J4E2s: Double,; NB: Struct{; aD5P8: Dict[String, Variant]; }; }. v [ Set(Set([null]), Set()); , WrappedArray( WrappedArray(WrappedArray(null, null, null, null, null, null)); , WrappedArray(WrappedArray(null, null, null), WrappedArray())); , 7465031920428684276; , WrappedArray( Map(false -> Map()); , Map(false -> Map(X:1505701634-MT:413708900 -> null, 2:908273550-4:1601452326 -> null)); , Map(); , Map(true -> Map(), false -> Map()); , Map()",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:7568,Integrability,Wrap,WrappedArray,7568,"p(Set() -> 19:759113415, Set(0.0, 1.9948878010656322E307, 22.324353631389556) -> 7:1349934030); , Map(Set() -> 6:10152252, Set(1.7976931348623157E308) -> 8:1499191709, Set(4.607043291623889E307) -> 5:316220126, Set(1.0) -> 20:1314749173, Set(4.9E-324) -> 10:1106413461); ); , [D3YSSo:1677841928:TA:*,null]; , WrappedArray( AuXYPeE9g:1581545128:G:GG; , VPnCju:2025837886:CAG:TGG; , GZoiw:11124631:TG:A; , C5c6HNsTe:1539490834:TA:AAC,C,AGC,GAA,GTCA,ACGC,CCT,GG; , z7jhYG8STJ:523877649:A:T,GGACA,G,*,TC,AAAC,AA,GG,AGA; , y0eEFwipq98:835986978:GT:TGTGT; , E4al5h:482756775:C:CGAC; , HAJ1jrSuaoq:1724809203:GA:TC; , w:1098499131:GT:GGT; , dHCyUjxqhr:1258701198:TA:AGC,CC,GTT,TGA,ACATGGA,GC,A,TGAGA,TAAC; ); , [-16,14:878485900-MT:1655450544]; , Map(4:1001834270 -> -1.4590773156850653E308, 10:2024397357 -> 41.20125040141795); , WrappedArray(-E, x', , mZW}Xaz, '(<vn:R, wNtR{u'T, h, , 0)C, PJ), :Q-#J7)N39j, g, , null); , k; , [true,null]; , 11:422731466-12:1318927466; , WrappedArray( Map(-4362806367897070770 -> -1); , Map(-64 -> 37, 8932649469340641424 -> -9223372036854775808, -2581551729758561812 -> 4866402986430542844, -21 -> -53); , null; , Map(48 -> -1, -9203674311477531463 -> -20)); ]; ```. # Example Draw 3; ```scala; t Struct{; YiG6N: Set[Set[Struct{; hAu0wTu: Empty; }]],; tGFebQm0H: Array[Array[Array[Empty]]],; _t24kfMLN: Long,; eSPJnjhs3: Array[Dict[Boolean, Dict[Interval, Empty]]],; B: Float,; pXm: Set[Genotype],; JRsi9GV: Array[Struct{; k0sLX9N: Empty; }],; pMq: Set[Dict[Set[Call], Empty]],; _: Empty,; J: AltAllele,; zo3Fqqd: Variant,; gxw1J4E2s: Double,; NB: Struct{; aD5P8: Dict[String, Variant]; }; }. v [ Set(Set([null]), Set()); , WrappedArray( WrappedArray(WrappedArray(null, null, null, null, null, null)); , WrappedArray(WrappedArray(null, null, null), WrappedArray())); , 7465031920428684276; , WrappedArray( Map(false -> Map()); , Map(false -> Map(X:1505701634-MT:413708900 -> null, 2:908273550-4:1601452326 -> null)); , Map(); , Map(true -> Map(), false -> Map()); , Map()",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:8255,Integrability,Wrap,WrappedArray,8255,"12:1318927466; , WrappedArray( Map(-4362806367897070770 -> -1); , Map(-64 -> 37, 8932649469340641424 -> -9223372036854775808, -2581551729758561812 -> 4866402986430542844, -21 -> -53); , null; , Map(48 -> -1, -9203674311477531463 -> -20)); ]; ```. # Example Draw 3; ```scala; t Struct{; YiG6N: Set[Set[Struct{; hAu0wTu: Empty; }]],; tGFebQm0H: Array[Array[Array[Empty]]],; _t24kfMLN: Long,; eSPJnjhs3: Array[Dict[Boolean, Dict[Interval, Empty]]],; B: Float,; pXm: Set[Genotype],; JRsi9GV: Array[Struct{; k0sLX9N: Empty; }],; pMq: Set[Dict[Set[Call], Empty]],; _: Empty,; J: AltAllele,; zo3Fqqd: Variant,; gxw1J4E2s: Double,; NB: Struct{; aD5P8: Dict[String, Variant]; }; }. v [ Set(Set([null]), Set()); , WrappedArray( WrappedArray(WrappedArray(null, null, null, null, null, null)); , WrappedArray(WrappedArray(null, null, null), WrappedArray())); , 7465031920428684276; , WrappedArray( Map(false -> Map()); , Map(false -> Map(X:1505701634-MT:413708900 -> null, 2:908273550-4:1601452326 -> null)); , Map(); , Map(true -> Map(), false -> Map()); , Map(); , Map(true -> Map()); , Map(false -> Map(2:1696581492-16:1148530363 -> null)); , Map(); , null; , Map(); , Map(false -> Map(11:1204664327-Y:452748442 -> null), true -> Map(1:759029796-8:660188853 -> null)); , Map(); , Map(true -> Map(6:1436710378-14:172736518 -> null))); , 0.0; , Set( ./.:.:.:.:GP=.; , ./.:666232109,323735949:1572839794:5865:PL=414367811,46987628,0; , ./.:37,16:69:123:PL=0,482,280; , 0/0:.:.:.:GP=1.0,0.0,0.0; , ./.:42,44:91:99:PL=453,0,746; , 0/1:.:.:.:GP=0.448516845703125,0.551483154296875,0.0; , 0/0:22,45:80:90:PL=0,535,934; ); , null; , Set( null; , Map(Set() -> null, Set(null) -> null); , Map(Set(2) -> null, Set() -> null); , Map(); , Map(Set(0) -> null, Set() -> null, Set(0, 2) -> null); , Map(Set(1) -> null); , Map(Set(0) -> null)); , null; , AAT/GATGA; , bm7TK_:989763744:CAGT:GG; , 4.9E-324; , [Map(null -> HaUqd5y:234050762:A:G; , x7d=h,D -> z:141633474:CTC:AGAT,AGT,AGCAA,AACG,TA,TCTG,GT,GAT,GC; , rqS3:WB'I -> ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:8269,Integrability,Wrap,WrappedArray,8269,"12:1318927466; , WrappedArray( Map(-4362806367897070770 -> -1); , Map(-64 -> 37, 8932649469340641424 -> -9223372036854775808, -2581551729758561812 -> 4866402986430542844, -21 -> -53); , null; , Map(48 -> -1, -9203674311477531463 -> -20)); ]; ```. # Example Draw 3; ```scala; t Struct{; YiG6N: Set[Set[Struct{; hAu0wTu: Empty; }]],; tGFebQm0H: Array[Array[Array[Empty]]],; _t24kfMLN: Long,; eSPJnjhs3: Array[Dict[Boolean, Dict[Interval, Empty]]],; B: Float,; pXm: Set[Genotype],; JRsi9GV: Array[Struct{; k0sLX9N: Empty; }],; pMq: Set[Dict[Set[Call], Empty]],; _: Empty,; J: AltAllele,; zo3Fqqd: Variant,; gxw1J4E2s: Double,; NB: Struct{; aD5P8: Dict[String, Variant]; }; }. v [ Set(Set([null]), Set()); , WrappedArray( WrappedArray(WrappedArray(null, null, null, null, null, null)); , WrappedArray(WrappedArray(null, null, null), WrappedArray())); , 7465031920428684276; , WrappedArray( Map(false -> Map()); , Map(false -> Map(X:1505701634-MT:413708900 -> null, 2:908273550-4:1601452326 -> null)); , Map(); , Map(true -> Map(), false -> Map()); , Map(); , Map(true -> Map()); , Map(false -> Map(2:1696581492-16:1148530363 -> null)); , Map(); , null; , Map(); , Map(false -> Map(11:1204664327-Y:452748442 -> null), true -> Map(1:759029796-8:660188853 -> null)); , Map(); , Map(true -> Map(6:1436710378-14:172736518 -> null))); , 0.0; , Set( ./.:.:.:.:GP=.; , ./.:666232109,323735949:1572839794:5865:PL=414367811,46987628,0; , ./.:37,16:69:123:PL=0,482,280; , 0/0:.:.:.:GP=1.0,0.0,0.0; , ./.:42,44:91:99:PL=453,0,746; , 0/1:.:.:.:GP=0.448516845703125,0.551483154296875,0.0; , 0/0:22,45:80:90:PL=0,535,934; ); , null; , Set( null; , Map(Set() -> null, Set(null) -> null); , Map(Set(2) -> null, Set() -> null); , Map(); , Map(Set(0) -> null, Set() -> null, Set(0, 2) -> null); , Map(Set(1) -> null); , Map(Set(0) -> null)); , null; , AAT/GATGA; , bm7TK_:989763744:CAGT:GG; , 4.9E-324; , [Map(null -> HaUqd5y:234050762:A:G; , x7d=h,D -> z:141633474:CTC:AGAT,AGT,AGCAA,AACG,TA,TCTG,GT,GAT,GC; , rqS3:WB'I -> ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:8282,Integrability,Wrap,WrappedArray,8282,"12:1318927466; , WrappedArray( Map(-4362806367897070770 -> -1); , Map(-64 -> 37, 8932649469340641424 -> -9223372036854775808, -2581551729758561812 -> 4866402986430542844, -21 -> -53); , null; , Map(48 -> -1, -9203674311477531463 -> -20)); ]; ```. # Example Draw 3; ```scala; t Struct{; YiG6N: Set[Set[Struct{; hAu0wTu: Empty; }]],; tGFebQm0H: Array[Array[Array[Empty]]],; _t24kfMLN: Long,; eSPJnjhs3: Array[Dict[Boolean, Dict[Interval, Empty]]],; B: Float,; pXm: Set[Genotype],; JRsi9GV: Array[Struct{; k0sLX9N: Empty; }],; pMq: Set[Dict[Set[Call], Empty]],; _: Empty,; J: AltAllele,; zo3Fqqd: Variant,; gxw1J4E2s: Double,; NB: Struct{; aD5P8: Dict[String, Variant]; }; }. v [ Set(Set([null]), Set()); , WrappedArray( WrappedArray(WrappedArray(null, null, null, null, null, null)); , WrappedArray(WrappedArray(null, null, null), WrappedArray())); , 7465031920428684276; , WrappedArray( Map(false -> Map()); , Map(false -> Map(X:1505701634-MT:413708900 -> null, 2:908273550-4:1601452326 -> null)); , Map(); , Map(true -> Map(), false -> Map()); , Map(); , Map(true -> Map()); , Map(false -> Map(2:1696581492-16:1148530363 -> null)); , Map(); , null; , Map(); , Map(false -> Map(11:1204664327-Y:452748442 -> null), true -> Map(1:759029796-8:660188853 -> null)); , Map(); , Map(true -> Map(6:1436710378-14:172736518 -> null))); , 0.0; , Set( ./.:.:.:.:GP=.; , ./.:666232109,323735949:1572839794:5865:PL=414367811,46987628,0; , ./.:37,16:69:123:PL=0,482,280; , 0/0:.:.:.:GP=1.0,0.0,0.0; , ./.:42,44:91:99:PL=453,0,746; , 0/1:.:.:.:GP=0.448516845703125,0.551483154296875,0.0; , 0/0:22,45:80:90:PL=0,535,934; ); , null; , Set( null; , Map(Set() -> null, Set(null) -> null); , Map(Set(2) -> null, Set() -> null); , Map(); , Map(Set(0) -> null, Set() -> null, Set(0, 2) -> null); , Map(Set(1) -> null); , Map(Set(0) -> null)); , null; , AAT/GATGA; , bm7TK_:989763744:CAGT:GG; , 4.9E-324; , [Map(null -> HaUqd5y:234050762:A:G; , x7d=h,D -> z:141633474:CTC:AGAT,AGT,AGCAA,AACG,TA,TCTG,GT,GAT,GC; , rqS3:WB'I -> ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:8335,Integrability,Wrap,WrappedArray,8335,"12:1318927466; , WrappedArray( Map(-4362806367897070770 -> -1); , Map(-64 -> 37, 8932649469340641424 -> -9223372036854775808, -2581551729758561812 -> 4866402986430542844, -21 -> -53); , null; , Map(48 -> -1, -9203674311477531463 -> -20)); ]; ```. # Example Draw 3; ```scala; t Struct{; YiG6N: Set[Set[Struct{; hAu0wTu: Empty; }]],; tGFebQm0H: Array[Array[Array[Empty]]],; _t24kfMLN: Long,; eSPJnjhs3: Array[Dict[Boolean, Dict[Interval, Empty]]],; B: Float,; pXm: Set[Genotype],; JRsi9GV: Array[Struct{; k0sLX9N: Empty; }],; pMq: Set[Dict[Set[Call], Empty]],; _: Empty,; J: AltAllele,; zo3Fqqd: Variant,; gxw1J4E2s: Double,; NB: Struct{; aD5P8: Dict[String, Variant]; }; }. v [ Set(Set([null]), Set()); , WrappedArray( WrappedArray(WrappedArray(null, null, null, null, null, null)); , WrappedArray(WrappedArray(null, null, null), WrappedArray())); , 7465031920428684276; , WrappedArray( Map(false -> Map()); , Map(false -> Map(X:1505701634-MT:413708900 -> null, 2:908273550-4:1601452326 -> null)); , Map(); , Map(true -> Map(), false -> Map()); , Map(); , Map(true -> Map()); , Map(false -> Map(2:1696581492-16:1148530363 -> null)); , Map(); , null; , Map(); , Map(false -> Map(11:1204664327-Y:452748442 -> null), true -> Map(1:759029796-8:660188853 -> null)); , Map(); , Map(true -> Map(6:1436710378-14:172736518 -> null))); , 0.0; , Set( ./.:.:.:.:GP=.; , ./.:666232109,323735949:1572839794:5865:PL=414367811,46987628,0; , ./.:37,16:69:123:PL=0,482,280; , 0/0:.:.:.:GP=1.0,0.0,0.0; , ./.:42,44:91:99:PL=453,0,746; , 0/1:.:.:.:GP=0.448516845703125,0.551483154296875,0.0; , 0/0:22,45:80:90:PL=0,535,934; ); , null; , Set( null; , Map(Set() -> null, Set(null) -> null); , Map(Set(2) -> null, Set() -> null); , Map(); , Map(Set(0) -> null, Set() -> null, Set(0, 2) -> null); , Map(Set(1) -> null); , Map(Set(0) -> null)); , null; , AAT/GATGA; , bm7TK_:989763744:CAGT:GG; , 4.9E-324; , [Map(null -> HaUqd5y:234050762:A:G; , x7d=h,D -> z:141633474:CTC:AGAT,AGT,AGCAA,AACG,TA,TCTG,GT,GAT,GC; , rqS3:WB'I -> ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:8348,Integrability,Wrap,WrappedArray,8348,"12:1318927466; , WrappedArray( Map(-4362806367897070770 -> -1); , Map(-64 -> 37, 8932649469340641424 -> -9223372036854775808, -2581551729758561812 -> 4866402986430542844, -21 -> -53); , null; , Map(48 -> -1, -9203674311477531463 -> -20)); ]; ```. # Example Draw 3; ```scala; t Struct{; YiG6N: Set[Set[Struct{; hAu0wTu: Empty; }]],; tGFebQm0H: Array[Array[Array[Empty]]],; _t24kfMLN: Long,; eSPJnjhs3: Array[Dict[Boolean, Dict[Interval, Empty]]],; B: Float,; pXm: Set[Genotype],; JRsi9GV: Array[Struct{; k0sLX9N: Empty; }],; pMq: Set[Dict[Set[Call], Empty]],; _: Empty,; J: AltAllele,; zo3Fqqd: Variant,; gxw1J4E2s: Double,; NB: Struct{; aD5P8: Dict[String, Variant]; }; }. v [ Set(Set([null]), Set()); , WrappedArray( WrappedArray(WrappedArray(null, null, null, null, null, null)); , WrappedArray(WrappedArray(null, null, null), WrappedArray())); , 7465031920428684276; , WrappedArray( Map(false -> Map()); , Map(false -> Map(X:1505701634-MT:413708900 -> null, 2:908273550-4:1601452326 -> null)); , Map(); , Map(true -> Map(), false -> Map()); , Map(); , Map(true -> Map()); , Map(false -> Map(2:1696581492-16:1148530363 -> null)); , Map(); , null; , Map(); , Map(false -> Map(11:1204664327-Y:452748442 -> null), true -> Map(1:759029796-8:660188853 -> null)); , Map(); , Map(true -> Map(6:1436710378-14:172736518 -> null))); , 0.0; , Set( ./.:.:.:.:GP=.; , ./.:666232109,323735949:1572839794:5865:PL=414367811,46987628,0; , ./.:37,16:69:123:PL=0,482,280; , 0/0:.:.:.:GP=1.0,0.0,0.0; , ./.:42,44:91:99:PL=453,0,746; , 0/1:.:.:.:GP=0.448516845703125,0.551483154296875,0.0; , 0/0:22,45:80:90:PL=0,535,934; ); , null; , Set( null; , Map(Set() -> null, Set(null) -> null); , Map(Set(2) -> null, Set() -> null); , Map(); , Map(Set(0) -> null, Set() -> null, Set(0, 2) -> null); , Map(Set(1) -> null); , Map(Set(0) -> null)); , null; , AAT/GATGA; , bm7TK_:989763744:CAGT:GG; , 4.9E-324; , [Map(null -> HaUqd5y:234050762:A:G; , x7d=h,D -> z:141633474:CTC:AGAT,AGT,AGCAA,AACG,TA,TCTG,GT,GAT,GC; , rqS3:WB'I -> ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:8380,Integrability,Wrap,WrappedArray,8380,"12:1318927466; , WrappedArray( Map(-4362806367897070770 -> -1); , Map(-64 -> 37, 8932649469340641424 -> -9223372036854775808, -2581551729758561812 -> 4866402986430542844, -21 -> -53); , null; , Map(48 -> -1, -9203674311477531463 -> -20)); ]; ```. # Example Draw 3; ```scala; t Struct{; YiG6N: Set[Set[Struct{; hAu0wTu: Empty; }]],; tGFebQm0H: Array[Array[Array[Empty]]],; _t24kfMLN: Long,; eSPJnjhs3: Array[Dict[Boolean, Dict[Interval, Empty]]],; B: Float,; pXm: Set[Genotype],; JRsi9GV: Array[Struct{; k0sLX9N: Empty; }],; pMq: Set[Dict[Set[Call], Empty]],; _: Empty,; J: AltAllele,; zo3Fqqd: Variant,; gxw1J4E2s: Double,; NB: Struct{; aD5P8: Dict[String, Variant]; }; }. v [ Set(Set([null]), Set()); , WrappedArray( WrappedArray(WrappedArray(null, null, null, null, null, null)); , WrappedArray(WrappedArray(null, null, null), WrappedArray())); , 7465031920428684276; , WrappedArray( Map(false -> Map()); , Map(false -> Map(X:1505701634-MT:413708900 -> null, 2:908273550-4:1601452326 -> null)); , Map(); , Map(true -> Map(), false -> Map()); , Map(); , Map(true -> Map()); , Map(false -> Map(2:1696581492-16:1148530363 -> null)); , Map(); , null; , Map(); , Map(false -> Map(11:1204664327-Y:452748442 -> null), true -> Map(1:759029796-8:660188853 -> null)); , Map(); , Map(true -> Map(6:1436710378-14:172736518 -> null))); , 0.0; , Set( ./.:.:.:.:GP=.; , ./.:666232109,323735949:1572839794:5865:PL=414367811,46987628,0; , ./.:37,16:69:123:PL=0,482,280; , 0/0:.:.:.:GP=1.0,0.0,0.0; , ./.:42,44:91:99:PL=453,0,746; , 0/1:.:.:.:GP=0.448516845703125,0.551483154296875,0.0; , 0/0:22,45:80:90:PL=0,535,934; ); , null; , Set( null; , Map(Set() -> null, Set(null) -> null); , Map(Set(2) -> null, Set() -> null); , Map(); , Map(Set(0) -> null, Set() -> null, Set(0, 2) -> null); , Map(Set(1) -> null); , Map(Set(0) -> null)); , null; , AAT/GATGA; , bm7TK_:989763744:CAGT:GG; , 4.9E-324; , [Map(null -> HaUqd5y:234050762:A:G; , x7d=h,D -> z:141633474:CTC:AGAT,AGT,AGCAA,AACG,TA,TCTG,GT,GAT,GC; , rqS3:WB'I -> ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:8423,Integrability,Wrap,WrappedArray,8423,"12:1318927466; , WrappedArray( Map(-4362806367897070770 -> -1); , Map(-64 -> 37, 8932649469340641424 -> -9223372036854775808, -2581551729758561812 -> 4866402986430542844, -21 -> -53); , null; , Map(48 -> -1, -9203674311477531463 -> -20)); ]; ```. # Example Draw 3; ```scala; t Struct{; YiG6N: Set[Set[Struct{; hAu0wTu: Empty; }]],; tGFebQm0H: Array[Array[Array[Empty]]],; _t24kfMLN: Long,; eSPJnjhs3: Array[Dict[Boolean, Dict[Interval, Empty]]],; B: Float,; pXm: Set[Genotype],; JRsi9GV: Array[Struct{; k0sLX9N: Empty; }],; pMq: Set[Dict[Set[Call], Empty]],; _: Empty,; J: AltAllele,; zo3Fqqd: Variant,; gxw1J4E2s: Double,; NB: Struct{; aD5P8: Dict[String, Variant]; }; }. v [ Set(Set([null]), Set()); , WrappedArray( WrappedArray(WrappedArray(null, null, null, null, null, null)); , WrappedArray(WrappedArray(null, null, null), WrappedArray())); , 7465031920428684276; , WrappedArray( Map(false -> Map()); , Map(false -> Map(X:1505701634-MT:413708900 -> null, 2:908273550-4:1601452326 -> null)); , Map(); , Map(true -> Map(), false -> Map()); , Map(); , Map(true -> Map()); , Map(false -> Map(2:1696581492-16:1148530363 -> null)); , Map(); , null; , Map(); , Map(false -> Map(11:1204664327-Y:452748442 -> null), true -> Map(1:759029796-8:660188853 -> null)); , Map(); , Map(true -> Map(6:1436710378-14:172736518 -> null))); , 0.0; , Set( ./.:.:.:.:GP=.; , ./.:666232109,323735949:1572839794:5865:PL=414367811,46987628,0; , ./.:37,16:69:123:PL=0,482,280; , 0/0:.:.:.:GP=1.0,0.0,0.0; , ./.:42,44:91:99:PL=453,0,746; , 0/1:.:.:.:GP=0.448516845703125,0.551483154296875,0.0; , 0/0:22,45:80:90:PL=0,535,934; ); , null; , Set( null; , Map(Set() -> null, Set(null) -> null); , Map(Set(2) -> null, Set() -> null); , Map(); , Map(Set(0) -> null, Set() -> null, Set(0, 2) -> null); , Map(Set(1) -> null); , Map(Set(0) -> null)); , null; , AAT/GATGA; , bm7TK_:989763744:CAGT:GG; , 4.9E-324; , [Map(null -> HaUqd5y:234050762:A:G; , x7d=h,D -> z:141633474:CTC:AGAT,AGT,AGCAA,AACG,TA,TCTG,GT,GAT,GC; , rqS3:WB'I -> ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:10960,Safety,avoid,avoid,10960,"richlet-Multinomial distribution](https://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution) with all α_n set to `parts`, which tends towards a uniformly distributed vector. Upon writing this PR, I realize it might be more desirable to set all α_n to 1. Commentary welcome. 3. When we choose the length of a random sequence, we choose it using the [Beta-Binomial Distribution](https://en.wikipedia.org/wiki/Beta-binomial_distribution) with `n` set to the maximum sequence size (usually the Gen's `size`), α set to `3` and β set to `6 * ln(n + 0.01)`. My choice of β is motivated by keeping the mass away from `0` when `n` is small. In particular, I'd like the mean to be greater than or equal to one, so `nα/(α+β) >= 1` so for, say, n=4, `β/α <= 3`. For large values of `n` I want values closer to zero than given by α=3, β=4. I really want commentary on how to choose sequence lengths. Choosing from a Beta-Binomial with a beta weighted by the natural log (shifted by 0.01 to avoid ln(1) = 0) seems very unnatural. Here's α=3, β=6, at n=4 and n=100:. <img width=""752"" alt=""screen shot 2017-06-05 at 2 40 04 pm"" src=""https://cloud.githubusercontent.com/assets/106194/26798031/1c14c0d4-49fd-11e7-9423-0cc46f412145.png"">; <img width=""752"" alt=""screen shot 2017-06-05 at 2 40 18 pm"" src=""https://cloud.githubusercontent.com/assets/106194/26798032/1c16b7d6-49fd-11e7-9a5c-04766ddab9ba.png"">. Note that the mean for n=100 is close to 35, pretty big. Now here's α=3, β=6*ln(n+0.01), at n=4 and n=100:. <img width=""752"" alt=""screen shot 2017-06-05 at 2 40 54 pm"" src=""https://cloud.githubusercontent.com/assets/106194/26798030/1bfc3e06-49fd-11e7-80d9-882e050c5087.png"">; <img width=""752"" alt=""screen shot 2017-06-05 at 2 40 34 pm"" src=""https://cloud.githubusercontent.com/assets/106194/26798033/1c1ad85c-49fd-11e7-8294-b8a5466ade4c.png"">. The n=4 case doesn't change much but the n=100 case biases towards smaller values, which is generally what we want for random sequences (if any algorithm works o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1902:10936,Testability,log,log,10936,"tition the `size` of a random generator, we partition using a [Dirichlet-Multinomial distribution](https://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution) with all α_n set to `parts`, which tends towards a uniformly distributed vector. Upon writing this PR, I realize it might be more desirable to set all α_n to 1. Commentary welcome. 3. When we choose the length of a random sequence, we choose it using the [Beta-Binomial Distribution](https://en.wikipedia.org/wiki/Beta-binomial_distribution) with `n` set to the maximum sequence size (usually the Gen's `size`), α set to `3` and β set to `6 * ln(n + 0.01)`. My choice of β is motivated by keeping the mass away from `0` when `n` is small. In particular, I'd like the mean to be greater than or equal to one, so `nα/(α+β) >= 1` so for, say, n=4, `β/α <= 3`. For large values of `n` I want values closer to zero than given by α=3, β=4. I really want commentary on how to choose sequence lengths. Choosing from a Beta-Binomial with a beta weighted by the natural log (shifted by 0.01 to avoid ln(1) = 0) seems very unnatural. Here's α=3, β=6, at n=4 and n=100:. <img width=""752"" alt=""screen shot 2017-06-05 at 2 40 04 pm"" src=""https://cloud.githubusercontent.com/assets/106194/26798031/1c14c0d4-49fd-11e7-9423-0cc46f412145.png"">; <img width=""752"" alt=""screen shot 2017-06-05 at 2 40 18 pm"" src=""https://cloud.githubusercontent.com/assets/106194/26798032/1c16b7d6-49fd-11e7-9a5c-04766ddab9ba.png"">. Note that the mean for n=100 is close to 35, pretty big. Now here's α=3, β=6*ln(n+0.01), at n=4 and n=100:. <img width=""752"" alt=""screen shot 2017-06-05 at 2 40 54 pm"" src=""https://cloud.githubusercontent.com/assets/106194/26798030/1bfc3e06-49fd-11e7-80d9-882e050c5087.png"">; <img width=""752"" alt=""screen shot 2017-06-05 at 2 40 34 pm"" src=""https://cloud.githubusercontent.com/assets/106194/26798033/1c1ad85c-49fd-11e7-8294-b8a5466ade4c.png"">. The n=4 case doesn't change much but the n=100 case biases towards smaller values, which is gener",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1902
https://github.com/hail-is/hail/pull/1903:1345,Integrability,Wrap,WrappedArray,1345," tqbHQbDv: Dict[AltAllele, Long],; HUh: Set[Double],; Xgb26Wlws: Double,; qe_XlLJt7_X: Dict[Double, Int],; U: Array[Call],; j: Double; }; v [ 1; , 5/6:.:.:.:GP=0.0,0.0,6.103515625E-4,0.0,0.0,0.0,0.0,0.00286865234375,0.0,0.058441162109375,0.0,0.0,0.0,0.00567626953125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.004730224609375,0.153839111328125,0.0,0.0,0.0,0.704742431640625,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.018402099609375,0.050689697265625,0.0,0.0,0.0,0.0,0.0,0.0,376902925948550232; , 2147483647; , 1; , true; , 15:396751431-18:1945111790; , Map(GT/C -> -9223372036854775808, GC/TGTG -> 0, GTCAC/AC -> 1, TCG/G -> -2853243060319614448, TTTG/G -> 0); , Set(0.0, 1.2590508536333097E308, -26.66700965052354, -1.0, 1.7976931348623157E308); , -44.18866673875504; , Map(4.9E-324 -> 1069254047, -29.21881886290265 -> 0, -7.511481628119398E307 -> -51783790, 78.3075905923555 -> -1679218199, -1.0 -> -1476797686, 66.69344244874847 -> -2147483648, -74.87563451361888 -> -50, 8.529797881337316E306 -> -38); , WrappedArray(2, 2, null, 2, 2, 1); , 1.7044473544408425E308; ]; ```; ```scala; t Struct{; jHUkH: Interval,; c: Struct{; EZyb77: Boolean,; qckA6k: Empty; },; K: Interval,; X: Locus,; BuujVaardN: Call,; drTH1J: Set[Locus],; DJL9uj7D: Variant,; vwuq: Set[Int],; cKAObAm1oh7: Boolean,; FqhLLOlV4p: Struct{; Ri631ZK2TiA: Empty; },; vtQ: Set[String],; HzHvw: Locus,; g: Double,; inwJHuBmLUM: Boolean,; Q: Float,; i: Genotype,; q: Float,; p_Wmn2Q: Variant,; Y6foXEa7F: Dict[Set[Float], Double],; SuXouO__uX: Int,; hrfM: Locus,; k: Variant,; V1WzY: Struct{; xWj: Struct{; bg: AltAllele; }; },; L3Ol_: Call,; Bmt: Variant,; EExpF_H: Struct{; DyAZQ1pL: Empty; },; JSc3FhxVxoM: Array[Dict[String, Empty]],; Iuu: Dict[Variant, Double],; qXBGmKS: Long,; QQfQtf3ct: Call,; bSsLYsTDI: Array[Array[Call]],; ad1iBzwaFZf: Dict[String, Locus],; Z2cD2KmFIkG: Call,; VSqH: AltAllele,; XVZqCYGf3_: Double,; NBdmoAaGkoL: Boolean,; r: Array[AltAllele],; W_NOSJIvTd: Double,; s3B8QiAqQ: Long,; lk: Float,; S1Fo: Float,; PEcTjU8vo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1903
https://github.com/hail-is/hail/pull/1903:2999,Integrability,Wrap,WrappedArray,2999,",; L3Ol_: Call,; Bmt: Variant,; EExpF_H: Struct{; DyAZQ1pL: Empty; },; JSc3FhxVxoM: Array[Dict[String, Empty]],; Iuu: Dict[Variant, Double],; qXBGmKS: Long,; QQfQtf3ct: Call,; bSsLYsTDI: Array[Array[Call]],; ad1iBzwaFZf: Dict[String, Locus],; Z2cD2KmFIkG: Call,; VSqH: AltAllele,; XVZqCYGf3_: Double,; NBdmoAaGkoL: Boolean,; r: Array[AltAllele],; W_NOSJIvTd: Double,; s3B8QiAqQ: Long,; lk: Float,; S1Fo: Float,; PEcTjU8vomh: Empty,; REgY: Double,; ZUB: Variant,; b9UtO: Set[Float],; sT8o_aQ: Array[Call],; Igr1: Call,; A6ditFSwmRK: Call,; EmxV: Boolean,; a7: Struct{; JtHJhnfateR: Empty; }; }; v [ 14:1955123271-18:1272376844; , [false,null]; , 14:1256937813-17:1583114455; , 3:4546980; , 2; , Set(null, 15:1947873743, 13:866526696, 6:1573282210, 19:891774512); , GCaO16Nsy8:77911856:ATA:TG; , Set(72); , false; , [null]; , Set(F&S~b); , 7:472895707; , null; , true; , Infinity; , ./.:22,28:75:108:PL=167,0,83; , Infinity; , rdQ:630981228:T:CTA; , Map(); , -28; , 11:810964873; , RFJrknBvIH:1425413812:TAT:GC; , [[GCCAC/A]]; , 1; , qxomnU6Nqr:1347703197:G:A; , [null]; , WrappedArray(); , Map(m:190688303:C:AG -> -1.7976931348623157E308, lcg7p:2050711280:G:GATC -> 4.9E-324, DBeo6xuPH:1588993816:T:CTA -> 8.537643765112484E307); , -5599614078518791215; , 1; , null; , Map(y -> 17:750270934); , 0; , G/CATG; , -1.4729264086204403E307; , false; , WrappedArray(); , null; , -9223372036854775808; , 12.945546; , -Infinity; , null; , null; , QWxPKk:173048886:GTC:TCTT; , null; , WrappedArray(); , null; , 2; , false; , [null]; ]; ```; ```scala; t Struct{; CKkepccXC: Float,; Np: Locus,; fP8kTXty9: Double,; Qp6uH: Set[Set[Double]],; K7wSG1F3: Variant,; lLo85q: AltAllele,; v1du: Genotype; }; v [ -Infinity; , 5:2134996951; , 0.0; , Set( Set(7.482063689522203E307, -4.3155177478799624E305, -1.428773456444566E308, 9.332975286117578E307); , Set(); , null; , Set(49.15738854346134, 1.5361471805543802E308); , Set(-7.448920624629132E306, 13.921804085458959, 4.9E-324, -40.02694783286595, -54.20909301114429); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1903
https://github.com/hail-is/hail/pull/1903:3273,Integrability,Wrap,WrappedArray,3273,"Td: Double,; s3B8QiAqQ: Long,; lk: Float,; S1Fo: Float,; PEcTjU8vomh: Empty,; REgY: Double,; ZUB: Variant,; b9UtO: Set[Float],; sT8o_aQ: Array[Call],; Igr1: Call,; A6ditFSwmRK: Call,; EmxV: Boolean,; a7: Struct{; JtHJhnfateR: Empty; }; }; v [ 14:1955123271-18:1272376844; , [false,null]; , 14:1256937813-17:1583114455; , 3:4546980; , 2; , Set(null, 15:1947873743, 13:866526696, 6:1573282210, 19:891774512); , GCaO16Nsy8:77911856:ATA:TG; , Set(72); , false; , [null]; , Set(F&S~b); , 7:472895707; , null; , true; , Infinity; , ./.:22,28:75:108:PL=167,0,83; , Infinity; , rdQ:630981228:T:CTA; , Map(); , -28; , 11:810964873; , RFJrknBvIH:1425413812:TAT:GC; , [[GCCAC/A]]; , 1; , qxomnU6Nqr:1347703197:G:A; , [null]; , WrappedArray(); , Map(m:190688303:C:AG -> -1.7976931348623157E308, lcg7p:2050711280:G:GATC -> 4.9E-324, DBeo6xuPH:1588993816:T:CTA -> 8.537643765112484E307); , -5599614078518791215; , 1; , null; , Map(y -> 17:750270934); , 0; , G/CATG; , -1.4729264086204403E307; , false; , WrappedArray(); , null; , -9223372036854775808; , 12.945546; , -Infinity; , null; , null; , QWxPKk:173048886:GTC:TCTT; , null; , WrappedArray(); , null; , 2; , false; , [null]; ]; ```; ```scala; t Struct{; CKkepccXC: Float,; Np: Locus,; fP8kTXty9: Double,; Qp6uH: Set[Set[Double]],; K7wSG1F3: Variant,; lLo85q: AltAllele,; v1du: Genotype; }; v [ -Infinity; , 5:2134996951; , 0.0; , Set( Set(7.482063689522203E307, -4.3155177478799624E305, -1.428773456444566E308, 9.332975286117578E307); , Set(); , null; , Set(49.15738854346134, 1.5361471805543802E308); , Set(-7.448920624629132E306, 13.921804085458959, 4.9E-324, -40.02694783286595, -54.20909301114429); , Set(-83.09343891814032, -9.62087242188959E307); , Set(1.0, -1.187010079565314E308); , Set(63.00077009047835); , Set(-1.3633961562402215E308, 1.0); , Set(12.662410481996574, 67.13193558644645); , Set(-78.81449395803094); , Set(1.7283261397633746E308, -8.611348087572712E307, -7.64522521854318E307); , Set(4.9E-324); , Set(1.0); , Set(-1.650631596617604E3",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1903
https://github.com/hail-is/hail/pull/1903:3402,Integrability,Wrap,WrappedArray,3402,"truct{; JtHJhnfateR: Empty; }; }; v [ 14:1955123271-18:1272376844; , [false,null]; , 14:1256937813-17:1583114455; , 3:4546980; , 2; , Set(null, 15:1947873743, 13:866526696, 6:1573282210, 19:891774512); , GCaO16Nsy8:77911856:ATA:TG; , Set(72); , false; , [null]; , Set(F&S~b); , 7:472895707; , null; , true; , Infinity; , ./.:22,28:75:108:PL=167,0,83; , Infinity; , rdQ:630981228:T:CTA; , Map(); , -28; , 11:810964873; , RFJrknBvIH:1425413812:TAT:GC; , [[GCCAC/A]]; , 1; , qxomnU6Nqr:1347703197:G:A; , [null]; , WrappedArray(); , Map(m:190688303:C:AG -> -1.7976931348623157E308, lcg7p:2050711280:G:GATC -> 4.9E-324, DBeo6xuPH:1588993816:T:CTA -> 8.537643765112484E307); , -5599614078518791215; , 1; , null; , Map(y -> 17:750270934); , 0; , G/CATG; , -1.4729264086204403E307; , false; , WrappedArray(); , null; , -9223372036854775808; , 12.945546; , -Infinity; , null; , null; , QWxPKk:173048886:GTC:TCTT; , null; , WrappedArray(); , null; , 2; , false; , [null]; ]; ```; ```scala; t Struct{; CKkepccXC: Float,; Np: Locus,; fP8kTXty9: Double,; Qp6uH: Set[Set[Double]],; K7wSG1F3: Variant,; lLo85q: AltAllele,; v1du: Genotype; }; v [ -Infinity; , 5:2134996951; , 0.0; , Set( Set(7.482063689522203E307, -4.3155177478799624E305, -1.428773456444566E308, 9.332975286117578E307); , Set(); , null; , Set(49.15738854346134, 1.5361471805543802E308); , Set(-7.448920624629132E306, 13.921804085458959, 4.9E-324, -40.02694783286595, -54.20909301114429); , Set(-83.09343891814032, -9.62087242188959E307); , Set(1.0, -1.187010079565314E308); , Set(63.00077009047835); , Set(-1.3633961562402215E308, 1.0); , Set(12.662410481996574, 67.13193558644645); , Set(-78.81449395803094); , Set(1.7283261397633746E308, -8.611348087572712E307, -7.64522521854318E307); , Set(4.9E-324); , Set(1.0); , Set(-1.650631596617604E308, -70.42650232974452, 1.0); , Set(-5.249660537956704E307, 1.378948908122762E308); ); , ufg2CarjGf:682440091:C:GG; , A/G; , 0/0:.:.:.:GP=0.737579345703125,0.251190185546875,0.01123046875; ]; ```; ```scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1903
https://github.com/hail-is/hail/pull/1903:4951,Integrability,Wrap,WrappedArray,4951,"ppedArray(); , null; , 2; , false; , [null]; ]; ```; ```scala; t Struct{; CKkepccXC: Float,; Np: Locus,; fP8kTXty9: Double,; Qp6uH: Set[Set[Double]],; K7wSG1F3: Variant,; lLo85q: AltAllele,; v1du: Genotype; }; v [ -Infinity; , 5:2134996951; , 0.0; , Set( Set(7.482063689522203E307, -4.3155177478799624E305, -1.428773456444566E308, 9.332975286117578E307); , Set(); , null; , Set(49.15738854346134, 1.5361471805543802E308); , Set(-7.448920624629132E306, 13.921804085458959, 4.9E-324, -40.02694783286595, -54.20909301114429); , Set(-83.09343891814032, -9.62087242188959E307); , Set(1.0, -1.187010079565314E308); , Set(63.00077009047835); , Set(-1.3633961562402215E308, 1.0); , Set(12.662410481996574, 67.13193558644645); , Set(-78.81449395803094); , Set(1.7283261397633746E308, -8.611348087572712E307, -7.64522521854318E307); , Set(4.9E-324); , Set(1.0); , Set(-1.650631596617604E308, -70.42650232974452, 1.0); , Set(-5.249660537956704E307, 1.378948908122762E308); ); , ufg2CarjGf:682440091:C:GG; , A/G; , 0/0:.:.:.:GP=0.737579345703125,0.251190185546875,0.01123046875; ]; ```; ```scala; t Struct{; Fm9ba: AltAllele,; t0_xi1m: Variant,; or8z4gwOp: Array[Set[AltAllele]],; OP: Interval,; bT: Array[Boolean],; iSJ9a0UwQpG: String,; WQwn: Array[Interval],; jGqh3xtcro: Double,; g0Kv11a: Variant,; ky36: AltAllele,; Jb8: Float,; MoBxPDdvGw: Float,; gW: Array[Interval],; a4: Struct{; B09aTY: Set[Long]; },; HkUrazxdp: Set[Struct{; L9XR6: Empty,; JJeICteE3Tm: Locus; }],; mBv9ljpHD: Int,; st: Array[Interval]; }; v [ C/AGTG; , AF:1132740736:CTG:GGCCT; , WrappedArray(null, Set(C/AC)); , 13:791385633-15:44039816; , WrappedArray(null, false, false, false, true); , caE; , null; , -1.0; , uAvZ6S5txx:551788563:CA:CC; , ACG/CA; , -9.697367; , 1.0; , WrappedArray(5:421782041-MT:51623882, 13:1157604585-X:1520033480, 2:246980474-5:355241032, 19:105200491-20:1422252263, 1:1169885632-7:583182317, 18:342278997-19:366035620); , [null]; , Set([null,4:557653812], [null,null]); , 2147483647; , WrappedArray(); ]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1903
https://github.com/hail-is/hail/pull/1903:5012,Integrability,Wrap,WrappedArray,5012,"ppedArray(); , null; , 2; , false; , [null]; ]; ```; ```scala; t Struct{; CKkepccXC: Float,; Np: Locus,; fP8kTXty9: Double,; Qp6uH: Set[Set[Double]],; K7wSG1F3: Variant,; lLo85q: AltAllele,; v1du: Genotype; }; v [ -Infinity; , 5:2134996951; , 0.0; , Set( Set(7.482063689522203E307, -4.3155177478799624E305, -1.428773456444566E308, 9.332975286117578E307); , Set(); , null; , Set(49.15738854346134, 1.5361471805543802E308); , Set(-7.448920624629132E306, 13.921804085458959, 4.9E-324, -40.02694783286595, -54.20909301114429); , Set(-83.09343891814032, -9.62087242188959E307); , Set(1.0, -1.187010079565314E308); , Set(63.00077009047835); , Set(-1.3633961562402215E308, 1.0); , Set(12.662410481996574, 67.13193558644645); , Set(-78.81449395803094); , Set(1.7283261397633746E308, -8.611348087572712E307, -7.64522521854318E307); , Set(4.9E-324); , Set(1.0); , Set(-1.650631596617604E308, -70.42650232974452, 1.0); , Set(-5.249660537956704E307, 1.378948908122762E308); ); , ufg2CarjGf:682440091:C:GG; , A/G; , 0/0:.:.:.:GP=0.737579345703125,0.251190185546875,0.01123046875; ]; ```; ```scala; t Struct{; Fm9ba: AltAllele,; t0_xi1m: Variant,; or8z4gwOp: Array[Set[AltAllele]],; OP: Interval,; bT: Array[Boolean],; iSJ9a0UwQpG: String,; WQwn: Array[Interval],; jGqh3xtcro: Double,; g0Kv11a: Variant,; ky36: AltAllele,; Jb8: Float,; MoBxPDdvGw: Float,; gW: Array[Interval],; a4: Struct{; B09aTY: Set[Long]; },; HkUrazxdp: Set[Struct{; L9XR6: Empty,; JJeICteE3Tm: Locus; }],; mBv9ljpHD: Int,; st: Array[Interval]; }; v [ C/AGTG; , AF:1132740736:CTG:GGCCT; , WrappedArray(null, Set(C/AC)); , 13:791385633-15:44039816; , WrappedArray(null, false, false, false, true); , caE; , null; , -1.0; , uAvZ6S5txx:551788563:CA:CC; , ACG/CA; , -9.697367; , 1.0; , WrappedArray(5:421782041-MT:51623882, 13:1157604585-X:1520033480, 2:246980474-5:355241032, 19:105200491-20:1422252263, 1:1169885632-7:583182317, 18:342278997-19:366035620); , [null]; , Set([null,4:557653812], [null,null]); , 2147483647; , WrappedArray(); ]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1903
https://github.com/hail-is/hail/pull/1903:5144,Integrability,Wrap,WrappedArray,5144,"ppedArray(); , null; , 2; , false; , [null]; ]; ```; ```scala; t Struct{; CKkepccXC: Float,; Np: Locus,; fP8kTXty9: Double,; Qp6uH: Set[Set[Double]],; K7wSG1F3: Variant,; lLo85q: AltAllele,; v1du: Genotype; }; v [ -Infinity; , 5:2134996951; , 0.0; , Set( Set(7.482063689522203E307, -4.3155177478799624E305, -1.428773456444566E308, 9.332975286117578E307); , Set(); , null; , Set(49.15738854346134, 1.5361471805543802E308); , Set(-7.448920624629132E306, 13.921804085458959, 4.9E-324, -40.02694783286595, -54.20909301114429); , Set(-83.09343891814032, -9.62087242188959E307); , Set(1.0, -1.187010079565314E308); , Set(63.00077009047835); , Set(-1.3633961562402215E308, 1.0); , Set(12.662410481996574, 67.13193558644645); , Set(-78.81449395803094); , Set(1.7283261397633746E308, -8.611348087572712E307, -7.64522521854318E307); , Set(4.9E-324); , Set(1.0); , Set(-1.650631596617604E308, -70.42650232974452, 1.0); , Set(-5.249660537956704E307, 1.378948908122762E308); ); , ufg2CarjGf:682440091:C:GG; , A/G; , 0/0:.:.:.:GP=0.737579345703125,0.251190185546875,0.01123046875; ]; ```; ```scala; t Struct{; Fm9ba: AltAllele,; t0_xi1m: Variant,; or8z4gwOp: Array[Set[AltAllele]],; OP: Interval,; bT: Array[Boolean],; iSJ9a0UwQpG: String,; WQwn: Array[Interval],; jGqh3xtcro: Double,; g0Kv11a: Variant,; ky36: AltAllele,; Jb8: Float,; MoBxPDdvGw: Float,; gW: Array[Interval],; a4: Struct{; B09aTY: Set[Long]; },; HkUrazxdp: Set[Struct{; L9XR6: Empty,; JJeICteE3Tm: Locus; }],; mBv9ljpHD: Int,; st: Array[Interval]; }; v [ C/AGTG; , AF:1132740736:CTG:GGCCT; , WrappedArray(null, Set(C/AC)); , 13:791385633-15:44039816; , WrappedArray(null, false, false, false, true); , caE; , null; , -1.0; , uAvZ6S5txx:551788563:CA:CC; , ACG/CA; , -9.697367; , 1.0; , WrappedArray(5:421782041-MT:51623882, 13:1157604585-X:1520033480, 2:246980474-5:355241032, 19:105200491-20:1422252263, 1:1169885632-7:583182317, 18:342278997-19:366035620); , [null]; , Set([null,4:557653812], [null,null]); , 2147483647; , WrappedArray(); ]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1903
https://github.com/hail-is/hail/pull/1903:5383,Integrability,Wrap,WrappedArray,5383,"ppedArray(); , null; , 2; , false; , [null]; ]; ```; ```scala; t Struct{; CKkepccXC: Float,; Np: Locus,; fP8kTXty9: Double,; Qp6uH: Set[Set[Double]],; K7wSG1F3: Variant,; lLo85q: AltAllele,; v1du: Genotype; }; v [ -Infinity; , 5:2134996951; , 0.0; , Set( Set(7.482063689522203E307, -4.3155177478799624E305, -1.428773456444566E308, 9.332975286117578E307); , Set(); , null; , Set(49.15738854346134, 1.5361471805543802E308); , Set(-7.448920624629132E306, 13.921804085458959, 4.9E-324, -40.02694783286595, -54.20909301114429); , Set(-83.09343891814032, -9.62087242188959E307); , Set(1.0, -1.187010079565314E308); , Set(63.00077009047835); , Set(-1.3633961562402215E308, 1.0); , Set(12.662410481996574, 67.13193558644645); , Set(-78.81449395803094); , Set(1.7283261397633746E308, -8.611348087572712E307, -7.64522521854318E307); , Set(4.9E-324); , Set(1.0); , Set(-1.650631596617604E308, -70.42650232974452, 1.0); , Set(-5.249660537956704E307, 1.378948908122762E308); ); , ufg2CarjGf:682440091:C:GG; , A/G; , 0/0:.:.:.:GP=0.737579345703125,0.251190185546875,0.01123046875; ]; ```; ```scala; t Struct{; Fm9ba: AltAllele,; t0_xi1m: Variant,; or8z4gwOp: Array[Set[AltAllele]],; OP: Interval,; bT: Array[Boolean],; iSJ9a0UwQpG: String,; WQwn: Array[Interval],; jGqh3xtcro: Double,; g0Kv11a: Variant,; ky36: AltAllele,; Jb8: Float,; MoBxPDdvGw: Float,; gW: Array[Interval],; a4: Struct{; B09aTY: Set[Long]; },; HkUrazxdp: Set[Struct{; L9XR6: Empty,; JJeICteE3Tm: Locus; }],; mBv9ljpHD: Int,; st: Array[Interval]; }; v [ C/AGTG; , AF:1132740736:CTG:GGCCT; , WrappedArray(null, Set(C/AC)); , 13:791385633-15:44039816; , WrappedArray(null, false, false, false, true); , caE; , null; , -1.0; , uAvZ6S5txx:551788563:CA:CC; , ACG/CA; , -9.697367; , 1.0; , WrappedArray(5:421782041-MT:51623882, 13:1157604585-X:1520033480, 2:246980474-5:355241032, 19:105200491-20:1422252263, 1:1169885632-7:583182317, 18:342278997-19:366035620); , [null]; , Set([null,4:557653812], [null,null]); , 2147483647; , WrappedArray(); ]; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1903
https://github.com/hail-is/hail/pull/1903:8,Modifiability,extend,extends,8,"This PR extends #1902, merge that one first. Now, 50% of the time a type is a Scalar type rather than a recursive one. Additionally, this PR adds a generator for pairs of types and values of that type. Here's a few samples:; ```scala; t Struct{; W: Call,; GL: Genotype,; nsfJ: Long,; tiX0pLA: Int,; B7vWAW: Call,; a6_mm: Boolean,; AGS1Eh: Interval,; tqbHQbDv: Dict[AltAllele, Long],; HUh: Set[Double],; Xgb26Wlws: Double,; qe_XlLJt7_X: Dict[Double, Int],; U: Array[Call],; j: Double; }; v [ 1; , 5/6:.:.:.:GP=0.0,0.0,6.103515625E-4,0.0,0.0,0.0,0.0,0.00286865234375,0.0,0.058441162109375,0.0,0.0,0.0,0.00567626953125,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.004730224609375,0.153839111328125,0.0,0.0,0.0,0.704742431640625,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.018402099609375,0.050689697265625,0.0,0.0,0.0,0.0,0.0,0.0,376902925948550232; , 2147483647; , 1; , true; , 15:396751431-18:1945111790; , Map(GT/C -> -9223372036854775808, GC/TGTG -> 0, GTCAC/AC -> 1, TCG/G -> -2853243060319614448, TTTG/G -> 0); , Set(0.0, 1.2590508536333097E308, -26.66700965052354, -1.0, 1.7976931348623157E308); , -44.18866673875504; , Map(4.9E-324 -> 1069254047, -29.21881886290265 -> 0, -7.511481628119398E307 -> -51783790, 78.3075905923555 -> -1679218199, -1.0 -> -1476797686, 66.69344244874847 -> -2147483648, -74.87563451361888 -> -50, 8.529797881337316E306 -> -38); , WrappedArray(2, 2, null, 2, 2, 1); , 1.7044473544408425E308; ]; ```; ```scala; t Struct{; jHUkH: Interval,; c: Struct{; EZyb77: Boolean,; qckA6k: Empty; },; K: Interval,; X: Locus,; BuujVaardN: Call,; drTH1J: Set[Locus],; DJL9uj7D: Variant,; vwuq: Set[Int],; cKAObAm1oh7: Boolean,; FqhLLOlV4p: Struct{; Ri631ZK2TiA: Empty; },; vtQ: Set[String],; HzHvw: Locus,; g: Double,; inwJHuBmLUM: Boolean,; Q: Float,; i: Genotype,; q: Float,; p_Wmn2Q: Variant,; Y6foXEa7F: Dict[Set[Float], Double],; SuXouO__uX: Int,; hrfM: Locus,; k: Variant,; V1WzY: Struct{; xWj: Struct{; bg: AltAllele; }; },; L3Ol_: Call,; Bmt: Variant,; EExpF_H: Struct{; DyAZQ1pL: Empty; },; JS",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1903
https://github.com/hail-is/hail/pull/1906:61,Integrability,depend,dependency,61,"Moved pyspark imports to be local to methods, removing numpy dependency",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1906
https://github.com/hail-is/hail/issues/1913:385,Availability,down,download,385,"I'm not 100% sure if this is true, but I've seen someone say on Stack Overflow that Macs only get BLAS and LAPACK natives after XCode is installed. We say on our Getting Started that these should automatically work on OSX, but we've also always required a C compiler as a getting started step. Now that we are going to distribute prebuilt packages, it may be the case that users won't download XCode because they don't need to and as such won't get the natives. We should verify whether or not natives are present prior to XCode installation and update docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1913
https://github.com/hail-is/hail/issues/1913:137,Deployability,install,installed,137,"I'm not 100% sure if this is true, but I've seen someone say on Stack Overflow that Macs only get BLAS and LAPACK natives after XCode is installed. We say on our Getting Started that these should automatically work on OSX, but we've also always required a C compiler as a getting started step. Now that we are going to distribute prebuilt packages, it may be the case that users won't download XCode because they don't need to and as such won't get the natives. We should verify whether or not natives are present prior to XCode installation and update docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1913
https://github.com/hail-is/hail/issues/1913:529,Deployability,install,installation,529,"I'm not 100% sure if this is true, but I've seen someone say on Stack Overflow that Macs only get BLAS and LAPACK natives after XCode is installed. We say on our Getting Started that these should automatically work on OSX, but we've also always required a C compiler as a getting started step. Now that we are going to distribute prebuilt packages, it may be the case that users won't download XCode because they don't need to and as such won't get the natives. We should verify whether or not natives are present prior to XCode installation and update docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1913
https://github.com/hail-is/hail/issues/1913:546,Deployability,update,update,546,"I'm not 100% sure if this is true, but I've seen someone say on Stack Overflow that Macs only get BLAS and LAPACK natives after XCode is installed. We say on our Getting Started that these should automatically work on OSX, but we've also always required a C compiler as a getting started step. Now that we are going to distribute prebuilt packages, it may be the case that users won't download XCode because they don't need to and as such won't get the natives. We should verify whether or not natives are present prior to XCode installation and update docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1913
https://github.com/hail-is/hail/pull/1929:142,Safety,safe,safe,142,"I think functionality wise this is all ready to go. Added to the docs as well, though those probably need another iteration. Either way, it's safe to start looking this over and give some feedback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1929
https://github.com/hail-is/hail/pull/1929:188,Usability,feedback,feedback,188,"I think functionality wise this is all ready to go. Added to the docs as well, though those probably need another iteration. Either way, it's safe to start looking this over and give some feedback.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1929
https://github.com/hail-is/hail/pull/1934:269,Integrability,depend,dependsOn,269,"@jbloom22 This is a start. Add your content to `jobs.md`. Feel free to change the styling on the banner. You can modify the html code in `www/jobs.xslt` and the css code in `www/style.css`. For iterating, I find it helpful to add a new gradle task:. ```; task testDocs(dependsOn: ['copyPDF', 'copyWebsiteContent', 'buildIndex', 'buildJobs']); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1934
https://github.com/hail-is/hail/pull/1934:260,Testability,test,testDocs,260,"@jbloom22 This is a start. Add your content to `jobs.md`. Feel free to change the styling on the banner. You can modify the html code in `www/jobs.xslt` and the css code in `www/style.css`. For iterating, I find it helpful to add a new gradle task:. ```; task testDocs(dependsOn: ['copyPDF', 'copyWebsiteContent', 'buildIndex', 'buildJobs']); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1934
https://github.com/hail-is/hail/pull/1937:89,Performance,perform,performance,89,Allows for local computation of ld matrix when the matrix is relatively small to improve performance.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1937
https://github.com/hail-is/hail/pull/1952:261,Availability,avail,available,261,"Addresses #1943 . [JVM Spec, Chapter 6](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html) states, regarding, DCMPG and DCMPL:; > NaN is unordered, so any double comparison fails if either or both of its operands are NaN. With both dcmpg and dcmpl available, any double comparison may be compiled to push the same result onto the operand stack whether the comparison fails on non-NaN values or fails because it encountered a NaN. For more information, see [§3.5](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-3.html#jvms-3.5). The `G` and `L` suffices refer to whether the presence of one or more `NaN`s should be indicated by returning ""greater than"" or ""less than"". Ergo, when we're checking `x > y` we use `DCMPL` so the NaN case produces `-1` with which the downstream comparison to `0` produces `false`. Confusingly, the simple intuition is, if you're checking **L**ess Than, you should use the **G** version. If you're checking **G**reater Than, you should use the **L** version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1952
https://github.com/hail-is/hail/pull/1952:785,Availability,down,downstream,785,"Addresses #1943 . [JVM Spec, Chapter 6](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html) states, regarding, DCMPG and DCMPL:; > NaN is unordered, so any double comparison fails if either or both of its operands are NaN. With both dcmpg and dcmpl available, any double comparison may be compiled to push the same result onto the operand stack whether the comparison fails on non-NaN values or fails because it encountered a NaN. For more information, see [§3.5](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-3.html#jvms-3.5). The `G` and `L` suffices refer to whether the presence of one or more `NaN`s should be indicated by returning ""greater than"" or ""less than"". Ergo, when we're checking `x > y` we use `DCMPL` so the NaN case produces `-1` with which the downstream comparison to `0` produces `false`. Confusingly, the simple intuition is, if you're checking **L**ess Than, you should use the **G** version. If you're checking **G**reater Than, you should use the **L** version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1952
https://github.com/hail-is/hail/pull/1952:849,Usability,simpl,simple,849,"Addresses #1943 . [JVM Spec, Chapter 6](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html) states, regarding, DCMPG and DCMPL:; > NaN is unordered, so any double comparison fails if either or both of its operands are NaN. With both dcmpg and dcmpl available, any double comparison may be compiled to push the same result onto the operand stack whether the comparison fails on non-NaN values or fails because it encountered a NaN. For more information, see [§3.5](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-3.html#jvms-3.5). The `G` and `L` suffices refer to whether the presence of one or more `NaN`s should be indicated by returning ""greater than"" or ""less than"". Ergo, when we're checking `x > y` we use `DCMPL` so the NaN case produces `-1` with which the downstream comparison to `0` produces `false`. Confusingly, the simple intuition is, if you're checking **L**ess Than, you should use the **G** version. If you're checking **G**reater Than, you should use the **L** version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1952
https://github.com/hail-is/hail/pull/1952:856,Usability,intuit,intuition,856,"Addresses #1943 . [JVM Spec, Chapter 6](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-6.html) states, regarding, DCMPG and DCMPL:; > NaN is unordered, so any double comparison fails if either or both of its operands are NaN. With both dcmpg and dcmpl available, any double comparison may be compiled to push the same result onto the operand stack whether the comparison fails on non-NaN values or fails because it encountered a NaN. For more information, see [§3.5](https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-3.html#jvms-3.5). The `G` and `L` suffices refer to whether the presence of one or more `NaN`s should be indicated by returning ""greater than"" or ""less than"". Ergo, when we're checking `x > y` we use `DCMPL` so the NaN case produces `-1` with which the downstream comparison to `0` produces `false`. Confusingly, the simple intuition is, if you're checking **L**ess Than, you should use the **G** version. If you're checking **G**reater Than, you should use the **L** version.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1952
https://github.com/hail-is/hail/pull/1957:154,Deployability,pipeline,pipeline,154,"Replace ProbabilityIterator with ProbabilityArray. This is slightly; faster and cleaner. Speeds up import_bgen, filter_variants(maf), linreg_multi_pheno; pipeline by about 5%. Remove samplePloidy array to reduce memory usage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1957
https://github.com/hail-is/hail/pull/1957:205,Energy Efficiency,reduce,reduce,205,"Replace ProbabilityIterator with ProbabilityArray. This is slightly; faster and cleaner. Speeds up import_bgen, filter_variants(maf), linreg_multi_pheno; pipeline by about 5%. Remove samplePloidy array to reduce memory usage.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1957
https://github.com/hail-is/hail/pull/1962:20,Integrability,Depend,Depends,20,In the UKBBv2 case. Depends on https://github.com/hail-is/hail/pull/1957,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1962
https://github.com/hail-is/hail/pull/1964:123,Testability,test,test,123,"- Skat ; class with the methods for running regression, computing skatStat, and calling C code to compute; p value; - Skat test suite; - Skat R ; R code which compares computed values against the R SKAT package",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1964
https://github.com/hail-is/hail/pull/1969:110,Performance,perform,performed,110,"toBlockMatrixDense method did not specify size of matrix in constructor, meaning that querying its size later performed an RDD action.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1969
https://github.com/hail-is/hail/pull/1972:28,Safety,unsafe,unsafe,28,Found this while working on unsafe stuff.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1972
https://github.com/hail-is/hail/pull/1973:121,Testability,test,test,121,"- Skat; class with the methods for running regression, computing skatStat, and calling C code to compute p value. - Skat test suite; - Skat R; R code which compares computed values against the R SKAT package",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1973
https://github.com/hail-is/hail/issues/1975:304,Performance,perform,perform,304,"Consider a NxM matrix, X, where N << M. The product X ⨯ Xᵀ has size NxN. If the block size is chosen on the order of N, say, N/2, then the number of partitions in the product is four. The number of partitions in X is much much larger. As a result, we miss out on some parallelism because four nodes must perform a tremendous number of multiplications. When N << M, we may consider using tree aggregations until N and M are the same order of magnitude. In particular, we can break the sum into chunks and use a tree aggregation. Consider:. ```; +---------+ +----+ +----+; | A | | B | = | C |; +---------+ | | +----+; | |; +----+; ```; We can slice A column-wise into three chunks and B column-wise into three chunks:. ```; +---------+ +----+ +----+; | : : | | .. | 1 = | C |; +---------+ | | 2 +----+; 1 2 3 | ˙˙ | 3; +----+; ```. multiply the chunks and then sum them:. ```; +----+ +----+ +----+ +----+ +----+ +----+ +----+; | A1 | | B1 | + | A2 | | B2 | + | A3 | | B3 | = | C |; +----+ +----+ +----+ +----+ +----+ +----+ +----+; ```. This would be a tree aggregate with tree of size three. I think we should trigger this whenever N < M/10, and we should generate a 10-ary tree of summations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1975
https://github.com/hail-is/hail/issues/1976:59,Availability,error,error,59,Trying to construct a genotype stream fails with assertion error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1976
https://github.com/hail-is/hail/issues/1976:49,Testability,assert,assertion,49,Trying to construct a genotype stream fails with assertion error,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1976
https://github.com/hail-is/hail/issues/1979:592,Energy Efficiency,efficient,efficiently,592,"[`BlockMatrixIsDistributedMatrix`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/distributedmatrix/BlockMatrixIsDistributedMatrix.scala) implements the [`DistributedMatrix`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/distributedmatrix/DistributedMatrix.scala) API for Spark's `BlockMatrix` type. We should rewrite `BlockMatrix` from scratch to use Breeze matrices because the Spark `DenseMatrix` type doesn't provide a rich interface, in particular there are no exposed mutation primitives. I hope that an implementation on top of Breeze can more efficiently implement `vectorAddToEveryColumn` and `vectorPointwiseMultiplyEveryColumn` and `vectorPointwiseMultiplyEveryRow`. Also, we can move into `is.hail.distributedmatrix` `BetterBlockMatrix` which we, rather illicitly, shove into the apache package during jar creation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1979
https://github.com/hail-is/hail/issues/1979:469,Integrability,interface,interface,469,"[`BlockMatrixIsDistributedMatrix`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/distributedmatrix/BlockMatrixIsDistributedMatrix.scala) implements the [`DistributedMatrix`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/distributedmatrix/DistributedMatrix.scala) API for Spark's `BlockMatrix` type. We should rewrite `BlockMatrix` from scratch to use Breeze matrices because the Spark `DenseMatrix` type doesn't provide a rich interface, in particular there are no exposed mutation primitives. I hope that an implementation on top of Breeze can more efficiently implement `vectorAddToEveryColumn` and `vectorPointwiseMultiplyEveryColumn` and `vectorPointwiseMultiplyEveryRow`. Also, we can move into `is.hail.distributedmatrix` `BetterBlockMatrix` which we, rather illicitly, shove into the apache package during jar creation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1979
https://github.com/hail-is/hail/issues/1979:351,Modifiability,rewrite,rewrite,351,"[`BlockMatrixIsDistributedMatrix`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/distributedmatrix/BlockMatrixIsDistributedMatrix.scala) implements the [`DistributedMatrix`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/distributedmatrix/DistributedMatrix.scala) API for Spark's `BlockMatrix` type. We should rewrite `BlockMatrix` from scratch to use Breeze matrices because the Spark `DenseMatrix` type doesn't provide a rich interface, in particular there are no exposed mutation primitives. I hope that an implementation on top of Breeze can more efficiently implement `vectorAddToEveryColumn` and `vectorPointwiseMultiplyEveryColumn` and `vectorPointwiseMultiplyEveryRow`. Also, we can move into `is.hail.distributedmatrix` `BetterBlockMatrix` which we, rather illicitly, shove into the apache package during jar creation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1979
https://github.com/hail-is/hail/issues/1979:507,Security,expose,exposed,507,"[`BlockMatrixIsDistributedMatrix`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/distributedmatrix/BlockMatrixIsDistributedMatrix.scala) implements the [`DistributedMatrix`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/distributedmatrix/DistributedMatrix.scala) API for Spark's `BlockMatrix` type. We should rewrite `BlockMatrix` from scratch to use Breeze matrices because the Spark `DenseMatrix` type doesn't provide a rich interface, in particular there are no exposed mutation primitives. I hope that an implementation on top of Breeze can more efficiently implement `vectorAddToEveryColumn` and `vectorPointwiseMultiplyEveryColumn` and `vectorPointwiseMultiplyEveryRow`. Also, we can move into `is.hail.distributedmatrix` `BetterBlockMatrix` which we, rather illicitly, shove into the apache package during jar creation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1979
https://github.com/hail-is/hail/pull/1981:229,Testability,test,test,229,I want to use them to answer this bioinformatics.SE question:. https://bioinformatics.stackexchange.com/questions/974/selecting-sites-from-vcf-which-have-an-alt-ad-10. ```; from hail import *; hc = HailContext(); (hc.import_vcf('test.vcf'); .filter_variants_expr('gs.exists(g => g.ad[1:].exists(d => d > 10))'); .count()); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1981
https://github.com/hail-is/hail/issues/1983:73,Availability,error,error,73,"From gitter:. ```; Konrad Karczewski @konradjk 14:12; ooooo that's a fun error:; Error summary: RuntimeException: Internal hail error, bad binding in function registry for `count' with argument types Aggregable[Variant], List(): Arity0Aggregator(Long,<function0>); i know what i did wrong, but that's not a particularly helpful error. Daniel King @danking 14:13; @konradjk can you share the expression that caused this?. Konrad Karczewski @konradjk 14:13; print hc.read(vds_path).query_variants('variants.count'); :facepalm:. Daniel King @danking 14:14; mhmm; terrible error message indeed; ```. Somehow we're looking up a `FieldType` but getting the `Arity0Aggregator` for `count` [which has `MethodType`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L1843) (see also [def'n of `registerAggregator`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L578)). I suspect something is wrong in either `FunctionRegistry.lookupConversion` or `TypeTag.unify` or `FunctionRegistry.lookup`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1983
https://github.com/hail-is/hail/issues/1983:81,Availability,Error,Error,81,"From gitter:. ```; Konrad Karczewski @konradjk 14:12; ooooo that's a fun error:; Error summary: RuntimeException: Internal hail error, bad binding in function registry for `count' with argument types Aggregable[Variant], List(): Arity0Aggregator(Long,<function0>); i know what i did wrong, but that's not a particularly helpful error. Daniel King @danking 14:13; @konradjk can you share the expression that caused this?. Konrad Karczewski @konradjk 14:13; print hc.read(vds_path).query_variants('variants.count'); :facepalm:. Daniel King @danking 14:14; mhmm; terrible error message indeed; ```. Somehow we're looking up a `FieldType` but getting the `Arity0Aggregator` for `count` [which has `MethodType`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L1843) (see also [def'n of `registerAggregator`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L578)). I suspect something is wrong in either `FunctionRegistry.lookupConversion` or `TypeTag.unify` or `FunctionRegistry.lookup`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1983
https://github.com/hail-is/hail/issues/1983:128,Availability,error,error,128,"From gitter:. ```; Konrad Karczewski @konradjk 14:12; ooooo that's a fun error:; Error summary: RuntimeException: Internal hail error, bad binding in function registry for `count' with argument types Aggregable[Variant], List(): Arity0Aggregator(Long,<function0>); i know what i did wrong, but that's not a particularly helpful error. Daniel King @danking 14:13; @konradjk can you share the expression that caused this?. Konrad Karczewski @konradjk 14:13; print hc.read(vds_path).query_variants('variants.count'); :facepalm:. Daniel King @danking 14:14; mhmm; terrible error message indeed; ```. Somehow we're looking up a `FieldType` but getting the `Arity0Aggregator` for `count` [which has `MethodType`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L1843) (see also [def'n of `registerAggregator`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L578)). I suspect something is wrong in either `FunctionRegistry.lookupConversion` or `TypeTag.unify` or `FunctionRegistry.lookup`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1983
https://github.com/hail-is/hail/issues/1983:328,Availability,error,error,328,"From gitter:. ```; Konrad Karczewski @konradjk 14:12; ooooo that's a fun error:; Error summary: RuntimeException: Internal hail error, bad binding in function registry for `count' with argument types Aggregable[Variant], List(): Arity0Aggregator(Long,<function0>); i know what i did wrong, but that's not a particularly helpful error. Daniel King @danking 14:13; @konradjk can you share the expression that caused this?. Konrad Karczewski @konradjk 14:13; print hc.read(vds_path).query_variants('variants.count'); :facepalm:. Daniel King @danking 14:14; mhmm; terrible error message indeed; ```. Somehow we're looking up a `FieldType` but getting the `Arity0Aggregator` for `count` [which has `MethodType`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L1843) (see also [def'n of `registerAggregator`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L578)). I suspect something is wrong in either `FunctionRegistry.lookupConversion` or `TypeTag.unify` or `FunctionRegistry.lookup`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1983
https://github.com/hail-is/hail/issues/1983:569,Availability,error,error,569,"From gitter:. ```; Konrad Karczewski @konradjk 14:12; ooooo that's a fun error:; Error summary: RuntimeException: Internal hail error, bad binding in function registry for `count' with argument types Aggregable[Variant], List(): Arity0Aggregator(Long,<function0>); i know what i did wrong, but that's not a particularly helpful error. Daniel King @danking 14:13; @konradjk can you share the expression that caused this?. Konrad Karczewski @konradjk 14:13; print hc.read(vds_path).query_variants('variants.count'); :facepalm:. Daniel King @danking 14:14; mhmm; terrible error message indeed; ```. Somehow we're looking up a `FieldType` but getting the `Arity0Aggregator` for `count` [which has `MethodType`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L1843) (see also [def'n of `registerAggregator`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L578)). I suspect something is wrong in either `FunctionRegistry.lookupConversion` or `TypeTag.unify` or `FunctionRegistry.lookup`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1983
https://github.com/hail-is/hail/issues/1983:575,Integrability,message,message,575,"From gitter:. ```; Konrad Karczewski @konradjk 14:12; ooooo that's a fun error:; Error summary: RuntimeException: Internal hail error, bad binding in function registry for `count' with argument types Aggregable[Variant], List(): Arity0Aggregator(Long,<function0>); i know what i did wrong, but that's not a particularly helpful error. Daniel King @danking 14:13; @konradjk can you share the expression that caused this?. Konrad Karczewski @konradjk 14:13; print hc.read(vds_path).query_variants('variants.count'); :facepalm:. Daniel King @danking 14:14; mhmm; terrible error message indeed; ```. Somehow we're looking up a `FieldType` but getting the `Arity0Aggregator` for `count` [which has `MethodType`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L1843) (see also [def'n of `registerAggregator`](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/FunctionRegistry.scala#L578)). I suspect something is wrong in either `FunctionRegistry.lookupConversion` or `TypeTag.unify` or `FunctionRegistry.lookup`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1983
https://github.com/hail-is/hail/pull/1984:60,Safety,safe,safe,60,"Definitely not ready to go in, but functionally working and safe to take a look. Did not change docs / python at all, so don't look there yet.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1984
https://github.com/hail-is/hail/pull/1986:84,Testability,test,test,84,This is a placeholder PR so I don't forget about this bug. I still need to devise a test case that triggers it. I saw it happen on the Broad on-prem cluster.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/1986
https://github.com/hail-is/hail/issues/1990:32,Energy Efficiency,reduce,reduce,32,"Basically this:. ```; [1,2,3,4].reduce(0, (x, y) => x + y, (x, x) => x + x); ```; In particular, TJ wants:. ```; m : Array[Array[Double]]; m.reduce([], (x, y) => x + y, (x, x) => x + x); ```; which should effectively compute the sums of the columns of this ""matrix""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1990
https://github.com/hail-is/hail/issues/1990:141,Energy Efficiency,reduce,reduce,141,"Basically this:. ```; [1,2,3,4].reduce(0, (x, y) => x + y, (x, x) => x + x); ```; In particular, TJ wants:. ```; m : Array[Array[Double]]; m.reduce([], (x, y) => x + y, (x, x) => x + x); ```; which should effectively compute the sums of the columns of this ""matrix""",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1990
https://github.com/hail-is/hail/issues/1996:22,Energy Efficiency,efficient,efficiently,22,We need a method that efficiently takes an `Array` of comparable elements and returns an array of indices which are sorted by the corresponding values. A reference implementation is:. ```scala; def sortedIndices[T: Comparable](xs: Array[T]): Array[Int] =; xs.zipWithIndex.sortBy(_._1).map(_._2); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/1996
https://github.com/hail-is/hail/pull/2000:0,Safety,Avoid,Avoid,0,Avoid creating List (!) of genotypes.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2000
https://github.com/hail-is/hail/pull/2001:23,Safety,unsafe,unsafe,23,I want to use this for unsafe testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2001
https://github.com/hail-is/hail/pull/2001:30,Testability,test,testing,30,I want to use this for unsafe testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2001
https://github.com/hail-is/hail/issues/2005:42,Modifiability,inherit,inherited,42,"Because, you know, that one is maternally inherited.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2005
https://github.com/hail-is/hail/pull/2007:15,Modifiability,refactor,refactoring,15,"A small bit of refactoring. I've moved schema and math for both LinearRegression and LinearRegressionMultiPheno to the LinearRegressionModel class, and now fit returns LinearRegressionStats which in turn have toAnnotation functions. This provides better separation of data prep and annotation from core math, in line with structure of LogisticRegression(Model), and sets stage for next step of generalizing genotype field. I've left LinearRegression3 as is for now, full consolidation may wait until 0.2.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2007
https://github.com/hail-is/hail/pull/2007:335,Testability,Log,LogisticRegression,335,"A small bit of refactoring. I've moved schema and math for both LinearRegression and LinearRegressionMultiPheno to the LinearRegressionModel class, and now fit returns LinearRegressionStats which in turn have toAnnotation functions. This provides better separation of data prep and annotation from core math, in line with structure of LogisticRegression(Model), and sets stage for next step of generalizing genotype field. I've left LinearRegression3 as is for now, full consolidation may wait until 0.2.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2007
https://github.com/hail-is/hail/issues/2009:142,Deployability,pipeline,pipeline,142,"The LDMatrix apply method calls collect on more or less the entire set of variants twice. Noticed this taking a lot of time in testing lmmreg pipeline, think it's worth trying to eliminate one if possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2009
https://github.com/hail-is/hail/issues/2009:127,Testability,test,testing,127,"The LDMatrix apply method calls collect on more or less the entire set of variants twice. Noticed this taking a lot of time in testing lmmreg pipeline, think it's worth trying to eliminate one if possible.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2009
https://github.com/hail-is/hail/pull/2034:130,Modifiability,variab,variables,130,- fixed structure of docs + links (hail/* -> docs/stable/*); - Added Hail version + supported spark versions + git hash as gradle variables; - Used these versions in Sphinx.; - Changed path of distribution links in getting started to point at current hash.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2034
https://github.com/hail-is/hail/pull/2034:115,Security,hash,hash,115,- fixed structure of docs + links (hail/* -> docs/stable/*); - Added Hail version + supported spark versions + git hash as gradle variables; - Used these versions in Sphinx.; - Changed path of distribution links in getting started to point at current hash.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2034
https://github.com/hail-is/hail/pull/2034:251,Security,hash,hash,251,- fixed structure of docs + links (hail/* -> docs/stable/*); - Added Hail version + supported spark versions + git hash as gradle variables; - Used these versions in Sphinx.; - Changed path of distribution links in getting started to point at current hash.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2034
https://github.com/hail-is/hail/pull/2039:319,Availability,error,error,319,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:901,Performance,optimiz,optimizations,901,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:89,Testability,test,tests,89,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:292,Testability,test,tests,292,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:328,Testability,test,test,328,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:615,Testability,test,tests,615,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:680,Testability,test,tests,680,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:695,Testability,test,tests,695,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:730,Testability,test,tests,730,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:757,Testability,test,tests,757,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:788,Testability,test,test,788,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:808,Testability,test,test,808,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2039:830,Testability,test,tests,830,"* wip: general VSM keys. cleanup. more cleanup. Complete. Addressed comments. Passes JVM tests with generalized row keys. Needs cleanup, work on pythons side. Minor cleanup. Cleanup around OrderedRDD. More minor cleanup. impex cleanup. VSMSubgen cleanup. Formatting. Passing Scala and Python tests. Fixed minor compile error in test. Minor fix. Addressed first round of comments. * Removed generic genotype. Support null genotypes in GenotypeStream. filterGenotypes just sets; filtered cells (genotypes) to null. This is a file-format breaking; change. Bumped the VDS file version, removed backwards compatability; tests. * Nuked generic from python. * More python cleanup, fixed tests. * Fixed tests. * Genotype can be null. All tests pass. * Fixed python tests. * Cleanup. * Fixed docs test. * Another doc test fix. * Fixed doc tests. * Added missing file. * Addressed comments. Added back SampleQC optimizations.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2039
https://github.com/hail-is/hail/pull/2047:122,Integrability,Depend,Depends,122,"Removed GenotypeStream.; Removed isLinearScale from VSM.; Removed write_partitioning.; {Variant, Sample}QC work over GDS. Depends on https://github.com/hail-is/hail/pull/2039",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2047
https://github.com/hail-is/hail/pull/2060:205,Deployability,install,install,205,"- Resolves #1105, #196; - For VDS and KT writes, writes history to `<output_path>/history.txt`; - For all other file types, writes history to `<output>.history.txt`; - Requires new Python dependency: `pip install autopep8`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2060
https://github.com/hail-is/hail/pull/2060:188,Integrability,depend,dependency,188,"- Resolves #1105, #196; - For VDS and KT writes, writes history to `<output_path>/history.txt`; - For all other file types, writes history to `<output>.history.txt`; - Requires new Python dependency: `pip install autopep8`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2060
https://github.com/hail-is/hail/issues/2062:14,Deployability,install,install,14,"I was able to install hail on my Macbook using a virtenv for python 2.7 and was able to import hail. . However, when I want to create a hail context as outlined in the installation guide, I get the following message: ; ``; Python 2.7.13 (default, Jul 18 2017, 09:16:53) ; [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-422>"", line 2, in __init__; File ""/Users/ih/languages/hail.is/hail/python/hail/typecheck/check.py"", line 226, in _typecheck; return f(*args, **kwargs); File ""/Users/ih/languages/hail.is/hail/python/hail/context.py"", line 68, in __init__; SparkContext._ensure_initialized(); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/context.py"", line 283, in _ensure_initialized; SparkContext._gateway = gateway or launch_gateway(conf); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/java_gateway.py"", line 77, in launch_gateway; proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ``; Any idea what is going on with my installation, did somebody encounter this before already?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2062
https://github.com/hail-is/hail/issues/2062:168,Deployability,install,installation,168,"I was able to install hail on my Macbook using a virtenv for python 2.7 and was able to import hail. . However, when I want to create a hail context as outlined in the installation guide, I get the following message: ; ``; Python 2.7.13 (default, Jul 18 2017, 09:16:53) ; [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-422>"", line 2, in __init__; File ""/Users/ih/languages/hail.is/hail/python/hail/typecheck/check.py"", line 226, in _typecheck; return f(*args, **kwargs); File ""/Users/ih/languages/hail.is/hail/python/hail/context.py"", line 68, in __init__; SparkContext._ensure_initialized(); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/context.py"", line 283, in _ensure_initialized; SparkContext._gateway = gateway or launch_gateway(conf); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/java_gateway.py"", line 77, in launch_gateway; proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ``; Any idea what is going on with my installation, did somebody encounter this before already?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2062
https://github.com/hail-is/hail/issues/2062:1571,Deployability,install,installation,1571,"I was able to install hail on my Macbook using a virtenv for python 2.7 and was able to import hail. . However, when I want to create a hail context as outlined in the installation guide, I get the following message: ; ``; Python 2.7.13 (default, Jul 18 2017, 09:16:53) ; [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-422>"", line 2, in __init__; File ""/Users/ih/languages/hail.is/hail/python/hail/typecheck/check.py"", line 226, in _typecheck; return f(*args, **kwargs); File ""/Users/ih/languages/hail.is/hail/python/hail/context.py"", line 68, in __init__; SparkContext._ensure_initialized(); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/context.py"", line 283, in _ensure_initialized; SparkContext._gateway = gateway or launch_gateway(conf); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/java_gateway.py"", line 77, in launch_gateway; proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ``; Any idea what is going on with my installation, did somebody encounter this before already?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2062
https://github.com/hail-is/hail/issues/2062:208,Integrability,message,message,208,"I was able to install hail on my Macbook using a virtenv for python 2.7 and was able to import hail. . However, when I want to create a hail context as outlined in the installation guide, I get the following message: ; ``; Python 2.7.13 (default, Jul 18 2017, 09:16:53) ; [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-422>"", line 2, in __init__; File ""/Users/ih/languages/hail.is/hail/python/hail/typecheck/check.py"", line 226, in _typecheck; return f(*args, **kwargs); File ""/Users/ih/languages/hail.is/hail/python/hail/context.py"", line 68, in __init__; SparkContext._ensure_initialized(); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/context.py"", line 283, in _ensure_initialized; SparkContext._gateway = gateway or launch_gateway(conf); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/java_gateway.py"", line 77, in launch_gateway; proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ``; Any idea what is going on with my installation, did somebody encounter this before already?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2062
https://github.com/hail-is/hail/issues/2062:181,Usability,guid,guide,181,"I was able to install hail on my Macbook using a virtenv for python 2.7 and was able to import hail. . However, when I want to create a hail context as outlined in the installation guide, I get the following message: ; ``; Python 2.7.13 (default, Jul 18 2017, 09:16:53) ; [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-422>"", line 2, in __init__; File ""/Users/ih/languages/hail.is/hail/python/hail/typecheck/check.py"", line 226, in _typecheck; return f(*args, **kwargs); File ""/Users/ih/languages/hail.is/hail/python/hail/context.py"", line 68, in __init__; SparkContext._ensure_initialized(); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/context.py"", line 283, in _ensure_initialized; SparkContext._gateway = gateway or launch_gateway(conf); File ""/Users/ih/hailenv/lib/python2.7/site-packages/pyspark/java_gateway.py"", line 77, in launch_gateway; proc = Popen(command, stdin=PIPE, preexec_fn=preexec_func, env=env); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/local/Cellar/python/2.7.13_1/Frameworks/Python.framework/Versions/2.7/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ``; Any idea what is going on with my installation, did somebody encounter this before already?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2062
https://github.com/hail-is/hail/pull/2063:228,Availability,error,errors,228,"- Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2063
https://github.com/hail-is/hail/pull/2063:151,Testability,log,logging,151,"- Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2063
https://github.com/hail-is/hail/pull/2063:163,Testability,log,log,163,"- Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2063
https://github.com/hail-is/hail/pull/2063:172,Testability,log,logging,172,"- Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2063
https://github.com/hail-is/hail/pull/2063:245,Testability,log,logged,245,"- Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2063
https://github.com/hail-is/hail/pull/2064:228,Availability,error,errors,228,"- Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2064
https://github.com/hail-is/hail/pull/2064:151,Testability,log,logging,151,"- Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2064
https://github.com/hail-is/hail/pull/2064:163,Testability,log,log,163,"- Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2064
https://github.com/hail-is/hail/pull/2064:172,Testability,log,logging,172,"- Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2064
https://github.com/hail-is/hail/pull/2064:245,Testability,log,logged,245,"- Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2064
https://github.com/hail-is/hail/issues/2067:33,Availability,down,downloadable,33,"When I ran the tutorial with the downloadable 'data/1kg.vds', it throws a fatal error ; ``HailException: Invalid VDS: old version [4]; Recreate VDS with current version of Hail.``. Would it be possible for you to provide an updated VDS or the underlying VCF to build a new VDS? I'm running Hail version devel-6d7d270",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2067
https://github.com/hail-is/hail/issues/2067:80,Availability,error,error,80,"When I ran the tutorial with the downloadable 'data/1kg.vds', it throws a fatal error ; ``HailException: Invalid VDS: old version [4]; Recreate VDS with current version of Hail.``. Would it be possible for you to provide an updated VDS or the underlying VCF to build a new VDS? I'm running Hail version devel-6d7d270",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2067
https://github.com/hail-is/hail/issues/2067:224,Deployability,update,updated,224,"When I ran the tutorial with the downloadable 'data/1kg.vds', it throws a fatal error ; ``HailException: Invalid VDS: old version [4]; Recreate VDS with current version of Hail.``. Would it be possible for you to provide an updated VDS or the underlying VCF to build a new VDS? I'm running Hail version devel-6d7d270",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2067
https://github.com/hail-is/hail/issues/2070:184,Availability,error,error,184,"I would like to load a single vcf that is present in the current working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2070
https://github.com/hail-is/hail/issues/2070:714,Availability,Error,Error,714,"I would like to load a single vcf that is present in the current working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2070
https://github.com/hail-is/hail/issues/2070:860,Availability,error,error,860,"I would like to load a single vcf that is present in the current working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2070
https://github.com/hail-is/hail/issues/2070:1052,Availability,Error,ErrorHandling,1052,"nt working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(Thread.java:745)`. `Hail version: 0.1-4238176`; `Error summar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2070
https://github.com/hail-is/hail/issues/2070:1078,Availability,Error,ErrorHandling,1078,". ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(Thread.java:745)`. `Hail version: 0.1-4238176`; `Error summary: HailException: ar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2070
https://github.com/hail-is/hail/issues/2070:2050,Availability,Error,Error,2050,"ollowing error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(Thread.java:745)`. `Hail version: 0.1-4238176`; `Error summary: HailException: arguments refer to no files`. It's probably something quick, but I can't seem to figure it out?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2070
https://github.com/hail-is/hail/issues/2070:190,Integrability,message,message,190,"I would like to load a single vcf that is present in the current working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2070
https://github.com/hail-is/hail/issues/2070:799,Integrability,protocol,protocol,799,"I would like to load a single vcf that is present in the current working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2070
https://github.com/hail-is/hail/issues/2070:16,Performance,load,load,16,"I would like to load a single vcf that is present in the current working directory. ; `vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`. However, I get the following error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2070
https://github.com/hail-is/hail/issues/2070:1177,Performance,Load,LoadVCF,1177,"ollowing error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(Thread.java:745)`. `Hail version: 0.1-4238176`; `Error summary: HailException: arguments refer to no files`. It's probably something quick, but I can't seem to figure it out?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2070
https://github.com/hail-is/hail/issues/2070:1198,Performance,Load,LoadVCF,1198,"ollowing error message:. `FatalError Traceback (most recent call last)`; `<ipython-input-15-90c48751816a> in <module>()`; `----> 1 vcf = hc.import_vcf('AID61507_SID56895.Improved.gatk.phased.vcf')`; `<decorator-gen-605> in import_vcf(self, path, force, force_bgz, header_file, min_partitions, ``drop_samples, store_gq, pp_as_pl, skip_bad_ad, generic, call_fields)`; `/Users/ih/hail/python/hail/java.pyc in handle_py4j(func, *args, **kwargs)`; ` 110 raise FatalError('%s\n\nJava stack trace:\n%s\n'`; ` 111 'Hail version: %s\n'`; `--> 112 'Error summary: %s' % (deepest, full, Env.hc().version, deepest))`; ` 113 except py4j.protocol.Py4JError as e:`; ` 114 if e.args[0].startswith('An error occurred while calling'):`; `FatalError: HailException: arguments refer to no files`; `Java stack trace:`; `is.hail.utils.HailException: arguments refer to no files`; 	`at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6)`; 	`at is.hail.utils.package$.fatal(package.scala:25)`; 	`at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105)`; 	`at is.hail.HailContext.importVCFsGeneric(HailContext.scala:558)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)`; 	`at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)`; 	`at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)`; 	`at java.lang.reflect.Method.invoke(Method.java:498)`; 	`at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)`; 	`at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)`; 	`at py4j.Gateway.invoke(Gateway.java:280)`; 	`at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)`; 	`at py4j.commands.CallCommand.execute(CallCommand.java:79)`; 	`at py4j.GatewayConnection.run(GatewayConnection.java:214)`; 	`at java.lang.Thread.run(Thread.java:745)`. `Hail version: 0.1-4238176`; `Error summary: HailException: arguments refer to no files`. It's probably something quick, but I can't seem to figure it out?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2070
https://github.com/hail-is/hail/pull/2074:403,Energy Efficiency,allocate,allocated,403,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:613,Performance,optimiz,optimization,613,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:1143,Performance,optimiz,optimizations,1143,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:1370,Performance,Load,LoadVCF,1370,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:163,Safety,unsafe,unsafe-rowstore-,163,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:482,Safety,Unsafe,UnsafeRow,482,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:505,Safety,Unsafe,UnsafeRowBuilder,505,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:835,Safety,Unsafe,UnsafeIndexedSeqAnnotation,835,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:902,Safety,unsafe,unsafe,902,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:1162,Safety,unsafe,unsafe-rowstore-,1162,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:1476,Safety,Unsafe,UnsafeRow,1476,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:564,Testability,test,tests,564,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/pull/2074:1401,Testability,Test,Tests,1401,"Spicy meatball for you, @tpoterba. I hope I didn't step on your feet too much. read{_table} and write now read and write rows, stolen and tweaked up from tpoterba/unsafe-rowstore-2. (Nice work!) Parquet is gone. Not having to scan all the partitions feels so nice, and I'm just working on tiny examples on my laptop. Added RegionValueBuilder which is useful for ... building region-based values (values allocated in a MemoryBuffer). `import_vcf` uses it to produces `RegionValues`/`UnsafeRow`. I left in `UnsafeRowBuilder`, but it is not being used (except by the tests). We should port over the region => region optimization, and remove it. I feel like this could be used to write our own non-consing Parquet importer easily (that supports nested fields!) Also, our own VCF parser is now trivial to drop in, esp. for genotypes. Added UnsafeIndexedSeqAnnotation and pulling native complex types out of unsafe rows. Cleaned up read/writing VDS/KT metadata files. Got rid of `RowGenotype`, wrote `buildGenotypeExtractor` to be much better. I handled the serialization issue a slightly different way. See `BroadcastTypeTree`. Including your Kryo optimizations from unsafe-rowstore-2 would be good, too. It is still not as fast as 0.1, but generic and getting closer. This change has a lot of upside. Making things mutable now is trivial (just remove to `region.copy()` in `LoadVCF`, `ReadRowsRDD`, etc.) Tests spend a lot of time in methods that should eventually go away (e.g. `UnsafeRow.read`). The main problem is that the rowstore with naive encodings is about 4x larger (compressed) than the corresponding 0.1 VDS (profile225, 2.0GB => 7.8GB) and a huge amount of time is spent in LZ4 compression. I have a plan for this.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2074
https://github.com/hail-is/hail/issues/2076:412,Availability,echo,echo,412,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:861,Availability,error,errors,861,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:1934,Availability,Error,Error,1934,"it)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:2161,Availability,Error,ErrorHandling,2161,"ttp://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:2187,Availability,Error,ErrorHandling,2187,"hanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```; How can",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:3120,Availability,Error,Error,3120,"ps://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```; How can I solve it ?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:95,Deployability,deploy,deploy,95,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:213,Modifiability,variab,variables,213,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:2282,Performance,Load,LoadVCF,2282,"ps://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```; How can I solve it ?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:2303,Performance,Load,LoadVCF,2303,"ps://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: 0.1-0320a61; Error summary: HailException: arguments refer to no files; ```; How can I solve it ?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:614,Testability,test,test,614,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:693,Testability,test,test,693,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:783,Testability,test,test,783,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:823,Testability,test,test,823,"Hi, I am using hail in spark, but encounter some problem.; I followed the ""Getting Started"" to deploy hail , and build Hail from source; (https://hail.is/docs/stable/getting_started.html). I set the environmental variables as follows:; ```; export SPARK_HOME=/opt/Software/spark/spark-2.0.2-bin-hadoop2.6; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:1366,Testability,log,log,1366,"$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(Hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:1398,Testability,log,logging,1398,"E/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```; I put the vcf file in hadoop， as follows:; ```; [hdfs@tele-1 root]$ hdfs dfs -ls /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.Nativ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:1610,Testability,test,test,1610,"s /hail/test; Found 1 items; -rw-r--r-- 3 hdfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:1650,Testability,test,test,1650,"dfs supergroup 21194 2017-08-08 18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/issues/2076:1690,Testability,test,test,1690,"18:20 /hail/test/BRCA1.raw_indel.vcf; ```; But when I excuted the command:; ```; hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); ```; there are some errors：; ```; [hdfs@tele-1 root]$ python; Python 2.7.13 |Anaconda 4.4.0 (64-bit)| (default, Dec 20 2016, 23:09:15) ; [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Anaconda is brought to you by Continuum Analytics.; Please check out: http://continuum.io/thanks and https://anaconda.org; >>> import hail; >>> hc = hail.HailContext(); Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4041; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/hail/test/BRCA1.raw_indel.vcf').write('/hail/test/brca1.vds'); hail: warning: `/hail/test/BRCA1.raw_indel.vcf' refers to no files; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-483>"", line 2, in import_vcf; File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: arguments refer to no files. Java stack trace:; is.hail.utils.HailException: arguments refer to no files; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:25); 	at is.hail.io.vcf.LoadVCF$.globAllVCFs(LoadVCF.scala:105); 	at is.hail.HailContext.importVCFs(HailContext.scala:523); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2076
https://github.com/hail-is/hail/pull/2081:60,Performance,optimiz,optimized,60,"Builds on: https://github.com/hail-is/hail/pull/2074. Added optimized unsafe row add to region value builder. Tests are faster than toward_fullgeneric_4, 0.1 (8m2s vs 9m18s, 0.1: 8m20s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2081
https://github.com/hail-is/hail/pull/2081:70,Safety,unsafe,unsafe,70,"Builds on: https://github.com/hail-is/hail/pull/2074. Added optimized unsafe row add to region value builder. Tests are faster than toward_fullgeneric_4, 0.1 (8m2s vs 9m18s, 0.1: 8m20s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2081
https://github.com/hail-is/hail/pull/2081:110,Testability,Test,Tests,110,"Builds on: https://github.com/hail-is/hail/pull/2074. Added optimized unsafe row add to region value builder. Tests are faster than toward_fullgeneric_4, 0.1 (8m2s vs 9m18s, 0.1: 8m20s).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2081
https://github.com/hail-is/hail/pull/2090:56,Testability,Test,Tests,56,- Added GenomeReference and Contig classes in Python; - Tests; - Documentation,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2090
https://github.com/hail-is/hail/pull/2093:566,Availability,error,error,566,"Made types final. @tpoterba I think you should rebase GenotypeView on this and use the new accessors there. We shouldn't be trying to optimize the traversal of types or the field access code now. The way to optimize these things is to make the types compile-time objects (as they should be) and these accessors will become code generators that turn lookups into things like `byteOffsets` into a compile-time constant. At some point I think we should go ""full unsafe"" by using off-heap allocation and change the (region, offset) pair into a Long. However, this makes error checking harder. I'll think about when to do that. I still think getting rid of the triple in VSM is the right next step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2093
https://github.com/hail-is/hail/pull/2093:134,Performance,optimiz,optimize,134,"Made types final. @tpoterba I think you should rebase GenotypeView on this and use the new accessors there. We shouldn't be trying to optimize the traversal of types or the field access code now. The way to optimize these things is to make the types compile-time objects (as they should be) and these accessors will become code generators that turn lookups into things like `byteOffsets` into a compile-time constant. At some point I think we should go ""full unsafe"" by using off-heap allocation and change the (region, offset) pair into a Long. However, this makes error checking harder. I'll think about when to do that. I still think getting rid of the triple in VSM is the right next step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2093
https://github.com/hail-is/hail/pull/2093:207,Performance,optimiz,optimize,207,"Made types final. @tpoterba I think you should rebase GenotypeView on this and use the new accessors there. We shouldn't be trying to optimize the traversal of types or the field access code now. The way to optimize these things is to make the types compile-time objects (as they should be) and these accessors will become code generators that turn lookups into things like `byteOffsets` into a compile-time constant. At some point I think we should go ""full unsafe"" by using off-heap allocation and change the (region, offset) pair into a Long. However, this makes error checking harder. I'll think about when to do that. I still think getting rid of the triple in VSM is the right next step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2093
https://github.com/hail-is/hail/pull/2093:459,Safety,unsafe,unsafe,459,"Made types final. @tpoterba I think you should rebase GenotypeView on this and use the new accessors there. We shouldn't be trying to optimize the traversal of types or the field access code now. The way to optimize these things is to make the types compile-time objects (as they should be) and these accessors will become code generators that turn lookups into things like `byteOffsets` into a compile-time constant. At some point I think we should go ""full unsafe"" by using off-heap allocation and change the (region, offset) pair into a Long. However, this makes error checking harder. I'll think about when to do that. I still think getting rid of the triple in VSM is the right next step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2093
https://github.com/hail-is/hail/pull/2093:91,Security,access,accessors,91,"Made types final. @tpoterba I think you should rebase GenotypeView on this and use the new accessors there. We shouldn't be trying to optimize the traversal of types or the field access code now. The way to optimize these things is to make the types compile-time objects (as they should be) and these accessors will become code generators that turn lookups into things like `byteOffsets` into a compile-time constant. At some point I think we should go ""full unsafe"" by using off-heap allocation and change the (region, offset) pair into a Long. However, this makes error checking harder. I'll think about when to do that. I still think getting rid of the triple in VSM is the right next step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2093
https://github.com/hail-is/hail/pull/2093:179,Security,access,access,179,"Made types final. @tpoterba I think you should rebase GenotypeView on this and use the new accessors there. We shouldn't be trying to optimize the traversal of types or the field access code now. The way to optimize these things is to make the types compile-time objects (as they should be) and these accessors will become code generators that turn lookups into things like `byteOffsets` into a compile-time constant. At some point I think we should go ""full unsafe"" by using off-heap allocation and change the (region, offset) pair into a Long. However, this makes error checking harder. I'll think about when to do that. I still think getting rid of the triple in VSM is the right next step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2093
https://github.com/hail-is/hail/pull/2093:301,Security,access,accessors,301,"Made types final. @tpoterba I think you should rebase GenotypeView on this and use the new accessors there. We shouldn't be trying to optimize the traversal of types or the field access code now. The way to optimize these things is to make the types compile-time objects (as they should be) and these accessors will become code generators that turn lookups into things like `byteOffsets` into a compile-time constant. At some point I think we should go ""full unsafe"" by using off-heap allocation and change the (region, offset) pair into a Long. However, this makes error checking harder. I'll think about when to do that. I still think getting rid of the triple in VSM is the right next step.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2093
https://github.com/hail-is/hail/pull/2096:2,Integrability,Depend,Depends,2,- Depends on #2086 and #2090,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2096
https://github.com/hail-is/hail/pull/2098:77,Performance,perform,performance,77,Reopening. Needed by https://github.com/hail-is/hail/pull/2097. Will address performance once Row is gone and we're full unsafe.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2098
https://github.com/hail-is/hail/pull/2098:121,Safety,unsafe,unsafe,121,Reopening. Needed by https://github.com/hail-is/hail/pull/2097. Will address performance once Row is gone and we're full unsafe.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2098
https://github.com/hail-is/hail/pull/2103:5,Usability,simpl,simplifies,5,This simplifies many of our methods with a `using` primitive for handling `Closeable` objects.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2103
https://github.com/hail-is/hail/pull/2109:290,Availability,error,errors,290,"* Reworked logging to route log output through Python stderr. - Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j. * Fix info for truncatables. * Address comments",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2109
https://github.com/hail-is/hail/pull/2109:22,Integrability,rout,route,22,"* Reworked logging to route log output through Python stderr. - Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j. * Fix info for truncatables. * Address comments",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2109
https://github.com/hail-is/hail/pull/2109:11,Testability,log,logging,11,"* Reworked logging to route log output through Python stderr. - Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j. * Fix info for truncatables. * Address comments",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2109
https://github.com/hail-is/hail/pull/2109:28,Testability,log,log,28,"* Reworked logging to route log output through Python stderr. - Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j. * Fix info for truncatables. * Address comments",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2109
https://github.com/hail-is/hail/pull/2109:213,Testability,log,logging,213,"* Reworked logging to route log output through Python stderr. - Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j. * Fix info for truncatables. * Address comments",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2109
https://github.com/hail-is/hail/pull/2109:225,Testability,log,log,225,"* Reworked logging to route log output through Python stderr. - Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j. * Fix info for truncatables. * Address comments",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2109
https://github.com/hail-is/hail/pull/2109:234,Testability,log,logging,234,"* Reworked logging to route log output through Python stderr. - Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j. * Fix info for truncatables. * Address comments",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2109
https://github.com/hail-is/hail/pull/2109:307,Testability,log,logged,307,"* Reworked logging to route log output through Python stderr. - Removed SLF4J, everything goes directly through log4j now.; - Removed the explicitly System.err.write calls inside info / warn.; - Separated console logging and log file logging; - Stripped the huge stack traces out of python errors; they are logged; to the screen / cell through log4j. * Fix info for truncatables. * Address comments",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2109
https://github.com/hail-is/hail/pull/2115:110,Availability,error,error,110,"If a KeyTable has no keys, then the KeyedRDD should have an empty row as the key rather than throwing a fatal error. This was causing problems when using `same` for KeyTables with no keys.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2115
https://github.com/hail-is/hail/pull/2122:86,Integrability,interface,interfaces,86,* works. * use hadoop. * fix inputstream reader and uris. * fix bug. * using existing interfaces. * fix comments,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2122
https://github.com/hail-is/hail/pull/2124:108,Availability,error,error,108,* works?. * address PR comments. * optimize imports. * use existing interfaces. * fix imports. * fix syntax error. * remove bad import. * add LDMatrix import. * fix tests for ldmatrix. * fix tests. * clean paths before writing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2124
https://github.com/hail-is/hail/pull/2124:68,Integrability,interface,interfaces,68,* works?. * address PR comments. * optimize imports. * use existing interfaces. * fix imports. * fix syntax error. * remove bad import. * add LDMatrix import. * fix tests for ldmatrix. * fix tests. * clean paths before writing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2124
https://github.com/hail-is/hail/pull/2124:35,Performance,optimiz,optimize,35,* works?. * address PR comments. * optimize imports. * use existing interfaces. * fix imports. * fix syntax error. * remove bad import. * add LDMatrix import. * fix tests for ldmatrix. * fix tests. * clean paths before writing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2124
https://github.com/hail-is/hail/pull/2124:165,Testability,test,tests,165,* works?. * address PR comments. * optimize imports. * use existing interfaces. * fix imports. * fix syntax error. * remove bad import. * add LDMatrix import. * fix tests for ldmatrix. * fix tests. * clean paths before writing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2124
https://github.com/hail-is/hail/pull/2124:191,Testability,test,tests,191,* works?. * address PR comments. * optimize imports. * use existing interfaces. * fix imports. * fix syntax error. * remove bad import. * add LDMatrix import. * fix tests for ldmatrix. * fix tests. * clean paths before writing,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2124
https://github.com/hail-is/hail/pull/2127:139,Performance,perform,performance,139,"Here is Nirvana PR, as approved by the folks over at Illumina. They have some tweaks and changes they want to make in the works to improve performance and documentation, but this is functional and ready for review.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2127
https://github.com/hail-is/hail/pull/2132:132,Availability,robust,robust,132,@danking the lmm change can be considered a bug fix since delta should never be negative. The log change should make the tests more robust to which JVM. Let me know if this fixes the failures and I'll PR against 0.1 as well.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2132
https://github.com/hail-is/hail/pull/2132:183,Availability,failure,failures,183,@danking the lmm change can be considered a bug fix since delta should never be negative. The log change should make the tests more robust to which JVM. Let me know if this fixes the failures and I'll PR against 0.1 as well.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2132
https://github.com/hail-is/hail/pull/2132:94,Testability,log,log,94,@danking the lmm change can be considered a bug fix since delta should never be negative. The log change should make the tests more robust to which JVM. Let me know if this fixes the failures and I'll PR against 0.1 as well.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2132
https://github.com/hail-is/hail/pull/2132:121,Testability,test,tests,121,@danking the lmm change can be considered a bug fix since delta should never be negative. The log change should make the tests more robust to which JVM. Let me know if this fixes the failures and I'll PR against 0.1 as well.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2132
https://github.com/hail-is/hail/issues/2136:419,Availability,repair,repairStandardHeaderLines,419,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:731,Performance,Load,LoadVCF,731,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:752,Performance,Load,LoadVCF,752,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:791,Performance,Load,LoadVCF,791,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:816,Performance,Load,LoadVCF,816,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:855,Performance,Load,LoadVCF,855,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:880,Performance,Load,LoadVCF,880,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:1394,Performance,Load,LoadVCF,1394,der.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:1409,Performance,Load,LoadVCF,1409,der.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:112,Security,Hash,HashMap,112,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:127,Security,Hash,HashMap,127,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:160,Security,Hash,HashMap,160,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:175,Security,Hash,HashMap,175,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:208,Security,Hash,HashMap,208,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/issues/2136:220,Security,Hash,HashMap,220,Much OOM. Related to #2108?. ```; Java stack trace:; java.lang.OutOfMemoryError: Java heap space; 	at java.util.HashMap.resize(HashMap.java:703); 	at java.util.HashMap.putVal(HashMap.java:662); 	at java.util.HashMap.put(HashMap.java:611); 	at htsjdk.variant.vcf.VCFHeader.buildVCFReaderMaps(VCFHeader.java:164); 	at htsjdk.variant.vcf.VCFHeader.<init>(VCFHeader.java:146); 	at htsjdk.variant.vcf.VCFStandardHeaderLines.repairStandardHeaderLines(VCFStandardHeaderLines.java:75); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseHeaderFromLines(AbstractVCFCodec.java:223); 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:111); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at is.hail.io.vcf.LoadVCF$.parseHeader(LoadVCF.scala:162); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at is.hail.io.vcf.LoadVCF$$anonfun$4.apply(LoadVCF.scala:205); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:205); 	at is.hail.HailContext.importVCFsGeneric(HailContext.scala:528); 	at is.hail.HailContext.importVCFs(HailContext.scala:484); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2136
https://github.com/hail-is/hail/pull/2144:206,Availability,redundant,redundant,206,"@jigold I made some changes to the annotation database web page, care to take a look?. Mainly got rid of the tree/query builder thing and moved that functionality to checkboxes in the documentation. Seemed redundant to have both.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2144
https://github.com/hail-is/hail/pull/2144:206,Safety,redund,redundant,206,"@jigold I made some changes to the annotation database web page, care to take a look?. Mainly got rid of the tree/query builder thing and moved that functionality to checkboxes in the documentation. Seemed redundant to have both.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2144
https://github.com/hail-is/hail/pull/2145:78,Availability,down,down,78,Default Mathjax CDN is no longer active. https://www.mathjax.org/cdn-shutting-down/,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2145
https://github.com/hail-is/hail/pull/2148:27,Performance,queue,queue,27,Merge after #2147 Priority queue.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2148
https://github.com/hail-is/hail/pull/2153:66,Deployability,update,updates,66,"Includes the logistic case of the SKAT algorithm. Changes include updates to the SKAT, SKATSuite, and SKATmodel Scala files including updating python tests and other documentation. . Another significant change in the code is that the tests in SKATSuite have been re-written to ensure less code is duplicated while also added in permutation and noise tests for the skat algorithm which are not apart of the CI testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2153
https://github.com/hail-is/hail/pull/2153:13,Testability,log,logistic,13,"Includes the logistic case of the SKAT algorithm. Changes include updates to the SKAT, SKATSuite, and SKATmodel Scala files including updating python tests and other documentation. . Another significant change in the code is that the tests in SKATSuite have been re-written to ensure less code is duplicated while also added in permutation and noise tests for the skat algorithm which are not apart of the CI testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2153
https://github.com/hail-is/hail/pull/2153:150,Testability,test,tests,150,"Includes the logistic case of the SKAT algorithm. Changes include updates to the SKAT, SKATSuite, and SKATmodel Scala files including updating python tests and other documentation. . Another significant change in the code is that the tests in SKATSuite have been re-written to ensure less code is duplicated while also added in permutation and noise tests for the skat algorithm which are not apart of the CI testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2153
https://github.com/hail-is/hail/pull/2153:234,Testability,test,tests,234,"Includes the logistic case of the SKAT algorithm. Changes include updates to the SKAT, SKATSuite, and SKATmodel Scala files including updating python tests and other documentation. . Another significant change in the code is that the tests in SKATSuite have been re-written to ensure less code is duplicated while also added in permutation and noise tests for the skat algorithm which are not apart of the CI testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2153
https://github.com/hail-is/hail/pull/2153:350,Testability,test,tests,350,"Includes the logistic case of the SKAT algorithm. Changes include updates to the SKAT, SKATSuite, and SKATmodel Scala files including updating python tests and other documentation. . Another significant change in the code is that the tests in SKATSuite have been re-written to ensure less code is duplicated while also added in permutation and noise tests for the skat algorithm which are not apart of the CI testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2153
https://github.com/hail-is/hail/pull/2153:409,Testability,test,testing,409,"Includes the logistic case of the SKAT algorithm. Changes include updates to the SKAT, SKATSuite, and SKATmodel Scala files including updating python tests and other documentation. . Another significant change in the code is that the tests in SKATSuite have been re-written to ensure less code is duplicated while also added in permutation and noise tests for the skat algorithm which are not apart of the CI testing.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2153
https://github.com/hail-is/hail/pull/2154:149,Testability,benchmark,benchmark,149,"It needs some more cleanup, I plan to make another review pass myself. I can definitely split it into multiple pieces if necessary. Will post latest benchmark numbers in a moment.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2154
https://github.com/hail-is/hail/issues/2159:713,Availability,avail,available,713,"- version `devel-6ee2919`; - source: git@github.com/hail-is/hail.git ; compiled with `gradle shadowJar`. Desired behavior:. When reading an old version of a VDS, hail should print a message such as:. ```; The vds ""/users/dking/projects/hail-data/profile225.vds"" cannot be read by this version of hail. It must be regenerated from the VCF source using this version of hail.; ```. Actual behavior:. ```; dking@wmb16-359 # python; >>> from hail import *; >>> hc = HailContext(); hc.reaUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; d9""/projecRunning on Apache Spark version 2.0.2; SparkUI available at http://10.10.99.215:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-6ee2919; WARNING: This is an unstable development build.; >>> hc.read(""/users/dking/projects/hail-data/profile225.vds""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-606>"", line 2, in read; File ""/Users/dking/projects/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: MappingException: Did not find value which can be converted into java.lang.String. Java stack trace:; org.json4s.package$MappingException: No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at org.json4s.reflect.package$.fail(package.scala:96); 	at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:462); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Traversab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2159
https://github.com/hail-is/hail/issues/2159:1162,Availability,Error,Error,1162," should print a message such as:. ```; The vds ""/users/dking/projects/hail-data/profile225.vds"" cannot be read by this version of hail. It must be regenerated from the VCF source using this version of hail.; ```. Actual behavior:. ```; dking@wmb16-359 # python; >>> from hail import *; >>> hc = HailContext(); hc.reaUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; d9""/projecRunning on Apache Spark version 2.0.2; SparkUI available at http://10.10.99.215:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-6ee2919; WARNING: This is an unstable development build.; >>> hc.read(""/users/dking/projects/hail-data/profile225.vds""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-606>"", line 2, in read; File ""/Users/dking/projects/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: MappingException: Did not find value which can be converted into java.lang.String. Java stack trace:; org.json4s.package$MappingException: No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at org.json4s.reflect.package$.fail(package.scala:96); 	at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:462); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2159
https://github.com/hail-is/hail/issues/2159:8640,Availability,Error,Error,8640,VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:72); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-6ee2919; Error summary: MappingException: Did not find value which can be converted into java.lang.String; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2159
https://github.com/hail-is/hail/issues/2159:182,Integrability,message,message,182,"- version `devel-6ee2919`; - source: git@github.com/hail-is/hail.git ; compiled with `gradle shadowJar`. Desired behavior:. When reading an old version of a VDS, hail should print a message such as:. ```; The vds ""/users/dking/projects/hail-data/profile225.vds"" cannot be read by this version of hail. It must be regenerated from the VCF source using this version of hail.; ```. Actual behavior:. ```; dking@wmb16-359 # python; >>> from hail import *; >>> hc = HailContext(); hc.reaUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; d9""/projecRunning on Apache Spark version 2.0.2; SparkUI available at http://10.10.99.215:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-6ee2919; WARNING: This is an unstable development build.; >>> hc.read(""/users/dking/projects/hail-data/profile225.vds""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-606>"", line 2, in read; File ""/Users/dking/projects/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: MappingException: Did not find value which can be converted into java.lang.String. Java stack trace:; org.json4s.package$MappingException: No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at org.json4s.reflect.package$.fail(package.scala:96); 	at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:462); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Traversab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2159
https://github.com/hail-is/hail/issues/2159:579,Testability,log,log,579,"- version `devel-6ee2919`; - source: git@github.com/hail-is/hail.git ; compiled with `gradle shadowJar`. Desired behavior:. When reading an old version of a VDS, hail should print a message such as:. ```; The vds ""/users/dking/projects/hail-data/profile225.vds"" cannot be read by this version of hail. It must be regenerated from the VCF source using this version of hail.; ```. Actual behavior:. ```; dking@wmb16-359 # python; >>> from hail import *; >>> hc = HailContext(); hc.reaUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; d9""/projecRunning on Apache Spark version 2.0.2; SparkUI available at http://10.10.99.215:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-6ee2919; WARNING: This is an unstable development build.; >>> hc.read(""/users/dking/projects/hail-data/profile225.vds""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-606>"", line 2, in read; File ""/Users/dking/projects/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: MappingException: Did not find value which can be converted into java.lang.String. Java stack trace:; org.json4s.package$MappingException: No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at org.json4s.reflect.package$.fail(package.scala:96); 	at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:462); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Traversab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2159
https://github.com/hail-is/hail/issues/2159:611,Testability,log,logging,611,"- version `devel-6ee2919`; - source: git@github.com/hail-is/hail.git ; compiled with `gradle shadowJar`. Desired behavior:. When reading an old version of a VDS, hail should print a message such as:. ```; The vds ""/users/dking/projects/hail-data/profile225.vds"" cannot be read by this version of hail. It must be regenerated from the VCF source using this version of hail.; ```. Actual behavior:. ```; dking@wmb16-359 # python; >>> from hail import *; >>> hc = HailContext(); hc.reaUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; d9""/projecRunning on Apache Spark version 2.0.2; SparkUI available at http://10.10.99.215:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-6ee2919; WARNING: This is an unstable development build.; >>> hc.read(""/users/dking/projects/hail-data/profile225.vds""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-606>"", line 2, in read; File ""/Users/dking/projects/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: MappingException: Did not find value which can be converted into java.lang.String. Java stack trace:; org.json4s.package$MappingException: No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at org.json4s.reflect.package$.fail(package.scala:96); 	at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:462); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(Traversab",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2159
https://github.com/hail-is/hail/issues/2159:1392,Usability,usab,usable,1392," python; >>> from hail import *; >>> hc = HailContext(); hc.reaUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; d9""/projecRunning on Apache Spark version 2.0.2; SparkUI available at http://10.10.99.215:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-6ee2919; WARNING: This is an unstable development build.; >>> hc.read(""/users/dking/projects/hail-data/profile225.vds""); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-606>"", line 2, in read; File ""/Users/dking/projects/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: MappingException: Did not find value which can be converted into java.lang.String. Java stack trace:; org.json4s.package$MappingException: No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at org.json4s.reflect.package$.fail(package.scala:96); 	at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$buildCtorArg(Extraction.scala:462); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at org.json4s.Extraction$ClassInstanceBuilder$$anonfun$14.apply(Extraction.scala:482); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at org.json4s.Extraction$ClassInstanceBuilder.org$json4s$Extraction$ClassInstanceBuilder$$instantia",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2159
https://github.com/hail-is/hail/pull/2160:209,Safety,avoid,avoiding,209,"- Eigen, EigenDistributed in Scala and Python; - EigenSuite; - assertVectorEqualityUpToSignDouble in TestUtils for comparing eigenvectors; - eigen methods in LDMatrix and KinshipMatrix; - toLocalMatrix on IRM avoiding BlockMatrix and test; - RichSparkMatrix with asBreeze to avoid copy; - asSpark and toArrayShallow on RichBreezeDenseMatrix. I think there is opportunity to have Eigen and EigenDistributed abstract a common class to replication. And for Kinship and LDMatrix as well, so that Kinship can also use the read and write that LDMatrix has.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2160
https://github.com/hail-is/hail/pull/2160:275,Safety,avoid,avoid,275,"- Eigen, EigenDistributed in Scala and Python; - EigenSuite; - assertVectorEqualityUpToSignDouble in TestUtils for comparing eigenvectors; - eigen methods in LDMatrix and KinshipMatrix; - toLocalMatrix on IRM avoiding BlockMatrix and test; - RichSparkMatrix with asBreeze to avoid copy; - asSpark and toArrayShallow on RichBreezeDenseMatrix. I think there is opportunity to have Eigen and EigenDistributed abstract a common class to replication. And for Kinship and LDMatrix as well, so that Kinship can also use the read and write that LDMatrix has.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2160
https://github.com/hail-is/hail/pull/2160:63,Testability,assert,assertVectorEqualityUpToSignDouble,63,"- Eigen, EigenDistributed in Scala and Python; - EigenSuite; - assertVectorEqualityUpToSignDouble in TestUtils for comparing eigenvectors; - eigen methods in LDMatrix and KinshipMatrix; - toLocalMatrix on IRM avoiding BlockMatrix and test; - RichSparkMatrix with asBreeze to avoid copy; - asSpark and toArrayShallow on RichBreezeDenseMatrix. I think there is opportunity to have Eigen and EigenDistributed abstract a common class to replication. And for Kinship and LDMatrix as well, so that Kinship can also use the read and write that LDMatrix has.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2160
https://github.com/hail-is/hail/pull/2160:101,Testability,Test,TestUtils,101,"- Eigen, EigenDistributed in Scala and Python; - EigenSuite; - assertVectorEqualityUpToSignDouble in TestUtils for comparing eigenvectors; - eigen methods in LDMatrix and KinshipMatrix; - toLocalMatrix on IRM avoiding BlockMatrix and test; - RichSparkMatrix with asBreeze to avoid copy; - asSpark and toArrayShallow on RichBreezeDenseMatrix. I think there is opportunity to have Eigen and EigenDistributed abstract a common class to replication. And for Kinship and LDMatrix as well, so that Kinship can also use the read and write that LDMatrix has.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2160
https://github.com/hail-is/hail/pull/2160:234,Testability,test,test,234,"- Eigen, EigenDistributed in Scala and Python; - EigenSuite; - assertVectorEqualityUpToSignDouble in TestUtils for comparing eigenvectors; - eigen methods in LDMatrix and KinshipMatrix; - toLocalMatrix on IRM avoiding BlockMatrix and test; - RichSparkMatrix with asBreeze to avoid copy; - asSpark and toArrayShallow on RichBreezeDenseMatrix. I think there is opportunity to have Eigen and EigenDistributed abstract a common class to replication. And for Kinship and LDMatrix as well, so that Kinship can also use the read and write that LDMatrix has.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2160
https://github.com/hail-is/hail/pull/2171:101,Testability,test,test,101,"- added RichBlockMatrix with toIndexedRowMatrixOrderedPartitioner; - added RichBlockMatrixSuite with test; - added orderedPartitionerInt() to OrderedRDD; - added test to OrderedRDDSuite; - added OrderedKeyIntImplicits with implicit val orderedKey. These are helpful when converting VDS to BlockMatrix to IRM, where the IRM is partitioned as the VDS when constructed so it can be joined back to the VDS without an additional shuffle. This will be used in low rank LMM.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2171
https://github.com/hail-is/hail/pull/2171:162,Testability,test,test,162,"- added RichBlockMatrix with toIndexedRowMatrixOrderedPartitioner; - added RichBlockMatrixSuite with test; - added orderedPartitionerInt() to OrderedRDD; - added test to OrderedRDDSuite; - added OrderedKeyIntImplicits with implicit val orderedKey. These are helpful when converting VDS to BlockMatrix to IRM, where the IRM is partitioned as the VDS when constructed so it can be joined back to the VDS without an additional shuffle. This will be used in low rank LMM.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2171
https://github.com/hail-is/hail/pull/2173:13,Availability,Error,Error,13,Fixes #2159. Error message is now:; ```; FatalError: HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String. Java stack trace:; is.hail.utils.HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:77); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:72); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.N,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2173
https://github.com/hail-is/hail/pull/2173:550,Availability,Error,ErrorHandling,550,Fixes #2159. Error message is now:; ```; FatalError: HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String. Java stack trace:; is.hail.utils.HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:77); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:72); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.N,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2173
https://github.com/hail-is/hail/pull/2173:576,Availability,Error,ErrorHandling,576,Fixes #2159. Error message is now:; ```; FatalError: HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String. Java stack trace:; is.hail.utils.HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:77); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:72); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.N,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2173
https://github.com/hail-is/hail/pull/2173:2659,Availability,Error,Error,2659,tils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-41347fd; Error summary: HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2173
https://github.com/hail-is/hail/pull/2173:19,Integrability,message,message,19,Fixes #2159. Error message is now:; ```; FatalError: HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String. Java stack trace:; is.hail.utils.HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:77); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:72); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.N,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2173
https://github.com/hail-is/hail/pull/2173:179,Usability,usab,usable,179,Fixes #2159. Error message is now:; ```; FatalError: HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String. Java stack trace:; is.hail.utils.HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:77); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:72); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.N,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2173
https://github.com/hail-is/hail/pull/2173:435,Usability,usab,usable,435,Fixes #2159. Error message is now:; ```; FatalError: HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String. Java stack trace:; is.hail.utils.HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:77); 	at is.hail.variant.VariantSampleMatrix$$anonfun$1.apply(VariantSampleMatrix.scala:72); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.is$hail$utils$richUtils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.N,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2173
https://github.com/hail-is/hail/pull/2173:2800,Usability,usab,usable,2800,tils$RichHadoopConfiguration$$using$extension(RichHadoopConfiguration.scala:226); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.readFile$extension(RichHadoopConfiguration.scala:251); 	at is.hail.variant.VariantSampleMatrix$.readFileMetadata(VariantSampleMatrix.scala:72); 	at is.hail.variant.VariantSampleMatrix$.read(VariantSampleMatrix.scala:51); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:434); 	at is.hail.HailContext$$anonfun$10.apply(HailContext.scala:433); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186); 	at is.hail.HailContext.readAll(HailContext.scala:433); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745). Hail version: devel-41347fd; Error summary: HailException: corrupt or outdated VDS: invalid metadata; Recreate VDS with current version of Hail.; Detailed exception:; No usable value for sample_schema; Did not find value which can be converted into java.lang.String; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2173
https://github.com/hail-is/hail/issues/2174:82,Testability,test,test,82,https://github.com/hail-is/hail/blob/284a5eafc9b4fad3de08c912c5b27b3adcbf238b/src/test/scala/is/hail/variant/GenotypeSuite.scala#L12-L27. All three genotype generators can generate a `null` `Genotype`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2174
https://github.com/hail-is/hail/pull/2177:0,Integrability,Depend,Depends,0,Depends on #2160. I have a Python interface and documentation in another branch but I think this function ought to be moved elsewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2177
https://github.com/hail-is/hail/pull/2177:34,Integrability,interface,interface,34,Depends on #2160. I have a Python interface and documentation in another branch but I think this function ought to be moved elsewhere.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2177
https://github.com/hail-is/hail/issues/2178:983,Availability,error,error,983,"I tried out the tiebreaking_expr with some dummyish data:. ```from hail import *; from hail.representation import *; import subprocess; import os; hc = HailContext(). vds = hc.read('gs://daly_atgu_finnish_swedish_exomes_v8/daly_atgu_finnish_swedish_exomes_99prosAndPSC.vds'). vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). intervals = KeyTable.import_interval_list('gs://ibd-exomes/5k.txt'); vds = vds.filter_variants_table(intervals, keep=True). vds = vds.variant_qc(); vds = vds.filter_variants_expr('va.qc.AF>0.01 && va.qc.callRate>0.99', keep=True). table = hc.import_table('gs://daly_atgu_finnish_swedish_exomes_v8/finnibdPSC_isCase.txt', impute=True).key_by('s'); vds = vds.annotate_samples_table(table, root='sa.pheno'). vds = vds.repartition(100); vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno.iscase) 1 else 0""). vds.export_samples('gs://daly_atgu_finnish_swedish_exomes_v8/99pros_psc_relBelowPIHAT35.tsv','s=s'). ```. But this results in an error:. ```hail: info: Reading table to impute column types; hail: info: Finished type imputation; Loading column `s' as type String (imputed); Loading column `case' as type Int (imputed); Struct{; pheno: Int; }; [Stage 6:====================================================>(3347 + 1) / 3348]Verify Output for is/hail/codegen/generated/C3:; Traceback (most recent call last):; File ""/tmp/0a89b0df-1299-4db1-9e90-0efc77501684/99pros_psc_relatedness_short.py"", line 26, in <module>; vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno) 1 else 0""); File ""<decorator-gen-322>"", line 2, in ibd_prune; File ""/home/ec2-user/BuildAgent/work/179f3a9ad532f105/python/hail/java.py"", line 112, in handle_py4j; hail.java.FatalError: IllegalStateException: Bytecode failed verification 2. Java stack trace:; java.lang.IllegalStateException: Bytecode failed verification 2; 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:196); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:208); 	at is.h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2178
https://github.com/hail-is/hail/issues/2178:3435,Availability,Error,Error,3435,handle_py4j; hail.java.FatalError: IllegalStateException: Bytecode failed verification 2. Java stack trace:; java.lang.IllegalStateException: Bytecode failed verification 2; 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:196); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:208); 	at is.hail.expr.CM.runWithDelayedValues(CM.scala:78); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$evalNoTypeCheck(Parser.scala:49); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:62); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:77); 	at is.hail.expr.Parser$.parseTypedExpr(Parser.scala:87); 	at is.hail.methods.IBDPrune$$anonfun$16.apply(IBD.scala:308); 	at is.hail.methods.IBDPrune$$anonfun$16.apply(IBD.scala:308); 	at scala.Option.map(Option.scala:146); 	at is.hail.methods.IBDPrune$.apply(IBD.scala:308); 	at is.hail.variant.VariantDatasetFunctions$.ibdPrune$extension(VariantDataset.scala:529); 	at is.hail.variant.VariantDatasetFunctions.ibdPrune(VariantDataset.scala:526); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-dfca956; Error summary: IllegalStateException: Bytecode failed verification 2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [0a89b0df-1299-4db1-9e90-0efc77501684] entered state [ERROR] while waiting for [DONE].```. Any idea what's wrong? Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2178
https://github.com/hail-is/hail/issues/2178:3505,Availability,ERROR,ERROR,3505,handle_py4j; hail.java.FatalError: IllegalStateException: Bytecode failed verification 2. Java stack trace:; java.lang.IllegalStateException: Bytecode failed verification 2; 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:196); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:208); 	at is.hail.expr.CM.runWithDelayedValues(CM.scala:78); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$evalNoTypeCheck(Parser.scala:49); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:62); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:77); 	at is.hail.expr.Parser$.parseTypedExpr(Parser.scala:87); 	at is.hail.methods.IBDPrune$$anonfun$16.apply(IBD.scala:308); 	at is.hail.methods.IBDPrune$$anonfun$16.apply(IBD.scala:308); 	at scala.Option.map(Option.scala:146); 	at is.hail.methods.IBDPrune$.apply(IBD.scala:308); 	at is.hail.variant.VariantDatasetFunctions$.ibdPrune$extension(VariantDataset.scala:529); 	at is.hail.variant.VariantDatasetFunctions.ibdPrune(VariantDataset.scala:526); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-dfca956; Error summary: IllegalStateException: Bytecode failed verification 2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [0a89b0df-1299-4db1-9e90-0efc77501684] entered state [ERROR] while waiting for [DONE].```. Any idea what's wrong? Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2178
https://github.com/hail-is/hail/issues/2178:3608,Availability,ERROR,ERROR,3608,handle_py4j; hail.java.FatalError: IllegalStateException: Bytecode failed verification 2. Java stack trace:; java.lang.IllegalStateException: Bytecode failed verification 2; 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:196); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:208); 	at is.hail.expr.CM.runWithDelayedValues(CM.scala:78); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$evalNoTypeCheck(Parser.scala:49); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:62); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:77); 	at is.hail.expr.Parser$.parseTypedExpr(Parser.scala:87); 	at is.hail.methods.IBDPrune$$anonfun$16.apply(IBD.scala:308); 	at is.hail.methods.IBDPrune$$anonfun$16.apply(IBD.scala:308); 	at scala.Option.map(Option.scala:146); 	at is.hail.methods.IBDPrune$.apply(IBD.scala:308); 	at is.hail.variant.VariantDatasetFunctions$.ibdPrune$extension(VariantDataset.scala:529); 	at is.hail.variant.VariantDatasetFunctions.ibdPrune(VariantDataset.scala:526); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-dfca956; Error summary: IllegalStateException: Bytecode failed verification 2; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [0a89b0df-1299-4db1-9e90-0efc77501684] entered state [ERROR] while waiting for [DONE].```. Any idea what's wrong? Thanks!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2178
https://github.com/hail-is/hail/issues/2178:1082,Performance,Load,Loading,1082,"xomes_v8/daly_atgu_finnish_swedish_exomes_99prosAndPSC.vds'). vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). intervals = KeyTable.import_interval_list('gs://ibd-exomes/5k.txt'); vds = vds.filter_variants_table(intervals, keep=True). vds = vds.variant_qc(); vds = vds.filter_variants_expr('va.qc.AF>0.01 && va.qc.callRate>0.99', keep=True). table = hc.import_table('gs://daly_atgu_finnish_swedish_exomes_v8/finnibdPSC_isCase.txt', impute=True).key_by('s'); vds = vds.annotate_samples_table(table, root='sa.pheno'). vds = vds.repartition(100); vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno.iscase) 1 else 0""). vds.export_samples('gs://daly_atgu_finnish_swedish_exomes_v8/99pros_psc_relBelowPIHAT35.tsv','s=s'). ```. But this results in an error:. ```hail: info: Reading table to impute column types; hail: info: Finished type imputation; Loading column `s' as type String (imputed); Loading column `case' as type Int (imputed); Struct{; pheno: Int; }; [Stage 6:====================================================>(3347 + 1) / 3348]Verify Output for is/hail/codegen/generated/C3:; Traceback (most recent call last):; File ""/tmp/0a89b0df-1299-4db1-9e90-0efc77501684/99pros_psc_relatedness_short.py"", line 26, in <module>; vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno) 1 else 0""); File ""<decorator-gen-322>"", line 2, in ibd_prune; File ""/home/ec2-user/BuildAgent/work/179f3a9ad532f105/python/hail/java.py"", line 112, in handle_py4j; hail.java.FatalError: IllegalStateException: Bytecode failed verification 2. Java stack trace:; java.lang.IllegalStateException: Bytecode failed verification 2; 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:196); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:208); 	at is.hail.expr.CM.runWithDelayedValues(CM.scala:78); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$evalNoTypeCheck(Parser.scala:49); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:62); 	at is.hail.expr.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2178
https://github.com/hail-is/hail/issues/2178:1127,Performance,Load,Loading,1127,"xomes_v8/daly_atgu_finnish_swedish_exomes_99prosAndPSC.vds'). vds = vds.filter_variants_expr('va.filters.isEmpty()', keep=True). intervals = KeyTable.import_interval_list('gs://ibd-exomes/5k.txt'); vds = vds.filter_variants_table(intervals, keep=True). vds = vds.variant_qc(); vds = vds.filter_variants_expr('va.qc.AF>0.01 && va.qc.callRate>0.99', keep=True). table = hc.import_table('gs://daly_atgu_finnish_swedish_exomes_v8/finnibdPSC_isCase.txt', impute=True).key_by('s'); vds = vds.annotate_samples_table(table, root='sa.pheno'). vds = vds.repartition(100); vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno.iscase) 1 else 0""). vds.export_samples('gs://daly_atgu_finnish_swedish_exomes_v8/99pros_psc_relBelowPIHAT35.tsv','s=s'). ```. But this results in an error:. ```hail: info: Reading table to impute column types; hail: info: Finished type imputation; Loading column `s' as type String (imputed); Loading column `case' as type Int (imputed); Struct{; pheno: Int; }; [Stage 6:====================================================>(3347 + 1) / 3348]Verify Output for is/hail/codegen/generated/C3:; Traceback (most recent call last):; File ""/tmp/0a89b0df-1299-4db1-9e90-0efc77501684/99pros_psc_relatedness_short.py"", line 26, in <module>; vds = vds.ibd_prune(0.35, tiebreaking_expr=""if (sa1.pheno) 1 else 0""); File ""<decorator-gen-322>"", line 2, in ibd_prune; File ""/home/ec2-user/BuildAgent/work/179f3a9ad532f105/python/hail/java.py"", line 112, in handle_py4j; hail.java.FatalError: IllegalStateException: Bytecode failed verification 2. Java stack trace:; java.lang.IllegalStateException: Bytecode failed verification 2; 	at is.hail.asm4s.FunctionBuilder.classAsBytes(FunctionBuilder.scala:196); 	at is.hail.asm4s.FunctionBuilder.result(FunctionBuilder.scala:208); 	at is.hail.expr.CM.runWithDelayedValues(CM.scala:78); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$evalNoTypeCheck(Parser.scala:49); 	at is.hail.expr.Parser$.is$hail$expr$Parser$$eval(Parser.scala:62); 	at is.hail.expr.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2178
https://github.com/hail-is/hail/issues/2180:131,Availability,avail,available,131,The `repr` for SparkContext displays like this: `<pyspark.context.SparkContext object at 0x7ff241c5f690>`. We don't have a history available for a SparkContext object. Not clear how to fix this other than use `sc` if a non-default arg is given for `sc` in HailContext constructor.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2180
https://github.com/hail-is/hail/issues/2180:172,Usability,clear,clear,172,The `repr` for SparkContext displays like this: `<pyspark.context.SparkContext object at 0x7ff241c5f690>`. We don't have a history available for a SparkContext object. Not clear how to fix this other than use `sc` if a non-default arg is given for `sc` in HailContext constructor.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2180
https://github.com/hail-is/hail/pull/2181:32,Usability,feedback,feedback,32,"First stage, esp. interested in feedback on structure of computation. Second stage will fill out docs and add global annotation for regression of `ys` against `covs` only.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2181
https://github.com/hail-is/hail/pull/2183:144,Performance,load,loading,144,import_vcf no longer scans variants. @tpoterba I imagine you want to fight me on this. I want to do binary search on chromosome boundaries when loading VCF files in which case this is quite expensive.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2183
https://github.com/hail-is/hail/pull/2184:48,Safety,unsafe,unsafe,48,"This was to simplify the bytecode generated for unsafe memory access calls vs Scala objects. If there was an improvement, it was smaller than the benchmark measurement noise. Also added some object+offset variants.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2184
https://github.com/hail-is/hail/pull/2184:62,Security,access,access,62,"This was to simplify the bytecode generated for unsafe memory access calls vs Scala objects. If there was an improvement, it was smaller than the benchmark measurement noise. Also added some object+offset variants.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2184
https://github.com/hail-is/hail/pull/2184:146,Testability,benchmark,benchmark,146,"This was to simplify the bytecode generated for unsafe memory access calls vs Scala objects. If there was an improvement, it was smaller than the benchmark measurement noise. Also added some object+offset variants.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2184
https://github.com/hail-is/hail/pull/2184:12,Usability,simpl,simplify,12,"This was to simplify the bytecode generated for unsafe memory access calls vs Scala objects. If there was an improvement, it was smaller than the benchmark measurement noise. Also added some object+offset variants.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2184
https://github.com/hail-is/hail/pull/2187:165,Integrability,interface,interface,165,Verify Field index is the index within the TStruct.; field => column in keytable. This was an old rename but looks like; some things got missed.; Simplify KT.select interface in Scala (and mathc Python).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2187
https://github.com/hail-is/hail/pull/2187:146,Usability,Simpl,Simplify,146,Verify Field index is the index within the TStruct.; field => column in keytable. This was an old rename but looks like; some things got missed.; Simplify KT.select interface in Scala (and mathc Python).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2187
https://github.com/hail-is/hail/pull/2188:178,Availability,down,downstream,178,"The previous idiom was mapAnnotations(...).copy(vaSignature =; newVASignature), but this results (temporarily) in a VDS with an; incorrect va type that which causes problems for downstream changes; and assertions like typecheck.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2188
https://github.com/hail-is/hail/pull/2188:202,Testability,assert,assertions,202,"The previous idiom was mapAnnotations(...).copy(vaSignature =; newVASignature), but this results (temporarily) in a VDS with an; incorrect va type that which causes problems for downstream changes; and assertions like typecheck.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2188
https://github.com/hail-is/hail/pull/2209:200,Availability,down,down,200,"I want this for interface purposes, but it's really not usable due to performance. It takes 2 minutes to collect sample.vcf. I think that py4j is the main culprit, but the history stuff also slows it down a ton.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2209
https://github.com/hail-is/hail/pull/2209:16,Integrability,interface,interface,16,"I want this for interface purposes, but it's really not usable due to performance. It takes 2 minutes to collect sample.vcf. I think that py4j is the main culprit, but the history stuff also slows it down a ton.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2209
https://github.com/hail-is/hail/pull/2209:70,Performance,perform,performance,70,"I want this for interface purposes, but it's really not usable due to performance. It takes 2 minutes to collect sample.vcf. I think that py4j is the main culprit, but the history stuff also slows it down a ton.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2209
https://github.com/hail-is/hail/pull/2209:56,Usability,usab,usable,56,"I want this for interface purposes, but it's really not usable due to performance. It takes 2 minutes to collect sample.vcf. I think that py4j is the main culprit, but the history stuff also slows it down a ton.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2209
https://github.com/hail-is/hail/issues/2217:200,Availability,error,error,200,On the web site:; https://hail.is/docs/stable/getting_started.html. The links to the current distributions are broken:; Current distribution for Spark 2.0.2; Current distribution for Spark 2.1.0. The error is:; NoSuchKeyThe specified key does not exist.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2217
https://github.com/hail-is/hail/pull/2219:82,Integrability,interface,interface,82,"Feedback welcome, not for merging. Goal with ScoreCovariance was to get a working interface rather than optimize speed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2219
https://github.com/hail-is/hail/pull/2219:104,Performance,optimiz,optimize,104,"Feedback welcome, not for merging. Goal with ScoreCovariance was to get a working interface rather than optimize speed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2219
https://github.com/hail-is/hail/pull/2219:0,Usability,Feedback,Feedback,0,"Feedback welcome, not for merging. Goal with ScoreCovariance was to get a working interface rather than optimize speed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2219
https://github.com/hail-is/hail/pull/2220:43,Testability,test,test,43,"Added typecheck to SkatSuite as regression test. keysType was being passed instead of keyType, which resulted for example in Array[String] instead of String as the type of key when the variantKeys was an Array[String] annotation and singleKey was False.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2220
https://github.com/hail-is/hail/pull/2221:32,Integrability,interface,interface,32,"I'd appreciate thoughts on this interface. Based this on our discussion for group/ungroup. The motivation for this PR is we needed a better way to select fields (including splat) for export when we remove export on `dataset`. . I started to modify the code for AST for ""select"" on Structs, but I found it to be extremely confusing and didn't think it was adding any benefit. Example:. ```; kt.annotate(""Y = select(X, a.b, a.c)""); ```; could just be; ```; kt.annotate(""Y = {b: X.a.b, c: X.a.c}"") ; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2221
https://github.com/hail-is/hail/pull/2226:3,Integrability,Depend,DependentFunCode,3,"A `DependentFunCode` is a function that additionally depends on the particular type of its arguments, even if it is otherwise generic. This likely provides a path towards refactoring `TNumeric` as well. cc: @cseed @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2226
https://github.com/hail-is/hail/pull/2226:53,Integrability,depend,depends,53,"A `DependentFunCode` is a function that additionally depends on the particular type of its arguments, even if it is otherwise generic. This likely provides a path towards refactoring `TNumeric` as well. cc: @cseed @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2226
https://github.com/hail-is/hail/pull/2226:171,Modifiability,refactor,refactoring,171,"A `DependentFunCode` is a function that additionally depends on the particular type of its arguments, even if it is otherwise generic. This likely provides a path towards refactoring `TNumeric` as well. cc: @cseed @jigold",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2226
https://github.com/hail-is/hail/pull/2228:145,Integrability,interface,interface,145,"Implicitly verify biallelic on non-split VDSes on the python side for methods that require it through the @require_biallelic decorator.; ; Scala interface now `require`s biallelic expectations. Other requirements should be written in this form, too (like TSampleString). This is based on the decision not to support a user-level Scala interface that mirror the Python. We've already left that with, say, history.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2228
https://github.com/hail-is/hail/pull/2228:335,Integrability,interface,interface,335,"Implicitly verify biallelic on non-split VDSes on the python side for methods that require it through the @require_biallelic decorator.; ; Scala interface now `require`s biallelic expectations. Other requirements should be written in this form, too (like TSampleString). This is based on the decision not to support a user-level Scala interface that mirror the Python. We've already left that with, say, history.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2228
https://github.com/hail-is/hail/issues/2231:137,Usability,simpl,simplest,137,"If the key columns do not appear first in the key table's `signature` (a `TStruct`), then `join` will mis-type the joined key table. The simplest example I could find:; ```python; In[1]: (KeyTable.range(0, 100).annotate('j = 1.0, i = 1'); .key_by(""i"").join(KeyTable.range(0, 100)); .schema); Out[1]: TStruct([u'index', u'j', u'i'], [TInt32(), TInt32(), TFloat64()]); ```. Note that `j` and `i`'s types have been switched. The type of `j` should be `TFloat64`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2231
https://github.com/hail-is/hail/pull/2236:522,Integrability,interface,interface,522,"Builds on: https://github.com/hail-is/hail/pull/2228. It is smaller, cleaner and more self-contained, but I could still break it into more pieces if needed. Some remarks:; - I removed the BroadcastTypeTree nonsense. This will kill KeyTable joins but we can fix that later.; - I sample keys as part of collecting the partition key info.; - I left off two features off the key ranges sampler vs OrderedRDD: I don't resample large partitions and I don't use weights to calculate the ranges.; - I included the minimal LoadVCF interface changes. VCF parser will come as a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2236
https://github.com/hail-is/hail/pull/2236:514,Performance,Load,LoadVCF,514,"Builds on: https://github.com/hail-is/hail/pull/2228. It is smaller, cleaner and more self-contained, but I could still break it into more pieces if needed. Some remarks:; - I removed the BroadcastTypeTree nonsense. This will kill KeyTable joins but we can fix that later.; - I sample keys as part of collecting the partition key info.; - I left off two features off the key ranges sampler vs OrderedRDD: I don't resample large partitions and I don't use weights to calculate the ranges.; - I included the minimal LoadVCF interface changes. VCF parser will come as a separate PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2236
https://github.com/hail-is/hail/pull/2244:114,Integrability,interface,interface,114,Here's what I have now. Higher-level feedback appreciated. Note there are 4 hidden methods to convert between old interface and new. `VariantDataset._to_new_variant_dataset`; `NewVariantDataset._to_old_variant_dataset`; `KeyTable._to_new_keytable`; `NewKeyTable._to_old_keytable`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2244
https://github.com/hail-is/hail/pull/2244:37,Usability,feedback,feedback,37,Here's what I have now. Higher-level feedback appreciated. Note there are 4 hidden methods to convert between old interface and new. `VariantDataset._to_new_variant_dataset`; `NewVariantDataset._to_old_variant_dataset`; `KeyTable._to_new_keytable`; `NewKeyTable._to_old_keytable`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2244
https://github.com/hail-is/hail/pull/2246:32,Performance,Load,LoadMatrix,32,"Builds on #2236. . Implements a LoadMatrix function that loads a VariantSampleMatrix from TSV of [rowID, ints... ] with a header containing column IDs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2246
https://github.com/hail-is/hail/pull/2246:57,Performance,load,loads,57,"Builds on #2236. . Implements a LoadMatrix function that loads a VariantSampleMatrix from TSV of [rowID, ints... ] with a header containing column IDs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2246
https://github.com/hail-is/hail/pull/2247:16,Availability,error,error,16,"…other headers, error won't be thrown",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2247
https://github.com/hail-is/hail/pull/2248:1719,Availability,down,down,1719,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2502,Availability,error,error,2502,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2570,Deployability,Update,Updated,2570,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2281,Energy Efficiency,reduce,reduce,2281,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:911,Integrability,rout,routine,911,"I've worked to improve Skat with regard to usability and code readability / organization and to address issues / bugs that could provoke a crash. - Added group size (number of variants) as a column in the returned key table. - Added an optional maxSize parameter so large groups only return their size rather than filling memory and killing the entire job. - Now computeGramianLargeN is used if n * m exceeds 8000 * 8000 (about 512MB of doubles) or maxSize * maxSize if maxSize is given and smaller than 8000. This seems a reasonable approach for now to prevent OOM without exposing additional memory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:1451,Integrability,rout,routines,1451,"en and smaller than 8000. This seems a reasonable approach for now to prevent OOM without exposing additional memory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:1922,Integrability,rout,route,1922,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2508,Integrability,message,message,2508,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:1256,Performance,tune,tune,1256,"ameter so large groups only return their size rather than filling memory and killing the entire job. - Now computeGramianLargeN is used if n * m exceeds 8000 * 8000 (about 512MB of doubles) or maxSize * maxSize if maxSize is given and smaller than 8000. This seems a reasonable approach for now to prevent OOM without exposing additional memory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 200",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2170,Safety,avoid,avoid,2170,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:1948,Testability,log,logistic,1948,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2133,Testability,test,test,2133,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2216,Testability,log,logistic,2216,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2350,Testability,log,logistic,2350,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2374,Testability,test,test,2374,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2417,Testability,test,tests,2417,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2436,Testability,test,test,2436,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:2553,Testability,log,logistic,2553,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:43,Usability,usab,usability,43,"I've worked to improve Skat with regard to usability and code readability / organization and to address issues / bugs that could provoke a crash. - Added group size (number of variants) as a column in the returned key table. - Added an optional maxSize parameter so large groups only return their size rather than filling memory and killing the entire job. - Now computeGramianLargeN is used if n * m exceeds 8000 * 8000 (about 512MB of doubles) or maxSize * maxSize if maxSize is given and smaller than 8000. This seems a reasonable approach for now to prevent OOM without exposing additional memory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:640,Usability,feedback,feedback,640,"I've worked to improve Skat with regard to usability and code readability / organization and to address issues / bugs that could provoke a crash. - Added group size (number of variants) as a column in the returned key table. - Added an optional maxSize parameter so large groups only return their size rather than filling memory and killing the entire job. - Now computeGramianLargeN is used if n * m exceeds 8000 * 8000 (about 512MB of doubles) or maxSize * maxSize if maxSize is given and smaller than 8000. This seems a reasonable approach for now to prevent OOM without exposing additional memory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2248:1669,Usability,Simpl,Simplified,1669,"emory parameters, but once we have some user feedback I'd like to consider re-implementing computeGramianLargeN to use BLAS3 outer product on blocks of (fewer than m) rows of the n x m matrix rather than inner product on all pairs of columns, which I think will boost speed and make it reasonable to kill the smallN routine entirely (the current largeN case benefits from dot product of sparse vectors when using hard calls, but that also goes away when we move to generic 0.2 and rip out the hardcall/dosage complexity). Then it will be natural for maxSize to control the number of rows in a block. - Added accuracy and iterations parameters to allow users to tune Davies, with R settings for Davies (1e-6, 10k) as default. This allows users to re-run groups with tiny p-values if desired to obtain greater accuracy. The R package runs additional p-value routines that may be faster when the p-value is very small, will keep in mind should this become an issue. - In remark above the Skat class, I've added an overview of how math in paper corresponds to implementation. - Simplified and re-organized the Skat class to cut down on the number and complexity of passed parameters and make the meaning of the code more transparent with respect to the overview. Killed the SkatModel class. - Fixed an oversight whereby the largeN route was never called by logistic. - Fixed a bug whereby a weight of null was passed to DoubleNumericConversion.to and then Option rather than the other way around to prevent null match exception. - Modified R test code to use Adjustment=False to avoid the small-sample adjustment made in the logistic case when running using than 2000 samples. I could then reduce the Balding-Nichols example from 2001 and 500 samples and run logistic on the smaller test set as well. - Further cleaned up the tests, and added a test of the size column and maxSize parameter. - More descriptive error message should Cholesky or inversion fail in logistic case. - Updated docs accordingly.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2248
https://github.com/hail-is/hail/pull/2249:123,Safety,avoid,avoids,123,"Builds on PC Relate Unified Filtering #2238. I think this practically much more valuable to our users than filtering. This avoids computation of unneeded statistics (they appear as NA in the output). I'm happy to rebase without unified filtering, but it's some work to do that, so I'd rather decide one way or the other on filtering first.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2249
https://github.com/hail-is/hail/pull/2252:61,Performance,cache,cache,61,"The fourth in a series of PCRelate Improvements. Sprinkling `cache` on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 36 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""phi"").count(); end = timer(); print(end-start); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2252
https://github.com/hail-is/hail/pull/2252:144,Testability,benchmark,benchmark,144,"The fourth in a series of PCRelate Improvements. Sprinkling `cache` on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 36 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""phi"").count(); end = timer(); print(end-start); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2252
https://github.com/hail-is/hail/pull/2253:48,Integrability,Depend,Depends,48,"The third in a series of PCRelate Improvements. Depends on #2249 . Sprinkling `cache` on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 40 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""all"").count(); end = timer(); print(end-start); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2253
https://github.com/hail-is/hail/pull/2253:79,Performance,cache,cache,79,"The third in a series of PCRelate Improvements. Depends on #2249 . Sprinkling `cache` on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 40 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""all"").count(); end = timer(); print(end-start); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2253
https://github.com/hail-is/hail/pull/2253:162,Testability,benchmark,benchmark,162,"The third in a series of PCRelate Improvements. Depends on #2249 . Sprinkling `cache` on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 40 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""all"").count(); end = timer(); print(end-start); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2253
https://github.com/hail-is/hail/pull/2262:187,Availability,error,errors,187,"Every suite is allocated by the gradle test framework, and then only those matching the requested filters are executed. Ergo, non-lazy fields on a suite will be executed (and may trigger errors) even though the suite wasn't requested by the gradle user.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2262
https://github.com/hail-is/hail/pull/2262:15,Energy Efficiency,allocate,allocated,15,"Every suite is allocated by the gradle test framework, and then only those matching the requested filters are executed. Ergo, non-lazy fields on a suite will be executed (and may trigger errors) even though the suite wasn't requested by the gradle user.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2262
https://github.com/hail-is/hail/pull/2262:39,Testability,test,test,39,"Every suite is allocated by the gradle test framework, and then only those matching the requested filters are executed. Ergo, non-lazy fields on a suite will be executed (and may trigger errors) even though the suite wasn't requested by the gradle user.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2262
https://github.com/hail-is/hail/pull/2264:47,Testability,test,tests,47,Our users will interact with a KeyTable so the tests should too.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2264
https://github.com/hail-is/hail/pull/2265:29,Usability,simpl,simpler,29,"cc: @cseed . This seems much simpler and saves a per-element allocation, but I'm not entirely sure I understand how `saveAsSequenceFile` differs from `saveAsHadoopFile`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2265
https://github.com/hail-is/hail/pull/2267:72,Deployability,install,installed,72,"`libsimdpp` only requires CMake for creating a distribution that can be installed on a given system. We only need the header files at compile time, so there's no need to install it. This removes our dependency on CMake and prevents recompilation of the C code on each call to `gradle test` (as well as `compileScala` and `compileTestScala`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2267
https://github.com/hail-is/hail/pull/2267:170,Deployability,install,install,170,"`libsimdpp` only requires CMake for creating a distribution that can be installed on a given system. We only need the header files at compile time, so there's no need to install it. This removes our dependency on CMake and prevents recompilation of the C code on each call to `gradle test` (as well as `compileScala` and `compileTestScala`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2267
https://github.com/hail-is/hail/pull/2267:199,Integrability,depend,dependency,199,"`libsimdpp` only requires CMake for creating a distribution that can be installed on a given system. We only need the header files at compile time, so there's no need to install it. This removes our dependency on CMake and prevents recompilation of the C code on each call to `gradle test` (as well as `compileScala` and `compileTestScala`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2267
https://github.com/hail-is/hail/pull/2267:284,Testability,test,test,284,"`libsimdpp` only requires CMake for creating a distribution that can be installed on a given system. We only need the header files at compile time, so there's no need to install it. This removes our dependency on CMake and prevents recompilation of the C code on each call to `gradle test` (as well as `compileScala` and `compileTestScala`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2267
https://github.com/hail-is/hail/issues/2272:3,Availability,Error,Error,3,```Error summary: Error: Multiple ES-Hadoop versions detected in the classpath; please use only one; jar:file:/tmp/7a54aa23-f38b-40e4-8068-3ea48ee212a0/hail-annotateAlleles01.jar; jar:file:/usr/lib/spark/jars/hail-0.1-6e815ac3d973-Spark-2.0.2.jar; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2272
https://github.com/hail-is/hail/issues/2272:18,Availability,Error,Error,18,```Error summary: Error: Multiple ES-Hadoop versions detected in the classpath; please use only one; jar:file:/tmp/7a54aa23-f38b-40e4-8068-3ea48ee212a0/hail-annotateAlleles01.jar; jar:file:/usr/lib/spark/jars/hail-0.1-6e815ac3d973-Spark-2.0.2.jar; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2272
https://github.com/hail-is/hail/issues/2272:53,Safety,detect,detected,53,```Error summary: Error: Multiple ES-Hadoop versions detected in the classpath; please use only one; jar:file:/tmp/7a54aa23-f38b-40e4-8068-3ea48ee212a0/hail-annotateAlleles01.jar; jar:file:/usr/lib/spark/jars/hail-0.1-6e815ac3d973-Spark-2.0.2.jar; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2272
https://github.com/hail-is/hail/pull/2274:77,Testability,test,test,77,@jigold identified `ibd_prune` as one of the slowest parts of the python doc test. This method is simply not effective. Users should use `pc_relate` or `ibd` in conjunction with `KeyTable.maximal_independent_set`. I didn't reimplement `ibd_prune` in terms of them simply to save time and because getting master builds going fast is important when we have limited build agents.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2274
https://github.com/hail-is/hail/pull/2274:98,Usability,simpl,simply,98,@jigold identified `ibd_prune` as one of the slowest parts of the python doc test. This method is simply not effective. Users should use `pc_relate` or `ibd` in conjunction with `KeyTable.maximal_independent_set`. I didn't reimplement `ibd_prune` in terms of them simply to save time and because getting master builds going fast is important when we have limited build agents.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2274
https://github.com/hail-is/hail/pull/2274:264,Usability,simpl,simply,264,@jigold identified `ibd_prune` as one of the slowest parts of the python doc test. This method is simply not effective. Users should use `pc_relate` or `ibd` in conjunction with `KeyTable.maximal_independent_set`. I didn't reimplement `ibd_prune` in terms of them simply to save time and because getting master builds going fast is important when we have limited build agents.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2274
https://github.com/hail-is/hail/pull/2275:39,Energy Efficiency,reduce,reduce,39,This test currently takes 3 minutes. I reduce the number of parameters we check.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2275
https://github.com/hail-is/hail/pull/2275:5,Testability,test,test,5,This test currently takes 3 minutes. I reduce the number of parameters we check.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2275
https://github.com/hail-is/hail/pull/2276:131,Energy Efficiency,adapt,adapt,131,"I based this on the row store in 0.2, in order to preserve partitioning on block matrices under read / write. @danking this should adapt to HailBlockMatrix with basically no change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2276
https://github.com/hail-is/hail/pull/2276:131,Modifiability,adapt,adapt,131,"I based this on the row store in 0.2, in order to preserve partitioning on block matrices under read / write. @danking this should adapt to HailBlockMatrix with basically no change.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2276
https://github.com/hail-is/hail/pull/2277:55,Testability,test,test-suite,55,"Curious if this makes our builds faster. Gradle is per-test-suite parallel, but not per-method-parallel.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2277
https://github.com/hail-is/hail/pull/2279:179,Integrability,interface,interface,179,"- Removed `export_samples`, `export_variants`, `export_genotypes`. To perform same functionality, need to convert to KeyTable and then use `select` and `export`. - Changed Python interface for `select`, `drop`, and `key_by` to take varargs rather than a String or List of String. - Select is not backwards compatible with 0.1. To select a column name with periods in it, must use backticks now. - Not certain whether left hand side of named expression should be treated as an identifier or an annotationIdentifier. Right now, it's treated as an identifier. If it's an identifier, than `A.B = 5` will have a signature of `(""A.B"", TInt)`. If it's an annotationIdentifier path, than the signature would be `(""A"", TStruct((""B"", TInt)))`. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2279
https://github.com/hail-is/hail/pull/2279:70,Performance,perform,perform,70,"- Removed `export_samples`, `export_variants`, `export_genotypes`. To perform same functionality, need to convert to KeyTable and then use `select` and `export`. - Changed Python interface for `select`, `drop`, and `key_by` to take varargs rather than a String or List of String. - Select is not backwards compatible with 0.1. To select a column name with periods in it, must use backticks now. - Not certain whether left hand side of named expression should be treated as an identifier or an annotationIdentifier. Right now, it's treated as an identifier. If it's an identifier, than `A.B = 5` will have a signature of `(""A.B"", TInt)`. If it's an annotationIdentifier path, than the signature would be `(""A"", TStruct((""B"", TInt)))`. Thoughts?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2279
https://github.com/hail-is/hail/pull/2280:48,Integrability,Depend,Depends,48,"The third in a series of PCRelate Improvements. Depends on #2249. Sprinkling cache on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 80 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""all"").count(); end = timer(); print(end-start); ```. Ready for a final look after #2270 lands. Creating a PR so that @konradjk and others can take it for a spin if desired.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2280
https://github.com/hail-is/hail/pull/2280:77,Performance,cache,cache,77,"The third in a series of PCRelate Improvements. Depends on #2249. Sprinkling cache on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 80 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""all"").count(); end = timer(); print(end-start); ```. Ready for a final look after #2270 lands. Creating a PR so that @konradjk and others can take it for a spin if desired.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2280
https://github.com/hail-is/hail/pull/2280:159,Testability,benchmark,benchmark,159,"The third in a series of PCRelate Improvements. Depends on #2249. Sprinkling cache on any RDD which is used more than once dramatically improved runtime. On a benchmark program (included below) PCRelate took 80 seconds with four cores on 1000 samples and 10,000 variants. ```; from hail import *; from timeit import default_timer as timer. hc = HailContext(); vds = hc.balding_nichols_model(20, 1000, 10000).repartition(10).persist(); vds.count(). start = timer(); vds.pc_relate(5, 0.01, min_kinship=0.1, desire=""all"").count(); end = timer(); print(end-start); ```. Ready for a final look after #2270 lands. Creating a PR so that @konradjk and others can take it for a spin if desired.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2280
https://github.com/hail-is/hail/pull/2285:32,Performance,optimiz,optimizer,32,This should play well with [the optimizer](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/expr/Relational.scala#L189-L192),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2285
https://github.com/hail-is/hail/pull/2288:149,Energy Efficiency,power,power,149,"I'd love feedback, especially on:; * How/whether to test these things; * How to organize a growing collection of hash families, with different speed/power tradeoffs, and different key and hash word-lengths. (These will be used in inner-most loops, so performance matters, and I don't have a good sense of what Scala abstractions hurt performance.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2288
https://github.com/hail-is/hail/pull/2288:251,Performance,perform,performance,251,"I'd love feedback, especially on:; * How/whether to test these things; * How to organize a growing collection of hash families, with different speed/power tradeoffs, and different key and hash word-lengths. (These will be used in inner-most loops, so performance matters, and I don't have a good sense of what Scala abstractions hurt performance.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2288
https://github.com/hail-is/hail/pull/2288:334,Performance,perform,performance,334,"I'd love feedback, especially on:; * How/whether to test these things; * How to organize a growing collection of hash families, with different speed/power tradeoffs, and different key and hash word-lengths. (These will be used in inner-most loops, so performance matters, and I don't have a good sense of what Scala abstractions hurt performance.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2288
https://github.com/hail-is/hail/pull/2288:113,Security,hash,hash,113,"I'd love feedback, especially on:; * How/whether to test these things; * How to organize a growing collection of hash families, with different speed/power tradeoffs, and different key and hash word-lengths. (These will be used in inner-most loops, so performance matters, and I don't have a good sense of what Scala abstractions hurt performance.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2288
https://github.com/hail-is/hail/pull/2288:188,Security,hash,hash,188,"I'd love feedback, especially on:; * How/whether to test these things; * How to organize a growing collection of hash families, with different speed/power tradeoffs, and different key and hash word-lengths. (These will be used in inner-most loops, so performance matters, and I don't have a good sense of what Scala abstractions hurt performance.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2288
https://github.com/hail-is/hail/pull/2288:52,Testability,test,test,52,"I'd love feedback, especially on:; * How/whether to test these things; * How to organize a growing collection of hash families, with different speed/power tradeoffs, and different key and hash word-lengths. (These will be used in inner-most loops, so performance matters, and I don't have a good sense of what Scala abstractions hurt performance.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2288
https://github.com/hail-is/hail/pull/2288:9,Usability,feedback,feedback,9,"I'd love feedback, especially on:; * How/whether to test these things; * How to organize a growing collection of hash families, with different speed/power tradeoffs, and different key and hash word-lengths. (These will be used in inner-most loops, so performance matters, and I don't have a good sense of what Scala abstractions hurt performance.)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2288
https://github.com/hail-is/hail/pull/2289:39,Safety,Unsafe,UnsafeRow,39,Unnecessarily specific (and crashes on UnsafeRow).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2289
https://github.com/hail-is/hail/pull/2296:0,Security,Expose,Expose,0,"Expose gIsDefined, isLinearScale.; Expose PX for TGenotypeView.; Fixed some offset bugs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2296
https://github.com/hail-is/hail/pull/2296:35,Security,Expose,Expose,35,"Expose gIsDefined, isLinearScale.; Expose PX for TGenotypeView.; Fixed some offset bugs.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2296
https://github.com/hail-is/hail/pull/2297:0,Integrability,depend,depends,0,depends on #2270,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2297
https://github.com/hail-is/hail/pull/2300:25,Deployability,update,updates,25,"Hi,. Here are our latest updates to the johnc branch originally created by John Compitello. We have improved the overall annotation performance by increasing the default block_size to 500K. Please let us know if you have any questions. Best,; Shuli & the Nirvana team",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2300
https://github.com/hail-is/hail/pull/2300:132,Performance,perform,performance,132,"Hi,. Here are our latest updates to the johnc branch originally created by John Compitello. We have improved the overall annotation performance by increasing the default block_size to 500K. Please let us know if you have any questions. Best,; Shuli & the Nirvana team",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2300
https://github.com/hail-is/hail/pull/2301:261,Performance,Cache,Cache,261,"Builds on: https://github.com/hail-is/hail/pull/2299. History of BTT is somewhat obscure. Now we just serialize the type in Unsafe{Row, IndexedSeq}. And some small additional improvements along the way:. Renamed UnsafeIndexedSeqAnnotation => UnsafeIndexedSeq.; Cache specialized Array types in UnsafeRow to avoid allocation.; Fixed ordering disagreement for Variant (added regression test).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2301
https://github.com/hail-is/hail/pull/2301:124,Safety,Unsafe,Unsafe,124,"Builds on: https://github.com/hail-is/hail/pull/2299. History of BTT is somewhat obscure. Now we just serialize the type in Unsafe{Row, IndexedSeq}. And some small additional improvements along the way:. Renamed UnsafeIndexedSeqAnnotation => UnsafeIndexedSeq.; Cache specialized Array types in UnsafeRow to avoid allocation.; Fixed ordering disagreement for Variant (added regression test).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2301
https://github.com/hail-is/hail/pull/2301:212,Safety,Unsafe,UnsafeIndexedSeqAnnotation,212,"Builds on: https://github.com/hail-is/hail/pull/2299. History of BTT is somewhat obscure. Now we just serialize the type in Unsafe{Row, IndexedSeq}. And some small additional improvements along the way:. Renamed UnsafeIndexedSeqAnnotation => UnsafeIndexedSeq.; Cache specialized Array types in UnsafeRow to avoid allocation.; Fixed ordering disagreement for Variant (added regression test).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2301
https://github.com/hail-is/hail/pull/2301:242,Safety,Unsafe,UnsafeIndexedSeq,242,"Builds on: https://github.com/hail-is/hail/pull/2299. History of BTT is somewhat obscure. Now we just serialize the type in Unsafe{Row, IndexedSeq}. And some small additional improvements along the way:. Renamed UnsafeIndexedSeqAnnotation => UnsafeIndexedSeq.; Cache specialized Array types in UnsafeRow to avoid allocation.; Fixed ordering disagreement for Variant (added regression test).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2301
https://github.com/hail-is/hail/pull/2301:294,Safety,Unsafe,UnsafeRow,294,"Builds on: https://github.com/hail-is/hail/pull/2299. History of BTT is somewhat obscure. Now we just serialize the type in Unsafe{Row, IndexedSeq}. And some small additional improvements along the way:. Renamed UnsafeIndexedSeqAnnotation => UnsafeIndexedSeq.; Cache specialized Array types in UnsafeRow to avoid allocation.; Fixed ordering disagreement for Variant (added regression test).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2301
https://github.com/hail-is/hail/pull/2301:307,Safety,avoid,avoid,307,"Builds on: https://github.com/hail-is/hail/pull/2299. History of BTT is somewhat obscure. Now we just serialize the type in Unsafe{Row, IndexedSeq}. And some small additional improvements along the way:. Renamed UnsafeIndexedSeqAnnotation => UnsafeIndexedSeq.; Cache specialized Array types in UnsafeRow to avoid allocation.; Fixed ordering disagreement for Variant (added regression test).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2301
https://github.com/hail-is/hail/pull/2301:384,Testability,test,test,384,"Builds on: https://github.com/hail-is/hail/pull/2299. History of BTT is somewhat obscure. Now we just serialize the type in Unsafe{Row, IndexedSeq}. And some small additional improvements along the way:. Renamed UnsafeIndexedSeqAnnotation => UnsafeIndexedSeq.; Cache specialized Array types in UnsafeRow to avoid allocation.; Fixed ordering disagreement for Variant (added regression test).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2301
https://github.com/hail-is/hail/pull/2302:86,Deployability,integrat,integrate,86,Builds on: https://github.com/hail-is/hail/pull/2301. We're on the home stretch. Will integrate this into VSM in the next PR.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2302
https://github.com/hail-is/hail/pull/2302:86,Integrability,integrat,integrate,86,Builds on: https://github.com/hail-is/hail/pull/2301. We're on the home stretch. Will integrate this into VSM in the next PR.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2302
https://github.com/hail-is/hail/pull/2303:36,Security,hash,hash,36,"Implements the two ""multiply-shift"" hash functions, as described in [High Speed Hashing for Integers and Strings](http://arxiv.org/abs/1504.06804v3), sections 2.3 and 3.3. They have the weakest still useful distributional properties, but are often sufficient, and very very fast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2303
https://github.com/hail-is/hail/pull/2303:80,Security,Hash,Hashing,80,"Implements the two ""multiply-shift"" hash functions, as described in [High Speed Hashing for Integers and Strings](http://arxiv.org/abs/1504.06804v3), sections 2.3 and 3.3. They have the weakest still useful distributional properties, but are often sufficient, and very very fast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2303
https://github.com/hail-is/hail/pull/2304:80,Energy Efficiency,Power,Powerful,80,"Implements simple tabulation and twisted tabulation hash methods. See [Fast and Powerful Hashing using Tabulation](http://arxiv.org/abs/1505.01523v5): simple tabulation is described in Section 2, twisted tabulation is described in Section 3, and Figure 1 on p.9 has C code for both.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2304
https://github.com/hail-is/hail/pull/2304:52,Security,hash,hash,52,"Implements simple tabulation and twisted tabulation hash methods. See [Fast and Powerful Hashing using Tabulation](http://arxiv.org/abs/1505.01523v5): simple tabulation is described in Section 2, twisted tabulation is described in Section 3, and Figure 1 on p.9 has C code for both.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2304
https://github.com/hail-is/hail/pull/2304:89,Security,Hash,Hashing,89,"Implements simple tabulation and twisted tabulation hash methods. See [Fast and Powerful Hashing using Tabulation](http://arxiv.org/abs/1505.01523v5): simple tabulation is described in Section 2, twisted tabulation is described in Section 3, and Figure 1 on p.9 has C code for both.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2304
https://github.com/hail-is/hail/pull/2304:11,Usability,simpl,simple,11,"Implements simple tabulation and twisted tabulation hash methods. See [Fast and Powerful Hashing using Tabulation](http://arxiv.org/abs/1505.01523v5): simple tabulation is described in Section 2, twisted tabulation is described in Section 3, and Figure 1 on p.9 has C code for both.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2304
https://github.com/hail-is/hail/pull/2304:151,Usability,simpl,simple,151,"Implements simple tabulation and twisted tabulation hash methods. See [Fast and Powerful Hashing using Tabulation](http://arxiv.org/abs/1505.01523v5): simple tabulation is described in Section 2, twisted tabulation is described in Section 3, and Figure 1 on p.9 has C code for both.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2304
https://github.com/hail-is/hail/pull/2310:62,Testability,test,test,62,"@patrick-schultz you can look at the docs by opening the docs test result link below, then navigating to artifacts then navigating to the index.html page in that folder structure.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2310
https://github.com/hail-is/hail/pull/2311:44,Security,hash,hash,44,"Implements a more involved tabulation based hash function which achieves 5-independence (which will be needed in one of the basic primitives for randomized linear algebra). This method is described in [Tabulation-Based 5-Independent Hashing with Applications to Linear Probing and Second Moment Estimation](http://www.cs.utexas.edu/~yzhang/papers/5-indep-sicomp12.pdf), which provides C code in section A.7.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2311
https://github.com/hail-is/hail/pull/2311:233,Security,Hash,Hashing,233,"Implements a more involved tabulation based hash function which achieves 5-independence (which will be needed in one of the basic primitives for randomized linear algebra). This method is described in [Tabulation-Based 5-Independent Hashing with Applications to Linear Probing and Second Moment Estimation](http://www.cs.utexas.edu/~yzhang/papers/5-indep-sicomp12.pdf), which provides C code in section A.7.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2311
https://github.com/hail-is/hail/issues/2314:680,Energy Efficiency,allocate,allocates,680,"The Balding Nichols Model currently does a bunch of allocation per-variant. We can avoid a lot of this by using a random seed per-partition, instead of per-variant. Moreover, we should modify the interface of `Distribution` such that it reads:. ```scala; trait Distribution {; def setSeed(seed: Long); def sample(): Double; }; ```. And the implementations should rely directly on java.util.Random:; ```scala; class UniformDist(...) {; ...; private val rand = new java.util.Random(); def setSeed(seed: Long) {; rand.setSeed(seed); }; def sample(): Double = rand.nextDouble(minVal, maxVal); }; ```; etc. Then we can reformulate the balding nichols model with a `mapPartitions` that allocates one Distribution per partition and seeds it once. If all the partition seeds come from one master seed, then the entire process is deterministic, but only requires allocation and seeding per partition.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2314
https://github.com/hail-is/hail/issues/2314:196,Integrability,interface,interface,196,"The Balding Nichols Model currently does a bunch of allocation per-variant. We can avoid a lot of this by using a random seed per-partition, instead of per-variant. Moreover, we should modify the interface of `Distribution` such that it reads:. ```scala; trait Distribution {; def setSeed(seed: Long); def sample(): Double; }; ```. And the implementations should rely directly on java.util.Random:; ```scala; class UniformDist(...) {; ...; private val rand = new java.util.Random(); def setSeed(seed: Long) {; rand.setSeed(seed); }; def sample(): Double = rand.nextDouble(minVal, maxVal); }; ```; etc. Then we can reformulate the balding nichols model with a `mapPartitions` that allocates one Distribution per partition and seeds it once. If all the partition seeds come from one master seed, then the entire process is deterministic, but only requires allocation and seeding per partition.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2314
https://github.com/hail-is/hail/issues/2314:83,Safety,avoid,avoid,83,"The Balding Nichols Model currently does a bunch of allocation per-variant. We can avoid a lot of this by using a random seed per-partition, instead of per-variant. Moreover, we should modify the interface of `Distribution` such that it reads:. ```scala; trait Distribution {; def setSeed(seed: Long); def sample(): Double; }; ```. And the implementations should rely directly on java.util.Random:; ```scala; class UniformDist(...) {; ...; private val rand = new java.util.Random(); def setSeed(seed: Long) {; rand.setSeed(seed); }; def sample(): Double = rand.nextDouble(minVal, maxVal); }; ```; etc. Then we can reformulate the balding nichols model with a `mapPartitions` that allocates one Distribution per partition and seeds it once. If all the partition seeds come from one master seed, then the entire process is deterministic, but only requires allocation and seeding per partition.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2314
https://github.com/hail-is/hail/pull/2316:183,Performance,optimiz,optimization,183,"I agree with this comment: https://stackoverflow.com/a/17329465/431282. > Lazy val is *not* free (or even cheap). Use it only if you absolutely need laziness for correctness, not for optimization. I think we should avoid lazy val. This was borne out when profiling `RegionValue` stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2316
https://github.com/hail-is/hail/pull/2316:215,Safety,avoid,avoid,215,"I agree with this comment: https://stackoverflow.com/a/17329465/431282. > Lazy val is *not* free (or even cheap). Use it only if you absolutely need laziness for correctness, not for optimization. I think we should avoid lazy val. This was borne out when profiling `RegionValue` stuff.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2316
https://github.com/hail-is/hail/pull/2317:10,Safety,unsafe,unsafeInsert,10,Includes `unsafeInsert`. @tpoterba: @danking got the random draw but I thought you'd like to take a look at this.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2317
https://github.com/hail-is/hail/pull/2320:97,Energy Efficiency,efficient,efficiently,97,"Two go full native, we still need:; - support for region values in expr language,; - support for efficiently inserting multiple values simultaneously.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2320
https://github.com/hail-is/hail/pull/2322:58,Integrability,rout,routines,58,mapValuesWithAll should go away eventually (as should all routines representing runtime values as Annotation).; This makes filterGenotypes semi-native in particular.; Removed unused filterGenotypes on VariantDataset.; Removed unused mapPartitionsWithAll.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2322
https://github.com/hail-is/hail/pull/2327:76,Availability,avail,available,76,> TMPDIR; > This variable shall represent a pathname of a directory made; > available for programs that need a place to create temporary; > files. http://pubs.opengroup.org/onlinepubs/9699919799/. Requested by the discuss user rca:. http://discuss.hail.is/t/hailcontext-tmp-dir/323,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2327
https://github.com/hail-is/hail/pull/2327:17,Modifiability,variab,variable,17,> TMPDIR; > This variable shall represent a pathname of a directory made; > available for programs that need a place to create temporary; > files. http://pubs.opengroup.org/onlinepubs/9699919799/. Requested by the discuss user rca:. http://discuss.hail.is/t/hailcontext-tmp-dir/323,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2327
https://github.com/hail-is/hail/pull/2329:364,Availability,toler,tolerate,364,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:556,Availability,error,error,556,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:734,Availability,error,error,734,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:562,Integrability,message,message,562,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:1475,Integrability,interface,interface,1475,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:1090,Safety,avoid,avoid,1090,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:69,Testability,test,test,69,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:141,Testability,test,test,141,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:202,Testability,test,test,202,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:287,Testability,test,test,287,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:295,Testability,test,test,295,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:355,Testability,test,tests,355,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:385,Testability,test,test,385,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:693,Testability,test,test,693,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:812,Testability,test,test,812,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:919,Testability,test,tests,919,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:1456,Testability,test,test,1456,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2329:108,Usability,simpl,simplify,108,"* fixed finally. * evict Spark BlockMatrix and friends. * remove old test suite. * a bunch more cleanups. * simplify grid partitioner. * fix test. * remove unneeded try-catches. * organization. * add a test suite for HBM. * help closure serializer. * use correct aggregation method, add test. * test+fix bug grid partitioner. * wip zippartitions. * teach tests to tolerate NaNs. * fix test. * kinda works again. * remove unnecessary trys. * handle transposition in map*. * clean up imports. * standardize langauge. * bunch of comments addressed. * improve error message. * fix python. * rename HailBlockMatrix -> BlockMatrix. * a bunch of comments addressed. * more comments addressed. * make test comment not confusing. * fix rebase error. * fixes. * fix. * fix bug in rirm. * gotta get that transpose right. * test fixes. * dan is a dummy. * commits got lost for sure. * realize transpose when writing. * add indexed tests for map2?WithIndex when transposed. * use Gen.denseMatrix. * use Gen.denseMatrix. * final fixes. * toLocalMatrix returns Spark matrix for backwards compatibility. * avoid an array copy. the BDMs produced by BlockMatrix.toLocalMatrix are in a; ""normal form"", i.e. offset 0, column-major stride, non-; transposed. Given this assumption we can quickly produce a; Spark-style local matrix. * dan is a dummy. * collect-in-order. collect doesn't guarantee order. * do not use BDM.data naively. This was the true root casue: an incorrect test. * fix python interface. * in python, java fields are methods. note the addition of parentheses",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2329
https://github.com/hail-is/hail/pull/2330:7,Availability,error,error,7,"Better error handling:. Attempt to verify correctness of byte code using automatically computed maxes and stack frames. ; - If that prints no messages and throws no exceptions, then everything worked fine.; - If that throws no exception, but prints a message, then verification failed. Print the verification failure message and throw an exception.; - If that throws an exception, then it is likely that auto-computing stack frames failed, so let's try to verify again without the stack frames; - If that prints no messages, then something weird happened, rethrow the original verification exception.; - If that prints a message, then verification failed. Print the verification failure message and throw an exception.; - I've never seen the max-only verifier throw an exception. I also added a bit that would print out the byte code if given a `PrintWriter` to print to. This functionality is currently unused, but was useful for debugging.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2330
https://github.com/hail-is/hail/pull/2330:309,Availability,failure,failure,309,"Better error handling:. Attempt to verify correctness of byte code using automatically computed maxes and stack frames. ; - If that prints no messages and throws no exceptions, then everything worked fine.; - If that throws no exception, but prints a message, then verification failed. Print the verification failure message and throw an exception.; - If that throws an exception, then it is likely that auto-computing stack frames failed, so let's try to verify again without the stack frames; - If that prints no messages, then something weird happened, rethrow the original verification exception.; - If that prints a message, then verification failed. Print the verification failure message and throw an exception.; - I've never seen the max-only verifier throw an exception. I also added a bit that would print out the byte code if given a `PrintWriter` to print to. This functionality is currently unused, but was useful for debugging.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2330
https://github.com/hail-is/hail/pull/2330:679,Availability,failure,failure,679,"Better error handling:. Attempt to verify correctness of byte code using automatically computed maxes and stack frames. ; - If that prints no messages and throws no exceptions, then everything worked fine.; - If that throws no exception, but prints a message, then verification failed. Print the verification failure message and throw an exception.; - If that throws an exception, then it is likely that auto-computing stack frames failed, so let's try to verify again without the stack frames; - If that prints no messages, then something weird happened, rethrow the original verification exception.; - If that prints a message, then verification failed. Print the verification failure message and throw an exception.; - I've never seen the max-only verifier throw an exception. I also added a bit that would print out the byte code if given a `PrintWriter` to print to. This functionality is currently unused, but was useful for debugging.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2330
https://github.com/hail-is/hail/pull/2330:142,Integrability,message,messages,142,"Better error handling:. Attempt to verify correctness of byte code using automatically computed maxes and stack frames. ; - If that prints no messages and throws no exceptions, then everything worked fine.; - If that throws no exception, but prints a message, then verification failed. Print the verification failure message and throw an exception.; - If that throws an exception, then it is likely that auto-computing stack frames failed, so let's try to verify again without the stack frames; - If that prints no messages, then something weird happened, rethrow the original verification exception.; - If that prints a message, then verification failed. Print the verification failure message and throw an exception.; - I've never seen the max-only verifier throw an exception. I also added a bit that would print out the byte code if given a `PrintWriter` to print to. This functionality is currently unused, but was useful for debugging.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2330
https://github.com/hail-is/hail/pull/2330:251,Integrability,message,message,251,"Better error handling:. Attempt to verify correctness of byte code using automatically computed maxes and stack frames. ; - If that prints no messages and throws no exceptions, then everything worked fine.; - If that throws no exception, but prints a message, then verification failed. Print the verification failure message and throw an exception.; - If that throws an exception, then it is likely that auto-computing stack frames failed, so let's try to verify again without the stack frames; - If that prints no messages, then something weird happened, rethrow the original verification exception.; - If that prints a message, then verification failed. Print the verification failure message and throw an exception.; - I've never seen the max-only verifier throw an exception. I also added a bit that would print out the byte code if given a `PrintWriter` to print to. This functionality is currently unused, but was useful for debugging.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2330
https://github.com/hail-is/hail/pull/2330:317,Integrability,message,message,317,"Better error handling:. Attempt to verify correctness of byte code using automatically computed maxes and stack frames. ; - If that prints no messages and throws no exceptions, then everything worked fine.; - If that throws no exception, but prints a message, then verification failed. Print the verification failure message and throw an exception.; - If that throws an exception, then it is likely that auto-computing stack frames failed, so let's try to verify again without the stack frames; - If that prints no messages, then something weird happened, rethrow the original verification exception.; - If that prints a message, then verification failed. Print the verification failure message and throw an exception.; - I've never seen the max-only verifier throw an exception. I also added a bit that would print out the byte code if given a `PrintWriter` to print to. This functionality is currently unused, but was useful for debugging.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2330
https://github.com/hail-is/hail/pull/2330:515,Integrability,message,messages,515,"Better error handling:. Attempt to verify correctness of byte code using automatically computed maxes and stack frames. ; - If that prints no messages and throws no exceptions, then everything worked fine.; - If that throws no exception, but prints a message, then verification failed. Print the verification failure message and throw an exception.; - If that throws an exception, then it is likely that auto-computing stack frames failed, so let's try to verify again without the stack frames; - If that prints no messages, then something weird happened, rethrow the original verification exception.; - If that prints a message, then verification failed. Print the verification failure message and throw an exception.; - I've never seen the max-only verifier throw an exception. I also added a bit that would print out the byte code if given a `PrintWriter` to print to. This functionality is currently unused, but was useful for debugging.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2330
https://github.com/hail-is/hail/pull/2330:621,Integrability,message,message,621,"Better error handling:. Attempt to verify correctness of byte code using automatically computed maxes and stack frames. ; - If that prints no messages and throws no exceptions, then everything worked fine.; - If that throws no exception, but prints a message, then verification failed. Print the verification failure message and throw an exception.; - If that throws an exception, then it is likely that auto-computing stack frames failed, so let's try to verify again without the stack frames; - If that prints no messages, then something weird happened, rethrow the original verification exception.; - If that prints a message, then verification failed. Print the verification failure message and throw an exception.; - I've never seen the max-only verifier throw an exception. I also added a bit that would print out the byte code if given a `PrintWriter` to print to. This functionality is currently unused, but was useful for debugging.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2330
https://github.com/hail-is/hail/pull/2330:687,Integrability,message,message,687,"Better error handling:. Attempt to verify correctness of byte code using automatically computed maxes and stack frames. ; - If that prints no messages and throws no exceptions, then everything worked fine.; - If that throws no exception, but prints a message, then verification failed. Print the verification failure message and throw an exception.; - If that throws an exception, then it is likely that auto-computing stack frames failed, so let's try to verify again without the stack frames; - If that prints no messages, then something weird happened, rethrow the original verification exception.; - If that prints a message, then verification failed. Print the verification failure message and throw an exception.; - I've never seen the max-only verifier throw an exception. I also added a bit that would print out the byte code if given a `PrintWriter` to print to. This functionality is currently unused, but was useful for debugging.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2330
https://github.com/hail-is/hail/pull/2347:14,Performance,load,loadLength,14,"- `TContainer.loadLength` is now on the object so creating byte code to; read the length does not require knowledge of the element type. - in `elementsOffset`, use the non-Code pattern of delegating to; `roundUpAlignment`. - add staged `TContainer.elementOffset`, `TContainer.loadElement`,; `TStruct.fieldOffset`, `UnsafeUtils.roundUpAlignment`. - fix bug: loadElement reads addresses using `loadAddress` instead of; `loadInt`. - add several `loadX` methods to `RichCodeMemoryBuffer`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2347
https://github.com/hail-is/hail/pull/2347:276,Performance,load,loadElement,276,"- `TContainer.loadLength` is now on the object so creating byte code to; read the length does not require knowledge of the element type. - in `elementsOffset`, use the non-Code pattern of delegating to; `roundUpAlignment`. - add staged `TContainer.elementOffset`, `TContainer.loadElement`,; `TStruct.fieldOffset`, `UnsafeUtils.roundUpAlignment`. - fix bug: loadElement reads addresses using `loadAddress` instead of; `loadInt`. - add several `loadX` methods to `RichCodeMemoryBuffer`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2347
https://github.com/hail-is/hail/pull/2347:357,Performance,load,loadElement,357,"- `TContainer.loadLength` is now on the object so creating byte code to; read the length does not require knowledge of the element type. - in `elementsOffset`, use the non-Code pattern of delegating to; `roundUpAlignment`. - add staged `TContainer.elementOffset`, `TContainer.loadElement`,; `TStruct.fieldOffset`, `UnsafeUtils.roundUpAlignment`. - fix bug: loadElement reads addresses using `loadAddress` instead of; `loadInt`. - add several `loadX` methods to `RichCodeMemoryBuffer`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2347
https://github.com/hail-is/hail/pull/2347:392,Performance,load,loadAddress,392,"- `TContainer.loadLength` is now on the object so creating byte code to; read the length does not require knowledge of the element type. - in `elementsOffset`, use the non-Code pattern of delegating to; `roundUpAlignment`. - add staged `TContainer.elementOffset`, `TContainer.loadElement`,; `TStruct.fieldOffset`, `UnsafeUtils.roundUpAlignment`. - fix bug: loadElement reads addresses using `loadAddress` instead of; `loadInt`. - add several `loadX` methods to `RichCodeMemoryBuffer`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2347
https://github.com/hail-is/hail/pull/2347:418,Performance,load,loadInt,418,"- `TContainer.loadLength` is now on the object so creating byte code to; read the length does not require knowledge of the element type. - in `elementsOffset`, use the non-Code pattern of delegating to; `roundUpAlignment`. - add staged `TContainer.elementOffset`, `TContainer.loadElement`,; `TStruct.fieldOffset`, `UnsafeUtils.roundUpAlignment`. - fix bug: loadElement reads addresses using `loadAddress` instead of; `loadInt`. - add several `loadX` methods to `RichCodeMemoryBuffer`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2347
https://github.com/hail-is/hail/pull/2347:443,Performance,load,loadX,443,"- `TContainer.loadLength` is now on the object so creating byte code to; read the length does not require knowledge of the element type. - in `elementsOffset`, use the non-Code pattern of delegating to; `roundUpAlignment`. - add staged `TContainer.elementOffset`, `TContainer.loadElement`,; `TStruct.fieldOffset`, `UnsafeUtils.roundUpAlignment`. - fix bug: loadElement reads addresses using `loadAddress` instead of; `loadInt`. - add several `loadX` methods to `RichCodeMemoryBuffer`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2347
https://github.com/hail-is/hail/pull/2347:315,Safety,Unsafe,UnsafeUtils,315,"- `TContainer.loadLength` is now on the object so creating byte code to; read the length does not require knowledge of the element type. - in `elementsOffset`, use the non-Code pattern of delegating to; `roundUpAlignment`. - add staged `TContainer.elementOffset`, `TContainer.loadElement`,; `TStruct.fieldOffset`, `UnsafeUtils.roundUpAlignment`. - fix bug: loadElement reads addresses using `loadAddress` instead of; `loadInt`. - add several `loadX` methods to `RichCodeMemoryBuffer`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2347
https://github.com/hail-is/hail/pull/2349:32,Availability,error,error,32,"Somehow, this didn't trigger an error in master but did in 0.1. Nonetheless, removing unnecessary data seems good.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2349
https://github.com/hail-is/hail/pull/2358:18,Testability,log,logic,18,Simplify the read logic by no longer reading multiple files at once. This functionality is preserved in the newly-added union function.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2358
https://github.com/hail-is/hail/pull/2358:0,Usability,Simpl,Simplify,0,Simplify the read logic by no longer reading multiple files at once. This functionality is preserved in the newly-added union function.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2358
https://github.com/hail-is/hail/pull/2359:242,Security,expose,exposed,242,"Generic: computation of split values is done in expression language.; Support HTS-like split of generic schema.; Removed fakeRef, which I don't think anyone ever used. If anyone wants it, can be added through expression language once that is exposed. @tpoterba FYI I removed fakeRef.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2359
https://github.com/hail-is/hail/pull/2365:22,Performance,perform,performance,22,Dramatically improves performance of PC Relate. Some prior discussion at #2280 .,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2365
https://github.com/hail-is/hail/pull/2366:205,Safety,avoid,avoid,205,"The native libraries sometimes play tricks to squeeze out better; precision if the memory layout is amenable to it. This causes some; of our tests to fail, particularly those related to transposition.; We avoid extreme values which should keep us from encountering; situations where naive arithmetic produces Infinity.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2366
https://github.com/hail-is/hail/pull/2366:141,Testability,test,tests,141,"The native libraries sometimes play tricks to squeeze out better; precision if the memory layout is amenable to it. This causes some; of our tests to fail, particularly those related to transposition.; We avoid extreme values which should keep us from encountering; situations where naive arithmetic produces Infinity.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2366
https://github.com/hail-is/hail/pull/2367:931,Performance,cache,cached,931,"@cseed I've implemented `readPartitions` on `HailContext` and `writePartitions` on `RichRDD`, re-implemented read/write of blocks on `BlockMatrix` to preserve partitioning using these, and moved MatrixValue and KeyTable row read/write to use these. readPartitions and writePartitions use `unsafeReadPartition` and `writePartition` in RichHadoopConfig. `readRow` includes `close()`, and I've added `close()` to `readBlock` as well. At first I used `using` (see `readPartition` which is remarked out), but this closes the resource before the iterator iterated in the readRows case. Yet somehow, `testWriteRead` in VSMSuite passes even with `readPartition`. This test just creates a random VDS, writes, reads, compares. I added `testWriteRead2` which imports `sample.vcf`, writes, reads, compares. And indeed, the latter fails with `readPartition`. I have no explanation for why the behavior would differ (could the former be somehow cached?). I'm not happy with the asymmetry or lake of read safety if closing isn't propagated backward (I think it is for DataInputStream), but would like to get your feedback before messing around with these functions further. I've left the filename that had ReadRowsRDD with that name so it's easier to compare, but will change prior to merging since that class is gone. No class jumps out, should I just use the first one, `ArrayInputStream`, or a name that's not a class?. `ReadRDDPartition` in file ReadRowsRDDs and `IntPartition` in `BlockMatrix` have the same definition (the latter is used for more than just reading). I'm thinking of just having IntPartition located somewhere other than BlockMatrix. Any thoughts on where?. I added `sqrt` on blockSize in `blockMatrixGen` in order to get more cases with `blockSize < min(rows, cols)`. Only 1 of 10 had this property in `readWriteIdentityRandom` and its an important case to check with respect to the GridPartitioner dealing with block indices corrected (particularly with transpose in the mix).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2367
https://github.com/hail-is/hail/pull/2367:289,Safety,unsafe,unsafeReadPartition,289,"@cseed I've implemented `readPartitions` on `HailContext` and `writePartitions` on `RichRDD`, re-implemented read/write of blocks on `BlockMatrix` to preserve partitioning using these, and moved MatrixValue and KeyTable row read/write to use these. readPartitions and writePartitions use `unsafeReadPartition` and `writePartition` in RichHadoopConfig. `readRow` includes `close()`, and I've added `close()` to `readBlock` as well. At first I used `using` (see `readPartition` which is remarked out), but this closes the resource before the iterator iterated in the readRows case. Yet somehow, `testWriteRead` in VSMSuite passes even with `readPartition`. This test just creates a random VDS, writes, reads, compares. I added `testWriteRead2` which imports `sample.vcf`, writes, reads, compares. And indeed, the latter fails with `readPartition`. I have no explanation for why the behavior would differ (could the former be somehow cached?). I'm not happy with the asymmetry or lake of read safety if closing isn't propagated backward (I think it is for DataInputStream), but would like to get your feedback before messing around with these functions further. I've left the filename that had ReadRowsRDD with that name so it's easier to compare, but will change prior to merging since that class is gone. No class jumps out, should I just use the first one, `ArrayInputStream`, or a name that's not a class?. `ReadRDDPartition` in file ReadRowsRDDs and `IntPartition` in `BlockMatrix` have the same definition (the latter is used for more than just reading). I'm thinking of just having IntPartition located somewhere other than BlockMatrix. Any thoughts on where?. I added `sqrt` on blockSize in `blockMatrixGen` in order to get more cases with `blockSize < min(rows, cols)`. Only 1 of 10 had this property in `readWriteIdentityRandom` and its an important case to check with respect to the GridPartitioner dealing with block indices corrected (particularly with transpose in the mix).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2367
https://github.com/hail-is/hail/pull/2367:990,Safety,safe,safety,990,"@cseed I've implemented `readPartitions` on `HailContext` and `writePartitions` on `RichRDD`, re-implemented read/write of blocks on `BlockMatrix` to preserve partitioning using these, and moved MatrixValue and KeyTable row read/write to use these. readPartitions and writePartitions use `unsafeReadPartition` and `writePartition` in RichHadoopConfig. `readRow` includes `close()`, and I've added `close()` to `readBlock` as well. At first I used `using` (see `readPartition` which is remarked out), but this closes the resource before the iterator iterated in the readRows case. Yet somehow, `testWriteRead` in VSMSuite passes even with `readPartition`. This test just creates a random VDS, writes, reads, compares. I added `testWriteRead2` which imports `sample.vcf`, writes, reads, compares. And indeed, the latter fails with `readPartition`. I have no explanation for why the behavior would differ (could the former be somehow cached?). I'm not happy with the asymmetry or lake of read safety if closing isn't propagated backward (I think it is for DataInputStream), but would like to get your feedback before messing around with these functions further. I've left the filename that had ReadRowsRDD with that name so it's easier to compare, but will change prior to merging since that class is gone. No class jumps out, should I just use the first one, `ArrayInputStream`, or a name that's not a class?. `ReadRDDPartition` in file ReadRowsRDDs and `IntPartition` in `BlockMatrix` have the same definition (the latter is used for more than just reading). I'm thinking of just having IntPartition located somewhere other than BlockMatrix. Any thoughts on where?. I added `sqrt` on blockSize in `blockMatrixGen` in order to get more cases with `blockSize < min(rows, cols)`. Only 1 of 10 had this property in `readWriteIdentityRandom` and its an important case to check with respect to the GridPartitioner dealing with block indices corrected (particularly with transpose in the mix).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2367
https://github.com/hail-is/hail/pull/2367:594,Testability,test,testWriteRead,594,"@cseed I've implemented `readPartitions` on `HailContext` and `writePartitions` on `RichRDD`, re-implemented read/write of blocks on `BlockMatrix` to preserve partitioning using these, and moved MatrixValue and KeyTable row read/write to use these. readPartitions and writePartitions use `unsafeReadPartition` and `writePartition` in RichHadoopConfig. `readRow` includes `close()`, and I've added `close()` to `readBlock` as well. At first I used `using` (see `readPartition` which is remarked out), but this closes the resource before the iterator iterated in the readRows case. Yet somehow, `testWriteRead` in VSMSuite passes even with `readPartition`. This test just creates a random VDS, writes, reads, compares. I added `testWriteRead2` which imports `sample.vcf`, writes, reads, compares. And indeed, the latter fails with `readPartition`. I have no explanation for why the behavior would differ (could the former be somehow cached?). I'm not happy with the asymmetry or lake of read safety if closing isn't propagated backward (I think it is for DataInputStream), but would like to get your feedback before messing around with these functions further. I've left the filename that had ReadRowsRDD with that name so it's easier to compare, but will change prior to merging since that class is gone. No class jumps out, should I just use the first one, `ArrayInputStream`, or a name that's not a class?. `ReadRDDPartition` in file ReadRowsRDDs and `IntPartition` in `BlockMatrix` have the same definition (the latter is used for more than just reading). I'm thinking of just having IntPartition located somewhere other than BlockMatrix. Any thoughts on where?. I added `sqrt` on blockSize in `blockMatrixGen` in order to get more cases with `blockSize < min(rows, cols)`. Only 1 of 10 had this property in `readWriteIdentityRandom` and its an important case to check with respect to the GridPartitioner dealing with block indices corrected (particularly with transpose in the mix).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2367
https://github.com/hail-is/hail/pull/2367:660,Testability,test,test,660,"@cseed I've implemented `readPartitions` on `HailContext` and `writePartitions` on `RichRDD`, re-implemented read/write of blocks on `BlockMatrix` to preserve partitioning using these, and moved MatrixValue and KeyTable row read/write to use these. readPartitions and writePartitions use `unsafeReadPartition` and `writePartition` in RichHadoopConfig. `readRow` includes `close()`, and I've added `close()` to `readBlock` as well. At first I used `using` (see `readPartition` which is remarked out), but this closes the resource before the iterator iterated in the readRows case. Yet somehow, `testWriteRead` in VSMSuite passes even with `readPartition`. This test just creates a random VDS, writes, reads, compares. I added `testWriteRead2` which imports `sample.vcf`, writes, reads, compares. And indeed, the latter fails with `readPartition`. I have no explanation for why the behavior would differ (could the former be somehow cached?). I'm not happy with the asymmetry or lake of read safety if closing isn't propagated backward (I think it is for DataInputStream), but would like to get your feedback before messing around with these functions further. I've left the filename that had ReadRowsRDD with that name so it's easier to compare, but will change prior to merging since that class is gone. No class jumps out, should I just use the first one, `ArrayInputStream`, or a name that's not a class?. `ReadRDDPartition` in file ReadRowsRDDs and `IntPartition` in `BlockMatrix` have the same definition (the latter is used for more than just reading). I'm thinking of just having IntPartition located somewhere other than BlockMatrix. Any thoughts on where?. I added `sqrt` on blockSize in `blockMatrixGen` in order to get more cases with `blockSize < min(rows, cols)`. Only 1 of 10 had this property in `readWriteIdentityRandom` and its an important case to check with respect to the GridPartitioner dealing with block indices corrected (particularly with transpose in the mix).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2367
https://github.com/hail-is/hail/pull/2367:1098,Usability,feedback,feedback,1098,"@cseed I've implemented `readPartitions` on `HailContext` and `writePartitions` on `RichRDD`, re-implemented read/write of blocks on `BlockMatrix` to preserve partitioning using these, and moved MatrixValue and KeyTable row read/write to use these. readPartitions and writePartitions use `unsafeReadPartition` and `writePartition` in RichHadoopConfig. `readRow` includes `close()`, and I've added `close()` to `readBlock` as well. At first I used `using` (see `readPartition` which is remarked out), but this closes the resource before the iterator iterated in the readRows case. Yet somehow, `testWriteRead` in VSMSuite passes even with `readPartition`. This test just creates a random VDS, writes, reads, compares. I added `testWriteRead2` which imports `sample.vcf`, writes, reads, compares. And indeed, the latter fails with `readPartition`. I have no explanation for why the behavior would differ (could the former be somehow cached?). I'm not happy with the asymmetry or lake of read safety if closing isn't propagated backward (I think it is for DataInputStream), but would like to get your feedback before messing around with these functions further. I've left the filename that had ReadRowsRDD with that name so it's easier to compare, but will change prior to merging since that class is gone. No class jumps out, should I just use the first one, `ArrayInputStream`, or a name that's not a class?. `ReadRDDPartition` in file ReadRowsRDDs and `IntPartition` in `BlockMatrix` have the same definition (the latter is used for more than just reading). I'm thinking of just having IntPartition located somewhere other than BlockMatrix. Any thoughts on where?. I added `sqrt` on blockSize in `blockMatrixGen` in order to get more cases with `blockSize < min(rows, cols)`. Only 1 of 10 had this property in `readWriteIdentityRandom` and its an important case to check with respect to the GridPartitioner dealing with block indices corrected (particularly with transpose in the mix).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2367
https://github.com/hail-is/hail/pull/2370:10,Performance,cache,caches,10,* all the caches!. * two caches and a fix. * space between methods. * cache comments,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2370
https://github.com/hail-is/hail/pull/2370:25,Performance,cache,caches,25,* all the caches!. * two caches and a fix. * space between methods. * cache comments,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2370
https://github.com/hail-is/hail/pull/2370:70,Performance,cache,cache,70,* all the caches!. * two caches and a fix. * space between methods. * cache comments,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2370
https://github.com/hail-is/hail/pull/2381:98,Integrability,rout,routine,98,"Copy new architecture of `split_multi`.; Added `left_aligned` option, removed `max_shift`.; Moved routine to VariantSampleMatrix, require row key is `Variant`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2381
https://github.com/hail-is/hail/pull/2382:14,Integrability,interface,interface,14,Implement old interface using expression language.; New interface is not yet exposed in Python (like split_multi). This will come in a later PR. This should use SplitMulti.unionMovedVariants from https://github.com/hail-is/hail/pull/2381. I will make this change as a separate PR after they both go in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2382
https://github.com/hail-is/hail/pull/2382:56,Integrability,interface,interface,56,Implement old interface using expression language.; New interface is not yet exposed in Python (like split_multi). This will come in a later PR. This should use SplitMulti.unionMovedVariants from https://github.com/hail-is/hail/pull/2381. I will make this change as a separate PR after they both go in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2382
https://github.com/hail-is/hail/pull/2382:77,Security,expose,exposed,77,Implement old interface using expression language.; New interface is not yet exposed in Python (like split_multi). This will come in a later PR. This should use SplitMulti.unionMovedVariants from https://github.com/hail-is/hail/pull/2381. I will make this change as a separate PR after they both go in.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2382
https://github.com/hail-is/hail/pull/2396:49,Performance,load,load,49,This encapsulates the type dispatch necessary to load and store; annotations.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2396
https://github.com/hail-is/hail/issues/2407:458,Availability,error,error,458,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.1-6e815ac. ### What you did: hc.import_bgen('*.bgen) X chromosome cannot be imported, which is a major issue when working on phenotypes linked to blood coagulation, for example. ### What went wrong (all error messages here, including the full java stack trace):. [Stage 1:===============================================> (747 + 9) / 871]Traceback (most recent call last):; File ""regression1.py"", line 22, in <module>; hc.import_bgen('/mnt/volume/imputed_genotypes/*.bgen', sample_file='/mnt/volume/imputed_genotypes/MT.sample').split_multi().write('/mnt/volume/imputed_genotypes/MT_intersect_imputed.vds'); File ""<decorator-gen-285>"", line 2, in write; File ""/usr/local/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycomput",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:979,Availability,Error,Error,979,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.1-6e815ac. ### What you did: hc.import_bgen('*.bgen) X chromosome cannot be imported, which is a major issue when working on phenotypes linked to blood coagulation, for example. ### What went wrong (all error messages here, including the full java stack trace):. [Stage 1:===============================================> (747 + 9) / 871]Traceback (most recent call last):; File ""regression1.py"", line 22, in <module>; hc.import_bgen('/mnt/volume/imputed_genotypes/*.bgen', sample_file='/mnt/volume/imputed_genotypes/MT.sample').split_multi().write('/mnt/volume/imputed_genotypes/MT_intersect_imputed.vds'); File ""<decorator-gen-285>"", line 2, in write; File ""/usr/local/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycomput",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:4128,Availability,failure,failure,4128,"cala:198); at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:494); at is.hail.variant.VariantDatasetFunctions$.write$extension(VariantDataset.scala:751); at is.hail.variant.VariantDatasetFunctions.write(VariantDataset.scala:721); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:4187,Availability,failure,failure,4187,"taFrameWriter.scala:494); at is.hail.variant.VariantDatasetFunctions$.write$extension(VariantDataset.scala:751); at is.hail.variant.VariantDatasetFunctions.write(VariantDataset.scala:721); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.H",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:5309,Availability,Error,ErrorHandling,5309,ting rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:202); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:195); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:5335,Availability,Error,ErrorHandling,5335,ache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:202); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:195); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:12518,Availability,Error,ErrorHandling,12518,led while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:202); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:195); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:12544,Availability,Error,ErrorHandling,12544,ws; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:202); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:195); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:14989,Availability,Error,Error,14989,"(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-6e815ac; Error summary: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; Unhandled exception in thread started by <bound method Thread.__bootstrap of <Thread(Thread-1, stopped daemon 140486823679744)>>; Traceback (most recent call last):; File ""/usr/lib/python2.7/threading.py"", line 783, in __bootstrap; self.__bootstrap_inner(); File ""/usr/lib/python2.7/threading.py"", line 823, in __bootstrap_inner; (self.name, _format_exc()))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:4768,Energy Efficiency,schedul,scheduler,4768,"ectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:44",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:4839,Energy Efficiency,schedul,scheduler,4839,"4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:7143,Energy Efficiency,schedul,scheduler,7143,:195); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:7183,Energy Efficiency,schedul,scheduler,7183,scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:7281,Energy Efficiency,schedul,scheduler,7281,scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:7378,Energy Efficiency,schedul,scheduler,7378,tasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:7629,Energy Efficiency,schedul,scheduler,7629,tWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:7709,Energy Efficiency,schedul,scheduler,7709,er$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.sql.execution.datasources.FileFormat,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:7814,Energy Efficiency,schedul,scheduler,7814,iter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127); at org.apache.spark.sql.execution.datas,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:7962,Energy Efficiency,schedul,scheduler,7962,ces.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:8050,Energy Efficiency,schedul,scheduler,8050,teTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.SQLExecutio,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:8147,Energy Efficiency,schedul,scheduler,8147,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.FileF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:8242,Energy Efficiency,schedul,scheduler,8242,.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.I,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:8405,Energy Efficiency,schedul,scheduler,8405,abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$l,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:11990,Energy Efficiency,schedul,scheduler,11990,gMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:12061,Energy Efficiency,schedul,scheduler,12061,.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.spark,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:14572,Energy Efficiency,schedul,scheduler,14572,"(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-6e815ac; Error summary: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; Unhandled exception in thread started by <bound method Thread.__bootstrap of <Thread(Thread-1, stopped daemon 140486823679744)>>; Traceback (most recent call last):; File ""/usr/lib/python2.7/threading.py"", line 783, in __bootstrap; self.__bootstrap_inner(); File ""/usr/lib/python2.7/threading.py"", line 823, in __bootstrap_inner; (self.name, _format_exc()))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:14643,Energy Efficiency,schedul,scheduler,14643,"(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-6e815ac; Error summary: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; Unhandled exception in thread started by <bound method Thread.__bootstrap of <Thread(Thread-1, stopped daemon 140486823679744)>>; Traceback (most recent call last):; File ""/usr/lib/python2.7/threading.py"", line 783, in __bootstrap; self.__bootstrap_inner(); File ""/usr/lib/python2.7/threading.py"", line 823, in __bootstrap_inner; (self.name, _format_exc()))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:464,Integrability,message,messages,464,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.1-6e815ac. ### What you did: hc.import_bgen('*.bgen) X chromosome cannot be imported, which is a major issue when working on phenotypes linked to blood coagulation, for example. ### What went wrong (all error messages here, including the full java stack trace):. [Stage 1:===============================================> (747 + 9) / 871]Traceback (most recent call last):; File ""regression1.py"", line 22, in <module>; hc.import_bgen('/mnt/volume/imputed_genotypes/*.bgen', sample_file='/mnt/volume/imputed_genotypes/MT.sample').split_multi().write('/mnt/volume/imputed_genotypes/MT_intersect_imputed.vds'); File ""<decorator-gen-285>"", line 2, in write; File ""/usr/local/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycomput",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:4961,Performance,concurren,concurrent,4961,"79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:5045,Performance,concurren,concurrent,5045,"run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(Ordere",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:12183,Performance,concurren,concurrent,12183,flectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:12267,Performance,concurren,concurrent,12267,.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.io.bgen.BgenRecordV12.getValue(BgenRecord.scala:203); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:76); at is.hail.io.bgen.BgenLoader$$anonfun$10$$anonfun$apply$5.apply(BgenLoader.scala:75); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:241); at is.hail.sparkextras.OrderedRDD$$anon$3.next(OrderedRDD.scala:234); at is.hail.sparkextras.OrderedRDD$$anonfun$apply$8$$anon$2.next(OrderedRDD.scala:20,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:14765,Performance,concurren,concurrent,14765,"(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-6e815ac; Error summary: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; Unhandled exception in thread started by <bound method Thread.__bootstrap of <Thread(Thread-1, stopped daemon 140486823679744)>>; Traceback (most recent call last):; File ""/usr/lib/python2.7/threading.py"", line 783, in __bootstrap; self.__bootstrap_inner(); File ""/usr/lib/python2.7/threading.py"", line 823, in __bootstrap_inner; (self.name, _format_exc()))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:14849,Performance,concurren,concurrent,14849,"(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748). Hail version: 0.1-6e815ac; Error summary: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'.; Unhandled exception in thread started by <bound method Thread.__bootstrap of <Thread(Thread-1, stopped daemon 140486823679744)>>; Traceback (most recent call last):; File ""/usr/lib/python2.7/threading.py"", line 783, in __bootstrap; self.__bootstrap_inner(); File ""/usr/lib/python2.7/threading.py"", line 823, in __bootstrap_inner; (self.name, _format_exc()))",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:1231,Safety,abort,aborted,1231,"--------. ### Hail version: 0.1-6e815ac. ### What you did: hc.import_bgen('*.bgen) X chromosome cannot be imported, which is a major issue when working on phenotypes linked to blood coagulation, for example. ### What went wrong (all error messages here, including the full java stack trace):. [Stage 1:===============================================> (747 + 9) / 871]Traceback (most recent call last):; File ""regression1.py"", line 22, in <module>; hc.import_bgen('/mnt/volume/imputed_genotypes/*.bgen', sample_file='/mnt/volume/imputed_genotypes/MT.sample').split_multi().write('/mnt/volume/imputed_genotypes/MT_intersect_imputed.vds'); File ""<decorator-gen-285>"", line 2, in write; File ""/usr/local/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: HailException: Hail only supports diploid genotypes. Found min ploidy equals `1' and max ploidy equals `2'. Java stack trace:; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121); at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58); at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56); at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74); at org.apac",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:4107,Safety,abort,aborted,4107,"cala:198); at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:494); at is.hail.variant.VariantDatasetFunctions$.write$extension(VariantDataset.scala:751); at is.hail.variant.VariantDatasetFunctions.write(VariantDataset.scala:721); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Job aborted due to stage failure: Task 754 in stage 1.0 failed 1 times, most recent failure: Lost task 754.0 in stage 1.0 (TID 1625, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:204); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:129); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$3.apply(FileFormatWriter.scala:128); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at jav",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:7313,Safety,abort,abortStage,7313,tor$$anon$11.next(Iterator.scala:409); at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:7410,Safety,abort,abortStage,7410,iter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:244); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:190); at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/issues/2407:7652,Safety,abort,abortStage,7652,che.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:188); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341); at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:193); ... 8 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2407
https://github.com/hail-is/hail/pull/2408:69,Integrability,interface,interface,69,I chose to use an Iterator that stasifies the spirit of the Iterator interface rather than special casing an iterator that does no copies,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2408
https://github.com/hail-is/hail/pull/2418:133,Energy Efficiency,energy,energy,133,I suspect we can eliminate any unnecessary overhead by staging this. I figured I should get feedback on it first before I expend the energy on that. I also include `VariantView` and `AltAlleleView` as examples of using `StructView` to create succinct views.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2418
https://github.com/hail-is/hail/pull/2418:92,Usability,feedback,feedback,92,I suspect we can eliminate any unnecessary overhead by staging this. I figured I should get feedback on it first before I expend the energy on that. I also include `VariantView` and `AltAlleleView` as examples of using `StructView` to create succinct views.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2418
https://github.com/hail-is/hail/pull/2419:238,Energy Efficiency,reduce,reduces,238,I would appreciate a review from anyone who has time. First of two (probably) for fast VCF parser. Main functionality still goes through HTSJDK. Next one will handle the genotypes. Signature for parseLines is a bit nuts but it definitely reduces the code size. @danking calling `hasNext` on my iterator invalidates it. I think this is inevitable and we should embrace it. Seems to work fine. Warn when filtering alleles due to invalid REF or symbolic alts. Much better than dropping data silently. Added `clear` to `ArrayStack` and `RegionValueBuilder`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2419
https://github.com/hail-is/hail/pull/2419:505,Usability,clear,clear,505,I would appreciate a review from anyone who has time. First of two (probably) for fast VCF parser. Main functionality still goes through HTSJDK. Next one will handle the genotypes. Signature for parseLines is a bit nuts but it definitely reduces the code size. @danking calling `hasNext` on my iterator invalidates it. I think this is inevitable and we should embrace it. Seems to work fine. Warn when filtering alleles due to invalid REF or symbolic alts. Much better than dropping data silently. Added `clear` to `ArrayStack` and `RegionValueBuilder`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2419
https://github.com/hail-is/hail/issues/2420:409,Availability,error,error,409,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.1. ### What you did: . kt = hc.import_table([""gnomad_coverage_chr1.tsv"", ""gnomad_coverage_chr2.tsv"", ..], min_partitions=10000). ### What went wrong (all error messages here, including the full java stack trace):. I expected the behavior with min_partitions=10000 to be the same as calling repartition(10000), but the keytable only had # of partitions = # of input .tsv files. I switched to just calling repartition(10000) after importing, and it works as expected. . May be duplicate of #508",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2420
https://github.com/hail-is/hail/issues/2420:415,Integrability,message,messages,415,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.1. ### What you did: . kt = hc.import_table([""gnomad_coverage_chr1.tsv"", ""gnomad_coverage_chr2.tsv"", ..], min_partitions=10000). ### What went wrong (all error messages here, including the full java stack trace):. I expected the behavior with min_partitions=10000 to be the same as calling repartition(10000), but the keytable only had # of partitions = # of input .tsv files. I switched to just calling repartition(10000) after importing, and it works as expected. . May be duplicate of #508",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2420
https://github.com/hail-is/hail/pull/2424:53,Safety,unsafe,unsafeInserter,53,@danking I think this is what you meant. I looked at unsafeInserter also but I don't see where/if it tries to expand missing structs?,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2424
https://github.com/hail-is/hail/pull/2426:82,Performance,Load,LoadVCF,82,"Intern types rather than recording singletons.; Remove canonical field stuff from LoadVCF (unused), move to LoadGDB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2426
https://github.com/hail-is/hail/pull/2426:108,Performance,Load,LoadGDB,108,"Intern types rather than recording singletons.; Remove canonical field stuff from LoadVCF (unused), move to LoadGDB.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2426
https://github.com/hail-is/hail/pull/2430:12,Modifiability,variab,variable-length,12,"Don't split variable-length encoded ints across compression blocks. Save a comparison in the inner loop. As usual, has seemingly negligible effect on performance. I'm sure it would be significant on the C side. I don't encode as signed yet. It makes the termination condition more complicated and I want to think on it a bit more.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2430
https://github.com/hail-is/hail/pull/2430:150,Performance,perform,performance,150,"Don't split variable-length encoded ints across compression blocks. Save a comparison in the inner loop. As usual, has seemingly negligible effect on performance. I'm sure it would be significant on the C side. I don't encode as signed yet. It makes the termination condition more complicated and I want to think on it a bit more.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2430
https://github.com/hail-is/hail/pull/2432:613,Integrability,interface,interface,613,"doesn't include the ability to use user-provided expr to generate double matrixes for PCA. @cseed @tpoterba I think you had feelings on whether or not PCA should keep the current behavior where it returns an annotated vsm. . Currently, I've kept VariantSampleMatrix.pca as something that annotates and returns a vsm (using annotateVariantsTable for loadings), and then there's another method (VariantSampleMatrix.pcaResults) that just returns a tuple of (scores, loadings, eigenvalues). I could refactor them such that pca() => pcaAsAnnotations() (or something) and pcaResults() => pca(), and updating the python interface to match?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2432
https://github.com/hail-is/hail/pull/2432:495,Modifiability,refactor,refactor,495,"doesn't include the ability to use user-provided expr to generate double matrixes for PCA. @cseed @tpoterba I think you had feelings on whether or not PCA should keep the current behavior where it returns an annotated vsm. . Currently, I've kept VariantSampleMatrix.pca as something that annotates and returns a vsm (using annotateVariantsTable for loadings), and then there's another method (VariantSampleMatrix.pcaResults) that just returns a tuple of (scores, loadings, eigenvalues). I could refactor them such that pca() => pcaAsAnnotations() (or something) and pcaResults() => pca(), and updating the python interface to match?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2432
https://github.com/hail-is/hail/pull/2432:349,Performance,load,loadings,349,"doesn't include the ability to use user-provided expr to generate double matrixes for PCA. @cseed @tpoterba I think you had feelings on whether or not PCA should keep the current behavior where it returns an annotated vsm. . Currently, I've kept VariantSampleMatrix.pca as something that annotates and returns a vsm (using annotateVariantsTable for loadings), and then there's another method (VariantSampleMatrix.pcaResults) that just returns a tuple of (scores, loadings, eigenvalues). I could refactor them such that pca() => pcaAsAnnotations() (or something) and pcaResults() => pca(), and updating the python interface to match?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2432
https://github.com/hail-is/hail/pull/2432:463,Performance,load,loadings,463,"doesn't include the ability to use user-provided expr to generate double matrixes for PCA. @cseed @tpoterba I think you had feelings on whether or not PCA should keep the current behavior where it returns an annotated vsm. . Currently, I've kept VariantSampleMatrix.pca as something that annotates and returns a vsm (using annotateVariantsTable for loadings), and then there's another method (VariantSampleMatrix.pcaResults) that just returns a tuple of (scores, loadings, eigenvalues). I could refactor them such that pca() => pcaAsAnnotations() (or something) and pcaResults() => pca(), and updating the python interface to match?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2432
https://github.com/hail-is/hail/pull/2438:208,Deployability,update,update,208,"- added reference and contig names/lengths to header in export_vcf; - added line in docs; - added a test; - renamed ExportVcfSuite to ExportVCFSuite to match classname and ImportVCFSuite. @jigold is going to update GenomeReference to capture MD5, at which point I'll make a PR to export other metadata.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2438
https://github.com/hail-is/hail/pull/2438:100,Testability,test,test,100,"- added reference and contig names/lengths to header in export_vcf; - added line in docs; - added a test; - renamed ExportVcfSuite to ExportVCFSuite to match classname and ImportVCFSuite. @jigold is going to update GenomeReference to capture MD5, at which point I'll make a PR to export other metadata.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2438
https://github.com/hail-is/hail/pull/2440:751,Integrability,interface,interface,751,"I believe I addressed all the comments from #2431. The one thing I couldn't quite figure out is that you suggested testing using a randomly generated vsm and then calling make_table().export() (which in Scala, best as I can figure, is makeKT().export(f)). This does some things that makes re-importing using LoadMatrix non-identical:; - the exported header includes a header entry for the row ID column also, which the data we were looking at before didn't (n entries in the first line, n + 1 entries in subsequent lines), which seems like a reasonable thing to want to deal with---I added a flag in LoadMatrix called hasRowKeyLabel which drops the first item of the header line if that's the case, although it's not exposed to HailContext/the Python interface.; - the exported sample IDs somehow get "".g""s tacked on the ends (presumably because they came from the ""g"" struct), so the sample IDs would never match.; I ended up writing my own export function for the test (since I assume we don't really want to be exporting to this format IRL)---please let me know if I should handle that differently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2440
https://github.com/hail-is/hail/pull/2440:308,Performance,Load,LoadMatrix,308,"I believe I addressed all the comments from #2431. The one thing I couldn't quite figure out is that you suggested testing using a randomly generated vsm and then calling make_table().export() (which in Scala, best as I can figure, is makeKT().export(f)). This does some things that makes re-importing using LoadMatrix non-identical:; - the exported header includes a header entry for the row ID column also, which the data we were looking at before didn't (n entries in the first line, n + 1 entries in subsequent lines), which seems like a reasonable thing to want to deal with---I added a flag in LoadMatrix called hasRowKeyLabel which drops the first item of the header line if that's the case, although it's not exposed to HailContext/the Python interface.; - the exported sample IDs somehow get "".g""s tacked on the ends (presumably because they came from the ""g"" struct), so the sample IDs would never match.; I ended up writing my own export function for the test (since I assume we don't really want to be exporting to this format IRL)---please let me know if I should handle that differently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2440
https://github.com/hail-is/hail/pull/2440:600,Performance,Load,LoadMatrix,600,"I believe I addressed all the comments from #2431. The one thing I couldn't quite figure out is that you suggested testing using a randomly generated vsm and then calling make_table().export() (which in Scala, best as I can figure, is makeKT().export(f)). This does some things that makes re-importing using LoadMatrix non-identical:; - the exported header includes a header entry for the row ID column also, which the data we were looking at before didn't (n entries in the first line, n + 1 entries in subsequent lines), which seems like a reasonable thing to want to deal with---I added a flag in LoadMatrix called hasRowKeyLabel which drops the first item of the header line if that's the case, although it's not exposed to HailContext/the Python interface.; - the exported sample IDs somehow get "".g""s tacked on the ends (presumably because they came from the ""g"" struct), so the sample IDs would never match.; I ended up writing my own export function for the test (since I assume we don't really want to be exporting to this format IRL)---please let me know if I should handle that differently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2440
https://github.com/hail-is/hail/pull/2440:717,Security,expose,exposed,717,"I believe I addressed all the comments from #2431. The one thing I couldn't quite figure out is that you suggested testing using a randomly generated vsm and then calling make_table().export() (which in Scala, best as I can figure, is makeKT().export(f)). This does some things that makes re-importing using LoadMatrix non-identical:; - the exported header includes a header entry for the row ID column also, which the data we were looking at before didn't (n entries in the first line, n + 1 entries in subsequent lines), which seems like a reasonable thing to want to deal with---I added a flag in LoadMatrix called hasRowKeyLabel which drops the first item of the header line if that's the case, although it's not exposed to HailContext/the Python interface.; - the exported sample IDs somehow get "".g""s tacked on the ends (presumably because they came from the ""g"" struct), so the sample IDs would never match.; I ended up writing my own export function for the test (since I assume we don't really want to be exporting to this format IRL)---please let me know if I should handle that differently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2440
https://github.com/hail-is/hail/pull/2440:115,Testability,test,testing,115,"I believe I addressed all the comments from #2431. The one thing I couldn't quite figure out is that you suggested testing using a randomly generated vsm and then calling make_table().export() (which in Scala, best as I can figure, is makeKT().export(f)). This does some things that makes re-importing using LoadMatrix non-identical:; - the exported header includes a header entry for the row ID column also, which the data we were looking at before didn't (n entries in the first line, n + 1 entries in subsequent lines), which seems like a reasonable thing to want to deal with---I added a flag in LoadMatrix called hasRowKeyLabel which drops the first item of the header line if that's the case, although it's not exposed to HailContext/the Python interface.; - the exported sample IDs somehow get "".g""s tacked on the ends (presumably because they came from the ""g"" struct), so the sample IDs would never match.; I ended up writing my own export function for the test (since I assume we don't really want to be exporting to this format IRL)---please let me know if I should handle that differently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2440
https://github.com/hail-is/hail/pull/2440:966,Testability,test,test,966,"I believe I addressed all the comments from #2431. The one thing I couldn't quite figure out is that you suggested testing using a randomly generated vsm and then calling make_table().export() (which in Scala, best as I can figure, is makeKT().export(f)). This does some things that makes re-importing using LoadMatrix non-identical:; - the exported header includes a header entry for the row ID column also, which the data we were looking at before didn't (n entries in the first line, n + 1 entries in subsequent lines), which seems like a reasonable thing to want to deal with---I added a flag in LoadMatrix called hasRowKeyLabel which drops the first item of the header line if that's the case, although it's not exposed to HailContext/the Python interface.; - the exported sample IDs somehow get "".g""s tacked on the ends (presumably because they came from the ""g"" struct), so the sample IDs would never match.; I ended up writing my own export function for the test (since I assume we don't really want to be exporting to this format IRL)---please let me know if I should handle that differently.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2440
https://github.com/hail-is/hail/pull/2441:41,Integrability,interface,interface,41,"Broke AltAllele and Variant each into an interface with a concrete and a region-value implementation. Decided in favor of allocation per-variant (the array of alleles) for simplicity. Had to change a few field references to method calls because `foo(0)` doesn't work when `foo` is a fake-field, it must be `foo()(0)`. I kept `ArrayView` since it does one simple job well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2441
https://github.com/hail-is/hail/pull/2441:172,Usability,simpl,simplicity,172,"Broke AltAllele and Variant each into an interface with a concrete and a region-value implementation. Decided in favor of allocation per-variant (the array of alleles) for simplicity. Had to change a few field references to method calls because `foo(0)` doesn't work when `foo` is a fake-field, it must be `foo()(0)`. I kept `ArrayView` since it does one simple job well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2441
https://github.com/hail-is/hail/pull/2441:355,Usability,simpl,simple,355,"Broke AltAllele and Variant each into an interface with a concrete and a region-value implementation. Decided in favor of allocation per-variant (the array of alleles) for simplicity. Had to change a few field references to method calls because `foo(0)` doesn't work when `foo` is a fake-field, it must be `foo()(0)`. I kept `ArrayView` since it does one simple job well.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2441
https://github.com/hail-is/hail/pull/2452:319,Security,hash,hashes,319,"* Implement arithemetic in the finite field of order 2^32, as; polynomials over F_2 modulo the irreducible polynomial; x^32 + x^7 + x^3 + x^2 + 1. * Define class `PolyHash`, implementing polynomials over above field,; using Horner's rule for polynomial evaluation. * Use random polynomials to fill tables in tabulation hashes, for; guaranteed 32-independent randomness.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2452
https://github.com/hail-is/hail/pull/2455:23,Availability,failure,failures,23,"Was throwing exception failures. Return optional by default. Handle; TInt32 since there can be two TInt32, required and optional.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2455
https://github.com/hail-is/hail/pull/2459:159,Deployability,Update,Updated,159,"Which exposes the generic split_multi interface.; When we nuke Genotype, I will rename split_multi_generic => split_multi and split_multi => split_multi_hts.; Updated the interface slightly. The rule I'm using is: v, va, g, etc. all denote the old annotations and newV denotes the new, split variant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2459
https://github.com/hail-is/hail/pull/2459:38,Integrability,interface,interface,38,"Which exposes the generic split_multi interface.; When we nuke Genotype, I will rename split_multi_generic => split_multi and split_multi => split_multi_hts.; Updated the interface slightly. The rule I'm using is: v, va, g, etc. all denote the old annotations and newV denotes the new, split variant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2459
https://github.com/hail-is/hail/pull/2459:171,Integrability,interface,interface,171,"Which exposes the generic split_multi interface.; When we nuke Genotype, I will rename split_multi_generic => split_multi and split_multi => split_multi_hts.; Updated the interface slightly. The rule I'm using is: v, va, g, etc. all denote the old annotations and newV denotes the new, split variant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2459
https://github.com/hail-is/hail/pull/2459:6,Security,expose,exposes,6,"Which exposes the generic split_multi interface.; When we nuke Genotype, I will rename split_multi_generic => split_multi and split_multi => split_multi_hts.; Updated the interface slightly. The rule I'm using is: v, va, g, etc. all denote the old annotations and newV denotes the new, split variant.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2459
https://github.com/hail-is/hail/pull/2460:19,Usability,feedback,feedback,19,@cseed opening for feedback as discussed,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2460
https://github.com/hail-is/hail/pull/2465:203,Integrability,interface,interface,203,"Also nuked filter_altered_genotypes which I don't think anyone was using, couldn't possibly have been the intended implementation, and can now be done generically. I copied the existing annotate_alleles interface but, in light of generics, I don't like it anymore. I will propose an alternative on the dev forum.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2465
https://github.com/hail-is/hail/pull/2467:293,Testability,test,test,293,"Stacked on #2466 . 1. Not sure when region value builders should be cleared. I put them in this PR. If they are needed, then a `clear()` needs to be added to `VariantSampleMatrix.join`. 2. `coalesce` is giving me a different number of partitions for OrderedRDD2 compared to OrderedRDD for the test of identical variants. I think it's because the function `calculateKeyRanges` is different between the two. 3. I know I should incorporate the RegionValueVariant and clean up my BitPackedVectorView, but I wanted to have something that's stable so Genotype can be ripped out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2467
https://github.com/hail-is/hail/pull/2467:68,Usability,clear,cleared,68,"Stacked on #2466 . 1. Not sure when region value builders should be cleared. I put them in this PR. If they are needed, then a `clear()` needs to be added to `VariantSampleMatrix.join`. 2. `coalesce` is giving me a different number of partitions for OrderedRDD2 compared to OrderedRDD for the test of identical variants. I think it's because the function `calculateKeyRanges` is different between the two. 3. I know I should incorporate the RegionValueVariant and clean up my BitPackedVectorView, but I wanted to have something that's stable so Genotype can be ripped out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2467
https://github.com/hail-is/hail/pull/2467:128,Usability,clear,clear,128,"Stacked on #2466 . 1. Not sure when region value builders should be cleared. I put them in this PR. If they are needed, then a `clear()` needs to be added to `VariantSampleMatrix.join`. 2. `coalesce` is giving me a different number of partitions for OrderedRDD2 compared to OrderedRDD for the test of identical variants. I think it's because the function `calculateKeyRanges` is different between the two. 3. I know I should incorporate the RegionValueVariant and clean up my BitPackedVectorView, but I wanted to have something that's stable so Genotype can be ripped out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2467
https://github.com/hail-is/hail/pull/2474:243,Integrability,interface,interface,243,"There was a bug converting Call to/from Java, in this line:. if annotation:. A call is represented by a 0 integer which is False in Python. I; changed all of these to `if annotation is not None`. Also, there was a bit of confusion in the call interface inherited; from genotype. A not call is just a missing Call value (in python); and a null Call = java.lang.Integer (in Java).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2474
https://github.com/hail-is/hail/pull/2474:253,Modifiability,inherit,inherited,253,"There was a bug converting Call to/from Java, in this line:. if annotation:. A call is represented by a 0 integer which is False in Python. I; changed all of these to `if annotation is not None`. Also, there was a bit of confusion in the call interface inherited; from genotype. A not call is just a missing Call value (in python); and a null Call = java.lang.Integer (in Java).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2474
https://github.com/hail-is/hail/pull/2476:99,Energy Efficiency,allocate,allocated,99,"Allocating memory off-heap but relying on the GC to deallocate will; never work. Since regions are allocated for iterators, it is; difficult to know when we can free the memory. When hasNext returns; false, but ending iteration early will lead to a memory leak. We; could define something like a CloseableIterator which closes itself at; the end of iteration and clients have to close if they exit early, but; this won't play nicely with Spark (or Scala iterators, for that; matter). This doesn't matter in the current setup but will make it somewhat; more difficult to push things into C/C++.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2476
https://github.com/hail-is/hail/pull/2481:203,Integrability,interface,interface,203,"Also nuked filter_altered_genotypes which I don't think anyone was using, couldn't possibly have been the intended implementation, and can now be done generically. I copied the existing annotate_alleles interface but, in light of generics, I don't like it anymore. I will propose an alternative on the dev forum.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2481
https://github.com/hail-is/hail/pull/2492:195,Integrability,interface,interface,195,"I added ""Reviewers"" who are folks that might be interested in this PR. The PR die came up Tim. If you prefer, I can probably break it up a bit. The big changes are:. - `RegionValueAggregator` an interface and associated set of concrete implementations that work on region values rather than `Annotation`s. - The magic happens in ExtractAggregators.scala and RunAggregtors.scala. The former rips out references to aggregations and replaces them with references to the 2nd (index 1) input. The latter packages up running an aggregator on a region value array with some ""context"" (i.e. the `EvalContext`).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2492
https://github.com/hail-is/hail/pull/2494:1062,Testability,test,test,1062,"@cseed after comments in #2482, I reworked this so that for each block row, each overlapping parent partition is iterated through once, writing out the full row into files corresponding to block columns. Based on the WriteBlocksRDDPartition comment and looking at BlockedRDD, now rather than computing the indices of all non-empty overlapping parent partitions, I switched to just computing the first and last overlapping parent partitions, and the number of rows to skip in the first partition, and put this in WriteBlocksRDDPartition. I'd appreciate feedback on this approach, and whether I've put the ""right"" data in WriteBlocksRDDPartition. In particular, I'm unsure why `computeBlockRowDependencies` would have been called multiple times given that it's a private val in the class. Is the class serialized to workers before function evaluation? Is parentPartitionBoundaries recomputed as well? I don't have a clear model. The function `computeBlockRowDependencies` could be moved directly into getDependencies; I kept it separate in order to write a simple test. I'm not a huge fan of the array of tuples but it seemed more natural than three arrays.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2494
https://github.com/hail-is/hail/pull/2494:552,Usability,feedback,feedback,552,"@cseed after comments in #2482, I reworked this so that for each block row, each overlapping parent partition is iterated through once, writing out the full row into files corresponding to block columns. Based on the WriteBlocksRDDPartition comment and looking at BlockedRDD, now rather than computing the indices of all non-empty overlapping parent partitions, I switched to just computing the first and last overlapping parent partitions, and the number of rows to skip in the first partition, and put this in WriteBlocksRDDPartition. I'd appreciate feedback on this approach, and whether I've put the ""right"" data in WriteBlocksRDDPartition. In particular, I'm unsure why `computeBlockRowDependencies` would have been called multiple times given that it's a private val in the class. Is the class serialized to workers before function evaluation? Is parentPartitionBoundaries recomputed as well? I don't have a clear model. The function `computeBlockRowDependencies` could be moved directly into getDependencies; I kept it separate in order to write a simple test. I'm not a huge fan of the array of tuples but it seemed more natural than three arrays.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2494
https://github.com/hail-is/hail/pull/2494:914,Usability,clear,clear,914,"@cseed after comments in #2482, I reworked this so that for each block row, each overlapping parent partition is iterated through once, writing out the full row into files corresponding to block columns. Based on the WriteBlocksRDDPartition comment and looking at BlockedRDD, now rather than computing the indices of all non-empty overlapping parent partitions, I switched to just computing the first and last overlapping parent partitions, and the number of rows to skip in the first partition, and put this in WriteBlocksRDDPartition. I'd appreciate feedback on this approach, and whether I've put the ""right"" data in WriteBlocksRDDPartition. In particular, I'm unsure why `computeBlockRowDependencies` would have been called multiple times given that it's a private val in the class. Is the class serialized to workers before function evaluation? Is parentPartitionBoundaries recomputed as well? I don't have a clear model. The function `computeBlockRowDependencies` could be moved directly into getDependencies; I kept it separate in order to write a simple test. I'm not a huge fan of the array of tuples but it seemed more natural than three arrays.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2494
https://github.com/hail-is/hail/pull/2494:1055,Usability,simpl,simple,1055,"@cseed after comments in #2482, I reworked this so that for each block row, each overlapping parent partition is iterated through once, writing out the full row into files corresponding to block columns. Based on the WriteBlocksRDDPartition comment and looking at BlockedRDD, now rather than computing the indices of all non-empty overlapping parent partitions, I switched to just computing the first and last overlapping parent partitions, and the number of rows to skip in the first partition, and put this in WriteBlocksRDDPartition. I'd appreciate feedback on this approach, and whether I've put the ""right"" data in WriteBlocksRDDPartition. In particular, I'm unsure why `computeBlockRowDependencies` would have been called multiple times given that it's a private val in the class. Is the class serialized to workers before function evaluation? Is parentPartitionBoundaries recomputed as well? I don't have a clear model. The function `computeBlockRowDependencies` could be moved directly into getDependencies; I kept it separate in order to write a simple test. I'm not a huge fan of the array of tuples but it seemed more natural than three arrays.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2494
https://github.com/hail-is/hail/pull/2496:106,Modifiability,variab,variable,106,"I'm getting this warning:. ```; /Users/jigold/hail/src/main/scala/is/hail/io/vcf/ExportVCF.scala:199: non-variable type argument String in type pattern scala.collection.immutable.Map[String,Any] (the underlying of Map[String,Any]) is unchecked since it is eliminated by erasure; case Some(x: Map[String, Any]) => getAttributes(path.tail, Some(x)); ```. If you have suggestions on how to improve the `getAttributes` function, I'd appreciate it!",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2496
https://github.com/hail-is/hail/pull/2507:281,Usability,intuit,intuition,281,"If we view IR as the least fixpoint of an algebra wherein recursive; references are replaced with a type parameter, a la:. IR[T] ::= I32() ...; | Cast(T, Type); | ...; | Let(T, String, T, Type); | ... Then IR is a functor and has a natural map operation. Recur is inspired by this intuition, but in the restricted situation of; Scala's built-in type recursion.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2507
https://github.com/hail-is/hail/pull/2511:42,Testability,test,tests,42,I think I also fixed a couple bugs in the tests where we set `rv`; instead of `rv2`.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2511
https://github.com/hail-is/hail/pull/2519:179,Availability,avail,available,179,"To properly implement IR sets, I need to staged UnsafeOrdering's, or, at the very least, I need to be able to call them from `Code`-land. Since objects at IR-compile-time are not available at IR-run-time (without shipping them to the nodes and passing them as arguments, which I'd like to avoid), I must be able to call static methods, or have fully code-ified versions of every UnsafeOrdering used in the IR. Whenever possible, I tried to call static methods. In a few cases, I couldn't figure out how to make that work, so I had to reimplement the operation in `Code`. I also had to introduce `BindingCode[T]` which is a type alias for `(FunctionBuilder, StagedBitSet) => Code[T]`. The function builder is used to allocate new variables and the `StagedBitSet` is used to compactly store boolean values. I am also somewhat confused by the `missingGreatest` parameter which existed on the original `UnsafeOrdering`s (which I refactored while Code-ifying). cc: @cseed, I guess this parameter is only sensible on compound data? It seems like there should be a:. ```; def compare(r1: MemoryBuffer, o1: Long, m1: Boolean, r2: MemoryBuffer, o2: Long, m2: Boolean): Int; ```. which correctly applies the `missingGreatest` parameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2519
https://github.com/hail-is/hail/pull/2519:716,Energy Efficiency,allocate,allocate,716,"To properly implement IR sets, I need to staged UnsafeOrdering's, or, at the very least, I need to be able to call them from `Code`-land. Since objects at IR-compile-time are not available at IR-run-time (without shipping them to the nodes and passing them as arguments, which I'd like to avoid), I must be able to call static methods, or have fully code-ified versions of every UnsafeOrdering used in the IR. Whenever possible, I tried to call static methods. In a few cases, I couldn't figure out how to make that work, so I had to reimplement the operation in `Code`. I also had to introduce `BindingCode[T]` which is a type alias for `(FunctionBuilder, StagedBitSet) => Code[T]`. The function builder is used to allocate new variables and the `StagedBitSet` is used to compactly store boolean values. I am also somewhat confused by the `missingGreatest` parameter which existed on the original `UnsafeOrdering`s (which I refactored while Code-ifying). cc: @cseed, I guess this parameter is only sensible on compound data? It seems like there should be a:. ```; def compare(r1: MemoryBuffer, o1: Long, m1: Boolean, r2: MemoryBuffer, o2: Long, m2: Boolean): Int; ```. which correctly applies the `missingGreatest` parameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2519
https://github.com/hail-is/hail/pull/2519:729,Modifiability,variab,variables,729,"To properly implement IR sets, I need to staged UnsafeOrdering's, or, at the very least, I need to be able to call them from `Code`-land. Since objects at IR-compile-time are not available at IR-run-time (without shipping them to the nodes and passing them as arguments, which I'd like to avoid), I must be able to call static methods, or have fully code-ified versions of every UnsafeOrdering used in the IR. Whenever possible, I tried to call static methods. In a few cases, I couldn't figure out how to make that work, so I had to reimplement the operation in `Code`. I also had to introduce `BindingCode[T]` which is a type alias for `(FunctionBuilder, StagedBitSet) => Code[T]`. The function builder is used to allocate new variables and the `StagedBitSet` is used to compactly store boolean values. I am also somewhat confused by the `missingGreatest` parameter which existed on the original `UnsafeOrdering`s (which I refactored while Code-ifying). cc: @cseed, I guess this parameter is only sensible on compound data? It seems like there should be a:. ```; def compare(r1: MemoryBuffer, o1: Long, m1: Boolean, r2: MemoryBuffer, o2: Long, m2: Boolean): Int; ```. which correctly applies the `missingGreatest` parameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2519
https://github.com/hail-is/hail/pull/2519:925,Modifiability,refactor,refactored,925,"To properly implement IR sets, I need to staged UnsafeOrdering's, or, at the very least, I need to be able to call them from `Code`-land. Since objects at IR-compile-time are not available at IR-run-time (without shipping them to the nodes and passing them as arguments, which I'd like to avoid), I must be able to call static methods, or have fully code-ified versions of every UnsafeOrdering used in the IR. Whenever possible, I tried to call static methods. In a few cases, I couldn't figure out how to make that work, so I had to reimplement the operation in `Code`. I also had to introduce `BindingCode[T]` which is a type alias for `(FunctionBuilder, StagedBitSet) => Code[T]`. The function builder is used to allocate new variables and the `StagedBitSet` is used to compactly store boolean values. I am also somewhat confused by the `missingGreatest` parameter which existed on the original `UnsafeOrdering`s (which I refactored while Code-ifying). cc: @cseed, I guess this parameter is only sensible on compound data? It seems like there should be a:. ```; def compare(r1: MemoryBuffer, o1: Long, m1: Boolean, r2: MemoryBuffer, o2: Long, m2: Boolean): Int; ```. which correctly applies the `missingGreatest` parameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2519
https://github.com/hail-is/hail/pull/2519:48,Safety,Unsafe,UnsafeOrdering,48,"To properly implement IR sets, I need to staged UnsafeOrdering's, or, at the very least, I need to be able to call them from `Code`-land. Since objects at IR-compile-time are not available at IR-run-time (without shipping them to the nodes and passing them as arguments, which I'd like to avoid), I must be able to call static methods, or have fully code-ified versions of every UnsafeOrdering used in the IR. Whenever possible, I tried to call static methods. In a few cases, I couldn't figure out how to make that work, so I had to reimplement the operation in `Code`. I also had to introduce `BindingCode[T]` which is a type alias for `(FunctionBuilder, StagedBitSet) => Code[T]`. The function builder is used to allocate new variables and the `StagedBitSet` is used to compactly store boolean values. I am also somewhat confused by the `missingGreatest` parameter which existed on the original `UnsafeOrdering`s (which I refactored while Code-ifying). cc: @cseed, I guess this parameter is only sensible on compound data? It seems like there should be a:. ```; def compare(r1: MemoryBuffer, o1: Long, m1: Boolean, r2: MemoryBuffer, o2: Long, m2: Boolean): Int; ```. which correctly applies the `missingGreatest` parameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2519
https://github.com/hail-is/hail/pull/2519:289,Safety,avoid,avoid,289,"To properly implement IR sets, I need to staged UnsafeOrdering's, or, at the very least, I need to be able to call them from `Code`-land. Since objects at IR-compile-time are not available at IR-run-time (without shipping them to the nodes and passing them as arguments, which I'd like to avoid), I must be able to call static methods, or have fully code-ified versions of every UnsafeOrdering used in the IR. Whenever possible, I tried to call static methods. In a few cases, I couldn't figure out how to make that work, so I had to reimplement the operation in `Code`. I also had to introduce `BindingCode[T]` which is a type alias for `(FunctionBuilder, StagedBitSet) => Code[T]`. The function builder is used to allocate new variables and the `StagedBitSet` is used to compactly store boolean values. I am also somewhat confused by the `missingGreatest` parameter which existed on the original `UnsafeOrdering`s (which I refactored while Code-ifying). cc: @cseed, I guess this parameter is only sensible on compound data? It seems like there should be a:. ```; def compare(r1: MemoryBuffer, o1: Long, m1: Boolean, r2: MemoryBuffer, o2: Long, m2: Boolean): Int; ```. which correctly applies the `missingGreatest` parameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2519
https://github.com/hail-is/hail/pull/2519:379,Safety,Unsafe,UnsafeOrdering,379,"To properly implement IR sets, I need to staged UnsafeOrdering's, or, at the very least, I need to be able to call them from `Code`-land. Since objects at IR-compile-time are not available at IR-run-time (without shipping them to the nodes and passing them as arguments, which I'd like to avoid), I must be able to call static methods, or have fully code-ified versions of every UnsafeOrdering used in the IR. Whenever possible, I tried to call static methods. In a few cases, I couldn't figure out how to make that work, so I had to reimplement the operation in `Code`. I also had to introduce `BindingCode[T]` which is a type alias for `(FunctionBuilder, StagedBitSet) => Code[T]`. The function builder is used to allocate new variables and the `StagedBitSet` is used to compactly store boolean values. I am also somewhat confused by the `missingGreatest` parameter which existed on the original `UnsafeOrdering`s (which I refactored while Code-ifying). cc: @cseed, I guess this parameter is only sensible on compound data? It seems like there should be a:. ```; def compare(r1: MemoryBuffer, o1: Long, m1: Boolean, r2: MemoryBuffer, o2: Long, m2: Boolean): Int; ```. which correctly applies the `missingGreatest` parameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2519
https://github.com/hail-is/hail/pull/2519:899,Safety,Unsafe,UnsafeOrdering,899,"To properly implement IR sets, I need to staged UnsafeOrdering's, or, at the very least, I need to be able to call them from `Code`-land. Since objects at IR-compile-time are not available at IR-run-time (without shipping them to the nodes and passing them as arguments, which I'd like to avoid), I must be able to call static methods, or have fully code-ified versions of every UnsafeOrdering used in the IR. Whenever possible, I tried to call static methods. In a few cases, I couldn't figure out how to make that work, so I had to reimplement the operation in `Code`. I also had to introduce `BindingCode[T]` which is a type alias for `(FunctionBuilder, StagedBitSet) => Code[T]`. The function builder is used to allocate new variables and the `StagedBitSet` is used to compactly store boolean values. I am also somewhat confused by the `missingGreatest` parameter which existed on the original `UnsafeOrdering`s (which I refactored while Code-ifying). cc: @cseed, I guess this parameter is only sensible on compound data? It seems like there should be a:. ```; def compare(r1: MemoryBuffer, o1: Long, m1: Boolean, r2: MemoryBuffer, o2: Long, m2: Boolean): Int; ```. which correctly applies the `missingGreatest` parameter.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2519
https://github.com/hail-is/hail/pull/2520:8,Testability,assert,asserts,8,"This PR asserts all inputs are consistent with the reference genome (default of GRCh37 if not supplied by the user). This PR **will break** user scripts if their data does not conform to GRCh37. If this occurs, the user can utilize the recode contigs option on import or create their own reference genome.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2520
https://github.com/hail-is/hail/issues/2527:355,Availability,Error,Error,355,"Summary;; I tried running hail with spark-submit and a .py script with a short pipeline to compare speed. Offending line:; ```; kt = vds_results.make_table('v = v', 'pval = va.pval').export(""output/test.txt""); ```; gives; ```; File ""<decorator-gen-93>"", line 2, in export; File ""/home/ludvig/Programs/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: SparkException: Job aborted due to stage failure: Task 0.0 in stage 5.0 (TID 1591) had a not serializable result: is.hail.io.bgen.BgenRecordV11$$anon$1; ```; ```; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.BgenRecordV11$$anon$1, value: BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:484,Availability,failure,failure,484,"Summary;; I tried running hail with spark-submit and a .py script with a short pipeline to compare speed. Offending line:; ```; kt = vds_results.make_table('v = v', 'pval = va.pval').export(""output/test.txt""); ```; gives; ```; File ""<decorator-gen-93>"", line 2, in export; File ""/home/ludvig/Programs/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: SparkException: Job aborted due to stage failure: Task 0.0 in stage 5.0 (TID 1591) had a not serializable result: is.hail.io.bgen.BgenRecordV11$$anon$1; ```; ```; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.BgenRecordV11$$anon$1, value: BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:6396,Availability,error,errors,6396,perationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:955); 	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1459); 	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438); 	at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1438); 	at is.hail.utils.richUtils.RichRDD$.writeTable$extension(RichRDD.scala:61); 	at is.hail.keytable.KeyTable.export(KeyTable.scala:537); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```; Edit: I get identical errors if I change export to .to_pandas() or try to write the vds_results to file with .write(). Full script attached (as .txt - it didn't let me attach a .py); [hail_test.txt](https://github.com/hail-is/hail/files/1535717/hail_test.txt). Submitted as; ```; spark-submit hail_test.py; ```; and my spark-defaults.conf only has the spark.local.dir set,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:79,Deployability,pipeline,pipeline,79,"Summary;; I tried running hail with spark-submit and a .py script with a short pipeline to compare speed. Offending line:; ```; kt = vds_results.make_table('v = v', 'pval = va.pval').export(""output/test.txt""); ```; gives; ```; File ""<decorator-gen-93>"", line 2, in export; File ""/home/ludvig/Programs/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: SparkException: Job aborted due to stage failure: Task 0.0 in stage 5.0 (TID 1591) had a not serializable result: is.hail.io.bgen.BgenRecordV11$$anon$1; ```; ```; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.BgenRecordV11$$anon$1, value: BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:1547,Energy Efficiency,schedul,scheduler,1547,"ult: is.hail.io.bgen.BgenRecordV11$$anon$1; ```; ```; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.BgenRecordV11$$anon$1, value: BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:1587,Energy Efficiency,schedul,scheduler,1587,"lization stack:; 	- object not serializable (class: is.hail.io.bgen.BgenRecordV11$$anon$1, value: BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:1686,Energy Efficiency,schedul,scheduler,1686,"1$$anon$1, value: BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProces",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:1784,Energy Efficiency,schedul,scheduler,1784,".0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:2038,Energy Efficiency,schedul,scheduler,2038,".:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkConte",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:2119,Energy Efficiency,schedul,scheduler,2119,"416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$an",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:2225,Energy Efficiency,schedul,scheduler,2225,"1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:2375,Energy Efficiency,schedul,scheduler,2375,"0000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.app",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:2464,Energy Efficiency,schedul,scheduler,2464,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RD,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:2562,Energy Efficiency,schedul,scheduler,2562,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScop,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:2658,Energy Efficiency,schedul,scheduler,2658,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.Pa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:2823,Energy Efficiency,schedul,scheduler,2823,.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:463,Safety,abort,aborted,463,"Summary;; I tried running hail with spark-submit and a .py script with a short pipeline to compare speed. Offending line:; ```; kt = vds_results.make_table('v = v', 'pval = va.pval').export(""output/test.txt""); ```; gives; ```; File ""<decorator-gen-93>"", line 2, in export; File ""/home/ludvig/Programs/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: SparkException: Job aborted due to stage failure: Task 0.0 in stage 5.0 (TID 1591) had a not serializable result: is.hail.io.bgen.BgenRecordV11$$anon$1; ```; ```; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.BgenRecordV11$$anon$1, value: BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:1718,Safety,abort,abortStage,1718,"RecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:1816,Safety,abort,abortStage,1816,"GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:2061,Safety,abort,abortStage,2061,"=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/issues/2527:198,Testability,test,test,198,"Summary;; I tried running hail with spark-submit and a .py script with a short pipeline to compare speed. Offending line:; ```; kt = vds_results.make_table('v = v', 'pval = va.pval').export(""output/test.txt""); ```; gives; ```; File ""<decorator-gen-93>"", line 2, in export; File ""/home/ludvig/Programs/hail/python/hail/java.py"", line 121, in handle_py4j; 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); hail.java.FatalError: SparkException: Job aborted due to stage failure: Task 0.0 in stage 5.0 (TID 1591) had a not serializable result: is.hail.io.bgen.BgenRecordV11$$anon$1; ```; ```; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.BgenRecordV11$$anon$1, value: BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. goes on for a while. field (class: scala.Tuple2, name: _2, type: class java.lang.Object); 	- object (class scala.Tuple2, ([rs149841286:10000179:AAAAAAAC:A,---],BgenRecordV11(0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0,. keeps on going like above until remaining stack trace:. at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2527
https://github.com/hail-is/hail/pull/2528:126,Modifiability,variab,variable,126,This is needed for future genome reference pull requests to be able to access the ordering from the GenomeReference after the variable GR has been substituted for.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2528
https://github.com/hail-is/hail/pull/2528:71,Security,access,access,71,This is needed for future genome reference pull requests to be able to access the ordering from the GenomeReference after the variable GR has been substituted for.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2528
https://github.com/hail-is/hail/pull/2530:76,Modifiability,parameteriz,parameterized,76,- Intervals cannot be in GenomeReference constructor because intervals; are parameterized by reference; - Made python genome reference operations lazy,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2530
https://github.com/hail-is/hail/pull/2533:76,Modifiability,parameteriz,parameterized,76,- Intervals cannot be in GenomeReference constructor because intervals; are parameterized by reference with Ordering[Locus],MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2533
https://github.com/hail-is/hail/issues/2537:71,Availability,error,error,71,"using ""chromosome"" when unexpected or leaving it out produces terrible error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2537
https://github.com/hail-is/hail/issues/2537:77,Integrability,message,messages,77,"using ""chromosome"" when unexpected or leaving it out produces terrible error messages.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2537
https://github.com/hail-is/hail/pull/2538:8,Testability,assert,assert,8,- Added assert to tests with `kt1.same(kt2)`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2538
https://github.com/hail-is/hail/pull/2538:18,Testability,test,tests,18,- Added assert to tests with `kt1.same(kt2)`,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2538
https://github.com/hail-is/hail/pull/2543:28,Testability,test,tests,28,Move functions only used in tests to RichVariantSampleMatrix defined; in src/test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2543
https://github.com/hail-is/hail/pull/2543:77,Testability,test,test,77,Move functions only used in tests to RichVariantSampleMatrix defined; in src/test.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2543
https://github.com/hail-is/hail/pull/2544:2,Deployability,update,updated,2,"- updated parameter names; - fixed tests and docs. As with burden tests, explode should be used to handle variants with multiple keys",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2544
https://github.com/hail-is/hail/pull/2544:35,Testability,test,tests,35,"- updated parameter names; - fixed tests and docs. As with burden tests, explode should be used to handle variants with multiple keys",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2544
https://github.com/hail-is/hail/pull/2544:66,Testability,test,tests,66,"- updated parameter names; - fixed tests and docs. As with burden tests, explode should be used to handle variants with multiple keys",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2544
https://github.com/hail-is/hail/pull/2549:222,Testability,test,tests,222,"explodeVariants and explodeSamples are failing on Set keys because of cast to IndexedSeq. And on key=[1, 2], explodeSamples is annotating each sample with [1,2] rather than with 1 and then 2. Here are fixes and regression tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2549
https://github.com/hail-is/hail/pull/2550:0,Integrability,depend,depends,0,depends on #2548 (not a stacked PR because I don't want them to have to go in together),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2550
https://github.com/hail-is/hail/pull/2554:396,Availability,Error,Error,396,"Output for a requirement failed exception now looks like:. ```; Traceback (most recent call last):; File ""hail_extract_cohorts.py"", line 37, in <module>; vds = vds.annotate_global('global.samples_to_exclude', set(samples_to_exclude_list), hail.TSet(hail.TString())); File ""<decorator-gen-390>"", line 2, in annotate_global; File ""/home/cotton/hail/python/hail/java.py"", line 167, in handle_py4j; 'Error summary: %s' % (e.desc, e.stackTrace, Env.hc().version, e.desc)); hail.java.FatalError: requirement failed. Java stack trace:; scala.Predef$.require(Predef.scala:212); 	 at is.hail.variant.VariantSampleMatrix.annotateGlobal(VariantSampleMatrix.scala:564); 	 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	 at java.lang.reflect.Method.invoke(Method.java:498); 	 at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	 at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	 at py4j.Gateway.invoke(Gateway.java:280); 	 at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	 at py4j.commands.CallCommand.execute(CallCommand.java:79); 	 at py4j.GatewayConnection.run(GatewayConnection.java:214); 	 at java.lang.Thread.run(Thread.java:748); Hail version: devel-75de081; Error summary: requirement failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2554
https://github.com/hail-is/hail/pull/2554:1409,Availability,Error,Error,1409,"Output for a requirement failed exception now looks like:. ```; Traceback (most recent call last):; File ""hail_extract_cohorts.py"", line 37, in <module>; vds = vds.annotate_global('global.samples_to_exclude', set(samples_to_exclude_list), hail.TSet(hail.TString())); File ""<decorator-gen-390>"", line 2, in annotate_global; File ""/home/cotton/hail/python/hail/java.py"", line 167, in handle_py4j; 'Error summary: %s' % (e.desc, e.stackTrace, Env.hc().version, e.desc)); hail.java.FatalError: requirement failed. Java stack trace:; scala.Predef$.require(Predef.scala:212); 	 at is.hail.variant.VariantSampleMatrix.annotateGlobal(VariantSampleMatrix.scala:564); 	 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	 at java.lang.reflect.Method.invoke(Method.java:498); 	 at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	 at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	 at py4j.Gateway.invoke(Gateway.java:280); 	 at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	 at py4j.commands.CallCommand.execute(CallCommand.java:79); 	 at py4j.GatewayConnection.run(GatewayConnection.java:214); 	 at java.lang.Thread.run(Thread.java:748); Hail version: devel-75de081; Error summary: requirement failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2554
https://github.com/hail-is/hail/pull/2555:169,Integrability,interface,interface,169,"cc: @catoverdrive @tpoterba . I collapsed two PRs into one and made major changes, so this needs a fresh look. Some high level direction:. - RegionValueAggregator is an interface that doesn't take `Any` as the seqOp argument. All of them have methods that can be called directly with the right input type. I emit code to call such methods. - TransformedRegionValueAggregator is just a little shim class that lets me override the seqOp of another aggregator with an AsmFunction4 (the result of compiling). - ZippedRegionValueAggregator just puts a bunch of RegionValueAggregators together (just foreach on the child aggregators) and puts all their results into an RV Struct. This RV Struct is referenced in the non-aggregating IR code (I've been calling the fragment that remains after aggs are removed ""stage0"" and calling aggregation ""stage1""). Passing all the aggregations directly as arguments to stage0 seems hard/impossible because the number of aggregations isn't known at scala-compile-time, but we stitch together the aggregations and the stage0 compiled IR in scala. - I added 4 IRs: AggIn (this is `gs`), AggMap, AggFilter, AggFlatMap, AggSum. I don't quite know how to add more aggregators. I think I will create new IRs for each one. Or maybe I can get a better interface. Comments welcome. - `compileAgg2` is a CPS-style compiler that lays out straight-line code that corresponds to aggregation. - The interface to generated aggregations isn't very nice right now. The user passes in a struct that contains the element and the scope values. When running an aggregator, the element is extracted first and then passed around (without the rest of the scope). References to the scope become `GetField` IRs that find the appropriate field in the argument. Using this is a bit hacky because we should mutate the fields of the scope struct as we loop over `gs` (e.g. `sa`). However, this is essentially what we do now (EvalContext is basically the scope struct), so I'm not so bothered by it. My",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2555
https://github.com/hail-is/hail/pull/2555:1274,Integrability,interface,interface,1274,"Rs into one and made major changes, so this needs a fresh look. Some high level direction:. - RegionValueAggregator is an interface that doesn't take `Any` as the seqOp argument. All of them have methods that can be called directly with the right input type. I emit code to call such methods. - TransformedRegionValueAggregator is just a little shim class that lets me override the seqOp of another aggregator with an AsmFunction4 (the result of compiling). - ZippedRegionValueAggregator just puts a bunch of RegionValueAggregators together (just foreach on the child aggregators) and puts all their results into an RV Struct. This RV Struct is referenced in the non-aggregating IR code (I've been calling the fragment that remains after aggs are removed ""stage0"" and calling aggregation ""stage1""). Passing all the aggregations directly as arguments to stage0 seems hard/impossible because the number of aggregations isn't known at scala-compile-time, but we stitch together the aggregations and the stage0 compiled IR in scala. - I added 4 IRs: AggIn (this is `gs`), AggMap, AggFilter, AggFlatMap, AggSum. I don't quite know how to add more aggregators. I think I will create new IRs for each one. Or maybe I can get a better interface. Comments welcome. - `compileAgg2` is a CPS-style compiler that lays out straight-line code that corresponds to aggregation. - The interface to generated aggregations isn't very nice right now. The user passes in a struct that contains the element and the scope values. When running an aggregator, the element is extracted first and then passed around (without the rest of the scope). References to the scope become `GetField` IRs that find the appropriate field in the argument. Using this is a bit hacky because we should mutate the fields of the scope struct as we loop over `gs` (e.g. `sa`). However, this is essentially what we do now (EvalContext is basically the scope struct), so I'm not so bothered by it. My tests are kind of hard to follow as a result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2555
https://github.com/hail-is/hail/pull/2555:1415,Integrability,interface,interface,1415,"Rs into one and made major changes, so this needs a fresh look. Some high level direction:. - RegionValueAggregator is an interface that doesn't take `Any` as the seqOp argument. All of them have methods that can be called directly with the right input type. I emit code to call such methods. - TransformedRegionValueAggregator is just a little shim class that lets me override the seqOp of another aggregator with an AsmFunction4 (the result of compiling). - ZippedRegionValueAggregator just puts a bunch of RegionValueAggregators together (just foreach on the child aggregators) and puts all their results into an RV Struct. This RV Struct is referenced in the non-aggregating IR code (I've been calling the fragment that remains after aggs are removed ""stage0"" and calling aggregation ""stage1""). Passing all the aggregations directly as arguments to stage0 seems hard/impossible because the number of aggregations isn't known at scala-compile-time, but we stitch together the aggregations and the stage0 compiled IR in scala. - I added 4 IRs: AggIn (this is `gs`), AggMap, AggFilter, AggFlatMap, AggSum. I don't quite know how to add more aggregators. I think I will create new IRs for each one. Or maybe I can get a better interface. Comments welcome. - `compileAgg2` is a CPS-style compiler that lays out straight-line code that corresponds to aggregation. - The interface to generated aggregations isn't very nice right now. The user passes in a struct that contains the element and the scope values. When running an aggregator, the element is extracted first and then passed around (without the rest of the scope). References to the scope become `GetField` IRs that find the appropriate field in the argument. Using this is a bit hacky because we should mutate the fields of the scope struct as we loop over `gs` (e.g. `sa`). However, this is essentially what we do now (EvalContext is basically the scope struct), so I'm not so bothered by it. My tests are kind of hard to follow as a result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2555
https://github.com/hail-is/hail/pull/2555:2002,Testability,test,tests,2002,"Rs into one and made major changes, so this needs a fresh look. Some high level direction:. - RegionValueAggregator is an interface that doesn't take `Any` as the seqOp argument. All of them have methods that can be called directly with the right input type. I emit code to call such methods. - TransformedRegionValueAggregator is just a little shim class that lets me override the seqOp of another aggregator with an AsmFunction4 (the result of compiling). - ZippedRegionValueAggregator just puts a bunch of RegionValueAggregators together (just foreach on the child aggregators) and puts all their results into an RV Struct. This RV Struct is referenced in the non-aggregating IR code (I've been calling the fragment that remains after aggs are removed ""stage0"" and calling aggregation ""stage1""). Passing all the aggregations directly as arguments to stage0 seems hard/impossible because the number of aggregations isn't known at scala-compile-time, but we stitch together the aggregations and the stage0 compiled IR in scala. - I added 4 IRs: AggIn (this is `gs`), AggMap, AggFilter, AggFlatMap, AggSum. I don't quite know how to add more aggregators. I think I will create new IRs for each one. Or maybe I can get a better interface. Comments welcome. - `compileAgg2` is a CPS-style compiler that lays out straight-line code that corresponds to aggregation. - The interface to generated aggregations isn't very nice right now. The user passes in a struct that contains the element and the scope values. When running an aggregator, the element is extracted first and then passed around (without the rest of the scope). References to the scope become `GetField` IRs that find the appropriate field in the argument. Using this is a bit hacky because we should mutate the fields of the scope struct as we loop over `gs` (e.g. `sa`). However, this is essentially what we do now (EvalContext is basically the scope struct), so I'm not so bothered by it. My tests are kind of hard to follow as a result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2555
https://github.com/hail-is/hail/pull/2558:0,Deployability,Update,Updated,0,Updated version of #2520. Will break import for some datasets!,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2558
https://github.com/hail-is/hail/pull/2559:69,Deployability,integrat,integrate,69,"I'll add more tests, and I'm still considering whether to rip out or integrate WriteBlocksRDD for IRM. But given how similar this is to the latter, I'm ready for feedback on the new code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2559
https://github.com/hail-is/hail/pull/2559:69,Integrability,integrat,integrate,69,"I'll add more tests, and I'm still considering whether to rip out or integrate WriteBlocksRDD for IRM. But given how similar this is to the latter, I'm ready for feedback on the new code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2559
https://github.com/hail-is/hail/pull/2559:14,Testability,test,tests,14,"I'll add more tests, and I'm still considering whether to rip out or integrate WriteBlocksRDD for IRM. But given how similar this is to the latter, I'm ready for feedback on the new code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2559
https://github.com/hail-is/hail/pull/2559:162,Usability,feedback,feedback,162,"I'll add more tests, and I'm still considering whether to rip out or integrate WriteBlocksRDD for IRM. But given how similar this is to the latter, I'm ready for feedback on the new code.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2559
https://github.com/hail-is/hail/pull/2564:216,Performance,load,loadPrimitive,216,Builds on: https://github.com/hail-is/hail/pull/2563. Didn't stack (yet). Had to remove the requiredness on gs since I can't produce it in the IR (minor). Fixed two bugs along the way: wrong TypeInfo in ArrayMap and loadPrimitive wasn't loading the array address.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2564
https://github.com/hail-is/hail/pull/2564:237,Performance,load,loading,237,Builds on: https://github.com/hail-is/hail/pull/2563. Didn't stack (yet). Had to remove the requiredness on gs since I can't produce it in the IR (minor). Fixed two bugs along the way: wrong TypeInfo in ArrayMap and loadPrimitive wasn't loading the array address.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2564
https://github.com/hail-is/hail/pull/2566:22,Testability,assert,assert,22,"…IntInsnNode, and add assert on allocLocal. cc @danking, we found where the problem was.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2566
https://github.com/hail-is/hail/pull/2568:75,Deployability,update,update,75,There is now just public allocate that takes an alignment and a size. Then update client code.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2568
https://github.com/hail-is/hail/pull/2568:25,Energy Efficiency,allocate,allocate,25,There is now just public allocate that takes an alignment and a size. Then update client code.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2568
https://github.com/hail-is/hail/pull/2573:89,Integrability,interface,interface,89,"Use in filterSamples. I might stop here for the moment. This seems about as good for the interface as one could hope modulo and easier way to write the IR. I'll come back to `InsertStruct` once physical types are in. @catoverdrive FYI this might be a useful idiom when adding the IR to key tables, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2573
https://github.com/hail-is/hail/pull/2580:17,Integrability,Depend,Dependent,17,Somewhat stubby. Dependent on #2578,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2580
https://github.com/hail-is/hail/pull/2580:9,Testability,stub,stubby,9,Somewhat stubby. Dependent on #2578,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2580
https://github.com/hail-is/hail/pull/2590:357,Safety,avoid,avoid,357,"Moved sample and variant QC methods from VDS to their own objects, and call the object apply methods from elsewhere (tests and python). If we're happy with this model (following the hail2 api) I'll change everything else. Long chains of `vds.f().g().h()` will need to get broken up as. ```; var vdss = ...; vds = f(vds); vds = g(vds); vds = h(vds); ```. to avoid unnecessary nesting (and clarity).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2590
https://github.com/hail-is/hail/pull/2590:117,Testability,test,tests,117,"Moved sample and variant QC methods from VDS to their own objects, and call the object apply methods from elsewhere (tests and python). If we're happy with this model (following the hail2 api) I'll change everything else. Long chains of `vds.f().g().h()` will need to get broken up as. ```; var vdss = ...; vds = f(vds); vds = g(vds); vds = h(vds); ```. to avoid unnecessary nesting (and clarity).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2590
https://github.com/hail-is/hail/pull/2594:6,Testability,test,test,6,Added test to invoke PCRelate on BN sample data (runs in ~3s).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2594
https://github.com/hail-is/hail/pull/2595:6,Testability,test,test,6,Added test to make sure python is calling the JVM correctly.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2595
https://github.com/hail-is/hail/pull/2603:698,Testability,test,tests,698,"AnnotateAllelesExpr, FilterAlleles and SplitMulti entrypoints moved to method objects.; Removed wasSplit from MatrixTable. Methods that require biallelics are only checked in python using the @require_biallelic decorator. They are checked every time.; Removed user-facing verify_biallelic.; Removed filter_multi in favor of filter_variants_expr('v.isBiallelic').; Renamed annotate_alleles_expr_generic => annotate_alleles_expr and annotate_alleles_expr => annotate_alleles_expr_hts, similarly for split_multi and filter_alleles.; Moved the _hts variants to Python, left duplicate Scala versions where needed.; Fixed bug in filter_alleles related to moving variants. This wasn't caught by gen-based tests because they rarely generate examples with duplicate loci.; Moved annotate_alleles tests to python. Each time we move a test to Python, an angel gets its wings.; Don't filter multi-allelics in ld_prune since it is marked as requiring biallelics. The change to not check biallelics in scala is potentially controversial. I appreciate your thoughts on that, @tpoterba.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2603
https://github.com/hail-is/hail/pull/2603:787,Testability,test,tests,787,"AnnotateAllelesExpr, FilterAlleles and SplitMulti entrypoints moved to method objects.; Removed wasSplit from MatrixTable. Methods that require biallelics are only checked in python using the @require_biallelic decorator. They are checked every time.; Removed user-facing verify_biallelic.; Removed filter_multi in favor of filter_variants_expr('v.isBiallelic').; Renamed annotate_alleles_expr_generic => annotate_alleles_expr and annotate_alleles_expr => annotate_alleles_expr_hts, similarly for split_multi and filter_alleles.; Moved the _hts variants to Python, left duplicate Scala versions where needed.; Fixed bug in filter_alleles related to moving variants. This wasn't caught by gen-based tests because they rarely generate examples with duplicate loci.; Moved annotate_alleles tests to python. Each time we move a test to Python, an angel gets its wings.; Don't filter multi-allelics in ld_prune since it is marked as requiring biallelics. The change to not check biallelics in scala is potentially controversial. I appreciate your thoughts on that, @tpoterba.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2603
https://github.com/hail-is/hail/pull/2603:824,Testability,test,test,824,"AnnotateAllelesExpr, FilterAlleles and SplitMulti entrypoints moved to method objects.; Removed wasSplit from MatrixTable. Methods that require biallelics are only checked in python using the @require_biallelic decorator. They are checked every time.; Removed user-facing verify_biallelic.; Removed filter_multi in favor of filter_variants_expr('v.isBiallelic').; Renamed annotate_alleles_expr_generic => annotate_alleles_expr and annotate_alleles_expr => annotate_alleles_expr_hts, similarly for split_multi and filter_alleles.; Moved the _hts variants to Python, left duplicate Scala versions where needed.; Fixed bug in filter_alleles related to moving variants. This wasn't caught by gen-based tests because they rarely generate examples with duplicate loci.; Moved annotate_alleles tests to python. Each time we move a test to Python, an angel gets its wings.; Don't filter multi-allelics in ld_prune since it is marked as requiring biallelics. The change to not check biallelics in scala is potentially controversial. I appreciate your thoughts on that, @tpoterba.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2603
https://github.com/hail-is/hail/issues/2604:298,Availability,error,error,298,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2604
https://github.com/hail-is/hail/issues/2604:304,Integrability,message,messages,304,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2604
https://github.com/hail-is/hail/pull/2606:283,Modifiability,rewrite,rewrite,283,"@cseed @catoverdrive This is how we should do aggregators. I implemented several as examples. It's a bit repetitive, maybe `@specialized` can help here? I'm kind of afraid of it. Some open issues:. - if the result of an aggregator is missing, I can't write it into the region, maybe rewrite `result` in continuation-passing style with a missing and non missing continuation? (is that function call indirection worth avoiding an allocation of a `Some(offset)` per-aggrgator-result?). - how do I take a user supplied function, e.g. `takeBy`? I keep avoiding lambda-like constructs. Do I introduce a new binding form, like `ApplyUnaryFunAggOp`. I don't like this path, but I also think adding a `Lambda` IR that isn't a full-fledged lambda will make the compiler look annoying/ugly too. This issue is basically the continuation of me punting on how to handle lambdas. - How do y'all feel about me eliminating some type-specific aggregators that can be implemented in terms of other ones (see AggOp.scala). If y'all are happy with this, I want to solve the missingness issue, and then farm out the last few (non-lambda-taking) aggregators to the team.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2606
https://github.com/hail-is/hail/pull/2606:416,Safety,avoid,avoiding,416,"@cseed @catoverdrive This is how we should do aggregators. I implemented several as examples. It's a bit repetitive, maybe `@specialized` can help here? I'm kind of afraid of it. Some open issues:. - if the result of an aggregator is missing, I can't write it into the region, maybe rewrite `result` in continuation-passing style with a missing and non missing continuation? (is that function call indirection worth avoiding an allocation of a `Some(offset)` per-aggrgator-result?). - how do I take a user supplied function, e.g. `takeBy`? I keep avoiding lambda-like constructs. Do I introduce a new binding form, like `ApplyUnaryFunAggOp`. I don't like this path, but I also think adding a `Lambda` IR that isn't a full-fledged lambda will make the compiler look annoying/ugly too. This issue is basically the continuation of me punting on how to handle lambdas. - How do y'all feel about me eliminating some type-specific aggregators that can be implemented in terms of other ones (see AggOp.scala). If y'all are happy with this, I want to solve the missingness issue, and then farm out the last few (non-lambda-taking) aggregators to the team.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2606
https://github.com/hail-is/hail/pull/2606:547,Safety,avoid,avoiding,547,"@cseed @catoverdrive This is how we should do aggregators. I implemented several as examples. It's a bit repetitive, maybe `@specialized` can help here? I'm kind of afraid of it. Some open issues:. - if the result of an aggregator is missing, I can't write it into the region, maybe rewrite `result` in continuation-passing style with a missing and non missing continuation? (is that function call indirection worth avoiding an allocation of a `Some(offset)` per-aggrgator-result?). - how do I take a user supplied function, e.g. `takeBy`? I keep avoiding lambda-like constructs. Do I introduce a new binding form, like `ApplyUnaryFunAggOp`. I don't like this path, but I also think adding a `Lambda` IR that isn't a full-fledged lambda will make the compiler look annoying/ugly too. This issue is basically the continuation of me punting on how to handle lambdas. - How do y'all feel about me eliminating some type-specific aggregators that can be implemented in terms of other ones (see AggOp.scala). If y'all are happy with this, I want to solve the missingness issue, and then farm out the last few (non-lambda-taking) aggregators to the team.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2606
https://github.com/hail-is/hail/issues/2612:298,Availability,error,error,298,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2612
https://github.com/hail-is/hail/issues/2612:304,Integrability,message,messages,304,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2612
https://github.com/hail-is/hail/issues/2613:298,Availability,error,error,298,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2613
https://github.com/hail-is/hail/issues/2613:304,Integrability,message,messages,304,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2613
https://github.com/hail-is/hail/issues/2634:16,Availability,error,error,16,Should throw an error on gt>2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2634
https://github.com/hail-is/hail/issues/2640:32,Availability,down,down,32,"This is going to cause problems down the line. Here's an example where inappropriate use of parentheses does something unexpected:; ```python; In [39]: x = functions.capture(5). In [40]: eval_expr((3 < x) & x > 5); Out[40]: True. In [41]: eval_expr((3 < x) & (x > 5)); Out[41]: False; ```; I'm actually not sure what the order of operations is in [40], but it's clearly wrong.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2640
https://github.com/hail-is/hail/issues/2640:362,Usability,clear,clearly,362,"This is going to cause problems down the line. Here's an example where inappropriate use of parentheses does something unexpected:; ```python; In [39]: x = functions.capture(5). In [40]: eval_expr((3 < x) & x > 5); Out[40]: True. In [41]: eval_expr((3 < x) & (x > 5)); Out[41]: False; ```; I'm actually not sure what the order of operations is in [40], but it's clearly wrong.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2640
https://github.com/hail-is/hail/issues/2641:88,Integrability,interface,interface,88,"Missing modulo and floor div. I think these are implemented in the engine, just not the interface.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2641
https://github.com/hail-is/hail/pull/2647:64,Integrability,rout,route,64,"This adds filter, filterCols, and filterRows to BlockMatrix, en route to moving kinship and LD matrix to from IRM to BlockMatrix. For example, filtering out rows/columns from a kinship matrix (e.g. to remove samples with missing covariate data); or filtering an LD matrix (e.g. to perform Leave-One-Chromosome-Out LMM analysis). Since the number of tasks equals the number of resulting partitions (blocks), these functions are suited to filtering out a small to medium subset (say, throwing out a few rows and columns, or half of the rows and columns) and may fail when filtering all but a small number of rows and columns due to all the blocks being sent to one executor. In the latter case, one can do better by writing the result of filterCols, then reading and applying filterRows. Testing locally, on a 16384 * 16384 matrix (block size 4096, 16 partitions, 128MB each), when only filtering columns (or rows), filterCols (or filterRows) tends to be faster than filter (likely due to less copying), and filterCols tends to be a bit faster than filterRows (which I think is due to lack of transpose and esp. better striding). When filtering both columns and rows, filter tends to be fastest but filter_columns followed by filter_rows is often comparable. It may be that the cost of an additional copy in the first step is offset by smaller strides resulting better caching in the second step. For example, filtering out every 31st row and column:; ```; filter; [9.550655126571655, 9.62321400642395]; filterCols.filterRows; [12.190248966217041, 12.07064700126648]; filterRows.filterCols; [14.239542007446289, 15.601837158203125]; ```; Filtering out every row and column with 0.5 probability, resulting in 4 partitions:; ```; filter; [4.625396966934204, 4.427164077758789, 4.435616970062256]; filterCols.filterRows; [4.647814035415649, 4.610292911529541, 4.655405044555664]; filterRows.filterCols; [4.866307973861694, 5.073392868041992, 5.470307111740112]; ```. On GCP, with 32 core highmem master and",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2647
https://github.com/hail-is/hail/pull/2647:281,Performance,perform,perform,281,"This adds filter, filterCols, and filterRows to BlockMatrix, en route to moving kinship and LD matrix to from IRM to BlockMatrix. For example, filtering out rows/columns from a kinship matrix (e.g. to remove samples with missing covariate data); or filtering an LD matrix (e.g. to perform Leave-One-Chromosome-Out LMM analysis). Since the number of tasks equals the number of resulting partitions (blocks), these functions are suited to filtering out a small to medium subset (say, throwing out a few rows and columns, or half of the rows and columns) and may fail when filtering all but a small number of rows and columns due to all the blocks being sent to one executor. In the latter case, one can do better by writing the result of filterCols, then reading and applying filterRows. Testing locally, on a 16384 * 16384 matrix (block size 4096, 16 partitions, 128MB each), when only filtering columns (or rows), filterCols (or filterRows) tends to be faster than filter (likely due to less copying), and filterCols tends to be a bit faster than filterRows (which I think is due to lack of transpose and esp. better striding). When filtering both columns and rows, filter tends to be fastest but filter_columns followed by filter_rows is often comparable. It may be that the cost of an additional copy in the first step is offset by smaller strides resulting better caching in the second step. For example, filtering out every 31st row and column:; ```; filter; [9.550655126571655, 9.62321400642395]; filterCols.filterRows; [12.190248966217041, 12.07064700126648]; filterRows.filterCols; [14.239542007446289, 15.601837158203125]; ```; Filtering out every row and column with 0.5 probability, resulting in 4 partitions:; ```; filter; [4.625396966934204, 4.427164077758789, 4.435616970062256]; filterCols.filterRows; [4.647814035415649, 4.610292911529541, 4.655405044555664]; filterRows.filterCols; [4.866307973861694, 5.073392868041992, 5.470307111740112]; ```. On GCP, with 32 core highmem master and",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2647
https://github.com/hail-is/hail/pull/2647:786,Testability,Test,Testing,786,"This adds filter, filterCols, and filterRows to BlockMatrix, en route to moving kinship and LD matrix to from IRM to BlockMatrix. For example, filtering out rows/columns from a kinship matrix (e.g. to remove samples with missing covariate data); or filtering an LD matrix (e.g. to perform Leave-One-Chromosome-Out LMM analysis). Since the number of tasks equals the number of resulting partitions (blocks), these functions are suited to filtering out a small to medium subset (say, throwing out a few rows and columns, or half of the rows and columns) and may fail when filtering all but a small number of rows and columns due to all the blocks being sent to one executor. In the latter case, one can do better by writing the result of filterCols, then reading and applying filterRows. Testing locally, on a 16384 * 16384 matrix (block size 4096, 16 partitions, 128MB each), when only filtering columns (or rows), filterCols (or filterRows) tends to be faster than filter (likely due to less copying), and filterCols tends to be a bit faster than filterRows (which I think is due to lack of transpose and esp. better striding). When filtering both columns and rows, filter tends to be fastest but filter_columns followed by filter_rows is often comparable. It may be that the cost of an additional copy in the first step is offset by smaller strides resulting better caching in the second step. For example, filtering out every 31st row and column:; ```; filter; [9.550655126571655, 9.62321400642395]; filterCols.filterRows; [12.190248966217041, 12.07064700126648]; filterRows.filterCols; [14.239542007446289, 15.601837158203125]; ```; Filtering out every row and column with 0.5 probability, resulting in 4 partitions:; ```; filter; [4.625396966934204, 4.427164077758789, 4.435616970062256]; filterCols.filterRows; [4.647814035415649, 4.610292911529541, 4.655405044555664]; filterRows.filterCols; [4.866307973861694, 5.073392868041992, 5.470307111740112]; ```. On GCP, with 32 core highmem master and",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2647
https://github.com/hail-is/hail/pull/2647:2738,Testability,log,log,2738,"625396966934204, 4.427164077758789, 4.435616970062256]; filterCols.filterRows; [4.647814035415649, 4.610292911529541, 4.655405044555664]; filterRows.filterCols; [4.866307973861694, 5.073392868041992, 5.470307111740112]; ```. On GCP, with 32 core highmem master and 176 cores (2 workers + 20 prempt, 8 core standard), reading and writing a 100k by 100k matrix M with 4096 block size (625 partitions, each 128MB, 80GB total) takes about 25s. Even with this 3.5 to 1 ratio of partitions to cores, there is a lot of volatility running the same operations multiple times. I've documented some experiments below, the main conclusion is that these functions should work well for the use cases above and while those are being developed and we experiment more on real data, I think it makes sense to include all three: filterCols, filterRows (implemented via filterCols and transpose), and filter. ```; from hail import *; from hail.linalg import *; import timeit; from random import randint. hc = HailContext(log='/hail/hail.log'). def time(name, f, number=1, repeat=3):; print('running:', name); d = timeit.repeat(f, number=number, repeat=repeat); print(name, d). size = 100000; block_size = 4096; mod = 31; alll = range(0, size). keep = filter(lambda i: i % mod != 0, alll); keepS = filter(lambda i: i % mod == 0, alll); keepR = filter(lambda i: randint(0,1) == 0, alll). pathC = 'gs://jbloom/block_filter/C.bm'; pathCfilt = 'gs://jbloom/block_filter/Cfilt.bm'. # 29s; def writeC():; BlockMatrix.random(size, size, block_size).write(pathC). # 25s; def readwrite():; BlockMatrix.read('gs://jbloom/block_filter/C.bm').write(pathC). ## keep all; # ('filtnone', [50.187114000320435, 37.279563903808594, 39.105873823165894]); # ('filtnone2', [42.968636989593506, 90.15687894821167, 53.434531927108765]); # ('filtnone3', [38.40539002418518, 106.12774705886841, 51.939454078674316]); def filtnone():; C = BlockMatrix.read(pathC); C.filter(alll, alll).write(pathCfilt); def filtnone2():; C = BlockMatrix.read(pathC",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2647
https://github.com/hail-is/hail/pull/2647:2754,Testability,log,log,2754,"27164077758789, 4.435616970062256]; filterCols.filterRows; [4.647814035415649, 4.610292911529541, 4.655405044555664]; filterRows.filterCols; [4.866307973861694, 5.073392868041992, 5.470307111740112]; ```. On GCP, with 32 core highmem master and 176 cores (2 workers + 20 prempt, 8 core standard), reading and writing a 100k by 100k matrix M with 4096 block size (625 partitions, each 128MB, 80GB total) takes about 25s. Even with this 3.5 to 1 ratio of partitions to cores, there is a lot of volatility running the same operations multiple times. I've documented some experiments below, the main conclusion is that these functions should work well for the use cases above and while those are being developed and we experiment more on real data, I think it makes sense to include all three: filterCols, filterRows (implemented via filterCols and transpose), and filter. ```; from hail import *; from hail.linalg import *; import timeit; from random import randint. hc = HailContext(log='/hail/hail.log'). def time(name, f, number=1, repeat=3):; print('running:', name); d = timeit.repeat(f, number=number, repeat=repeat); print(name, d). size = 100000; block_size = 4096; mod = 31; alll = range(0, size). keep = filter(lambda i: i % mod != 0, alll); keepS = filter(lambda i: i % mod == 0, alll); keepR = filter(lambda i: randint(0,1) == 0, alll). pathC = 'gs://jbloom/block_filter/C.bm'; pathCfilt = 'gs://jbloom/block_filter/Cfilt.bm'. # 29s; def writeC():; BlockMatrix.random(size, size, block_size).write(pathC). # 25s; def readwrite():; BlockMatrix.read('gs://jbloom/block_filter/C.bm').write(pathC). ## keep all; # ('filtnone', [50.187114000320435, 37.279563903808594, 39.105873823165894]); # ('filtnone2', [42.968636989593506, 90.15687894821167, 53.434531927108765]); # ('filtnone3', [38.40539002418518, 106.12774705886841, 51.939454078674316]); def filtnone():; C = BlockMatrix.read(pathC); C.filter(alll, alll).write(pathCfilt); def filtnone2():; C = BlockMatrix.read(pathC); C.filter_rows(all",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2647
https://github.com/hail-is/hail/issues/2653:436,Availability,error,error,436,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-45429b1. ### What you did:; ```; eval_expr(functions.cond(functions.capture(True), 'T', 'F') + functions.cond(functions.capture(True), 'T', 'F')); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-20-3b0a2ce317c2> in <module>(); ----> 1 eval_expr(functions.cond(functions.capture(True), 'T', 'F') + functions.cond(functions.capture(True), 'T', 'F')). <decorator-gen-365> in eval_expr(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr(expression); 3576 Result of evaluating `expression`.; 3577 """"""; -> 3578 return eval_expr_typed(expression)[0]; 3579; 3580. <decorator-gen-366> in eval_expr_typed(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2653
https://github.com/hail-is/hail/issues/2653:2207,Availability,Error,Error,2207,"ion.py in eval_expr(expression); 3576 Result of evaluating `expression`.; 3577 """"""; -> 3578 return eval_expr_typed(expression)[0]; 3579; 3580. <decorator-gen-366> in eval_expr_typed(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^. Java stack trace:; is.hail.utils.HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2653
https://github.com/hail-is/hail/issues/2653:2347,Availability,error,error,2347,". /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^. Java stack trace:; is.hail.utils.HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.Reflec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2653
https://github.com/hail-is/hail/issues/2653:2657,Availability,Error,ErrorHandling,2657,"/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^. Java stack trace:; is.hail.utils.HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2653
https://github.com/hail-is/hail/issues/2653:2683,Availability,Error,ErrorHandling,2683,"in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^. Java stack trace:; is.hail.utils.HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2653
https://github.com/hail-is/hail/issues/2653:2789,Availability,error,error,2789,"xpr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^. Java stack trace:; is.hail.utils.HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-45429b1; Error summary: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" el",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2653
https://github.com/hail-is/hail/issues/2653:3711,Availability,Error,Error,3711,"dcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^. Java stack trace:; is.hail.utils.HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-45429b1; Error summary: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2653
https://github.com/hail-is/hail/issues/2653:442,Integrability,message,messages,442,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-45429b1. ### What you did:; ```; eval_expr(functions.cond(functions.capture(True), 'T', 'F') + functions.cond(functions.capture(True), 'T', 'F')); ```. ### What went wrong (all error messages here, including the full java stack trace):. ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-20-3b0a2ce317c2> in <module>(); ----> 1 eval_expr(functions.cond(functions.capture(True), 'T', 'F') + functions.cond(functions.capture(True), 'T', 'F')). <decorator-gen-365> in eval_expr(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr(expression); 3576 Result of evaluating `expression`.; 3577 """"""; -> 3578 return eval_expr_typed(expression)[0]; 3579; 3580. <decorator-gen-366> in eval_expr_typed(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2653
https://github.com/hail-is/hail/issues/2653:2289,Integrability,protocol,protocol,2289,"3579; 3580. <decorator-gen-366> in eval_expr_typed(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^. Java stack trace:; is.hail.utils.HailException: `(' expected but `i' found; <input>:1:(if (true) ""T"" else ""F"" + if (true) ""T"" else ""F""); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvo",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2653
https://github.com/hail-is/hail/issues/2655:380,Availability,error,error,380,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-45429b1. ### What you did:; ```; x = functions.capture(5); y = -1.2e-7; eval_expr(x * y); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-53-69c81b591366> in <module>(); ----> 1 eval_expr(x * y). <decorator-gen-365> in eval_expr(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr(expression); 3576 Result of evaluating `expression`.; 3577 """"""; -> 3578 return eval_expr_typed(expression)[0]; 3579; 3580. <decorator-gen-366> in eval_expr_typed(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava st",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2655
https://github.com/hail-is/hail/issues/2655:2055,Availability,Error,Error,2055,"ion.py in eval_expr(expression); 3576 Result of evaluating `expression`.; 3577 """"""; -> 3578 return eval_expr_typed(expression)[0]; 3579; 3580. <decorator-gen-366> in eval_expr_typed(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2655
https://github.com/hail-is/hail/issues/2655:2195,Availability,error,error,2195,". /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2655
https://github.com/hail-is/hail/issues/2655:2435,Availability,Error,ErrorHandling,2435,"oop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2655
https://github.com/hail-is/hail/issues/2655:2461,Availability,Error,ErrorHandling,2461,"tadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2655
https://github.com/hail-is/hail/issues/2655:2567,Availability,error,error,2567,"ped(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-45429b1; Error summary: HailException: `)' expected but `e' found; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2655
https://github.com/hail-is/hail/issues/2655:3489,Availability,Error,Error,3489,"ped(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-45429b1; Error summary: HailException: `)' expected but `e' found; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2655
https://github.com/hail-is/hail/issues/2655:386,Integrability,message,messages,386,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-45429b1. ### What you did:; ```; x = functions.capture(5); y = -1.2e-7; eval_expr(x * y); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-53-69c81b591366> in <module>(); ----> 1 eval_expr(x * y). <decorator-gen-365> in eval_expr(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr(expression); 3576 Result of evaluating `expression`.; 3577 """"""; -> 3578 return eval_expr_typed(expression)[0]; 3579; 3580. <decorator-gen-366> in eval_expr_typed(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava st",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2655
https://github.com/hail-is/hail/issues/2655:2137,Integrability,protocol,protocol,2137,"3579; 3580. <decorator-gen-366> in eval_expr_typed(expression). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in args_to_expr(func, *args); 176 @decorator; 177 def args_to_expr(func, *args):; --> 178 return func(*(to_expr(a) for a in args)); 179; 180. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/expr/expression.py in eval_expr_typed(expression); 3612 if len(expression._joins) > 0:; 3613 raise ExpressionException(""'eval_expr' methods do not support joins or broadcasts""); -> 3614 r, t = Env.hc().eval_expr_typed(expression._ast.to_hql()); 3615 return r, t; 3616. <decorator-gen-1049> in eval_expr_typed(self, expr). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-45429b164934.zip/hail/utils/java.py in handle_py4j(func, *args, **kwargs); 153 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 154 'Hail version: %s\n'; --> 155 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)); 156 except py4j.protocol.Py4JError as e:; 157 if e.args[0].startswith('An error occurred while calling'):. FatalError: HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^. Java stack trace:; is.hail.utils.HailException: `)' expected but `e' found; <input>:1:(5 * -1.2e-07); ^; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:27); 	at is.hail.expr.ParserUtils$.error(Parser.scala:32); 	at is.hail.expr.RichParser.parse(Parser.scala:16); 	at is.hail.expr.Parser$.parseExpr(Parser.scala:85); 	at is.hail.HailContext.eval(HailContext.scala:613); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngi",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2655
https://github.com/hail-is/hail/pull/2656:72,Integrability,depend,dependencies,72,"passes all tests, builds package locally; ripped out (now unused) mongo dependencies",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2656
https://github.com/hail-is/hail/pull/2656:11,Testability,test,tests,11,"passes all tests, builds package locally; ripped out (now unused) mongo dependencies",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2656
https://github.com/hail-is/hail/pull/2661:11,Integrability,rout,route,11,This is en route to removing IndexedRowMatrix and ExportableMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2661
https://github.com/hail-is/hail/pull/2665:0,Integrability,Depend,Depends,0,"Depends on (and includes changes in) #2661. This PR adds:; RowPartitioner and RowPartitionerSuite; RowMatrix.readBlockMatrix and tests in RowMatrixSuite; forceRowMajor parameter to BlockMatrix.write, included in tests; forceRowMajor to richDenseMatrixDouble. Currently I use skipBytes (which reads and throws away) to advance the DataInputStream. @cseed do you have advice on how to use seek instead?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2665
https://github.com/hail-is/hail/pull/2665:129,Testability,test,tests,129,"Depends on (and includes changes in) #2661. This PR adds:; RowPartitioner and RowPartitionerSuite; RowMatrix.readBlockMatrix and tests in RowMatrixSuite; forceRowMajor parameter to BlockMatrix.write, included in tests; forceRowMajor to richDenseMatrixDouble. Currently I use skipBytes (which reads and throws away) to advance the DataInputStream. @cseed do you have advice on how to use seek instead?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2665
https://github.com/hail-is/hail/pull/2665:212,Testability,test,tests,212,"Depends on (and includes changes in) #2661. This PR adds:; RowPartitioner and RowPartitionerSuite; RowMatrix.readBlockMatrix and tests in RowMatrixSuite; forceRowMajor parameter to BlockMatrix.write, included in tests; forceRowMajor to richDenseMatrixDouble. Currently I use skipBytes (which reads and throws away) to advance the DataInputStream. @cseed do you have advice on how to use seek instead?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2665
https://github.com/hail-is/hail/issues/2666:350,Availability,error,error,350,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-6191d4c. ### What you did: vds.vep(""/vep/vep-gcloud.properties”). ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/2f781c88-64f1-4345-a2b9-433c49d5a099/script_to_submit.py"", line 8, in <module>; vdsvep = vds.vep(""/vep/vep-gcloud.properties""); File ""<decorator-gen-878>"", line 2, in vep; File ""/tmp/2f781c88-64f1-4345-a2b9-433c49d5a099/hail-devel-6191d4ce69aa.zip/hail/utils/java.py"", line 167, in handle_py4j; hail.utils.java.FatalError: Can't zip RDDs with unequal numbers of partitions: List(16979, 16992). Java stack trace:; org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:57); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	 at scala.Option.getOrElse(Option.scala:121); 	 at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	 at scala.Option.getOrElse(Option.scala:121); 	 at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	 at is.hail.sparkextras.OrderedRDD$.coerce(OrderedRDD.scala:58); 	 at is.hail.sparkextras.OrderedRDD$.apply(OrderedRDD.scala:48); 	 at is.hail.utils.richUtils.RichPairRDD$.toOrderedRDD$extension1(RichPairRDD.scala:44); 	 at is.hail.variant.MatrixTable$.fromLegacy(MatrixTable.scala:84); 	 at is.hail.variant.MatrixTable.copyLegacy(MatrixTable.scala:2019); 	 at is.hail.methods.VEP$.annotate(VEP.scala:407); 	 at is.hail.methods.VEP$.apply(VEP.scala:41",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2666
https://github.com/hail-is/hail/issues/2666:2799,Availability,Error,Error,2799,"ark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:57); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	 at scala.Option.getOrElse(Option.scala:121); 	 at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	 at scala.Option.getOrElse(Option.scala:121); 	 at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	 at is.hail.sparkextras.OrderedRDD$.coerce(OrderedRDD.scala:58); 	 at is.hail.sparkextras.OrderedRDD$.apply(OrderedRDD.scala:48); 	 at is.hail.utils.richUtils.RichPairRDD$.toOrderedRDD$extension1(RichPairRDD.scala:44); 	 at is.hail.variant.MatrixTable$.fromLegacy(MatrixTable.scala:84); 	 at is.hail.variant.MatrixTable.copyLegacy(MatrixTable.scala:2019); 	 at is.hail.methods.VEP$.annotate(VEP.scala:407); 	 at is.hail.methods.VEP$.apply(VEP.scala:412); 	 at is.hail.methods.VEP.apply(VEP.scala); 	 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	 at java.lang.reflect.Method.invoke(Method.java:498); 	 at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	 at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	 at py4j.Gateway.invoke(Gateway.java:280); 	 at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	 at py4j.commands.CallCommand.execute(CallCommand.java:79); 	 at py4j.GatewayConnection.run(GatewayConnection.java:214); 	 at java.lang.Thread.run(Thread.java:748); Hail version: devel-6191d4c; Error summary: Can't zip RDDs with unequal numbers of partitions: List(16979, 16992)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2666
https://github.com/hail-is/hail/issues/2666:356,Integrability,message,messages,356,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: devel-6191d4c. ### What you did: vds.vep(""/vep/vep-gcloud.properties”). ### What went wrong (all error messages here, including the full java stack trace):. Traceback (most recent call last):; File ""/tmp/2f781c88-64f1-4345-a2b9-433c49d5a099/script_to_submit.py"", line 8, in <module>; vdsvep = vds.vep(""/vep/vep-gcloud.properties""); File ""<decorator-gen-878>"", line 2, in vep; File ""/tmp/2f781c88-64f1-4345-a2b9-433c49d5a099/hail-devel-6191d4ce69aa.zip/hail/utils/java.py"", line 167, in handle_py4j; hail.utils.java.FatalError: Can't zip RDDs with unequal numbers of partitions: List(16979, 16992). Java stack trace:; org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:57); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	 at scala.Option.getOrElse(Option.scala:121); 	 at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	 at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); 	 at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); 	 at scala.Option.getOrElse(Option.scala:121); 	 at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); 	 at is.hail.sparkextras.OrderedRDD$.coerce(OrderedRDD.scala:58); 	 at is.hail.sparkextras.OrderedRDD$.apply(OrderedRDD.scala:48); 	 at is.hail.utils.richUtils.RichPairRDD$.toOrderedRDD$extension1(RichPairRDD.scala:44); 	 at is.hail.variant.MatrixTable$.fromLegacy(MatrixTable.scala:84); 	 at is.hail.variant.MatrixTable.copyLegacy(MatrixTable.scala:2019); 	 at is.hail.methods.VEP$.annotate(VEP.scala:407); 	 at is.hail.methods.VEP$.apply(VEP.scala:41",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2666
https://github.com/hail-is/hail/pull/2671:132,Integrability,depend,depend,132,"I'm curious what people think about this. Did this on one of my flights. Moved all the types to a separate package (so that you can depend on hail types without depending on the AST/IR). When doing this with IntelliJ, I had to move each class to a separate file. This doesn't seem terrible actually because `Type.scala` is rather large and makes IntelliJ angry. On the other hand, we can't seal `Type` if its subclasses are not in the same file. cc: @tpoterba @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2671
https://github.com/hail-is/hail/pull/2671:161,Integrability,depend,depending,161,"I'm curious what people think about this. Did this on one of my flights. Moved all the types to a separate package (so that you can depend on hail types without depending on the AST/IR). When doing this with IntelliJ, I had to move each class to a separate file. This doesn't seem terrible actually because `Type.scala` is rather large and makes IntelliJ angry. On the other hand, we can't seal `Type` if its subclasses are not in the same file. cc: @tpoterba @cseed",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2671
https://github.com/hail-is/hail/pull/2675:169,Modifiability,Extend,ExtendedOrdering,169,"I want to make ordering a val instead of a def so it doesn't have to be recreated, but that means I need a way to pass missingGreatest as a parameter in compare. Hence, ExtendedOrdering. There are variants of compare, etc. that default missingGreatest = true since that is the default. All the overrides are necessary because lt, etc. can't always be determined from compare (e.g. nan compares false with everything for all comparison operations). This is used by a future PR for generic interval comparison.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2675
https://github.com/hail-is/hail/pull/2685:0,Integrability,Depend,Depends,0,Depends on #2682 .,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2685
https://github.com/hail-is/hail/pull/2688:100,Testability,test,testing,100,"@cseed I think this should clean up the entire temp directory when we exit, although I will do some testing. Anything we create in that temp directory (or using create_temp_file) should get cleaned up automatically.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2688
https://github.com/hail-is/hail/issues/2691:111,Availability,Error,Error,111,"```; In [10]: eval_expr_typed(functions.capture([1,2,3]).map(lambda x: x.to_int64()).append([1.0, 2.0, 3.0])); Error summary: HailException: No function found with name `append' and argument types (Array[Int64], Array[Float64]); <input>:1:[ 1, 2, 3 ].map(__uid_2 => `__uid_2`.toInt64()).append([ 1.0, 2.0, 3.0 ]); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2691
https://github.com/hail-is/hail/issues/2694:55,Integrability,message,message,55,"Don't use typecheck decorators here, we need a clearer message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2694
https://github.com/hail-is/hail/issues/2694:47,Usability,clear,clearer,47,"Don't use typecheck decorators here, we need a clearer message.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2694
https://github.com/hail-is/hail/pull/2696:145,Integrability,interface,interface,145,"I couldn't find any tests of `sample_variants` in either the Python or Scala suites, nor could I think of any good tests. But it's such a direct interface to `RDD.sample`, that seems okay to me. I can add tests if anyone has suggestions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2696
https://github.com/hail-is/hail/pull/2696:20,Testability,test,tests,20,"I couldn't find any tests of `sample_variants` in either the Python or Scala suites, nor could I think of any good tests. But it's such a direct interface to `RDD.sample`, that seems okay to me. I can add tests if anyone has suggestions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2696
https://github.com/hail-is/hail/pull/2696:115,Testability,test,tests,115,"I couldn't find any tests of `sample_variants` in either the Python or Scala suites, nor could I think of any good tests. But it's such a direct interface to `RDD.sample`, that seems okay to me. I can add tests if anyone has suggestions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2696
https://github.com/hail-is/hail/pull/2696:205,Testability,test,tests,205,"I couldn't find any tests of `sample_variants` in either the Python or Scala suites, nor could I think of any good tests. But it's such a direct interface to `RDD.sample`, that seems okay to me. I can add tests if anyone has suggestions.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2696
https://github.com/hail-is/hail/pull/2697:68,Testability,test,tests,68,@tpoterba this isn't quite ready yet--I'm still struggling with the tests failing--but if you wanted to take a quick look I think I finished the other parts. If you want me to break this out into multiple PRs let me know.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2697
https://github.com/hail-is/hail/pull/2698:321,Modifiability,variab,variable,321,"This is small addition on top of #2665, only review last commit if that has yet to go in. Here's the logic/plan: rows refers to actual rows as in RowMatrix or MatrixTable. nRows refers to the number of rows, as already used in GridPartitioner, RowMatrix, etc. Breeze uses mat.rows for nRows, but we'll still use nRows as variable name of number of rows in Hail's linear algebra. In Python, we'll use num_rows and num_cols.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2698
https://github.com/hail-is/hail/pull/2698:101,Testability,log,logic,101,"This is small addition on top of #2665, only review last commit if that has yet to go in. Here's the logic/plan: rows refers to actual rows as in RowMatrix or MatrixTable. nRows refers to the number of rows, as already used in GridPartitioner, RowMatrix, etc. Breeze uses mat.rows for nRows, but we'll still use nRows as variable name of number of rows in Hail's linear algebra. In Python, we'll use num_rows and num_cols.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2698
https://github.com/hail-is/hail/pull/2699:9,Deployability,patch,patches,9,"Previous patches made the interval _type_ generic (e.g. Interval[T] instead of just Interval which implied interval of locus). This patch makes the intervals themselves generic. The interval doesn't carry the type (it's just a container for the start, end) so various interval operations need the ordering on the type that the interval is over to be passed in. Then everything else is follows from this. The next PR makes the intervals generic on the Python side. Along the way, I allows intervals to be improper, e.g., [7, 5), which are effectively empty. Note, intervals compare for equality if the endpoints are equal, so [7, 5) != [8, 5) even though they are both empty.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2699
https://github.com/hail-is/hail/pull/2699:132,Deployability,patch,patch,132,"Previous patches made the interval _type_ generic (e.g. Interval[T] instead of just Interval which implied interval of locus). This patch makes the intervals themselves generic. The interval doesn't carry the type (it's just a container for the start, end) so various interval operations need the ordering on the type that the interval is over to be passed in. Then everything else is follows from this. The next PR makes the intervals generic on the Python side. Along the way, I allows intervals to be improper, e.g., [7, 5), which are effectively empty. Note, intervals compare for equality if the endpoints are equal, so [7, 5) != [8, 5) even though they are both empty.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2699
https://github.com/hail-is/hail/pull/2705:110,Modifiability,variab,variable,110,"The type of this RDD is rather complicated because we prepend a String to; an RDD of a universally quantified variable. `writeTextTable` works with any subtypes of AnyRef because it only requires; that the object defines `toString`. Ergo, we don't actually need to know; anything about the type parameter of the RDD. The explicit annotation; that I've added prevents Scala from inferring a complex type that cannot; be expressed without `forSome`. Such types trigger a warning from the; Scala compiler.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2705
https://github.com/hail-is/hail/pull/2712:125,Testability,test,test,125,Instead of building variant map (which is actually broken because variants are not necessarily unique!); Move MT.variants to test code. This eliminates another use of MT.rdd.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2712
https://github.com/hail-is/hail/pull/2714:214,Integrability,depend,depending,214,"renamed fromKeyTable => fromTable.; made generic (single key until we have compound keys in MT). The two FIXMEs will use infrastructure from other, pending PRs. To avoid stacking, I'll rebase one side or the other depending on which goes in first.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2714
https://github.com/hail-is/hail/pull/2714:164,Safety,avoid,avoid,164,"renamed fromKeyTable => fromTable.; made generic (single key until we have compound keys in MT). The two FIXMEs will use infrastructure from other, pending PRs. To avoid stacking, I'll rebase one side or the other depending on which goes in first.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2714
https://github.com/hail-is/hail/pull/2718:1634,Security,expose,exposed,1634,"Matrix can be promoted to a KeyedBlockMatrix even if it's dimensions exceed Int.MaxValue; the simple rule is that you can't set keys on a dimension that is too large. This is convenient if you have a matrix where one dimension is huge but you still want keys on the other dimension. Checking keys helps ensure correctness, and I think the ability to set/drop keys on keyed matrices should be useful for linear algebra where a matrix operand comes un-keyed or you don't care about the keys on that operand. This key persistence is also natural when thinking about operations that add (unkeyed) scalars or vectors to keyed matrices (We'd later add optionally-keyed vectors as well, where keys are checked in vector addition, matrix/vector mult, vectorAddToEveryRow, etc). Keys are stored and checked on master, so for large dimensions users may want to rekey with simpler keys, via a map on the python side or with just indices. For simplicity, and since there's basically no additional overhead using an unkeyed KeyedBlockMatrix versus its underlying BlockMatrix, I think we should consider only having (optionally) keyed matrices exposed on the Python side (so a Python BlockMatrix is a Scala KeyedBlockMatrix, and you can do linear algebra numpy-style by just having no keys set). I plan to add writeKeyedBlockMatrix to MatrixTable in next step, with parameters to on whether to retain the keys (e.g., key_rows = true). On the Python side, this could then replace write_block_matrix rather than being in addition to it. Later, LocalMatrix and RowMatrix would follow the same pattern of optionally keyed versions in Scala, with a common Matrix and KeyedMatrix abstraction. And in Python users would just have Matrix backed by KeyedMatrix on the Scala side. PS. In the filters, I switched to names keepRows and keepCols in KeyedBlockMatrix instead of rowsToKeep and colsToKeep, so the changes in BlockMatrix and BlockMatrixSuite are just renaming for consistency (as well as removing one unused line).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2718
https://github.com/hail-is/hail/pull/2718:598,Usability,simpl,simple,598,"KeyedBlockMatrix is a BlockMatrix with optional row and column Keys. I chose to make them optional when considering the need to constantly check key agreement. Here if both operands have row keys in a binary operation like add, then the keys are checked for agreement. The result has rowKeys iff at least one operand has rowKeys (and they agree). Similar rules apply to column keys and other operations where sensible. When promoting a BlockMatrix to a KeyedBlockMatrix, the keys are set to None. A BlockMatrix can be promoted to a KeyedBlockMatrix even if it's dimensions exceed Int.MaxValue; the simple rule is that you can't set keys on a dimension that is too large. This is convenient if you have a matrix where one dimension is huge but you still want keys on the other dimension. Checking keys helps ensure correctness, and I think the ability to set/drop keys on keyed matrices should be useful for linear algebra where a matrix operand comes un-keyed or you don't care about the keys on that operand. This key persistence is also natural when thinking about operations that add (unkeyed) scalars or vectors to keyed matrices (We'd later add optionally-keyed vectors as well, where keys are checked in vector addition, matrix/vector mult, vectorAddToEveryRow, etc). Keys are stored and checked on master, so for large dimensions users may want to rekey with simpler keys, via a map on the python side or with just indices. For simplicity, and since there's basically no additional overhead using an unkeyed KeyedBlockMatrix versus its underlying BlockMatrix, I think we should consider only having (optionally) keyed matrices exposed on the Python side (so a Python BlockMatrix is a Scala KeyedBlockMatrix, and you can do linear algebra numpy-style by just having no keys set). I plan to add writeKeyedBlockMatrix to MatrixTable in next step, with parameters to on whether to retain the keys (e.g., key_rows = true). On the Python side, this could then replace write_block_matrix rather than b",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2718
https://github.com/hail-is/hail/pull/2718:1366,Usability,simpl,simpler,1366,"lar rules apply to column keys and other operations where sensible. When promoting a BlockMatrix to a KeyedBlockMatrix, the keys are set to None. A BlockMatrix can be promoted to a KeyedBlockMatrix even if it's dimensions exceed Int.MaxValue; the simple rule is that you can't set keys on a dimension that is too large. This is convenient if you have a matrix where one dimension is huge but you still want keys on the other dimension. Checking keys helps ensure correctness, and I think the ability to set/drop keys on keyed matrices should be useful for linear algebra where a matrix operand comes un-keyed or you don't care about the keys on that operand. This key persistence is also natural when thinking about operations that add (unkeyed) scalars or vectors to keyed matrices (We'd later add optionally-keyed vectors as well, where keys are checked in vector addition, matrix/vector mult, vectorAddToEveryRow, etc). Keys are stored and checked on master, so for large dimensions users may want to rekey with simpler keys, via a map on the python side or with just indices. For simplicity, and since there's basically no additional overhead using an unkeyed KeyedBlockMatrix versus its underlying BlockMatrix, I think we should consider only having (optionally) keyed matrices exposed on the Python side (so a Python BlockMatrix is a Scala KeyedBlockMatrix, and you can do linear algebra numpy-style by just having no keys set). I plan to add writeKeyedBlockMatrix to MatrixTable in next step, with parameters to on whether to retain the keys (e.g., key_rows = true). On the Python side, this could then replace write_block_matrix rather than being in addition to it. Later, LocalMatrix and RowMatrix would follow the same pattern of optionally keyed versions in Scala, with a common Matrix and KeyedMatrix abstraction. And in Python users would just have Matrix backed by KeyedMatrix on the Scala side. PS. In the filters, I switched to names keepRows and keepCols in KeyedBlockMatrix instead o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2718
https://github.com/hail-is/hail/pull/2718:1435,Usability,simpl,simplicity,1435,"Matrix can be promoted to a KeyedBlockMatrix even if it's dimensions exceed Int.MaxValue; the simple rule is that you can't set keys on a dimension that is too large. This is convenient if you have a matrix where one dimension is huge but you still want keys on the other dimension. Checking keys helps ensure correctness, and I think the ability to set/drop keys on keyed matrices should be useful for linear algebra where a matrix operand comes un-keyed or you don't care about the keys on that operand. This key persistence is also natural when thinking about operations that add (unkeyed) scalars or vectors to keyed matrices (We'd later add optionally-keyed vectors as well, where keys are checked in vector addition, matrix/vector mult, vectorAddToEveryRow, etc). Keys are stored and checked on master, so for large dimensions users may want to rekey with simpler keys, via a map on the python side or with just indices. For simplicity, and since there's basically no additional overhead using an unkeyed KeyedBlockMatrix versus its underlying BlockMatrix, I think we should consider only having (optionally) keyed matrices exposed on the Python side (so a Python BlockMatrix is a Scala KeyedBlockMatrix, and you can do linear algebra numpy-style by just having no keys set). I plan to add writeKeyedBlockMatrix to MatrixTable in next step, with parameters to on whether to retain the keys (e.g., key_rows = true). On the Python side, this could then replace write_block_matrix rather than being in addition to it. Later, LocalMatrix and RowMatrix would follow the same pattern of optionally keyed versions in Scala, with a common Matrix and KeyedMatrix abstraction. And in Python users would just have Matrix backed by KeyedMatrix on the Scala side. PS. In the filters, I switched to names keepRows and keepCols in KeyedBlockMatrix instead of rowsToKeep and colsToKeep, so the changes in BlockMatrix and BlockMatrixSuite are just renaming for consistency (as well as removing one unused line).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2718
https://github.com/hail-is/hail/pull/2720:56,Testability,test,tests,56,"along the way, move rdd-based variantsAndAnnotations to tests code",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2720
https://github.com/hail-is/hail/pull/2722:249,Safety,safe,safer,249,"Builds on: https://github.com/hail-is/hail/pull/2711 (genericintervals7). You probably want to wait until that goes in to review. @konradjk Unfortunately, we don't have automated tests for VEP yet. I'm bump the priority on that to make changing VEP safer, but in the mean time, can I ask you to run this on a small example to make sure we didn't break anything? Thanks!. @jbloom22 Can you do the same for Nirvana?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2722
https://github.com/hail-is/hail/pull/2722:179,Testability,test,tests,179,"Builds on: https://github.com/hail-is/hail/pull/2711 (genericintervals7). You probably want to wait until that goes in to review. @konradjk Unfortunately, we don't have automated tests for VEP yet. I'm bump the priority on that to make changing VEP safer, but in the mean time, can I ask you to run this on a small example to make sure we didn't break anything? Thanks!. @jbloom22 Can you do the same for Nirvana?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2722
https://github.com/hail-is/hail/pull/2725:81,Availability,error,errors,81,"This isn't quite ready, but PR'ing to draw a line in the sand. There are compile errors in concordance, nirvana, vep and MT. They should all be fixed by the pending PRs with the exception of a few additional fixups in MT (fromLegacy, same, etc.) I will rebase this as those PRs go in and fix up anything else that remains.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2725
https://github.com/hail-is/hail/issues/2731:120,Safety,Unsafe,UnsafeIndexedSeq,120,this line in OrderedRVD.coalesce:; ``` ; val newRangeBounds = newPartEnd.init.map(partitioner.rangeBounds).asInstanceOf[UnsafeIndexedSeq]; ```. Robert's stack trace:; ```; hail.utils.java.FatalError: ClassCastException: [Ljava.lang.Object; cannot be cast to is.hail.annotations.UnsafeIndexedSeq. Java stack trace:; java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to is.hail.annotations.UnsafeIndexedSeq; at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:217); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2331); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2731
https://github.com/hail-is/hail/issues/2731:278,Safety,Unsafe,UnsafeIndexedSeq,278,this line in OrderedRVD.coalesce:; ``` ; val newRangeBounds = newPartEnd.init.map(partitioner.rangeBounds).asInstanceOf[UnsafeIndexedSeq]; ```. Robert's stack trace:; ```; hail.utils.java.FatalError: ClassCastException: [Ljava.lang.Object; cannot be cast to is.hail.annotations.UnsafeIndexedSeq. Java stack trace:; java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to is.hail.annotations.UnsafeIndexedSeq; at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:217); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2331); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2731
https://github.com/hail-is/hail/issues/2731:403,Safety,Unsafe,UnsafeIndexedSeq,403,this line in OrderedRVD.coalesce:; ``` ; val newRangeBounds = newPartEnd.init.map(partitioner.rangeBounds).asInstanceOf[UnsafeIndexedSeq]; ```. Robert's stack trace:; ```; hail.utils.java.FatalError: ClassCastException: [Ljava.lang.Object; cannot be cast to is.hail.annotations.UnsafeIndexedSeq. Java stack trace:; java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to is.hail.annotations.UnsafeIndexedSeq; at is.hail.rvd.OrderedRVD.coalesce(OrderedRVD.scala:217); at is.hail.variant.MatrixTable.coalesce(MatrixTable.scala:2331); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2731
https://github.com/hail-is/hail/pull/2735:67,Availability,error,error,67,"** Actually used python stdlib package `difflib`. This PR improves error messages for things like:. ```text; In [3]: ds.INFO; AttributeError: MatrixTable instance has no field, method, or property 'INFO'; Did you mean:; Data field: 'info' [row]; ```; ```text; In [4]: ds['INFO']; LookupError: MatrixTable instance has no field 'INFO'; Did you mean:; 'info' [row field]; Hint: use 'describe()' to show the names of all data fields.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2735
https://github.com/hail-is/hail/pull/2735:73,Integrability,message,messages,73,"** Actually used python stdlib package `difflib`. This PR improves error messages for things like:. ```text; In [3]: ds.INFO; AttributeError: MatrixTable instance has no field, method, or property 'INFO'; Did you mean:; Data field: 'info' [row]; ```; ```text; In [4]: ds['INFO']; LookupError: MatrixTable instance has no field 'INFO'; Did you mean:; 'info' [row field]; Hint: use 'describe()' to show the names of all data fields.; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2735
https://github.com/hail-is/hail/pull/2737:359,Availability,error,error,359,"This addresses a user issue whereby adding an indicator covariate that only varied in controls caused Firth to fail. Currently the standard logistic MLE is computed even for Firth regression so this beta can be used to initialize the Firth per-variant models. But if the data has a (quasi-)separated coordinate, the standard MLE may not converge, throwing an error before the Firth models are fit. With the changes in this PR, if the MLE does not converge, Firth falls back to initializing with the intercept-only estimate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2737
https://github.com/hail-is/hail/pull/2737:140,Testability,log,logistic,140,"This addresses a user issue whereby adding an indicator covariate that only varied in controls caused Firth to fail. Currently the standard logistic MLE is computed even for Firth regression so this beta can be used to initialize the Firth per-variant models. But if the data has a (quasi-)separated coordinate, the standard MLE may not converge, throwing an error before the Firth models are fit. With the changes in this PR, if the MLE does not converge, Firth falls back to initializing with the intercept-only estimate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2737
https://github.com/hail-is/hail/issues/2743:620,Availability,error,error,620,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel. ### What you did:; ```; hc = HailContext(); past_vds = hc.read(args.past); future_vds = hc.read(args.future). bi_past_vds = past_vds.filter_rows(past_vds.v.is_biallelic()); bi_future_vds = future_vds.filter_rows(future_vds.v.is_biallelic()); bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-08a1543; WARNING: This is an unstable development build.; 2018-01-17 18:32:09 Hail: INFO: Found 729 overlapping samples; Left: 729 total samples; Right: 729 total samples; 2018-01-17 18:32:10 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 0:====================================================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:1867,Availability,failure,failure,1867,"FO: Found 729 overlapping samples; Left: 729 total samples; Right: 729 total samples; 2018-01-17 18:32:10 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 0:====================================================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:1926,Availability,failure,failure,1926,"ight: 729 total samples; 2018-01-17 18:32:10 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 0:====================================================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:10289,Availability,Error,Error,10289,rVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-08a1543; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:4178,Energy Efficiency,schedul,scheduler,4178,$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:4250,Energy Efficiency,schedul,scheduler,4250,rator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSchedu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:4614,Energy Efficiency,schedul,scheduler,4614,extras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:4654,Energy Efficiency,schedul,scheduler,4654,t is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:4753,Energy Efficiency,schedul,scheduler,4753,apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:4851,Energy Efficiency,schedul,scheduler,4851, org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:5105,Energy Efficiency,schedul,scheduler,5105,.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:5186,Energy Efficiency,schedul,scheduler,5186,ultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:5292,Energy Efficiency,schedul,scheduler,5292,he.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:5442,Energy Efficiency,schedul,scheduler,5442,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:5531,Energy Efficiency,schedul,scheduler,5531,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:5629,Energy Efficiency,schedul,scheduler,5629,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:911); 	at is.hail.sparkextras.OrderedRDD$.coerce(OrderedRDD.scala:75); 	at is.hai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:5725,Energy Efficiency,schedul,scheduler,5725,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:911); 	at is.hail.sparkextras.OrderedRDD$.coerce(OrderedRDD.scala:75); 	at is.hail.sparkextras.OrderedRDD$.apply(OrderedRDD.scala:49); 	at is.hail.utils.richUtils.RichPairRDD$.t,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:5890,Energy Efficiency,schedul,scheduler,5890,.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:911); 	at is.hail.sparkextras.OrderedRDD$.coerce(OrderedRDD.scala:75); 	at is.hail.sparkextras.OrderedRDD$.apply(OrderedRDD.scala:49); 	at is.hail.utils.richUtils.RichPairRDD$.toOrderedRDD$extension1(RichPairRDD.scala:44); 	at is.hail.variant.MatrixTable.typedRDD(MatrixTable.scala:475); 	at is.hail.methods.CalculateConcordance$.apply(Calcul,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:9865,Energy Efficiency,schedul,scheduler,9865,rVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-08a1543; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:9937,Energy Efficiency,schedul,scheduler,9937,rVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-08a1543; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:626,Integrability,message,messages,626,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel. ### What you did:; ```; hc = HailContext(); past_vds = hc.read(args.past); future_vds = hc.read(args.future). bi_past_vds = past_vds.filter_rows(past_vds.v.is_biallelic()); bi_future_vds = future_vds.filter_rows(future_vds.v.is_biallelic()); bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version devel-08a1543; WARNING: This is an unstable development build.; 2018-01-17 18:32:09 Hail: INFO: Found 729 overlapping samples; Left: 729 total samples; Right: 729 total samples; 2018-01-17 18:32:10 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 0:====================================================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2139,Performance,load,loadInt,2139,"set; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2197,Performance,load,loadLength,2197,"> OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:4374,Performance,concurren,concurrent,4374,cala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:4459,Performance,concurren,concurrent,4459,ala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:7826,Performance,load,loadInt,7826,a:475); 	at is.hail.methods.CalculateConcordance$.apply(CalculateConcordance.scala:108); 	at is.hail.methods.CalculateConcordance.apply(CalculateConcordance.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:7884,Performance,load,loadLength,7884,ulateConcordance.scala:108); 	at is.hail.methods.CalculateConcordance.apply(CalculateConcordance.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:10061,Performance,concurren,concurrent,10061,rVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-08a1543; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:10146,Performance,concurren,concurrent,10146,rVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-08a1543; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:1846,Safety,abort,aborted,1846,"FO: Found 729 overlapping samples; Left: 729 total samples; Right: 729 total samples; 2018-01-17 18:32:10 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 0:====================================================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2251,Safety,Unsafe,UnsafeRow,2251," (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Itera",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2273,Safety,Unsafe,UnsafeRow,2273,"ll last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.has",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2319,Safety,Unsafe,UnsafeRow,2319,"88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.Or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2341,Safety,Unsafe,UnsafeRow,2341,"ce.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2387,Safety,Unsafe,UnsafeRow,2387,"/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2412,Safety,Unsafe,UnsafeRow,2412,"9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2458,Safety,Unsafe,UnsafeRow,2458,"; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2488,Safety,Unsafe,UnsafeRow,2488,"ples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Itera",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2534,Safety,Unsafe,UnsafeRow,2534,"future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sp",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2550,Safety,Unsafe,UnsafeRow,2550,"File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.Par",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2596,Safety,Unsafe,UnsafeRow,2596,"ce; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2610,Safety,Unsafe,UnsafeRow,2610,"mp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:4785,Safety,abort,abortStage,4785,anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:4883,Safety,abort,abortStage,4883,RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:5128,Safety,abort,abortStage,5128,(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:7938,Safety,Unsafe,UnsafeRow,7938,alculateConcordance.apply(CalculateConcordance.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:7960,Safety,Unsafe,UnsafeRow,7960,nce.apply(CalculateConcordance.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.has,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:8006,Safety,Unsafe,UnsafeRow,8006,ct.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.Or,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:8028,Safety,Unsafe,UnsafeRow,8028,ccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfu,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:8074,Safety,Unsafe,UnsafeRow,8074,NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:8099,Safety,Unsafe,UnsafeRow,8099,orImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:8145,Safety,Unsafe,UnsafeRow,8145,sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at sc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:8175,Safety,Unsafe,UnsafeRow,8175,ngMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Itera,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:8221,Safety,Unsafe,UnsafeRow,8221,.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:8237,Safety,Unsafe,UnsafeRow,8237,t java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.Par,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:8283,Safety,Unsafe,UnsafeRow,8283,); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:8297,Safety,Unsafe,UnsafeRow,8297,reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:1756,Testability,Assert,AssertionError,1756,"n devel-08a1543; WARNING: This is an unstable development build.; 2018-01-17 18:32:09 Hail: INFO: Found 729 overlapping samples; Left: 729 total samples; Right: 729 total samples; 2018-01-17 18:32:10 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 0:====================================================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:1772,Testability,assert,assertion,1772,"n devel-08a1543; WARNING: This is an unstable development build.; 2018-01-17 18:32:09 Hail: INFO: Found 729 overlapping samples; Left: 729 total samples; Right: 729 total samples; 2018-01-17 18:32:10 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 0:====================================================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2030,Testability,Assert,AssertionError,2030,"========================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.has",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2046,Testability,assert,assertion,2046,"========================>(4627 + 1) / 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.has",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:2082,Testability,assert,assert,2082," 4628]2018-01-17 18:47:04 Hail: INFO: Coerced sorted dataset; 2018-01-17 18:47:04 Hail: WARN: converting OrderedRVD => OrderedRDD; [Stage 1:> (7 + 28) / 4969]Traceback (most recent call last):; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 38, in <module>; main(args); File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/concordance.py"", line 19, in main; bi_summary, bi_samples, bi_variants = methods.concordance(bi_past_vds, bi_future_vds); File ""<decorator-gen-1304>"", line 2, in concordance; File ""/tmp/f93de2d1-2d89-43f9-9868-f266eb88a6f1/hail-devel-08a15431a0ef.zip/hail/utils/java.py"", line 155, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 1.0 failed 20 times, most recent failure: Lost task 30.19 in stage 1.0 (TID 4847, lfdev2-sw-f5w2.c.broad-mpg-gnomad.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:7717,Testability,Assert,AssertionError,7717,DD$extension1(RichPairRDD.scala:44); 	at is.hail.variant.MatrixTable.typedRDD(MatrixTable.scala:475); 	at is.hail.methods.CalculateConcordance$.apply(CalculateConcordance.scala:108); 	at is.hail.methods.CalculateConcordance.apply(CalculateConcordance.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.has,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:7733,Testability,assert,assertion,7733,DD$extension1(RichPairRDD.scala:44); 	at is.hail.variant.MatrixTable.typedRDD(MatrixTable.scala:475); 	at is.hail.methods.CalculateConcordance$.apply(CalculateConcordance.scala:108); 	at is.hail.methods.CalculateConcordance.apply(CalculateConcordance.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.has,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:7769,Testability,assert,assert,7769,	at is.hail.variant.MatrixTable.typedRDD(MatrixTable.scala:475); 	at is.hail.methods.CalculateConcordance$.apply(CalculateConcordance.scala:108); 	at is.hail.methods.CalculateConcordance.apply(CalculateConcordance.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:10304,Testability,Assert,AssertionError,10304,rVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-08a1543; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/issues/2743:10320,Testability,assert,assertion,10320,rVariants$$anonfun$10.apply(Relational.scala:503); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:500); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:658); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$5$$anon$2.hasNext(OrderedRDD.scala:194); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.sparkextras.PartitionKeyInfo$.apply(PartitionKeyInfo.scala:30); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:72); 	at is.hail.sparkextras.OrderedRDD$$anonfun$4.apply(OrderedRDD.scala:70); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-08a1543; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2743
https://github.com/hail-is/hail/pull/2746:80,Integrability,interface,interface,80,"- I ported over the parallel write changes from 0.2 to 0.1. However, I left the interface of parallel being a Boolean where False = concatenate and True = parallel with a header per shard.; - This is a combination of previous PRs: #2265, #2263, #2354, #2587, #2630 ; - Addresses #2245 for @maryhaas",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2746
https://github.com/hail-is/hail/pull/2750:152,Deployability,update,updated,152,"I changed the fam_expr string argument to **fam_args that are checked in Python. I also changed the args to Python stye (is_case instead of isCase) and updated the import_fam and importFam docs/tests. Porting of import_plink will need similar translation. I've temporarily commented out the last two `assertRaises` tests as they've uncovered a bug in other code, talking to Tim about it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2750
https://github.com/hail-is/hail/pull/2750:194,Testability,test,tests,194,"I changed the fam_expr string argument to **fam_args that are checked in Python. I also changed the args to Python stye (is_case instead of isCase) and updated the import_fam and importFam docs/tests. Porting of import_plink will need similar translation. I've temporarily commented out the last two `assertRaises` tests as they've uncovered a bug in other code, talking to Tim about it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2750
https://github.com/hail-is/hail/pull/2750:301,Testability,assert,assertRaises,301,"I changed the fam_expr string argument to **fam_args that are checked in Python. I also changed the args to Python stye (is_case instead of isCase) and updated the import_fam and importFam docs/tests. Porting of import_plink will need similar translation. I've temporarily commented out the last two `assertRaises` tests as they've uncovered a bug in other code, talking to Tim about it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2750
https://github.com/hail-is/hail/pull/2750:315,Testability,test,tests,315,"I changed the fam_expr string argument to **fam_args that are checked in Python. I also changed the args to Python stye (is_case instead of isCase) and updated the import_fam and importFam docs/tests. Porting of import_plink will need similar translation. I've temporarily commented out the last two `assertRaises` tests as they've uncovered a bug in other code, talking to Tim about it.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2750
https://github.com/hail-is/hail/pull/2751:203,Testability,test,testing,203,"@jbloom22 I put in a rough version of a Python `BlockMatrix.to_local_matrix()`. It won't do super large matrices---it can only accommodate `MAXINT / 8 - 3` entries---but I needed something like this for testing. I was looking at maybe removing `ComputeRRM` from Scala, but it looks like a lot of the lmmreg tests are using it, so I didn't touch it for now; do you know how easy that is work around?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2751
https://github.com/hail-is/hail/pull/2751:307,Testability,test,tests,307,"@jbloom22 I put in a rough version of a Python `BlockMatrix.to_local_matrix()`. It won't do super large matrices---it can only accommodate `MAXINT / 8 - 3` entries---but I needed something like this for testing. I was looking at maybe removing `ComputeRRM` from Scala, but it looks like a lot of the lmmreg tests are using it, so I didn't touch it for now; do you know how easy that is work around?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2751
https://github.com/hail-is/hail/issues/2755:371,Availability,error,error,371,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-64a1fac. ### What you did:; run split_multi_hts on a VDS with only GT genotype field. ### What went wrong (all error messages here, including the full java stack trace):; ```; is.hail.utils.HailException: Struct has no field `AD'; Available fields:; GT: Call; <input>:4: newad = if (isDefined(g.AD)); ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.expr.ParserUtils$.error(Parser.scala:33); at is.hail.expr.AST.parseError(AST.scala:255); at is.hail.expr.Select.typecheckThis(AST.scala:333); at is.hail.expr.AST.typecheckThis(AST.scala:246); at is.hail.expr.AST.typecheck(AST.scala:252); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Apply.typecheck(AST.scala:663); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:850); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:849); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2755
https://github.com/hail-is/hail/issues/2755:491,Availability,Avail,Available,491,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-64a1fac. ### What you did:; run split_multi_hts on a VDS with only GT genotype field. ### What went wrong (all error messages here, including the full java stack trace):; ```; is.hail.utils.HailException: Struct has no field `AD'; Available fields:; GT: Call; <input>:4: newad = if (isDefined(g.AD)); ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.expr.ParserUtils$.error(Parser.scala:33); at is.hail.expr.AST.parseError(AST.scala:255); at is.hail.expr.Select.typecheckThis(AST.scala:333); at is.hail.expr.AST.typecheckThis(AST.scala:246); at is.hail.expr.AST.typecheck(AST.scala:252); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Apply.typecheck(AST.scala:663); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:850); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:849); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2755
https://github.com/hail-is/hail/issues/2755:581,Availability,Error,ErrorHandling,581,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-64a1fac. ### What you did:; run split_multi_hts on a VDS with only GT genotype field. ### What went wrong (all error messages here, including the full java stack trace):; ```; is.hail.utils.HailException: Struct has no field `AD'; Available fields:; GT: Call; <input>:4: newad = if (isDefined(g.AD)); ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.expr.ParserUtils$.error(Parser.scala:33); at is.hail.expr.AST.parseError(AST.scala:255); at is.hail.expr.Select.typecheckThis(AST.scala:333); at is.hail.expr.AST.typecheckThis(AST.scala:246); at is.hail.expr.AST.typecheck(AST.scala:252); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Apply.typecheck(AST.scala:663); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:850); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:849); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2755
https://github.com/hail-is/hail/issues/2755:607,Availability,Error,ErrorHandling,607,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-64a1fac. ### What you did:; run split_multi_hts on a VDS with only GT genotype field. ### What went wrong (all error messages here, including the full java stack trace):; ```; is.hail.utils.HailException: Struct has no field `AD'; Available fields:; GT: Call; <input>:4: newad = if (isDefined(g.AD)); ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.expr.ParserUtils$.error(Parser.scala:33); at is.hail.expr.AST.parseError(AST.scala:255); at is.hail.expr.Select.typecheckThis(AST.scala:333); at is.hail.expr.AST.typecheckThis(AST.scala:246); at is.hail.expr.AST.typecheck(AST.scala:252); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Apply.typecheck(AST.scala:663); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:850); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:849); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2755
https://github.com/hail-is/hail/issues/2755:711,Availability,error,error,711,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-64a1fac. ### What you did:; run split_multi_hts on a VDS with only GT genotype field. ### What went wrong (all error messages here, including the full java stack trace):; ```; is.hail.utils.HailException: Struct has no field `AD'; Available fields:; GT: Call; <input>:4: newad = if (isDefined(g.AD)); ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.expr.ParserUtils$.error(Parser.scala:33); at is.hail.expr.AST.parseError(AST.scala:255); at is.hail.expr.Select.typecheckThis(AST.scala:333); at is.hail.expr.AST.typecheckThis(AST.scala:246); at is.hail.expr.AST.typecheck(AST.scala:252); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Apply.typecheck(AST.scala:663); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:850); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:849); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2755
https://github.com/hail-is/hail/issues/2755:377,Integrability,message,messages,377,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-64a1fac. ### What you did:; run split_multi_hts on a VDS with only GT genotype field. ### What went wrong (all error messages here, including the full java stack trace):; ```; is.hail.utils.HailException: Struct has no field `AD'; Available fields:; GT: Call; <input>:4: newad = if (isDefined(g.AD)); ^; at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); at is.hail.utils.package$.fatal(package.scala:27); at is.hail.expr.ParserUtils$.error(Parser.scala:33); at is.hail.expr.AST.parseError(AST.scala:255); at is.hail.expr.Select.typecheckThis(AST.scala:333); at is.hail.expr.AST.typecheckThis(AST.scala:246); at is.hail.expr.AST.typecheck(AST.scala:252); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Apply.typecheck(AST.scala:663); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at is.hail.expr.AST$$anonfun$typecheck$1.apply(AST.scala:251); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186); at is.hail.expr.AST.typecheck(AST.scala:251); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:850); at is.hail.expr.Let$$anonfun$typecheck$18.apply(AST.scala:849); at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2755
https://github.com/hail-is/hail/pull/2762:58,Performance,cache,cached,58,"After profiling read/count on a 600M ExAC sites KeyTable, cached types/requiredness (requiredness was the big one, calling a virtual function) in arrays. ```; MASTER; In [4]: %timeit -n 10 df.count(); 4.41 s per loop; In [3]: %timeit -n 10 df.count(); 10 loops, best of 3: 4.63 s per loop. avg of two: 4.525 s per loop; ```; ```; THIS BRANCH; In [7]: %timeit -n 10 df.count(); 10 loops, best of 3: 2.94 s per loop; In [3]: %timeit -n 10 df.count(); 10 loops, best of 3: 3.34 s per loop. avg of two: 3.14 s per loop; ```. 44% increase in decoding throughput for KeyTable, can't imagine VDS would lag far behind.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2762
https://github.com/hail-is/hail/pull/2762:546,Performance,throughput,throughput,546,"After profiling read/count on a 600M ExAC sites KeyTable, cached types/requiredness (requiredness was the big one, calling a virtual function) in arrays. ```; MASTER; In [4]: %timeit -n 10 df.count(); 4.41 s per loop; In [3]: %timeit -n 10 df.count(); 10 loops, best of 3: 4.63 s per loop. avg of two: 4.525 s per loop; ```; ```; THIS BRANCH; In [7]: %timeit -n 10 df.count(); 10 loops, best of 3: 2.94 s per loop; In [3]: %timeit -n 10 df.count(); 10 loops, best of 3: 3.34 s per loop. avg of two: 3.14 s per loop; ```. 44% increase in decoding throughput for KeyTable, can't imagine VDS would lag far behind.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2762
https://github.com/hail-is/hail/issues/2763:429,Availability,error,error,429,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2763:576,Availability,failure,failure,576,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2763:634,Availability,failure,failure,634,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2763:435,Integrability,message,messages,435,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2763:835,Performance,load,loadInt,835,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2763:895,Performance,load,loadLength,895,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2763:960,Performance,load,loadLength,960,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2763:555,Safety,abort,aborted,555,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2763:496,Testability,Assert,AssertionError,496,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2763:728,Testability,Assert,AssertionError,728,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2763:744,Testability,assert,assertion,744,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2763:779,Testability,assert,assert,779,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-92bbe4b. ### What you did:; I used `join` to combine two VDS. Empty sample schema, different variant schema, no overlapping samples, hardcalls. ### What went wrong (all error messages here, including the full java stack trace):; Got an AssertionError:; ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 20 times, most recent failure: Lost task 0.19 in stage 5.0 (TID 677, mycluster3-w-1.c.ccdg-wgs.internal): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.Region.loadInt(Region.scala:36); at is.hail.expr.types.TContainer$.loadLength(TContainer.scala:9); at is.hail.expr.types.TContainer.loadLength(TContainer.scala:27); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1702); at is.hail.variant.MatrixTable$$anonfun$105$$anonfun$apply$58.apply(MatrixTable.scala:1685); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:661); at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.next(OrderedRVD.scala:655); ```. I can make it work by copying the variant annotation from one VDS to the other before calling `join`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2763
https://github.com/hail-is/hail/issues/2765:22,Integrability,message,messages,22,"Describe, LookupError messages, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2765
https://github.com/hail-is/hail/pull/2768:213,Security,expose,exposed,213,Made relevant field and function names pythonic and consistent along the way. I've ported the TDT python implementation and test quite literally. The implementation should use sum on arrays but isn't yet properly exposed in api2. I'm confused why -1 is used as a ploidy but will leave further improvements for later.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2768
https://github.com/hail-is/hail/pull/2768:124,Testability,test,test,124,Made relevant field and function names pythonic and consistent along the way. I've ported the TDT python implementation and test quite literally. The implementation should use sum on arrays but isn't yet properly exposed in api2. I'm confused why -1 is used as a ploidy but will leave further improvements for later.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2768
https://github.com/hail-is/hail/pull/2772:534,Integrability,interface,interface,534,"Builds off of https://github.com/hail-is/hail/pull/2725 which should be merged in shortly. You can view just this commit here: https://github.com/hail-is/hail/commit/d72c0e84bdea76228fd674dae9370af166c752df. @tpoterba FYI, can you look at the new MatrixType signature and make sure you're happy with it? https://github.com/hail-is/hail/commit/d72c0e84bdea76228fd674dae9370af166c752df#diff-191ea9c8cfc065f421b851203ddcd2a5R44. towards file format changes and matrix compound keys; moved MatrixType => ...expr.types package; MatrixType interface unchanged but members are generic and support compound keys; removed VSMMetadata (same information as MatrixType); renamed VSMFileMetadata => MatrixFileMetadata (can probably go away in next file format change); In the MatrixTable context, ""rowType"" now refers collectively to the row-indexed fields. rowType used to refer to the type of the region value in the OrderedRVD. That is now called ""rvRowType"". Where there is no confusion (inside TableType, RVD, we just use the single rowType). Changed all usages.; Renamed VSMLocalValue => MatrixLocalValue and moved to be next to MatrixValue in expr.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2772
https://github.com/hail-is/hail/pull/2783:6,Testability,log,logreg,6,"Port `logreg` and `lmmreg` to api2. The tests in api1 weren't doing anything that the doctests don't already do, so I haven't added any more python tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2783
https://github.com/hail-is/hail/pull/2783:40,Testability,test,tests,40,"Port `logreg` and `lmmreg` to api2. The tests in api1 weren't doing anything that the doctests don't already do, so I haven't added any more python tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2783
https://github.com/hail-is/hail/pull/2783:148,Testability,test,tests,148,"Port `logreg` and `lmmreg` to api2. The tests in api1 weren't doing anything that the doctests don't already do, so I haven't added any more python tests.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2783
https://github.com/hail-is/hail/pull/2785:2,Deployability,update,updated,2,- updated hail 0.2 tutorial; - added test for struct unpacking with annotate_cols/rows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2785
https://github.com/hail-is/hail/pull/2785:37,Testability,test,test,37,- updated hail 0.2 tutorial; - added test for struct unpacking with annotate_cols/rows,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2785
https://github.com/hail-is/hail/pull/2795:0,Integrability,depend,depends,0,depends on #2718,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2795
https://github.com/hail-is/hail/issues/2802:8,Deployability,pipeline,pipeline,8,"Beryl's pipeline:. ```from hail2 import *; hc = HailContext(). gtex_tx_summary_vds_path = ""gs://gnomad-berylc/tx-annotation/GTEx.V6.tx_medians.012318.vds/""; gtex = hc.read(gtex_tx_summary_vds_path); gtex_table = gtex.rows_table().key_by(""transcript_id""). vds_path = ""gs://gnomad-berylc/tx-annotation/hail0.2/clinvar_alleles.single.b37.hail0.2.vep.vds/""; vds = hc.read(vds_path). # Explode the vds for the transcript consequences; vds = vds.explode_rows(vds.vep.transcript_consequences). to_keep = [Variant.parse('14:29236865:A:AG')]; vds = vds.filter_rows_list(to_keep, keep=True). # Annotate vds with the gtex values (ie. join them); vds = vds.annotate_rows(tx_data=gtex_table[vds.vep.transcript_consequences.transcript_id]). kt = vds.rows_table(); kt.select(kt.vep.transcript_consequences.transcript_id, kt.tx_data).show(); ```. Result:. ```; +-----------------+-------------------+-----------------+; | transcript_id | tx_data.v | tx_data.gene_id |; +-----------------+-------------------+-----------------+; | String | String | String |; +-----------------+-------------------+-----------------+; | ENST00000313071 | ENST00000551395.1 | ENSG00000257126 |; | ENST00000382535 | ENST00000551395.1 | ENSG00000257126 |; | ENST00000546560 | ENST00000551395.1 | ENSG00000257126 |; | ENST00000549487 | ENST00000551395.1 | ENSG00000257126 |; | ENST00000551395 | ENST00000551395.1 | ENSG00000257126 |; +-----------------+-------------------+-----------------+; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2802
https://github.com/hail-is/hail/issues/2803:250,Availability,failure,failure,250,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:309,Availability,failure,failure,309,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:86,Deployability,pipeline,pipeline,86,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:2771,Energy Efficiency,schedul,scheduler,2771,nfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:2851,Energy Efficiency,schedul,scheduler,2851,nfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:2931,Energy Efficiency,schedul,scheduler,2931,nfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:519,Performance,load,loadInt,519,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:577,Performance,load,loadLength,577,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:3055,Performance,concurren,concurrent,3055,nfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:3140,Performance,concurren,concurrent,3140,nfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:229,Safety,abort,aborted,229,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:631,Safety,Unsafe,UnsafeRow,631,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:653,Safety,Unsafe,UnsafeRow,653,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:699,Safety,Unsafe,UnsafeRow,699,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:721,Safety,Unsafe,UnsafeRow,721,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:767,Safety,Unsafe,UnsafeRow,767,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:792,Safety,Unsafe,UnsafeRow,792,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:838,Safety,Unsafe,UnsafeRow,838,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:868,Safety,Unsafe,UnsafeRow,868,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:914,Safety,Unsafe,UnsafeRow,914,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:930,Safety,Unsafe,UnsafeRow,930,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:976,Safety,Unsafe,UnsafeRow,976,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:990,Safety,Unsafe,UnsafeRow,990,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:410,Testability,Assert,AssertionError,410,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:426,Testability,assert,assertion,426,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/issues/2803:462,Testability,assert,assert,462,"Replicable with the following:. ```; ds = hc.read('gs://future-variant-calling/future-pipeline/future.vds'); ds.filter_rows(ds.v.num_alleles() == 2).count_rows(); ```. ```; Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 66 in stage 2.0 failed 20 times, most recent failure: Lost task 66.19 in stage 2.0 (TID 2061, tim-debug-sw-h2hs.c.broad-ctsa.internal): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.Region.loadInt(Region.scala:36); 	at is.hail.expr.types.TBinary$.loadLength(TBinary.scala:62); 	at is.hail.annotations.UnsafeRow$.readBinary(UnsafeRow.scala:128); 	at is.hail.annotations.UnsafeRow$.readString(UnsafeRow.scala:139); 	at is.hail.annotations.UnsafeRow$.readAltAllele(UnsafeRow.scala:152); 	at is.hail.annotations.UnsafeRow$.readArrayAltAllele(UnsafeRow.scala:164); 	at is.hail.annotations.UnsafeRow$.read(UnsafeRow.scala:210); 	at is.hail.annotations.UnsafeRow.get(UnsafeRow.scala:257); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:428); 	at is.hail.expr.FilterVariants$$anonfun$10.apply(Relational.scala:425); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.hasNext(OrderedRVD.scala:694); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:691); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:170); 	at is.hail.methods.SampleQC$$anonfun$results$1.apply(SampleQC.scala:166); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2803
https://github.com/hail-is/hail/pull/2807:174,Testability,test,test,174,Fixes https://github.com/hail-is/hail/issues/2802. fixed a bug where a matrix with duplicated row keys would join incorrectly witha table via a computed key (vds_key); added test for this case; moved the vds_key logic and tests to python in api2; removed vds_key from api1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2807
https://github.com/hail-is/hail/pull/2807:212,Testability,log,logic,212,Fixes https://github.com/hail-is/hail/issues/2802. fixed a bug where a matrix with duplicated row keys would join incorrectly witha table via a computed key (vds_key); added test for this case; moved the vds_key logic and tests to python in api2; removed vds_key from api1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2807
https://github.com/hail-is/hail/pull/2807:222,Testability,test,tests,222,Fixes https://github.com/hail-is/hail/issues/2802. fixed a bug where a matrix with duplicated row keys would join incorrectly witha table via a computed key (vds_key); added test for this case; moved the vds_key logic and tests to python in api2; removed vds_key from api1,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2807
https://github.com/hail-is/hail/issues/2814:77,Integrability,depend,depends,77,"Deleting in col key branch. ```. def test_trio_matrix(self):; """"""; This test depends on certain properties of the trio matrix VCF; and pedigree structure. This test is NOT a valid test if the pedigree includes quads:; the trio_matrix method will duplicate the parents appropriately,; but the genotypes_table and samples_table orthogonal paths would; require another duplication/explode that we haven't written.; """"""; ped = Pedigree.read(test_file('triomatrix.fam')); famkt = KeyTable.import_fam(test_file('triomatrix.fam')). vds = hc.import_vcf(test_file('triomatrix.vcf'))\; .annotate_samples_table(famkt, root='sa.fam'). dads = famkt.filter('isDefined(pat_id)')\; .annotate('is_dad = true')\; .select(['pat_id', 'is_dad'])\; .key_by('pat_id'). moms = famkt.filter('isDefined(mat_id)') \; .annotate('is_mom = true') \; .select(['mat_id', 'is_mom']) \; .key_by('mat_id'). # test genotypes; gkt = (vds.genotypes_table(); .key_by('s'); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('v = v, fam = fam.fam_id',; 'data = GT.map(_ => {role: if (is_dad) 1 else if (is_mom) 2 else 0, g: {GT: GT, AD: AD, GQ: GQ, DP: DP, PL: PL}}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['v', 'fam', 'data'])). tkt = (vds.trio_matrix(ped, complete_trios=True); .genotypes_table(); .annotate('fam = proband.fields.fam.fam_id, data = [{role: 0, g: proband}, {role: 1, g: father}, {role: 2, g: mother}]'); .select(['v', 'fam', 'data']); .explode('data'); .filter('isDefined(data.g)'); .key_by(['v', 'fam'])). self.assertTrue(gkt.same(tkt)). # test annotations; g_sa = (vds.samples_table(); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('fam = fam.fam_id',; 'data = map(sa => {role: if (is_dad) 1 else if (is_mom) 2 else 0, sa: sa}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['fam', 'data']",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2814
https://github.com/hail-is/hail/issues/2814:72,Testability,test,test,72,"Deleting in col key branch. ```. def test_trio_matrix(self):; """"""; This test depends on certain properties of the trio matrix VCF; and pedigree structure. This test is NOT a valid test if the pedigree includes quads:; the trio_matrix method will duplicate the parents appropriately,; but the genotypes_table and samples_table orthogonal paths would; require another duplication/explode that we haven't written.; """"""; ped = Pedigree.read(test_file('triomatrix.fam')); famkt = KeyTable.import_fam(test_file('triomatrix.fam')). vds = hc.import_vcf(test_file('triomatrix.vcf'))\; .annotate_samples_table(famkt, root='sa.fam'). dads = famkt.filter('isDefined(pat_id)')\; .annotate('is_dad = true')\; .select(['pat_id', 'is_dad'])\; .key_by('pat_id'). moms = famkt.filter('isDefined(mat_id)') \; .annotate('is_mom = true') \; .select(['mat_id', 'is_mom']) \; .key_by('mat_id'). # test genotypes; gkt = (vds.genotypes_table(); .key_by('s'); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('v = v, fam = fam.fam_id',; 'data = GT.map(_ => {role: if (is_dad) 1 else if (is_mom) 2 else 0, g: {GT: GT, AD: AD, GQ: GQ, DP: DP, PL: PL}}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['v', 'fam', 'data'])). tkt = (vds.trio_matrix(ped, complete_trios=True); .genotypes_table(); .annotate('fam = proband.fields.fam.fam_id, data = [{role: 0, g: proband}, {role: 1, g: father}, {role: 2, g: mother}]'); .select(['v', 'fam', 'data']); .explode('data'); .filter('isDefined(data.g)'); .key_by(['v', 'fam'])). self.assertTrue(gkt.same(tkt)). # test annotations; g_sa = (vds.samples_table(); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('fam = fam.fam_id',; 'data = map(sa => {role: if (is_dad) 1 else if (is_mom) 2 else 0, sa: sa}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['fam', 'data']",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2814
https://github.com/hail-is/hail/issues/2814:160,Testability,test,test,160,"Deleting in col key branch. ```. def test_trio_matrix(self):; """"""; This test depends on certain properties of the trio matrix VCF; and pedigree structure. This test is NOT a valid test if the pedigree includes quads:; the trio_matrix method will duplicate the parents appropriately,; but the genotypes_table and samples_table orthogonal paths would; require another duplication/explode that we haven't written.; """"""; ped = Pedigree.read(test_file('triomatrix.fam')); famkt = KeyTable.import_fam(test_file('triomatrix.fam')). vds = hc.import_vcf(test_file('triomatrix.vcf'))\; .annotate_samples_table(famkt, root='sa.fam'). dads = famkt.filter('isDefined(pat_id)')\; .annotate('is_dad = true')\; .select(['pat_id', 'is_dad'])\; .key_by('pat_id'). moms = famkt.filter('isDefined(mat_id)') \; .annotate('is_mom = true') \; .select(['mat_id', 'is_mom']) \; .key_by('mat_id'). # test genotypes; gkt = (vds.genotypes_table(); .key_by('s'); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('v = v, fam = fam.fam_id',; 'data = GT.map(_ => {role: if (is_dad) 1 else if (is_mom) 2 else 0, g: {GT: GT, AD: AD, GQ: GQ, DP: DP, PL: PL}}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['v', 'fam', 'data'])). tkt = (vds.trio_matrix(ped, complete_trios=True); .genotypes_table(); .annotate('fam = proband.fields.fam.fam_id, data = [{role: 0, g: proband}, {role: 1, g: father}, {role: 2, g: mother}]'); .select(['v', 'fam', 'data']); .explode('data'); .filter('isDefined(data.g)'); .key_by(['v', 'fam'])). self.assertTrue(gkt.same(tkt)). # test annotations; g_sa = (vds.samples_table(); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('fam = fam.fam_id',; 'data = map(sa => {role: if (is_dad) 1 else if (is_mom) 2 else 0, sa: sa}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['fam', 'data']",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2814
https://github.com/hail-is/hail/issues/2814:180,Testability,test,test,180,"Deleting in col key branch. ```. def test_trio_matrix(self):; """"""; This test depends on certain properties of the trio matrix VCF; and pedigree structure. This test is NOT a valid test if the pedigree includes quads:; the trio_matrix method will duplicate the parents appropriately,; but the genotypes_table and samples_table orthogonal paths would; require another duplication/explode that we haven't written.; """"""; ped = Pedigree.read(test_file('triomatrix.fam')); famkt = KeyTable.import_fam(test_file('triomatrix.fam')). vds = hc.import_vcf(test_file('triomatrix.vcf'))\; .annotate_samples_table(famkt, root='sa.fam'). dads = famkt.filter('isDefined(pat_id)')\; .annotate('is_dad = true')\; .select(['pat_id', 'is_dad'])\; .key_by('pat_id'). moms = famkt.filter('isDefined(mat_id)') \; .annotate('is_mom = true') \; .select(['mat_id', 'is_mom']) \; .key_by('mat_id'). # test genotypes; gkt = (vds.genotypes_table(); .key_by('s'); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('v = v, fam = fam.fam_id',; 'data = GT.map(_ => {role: if (is_dad) 1 else if (is_mom) 2 else 0, g: {GT: GT, AD: AD, GQ: GQ, DP: DP, PL: PL}}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['v', 'fam', 'data'])). tkt = (vds.trio_matrix(ped, complete_trios=True); .genotypes_table(); .annotate('fam = proband.fields.fam.fam_id, data = [{role: 0, g: proband}, {role: 1, g: father}, {role: 2, g: mother}]'); .select(['v', 'fam', 'data']); .explode('data'); .filter('isDefined(data.g)'); .key_by(['v', 'fam'])). self.assertTrue(gkt.same(tkt)). # test annotations; g_sa = (vds.samples_table(); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('fam = fam.fam_id',; 'data = map(sa => {role: if (is_dad) 1 else if (is_mom) 2 else 0, sa: sa}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['fam', 'data']",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2814
https://github.com/hail-is/hail/issues/2814:874,Testability,test,test,874,"Deleting in col key branch. ```. def test_trio_matrix(self):; """"""; This test depends on certain properties of the trio matrix VCF; and pedigree structure. This test is NOT a valid test if the pedigree includes quads:; the trio_matrix method will duplicate the parents appropriately,; but the genotypes_table and samples_table orthogonal paths would; require another duplication/explode that we haven't written.; """"""; ped = Pedigree.read(test_file('triomatrix.fam')); famkt = KeyTable.import_fam(test_file('triomatrix.fam')). vds = hc.import_vcf(test_file('triomatrix.vcf'))\; .annotate_samples_table(famkt, root='sa.fam'). dads = famkt.filter('isDefined(pat_id)')\; .annotate('is_dad = true')\; .select(['pat_id', 'is_dad'])\; .key_by('pat_id'). moms = famkt.filter('isDefined(mat_id)') \; .annotate('is_mom = true') \; .select(['mat_id', 'is_mom']) \; .key_by('mat_id'). # test genotypes; gkt = (vds.genotypes_table(); .key_by('s'); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('v = v, fam = fam.fam_id',; 'data = GT.map(_ => {role: if (is_dad) 1 else if (is_mom) 2 else 0, g: {GT: GT, AD: AD, GQ: GQ, DP: DP, PL: PL}}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['v', 'fam', 'data'])). tkt = (vds.trio_matrix(ped, complete_trios=True); .genotypes_table(); .annotate('fam = proband.fields.fam.fam_id, data = [{role: 0, g: proband}, {role: 1, g: father}, {role: 2, g: mother}]'); .select(['v', 'fam', 'data']); .explode('data'); .filter('isDefined(data.g)'); .key_by(['v', 'fam'])). self.assertTrue(gkt.same(tkt)). # test annotations; g_sa = (vds.samples_table(); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('fam = fam.fam_id',; 'data = map(sa => {role: if (is_dad) 1 else if (is_mom) 2 else 0, sa: sa}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['fam', 'data']",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2814
https://github.com/hail-is/hail/issues/2814:1607,Testability,assert,assertTrue,1607,"cation/explode that we haven't written.; """"""; ped = Pedigree.read(test_file('triomatrix.fam')); famkt = KeyTable.import_fam(test_file('triomatrix.fam')). vds = hc.import_vcf(test_file('triomatrix.vcf'))\; .annotate_samples_table(famkt, root='sa.fam'). dads = famkt.filter('isDefined(pat_id)')\; .annotate('is_dad = true')\; .select(['pat_id', 'is_dad'])\; .key_by('pat_id'). moms = famkt.filter('isDefined(mat_id)') \; .annotate('is_mom = true') \; .select(['mat_id', 'is_mom']) \; .key_by('mat_id'). # test genotypes; gkt = (vds.genotypes_table(); .key_by('s'); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('v = v, fam = fam.fam_id',; 'data = GT.map(_ => {role: if (is_dad) 1 else if (is_mom) 2 else 0, g: {GT: GT, AD: AD, GQ: GQ, DP: DP, PL: PL}}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['v', 'fam', 'data'])). tkt = (vds.trio_matrix(ped, complete_trios=True); .genotypes_table(); .annotate('fam = proband.fields.fam.fam_id, data = [{role: 0, g: proband}, {role: 1, g: father}, {role: 2, g: mother}]'); .select(['v', 'fam', 'data']); .explode('data'); .filter('isDefined(data.g)'); .key_by(['v', 'fam'])). self.assertTrue(gkt.same(tkt)). # test annotations; g_sa = (vds.samples_table(); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('fam = fam.fam_id',; 'data = map(sa => {role: if (is_dad) 1 else if (is_mom) 2 else 0, sa: sa}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['fam', 'data'])). t_sa = (vds.trio_matrix(ped, complete_trios=True); .samples_table(); .annotate('fam = sa.proband.fields.fam.fam_id, data = [{role: 0, sa: sa.proband.fields}, '; '{role: 1, sa: sa.father.fields}, '; '{role: 2, sa: sa.mother.fields}]'); .select(['fam', 'data']); .explode('data'); .filter('isDefined(data.sa)'); .key_by(['fam'])). self.assertTrue(g_sa.same(t_sa)); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2814
https://github.com/hail-is/hail/issues/2814:1636,Testability,test,test,1636,"cation/explode that we haven't written.; """"""; ped = Pedigree.read(test_file('triomatrix.fam')); famkt = KeyTable.import_fam(test_file('triomatrix.fam')). vds = hc.import_vcf(test_file('triomatrix.vcf'))\; .annotate_samples_table(famkt, root='sa.fam'). dads = famkt.filter('isDefined(pat_id)')\; .annotate('is_dad = true')\; .select(['pat_id', 'is_dad'])\; .key_by('pat_id'). moms = famkt.filter('isDefined(mat_id)') \; .annotate('is_mom = true') \; .select(['mat_id', 'is_mom']) \; .key_by('mat_id'). # test genotypes; gkt = (vds.genotypes_table(); .key_by('s'); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('v = v, fam = fam.fam_id',; 'data = GT.map(_ => {role: if (is_dad) 1 else if (is_mom) 2 else 0, g: {GT: GT, AD: AD, GQ: GQ, DP: DP, PL: PL}}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['v', 'fam', 'data'])). tkt = (vds.trio_matrix(ped, complete_trios=True); .genotypes_table(); .annotate('fam = proband.fields.fam.fam_id, data = [{role: 0, g: proband}, {role: 1, g: father}, {role: 2, g: mother}]'); .select(['v', 'fam', 'data']); .explode('data'); .filter('isDefined(data.g)'); .key_by(['v', 'fam'])). self.assertTrue(gkt.same(tkt)). # test annotations; g_sa = (vds.samples_table(); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('fam = fam.fam_id',; 'data = map(sa => {role: if (is_dad) 1 else if (is_mom) 2 else 0, sa: sa}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['fam', 'data'])). t_sa = (vds.trio_matrix(ped, complete_trios=True); .samples_table(); .annotate('fam = sa.proband.fields.fam.fam_id, data = [{role: 0, sa: sa.proband.fields}, '; '{role: 1, sa: sa.father.fields}, '; '{role: 2, sa: sa.mother.fields}]'); .select(['fam', 'data']); .explode('data'); .filter('isDefined(data.sa)'); .key_by(['fam'])). self.assertTrue(g_sa.same(t_sa)); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2814
https://github.com/hail-is/hail/issues/2814:2339,Testability,assert,assertTrue,2339,"cation/explode that we haven't written.; """"""; ped = Pedigree.read(test_file('triomatrix.fam')); famkt = KeyTable.import_fam(test_file('triomatrix.fam')). vds = hc.import_vcf(test_file('triomatrix.vcf'))\; .annotate_samples_table(famkt, root='sa.fam'). dads = famkt.filter('isDefined(pat_id)')\; .annotate('is_dad = true')\; .select(['pat_id', 'is_dad'])\; .key_by('pat_id'). moms = famkt.filter('isDefined(mat_id)') \; .annotate('is_mom = true') \; .select(['mat_id', 'is_mom']) \; .key_by('mat_id'). # test genotypes; gkt = (vds.genotypes_table(); .key_by('s'); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('v = v, fam = fam.fam_id',; 'data = GT.map(_ => {role: if (is_dad) 1 else if (is_mom) 2 else 0, g: {GT: GT, AD: AD, GQ: GQ, DP: DP, PL: PL}}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['v', 'fam', 'data'])). tkt = (vds.trio_matrix(ped, complete_trios=True); .genotypes_table(); .annotate('fam = proband.fields.fam.fam_id, data = [{role: 0, g: proband}, {role: 1, g: father}, {role: 2, g: mother}]'); .select(['v', 'fam', 'data']); .explode('data'); .filter('isDefined(data.g)'); .key_by(['v', 'fam'])). self.assertTrue(gkt.same(tkt)). # test annotations; g_sa = (vds.samples_table(); .join(dads, how='left'); .join(moms, how='left'); .annotate('is_dad = isDefined(is_dad), is_mom = isDefined(is_mom)'); .aggregate_by_key('fam = fam.fam_id',; 'data = map(sa => {role: if (is_dad) 1 else if (is_mom) 2 else 0, sa: sa}).collect()'); .filter('data.length() == 3'); .explode('data'); .select(['fam', 'data'])). t_sa = (vds.trio_matrix(ped, complete_trios=True); .samples_table(); .annotate('fam = sa.proband.fields.fam.fam_id, data = [{role: 0, sa: sa.proband.fields}, '; '{role: 1, sa: sa.father.fields}, '; '{role: 2, sa: sa.mother.fields}]'); .select(['fam', 'data']); .explode('data'); .filter('isDefined(data.sa)'); .key_by(['fam'])). self.assertTrue(g_sa.same(t_sa)); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2814
https://github.com/hail-is/hail/issues/2818:5,Availability,Error,Error,5,```; Error summary: FileNotFoundException: Item not found: gnomad-berylc/tx-annotation/hail0.2/gnomad.exomes.r2.0.2.tx_annotated.PLIgenes.012618.kt; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2818
https://github.com/hail-is/hail/pull/2821:946,Availability,robust,robust,946,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:209,Performance,load,loads,209,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:1232,Performance,perform,performs,1232,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:94,Safety,avoid,avoiding,94,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:116,Safety,avoid,avoids,116,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:612,Testability,test,tested,612,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:626,Testability,test,test,626,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:880,Testability,test,tests,880,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:953,Testability,test,test,953,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:1079,Testability,test,testing,1079,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:1321,Testability,test,tests,1321,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:1384,Testability,test,tests,1384,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/pull/2821:678,Usability,simpl,simple,678,"cc: @cseed @konradjk . This PCRelate should handle larger data sizes than the previous one by avoiding shuffles. It avoids the shuffle by writing the vds to a temporary directory in block matrix form. It then loads this BlockMatrix directly. Form that point forward, the PCRelate algorithm is just non-shuffling linear algebra (however: matrix multiplication will require each node to communicate with approximately `n+m` other nodes). I'm vaguely uncomfortable with two things:. 1. I've added some hail expr lang to mean impute missing values. This is written in python. As such, correctly mean imputing is not tested by our test system any more. The mean imputation is pretty simple, so maybe we should just verify my code is right?. 2. I noticed that at some earlier point PCA was moved outside of Java as well. This also makes me uncomfortable for the same reason. Moving the tests into python is a fair lift because, AFAIK, we don't have as robust test infrastructure over there. I'm torn between the desire to get this out for @konradjk and the desire to follow our normal testing standards. ---. I've played a bit with this locally, but have not tried it on a large cluster. @konradjk, I would be very interested in how this performs on a large dataset, if you would be so kind. Please don't try this until the CI tests and doc builds pass on this PR though. I haven't run the tests locally, so I'm not certain it passes them :P",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2821
https://github.com/hail-is/hail/issues/2822:324,Availability,error,errors,324,"The standard field registrations lead to TERRIBLE behavior -- . this line in a VCF:; ```; ##INFO=<ID=MQ0,Number=.,Type=Integer,Description=""Number of MAPQ == 0 reads"">; ```. ...gets returned in our code as...; ```; INFO=<ID=MQ0,Number=1,Type=Integer,Description=""Total Mapping Quality Zero Reads"">; ```. ...leading to match errors when we get to the values.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2822
https://github.com/hail-is/hail/pull/2824:80,Deployability,integrat,integrates,80,"Once this goes in I will make a second PR (to breaking) that that removes api1, integrates api2, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2824
https://github.com/hail-is/hail/pull/2824:80,Integrability,integrat,integrates,80,"Once this goes in I will make a second PR (to breaking) that that removes api1, integrates api2, etc.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2824
https://github.com/hail-is/hail/pull/2828:259,Availability,failure,failure,259,Builds on: https://github.com/hail-is/hail/pull/2825. added RVD (should be UnpartitionedRVD) and OrderedRVD; allows to add new rvd types (HashedRVD); added list of partition files to current specs (to support safe object storage write strategy in presence of failure),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2828
https://github.com/hail-is/hail/pull/2828:209,Safety,safe,safe,209,Builds on: https://github.com/hail-is/hail/pull/2825. added RVD (should be UnpartitionedRVD) and OrderedRVD; allows to add new rvd types (HashedRVD); added list of partition files to current specs (to support safe object storage write strategy in presence of failure),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2828
https://github.com/hail-is/hail/pull/2828:138,Security,Hash,HashedRVD,138,Builds on: https://github.com/hail-is/hail/pull/2825. added RVD (should be UnpartitionedRVD) and OrderedRVD; allows to add new rvd types (HashedRVD); added list of partition files to current specs (to support safe object storage write strategy in presence of failure),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2828
https://github.com/hail-is/hail/pull/2829:26,Testability,test,tests,26,"Still need to fix the doc tests. Just getting it up on the board. builds on: https://github.com/hail-is/hail/pull/2824. removed hail2; moved hail2 => hail; moved api2 => hail; scala + python tests passing, docs still need some work",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2829
https://github.com/hail-is/hail/pull/2829:191,Testability,test,tests,191,"Still need to fix the doc tests. Just getting it up on the board. builds on: https://github.com/hail-is/hail/pull/2824. removed hail2; moved hail2 => hail; moved api2 => hail; scala + python tests passing, docs still need some work",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2829
https://github.com/hail-is/hail/issues/2839:755,Integrability,wrap,wrapper,755,"```; x = functions.capture(True); y = functions.capture(2); eval_expr(x * y); ```; Hoped for 2, got:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-21-69c81b591366> in <module>(); ----> 1 eval_expr(x * y). /Users/konradk/Dropbox/src/python/hail/expr/expression.py in __rmul__(self, other); 2598; 2599 def __rmul__(self, other):; -> 2600 return self._bin_op_numeric_reverse(""*"", other); 2601; 2602 def __div__(self, other):. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in _bin_op_numeric_reverse(self, name, other, ret_type_f); 2395 def _bin_op_numeric_reverse(self, name, other, ret_type_f=None):; 2396 other = to_expr(other); -> 2397 ret_type, wrapper = self._bin_op_ret_typ(other); 2398 if not ret_type:; 2399 raise NotImplementedError(""'{}' {} '{}'"".format(. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in _bin_op_ret_typ(self, other); 2377 t = other._type; 2378 wrapper = lambda t: t; -> 2379 t = unify_types(self._type, t); 2380 if not t:; 2381 return None. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in unify_types(*ts); 264 assert classes == {TInt32, TInt64}; 265 return TInt64(); --> 266 elif all(isinstance(TArray, t) for t in ts):; 267 et = unify_types(*(t.element_type for t in ts)); 268 if et:. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in <genexpr>((t,)); 264 assert classes == {TInt32, TInt64}; 265 return TInt64(); --> 266 elif all(isinstance(TArray, t) for t in ts):; 267 et = unify_types(*(t.element_type for t in ts)); 268 if et:. TypeError: isinstance() arg 2 must be a class, type, or tuple of classes and types; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2839
https://github.com/hail-is/hail/issues/2839:990,Integrability,wrap,wrapper,990,"```; x = functions.capture(True); y = functions.capture(2); eval_expr(x * y); ```; Hoped for 2, got:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-21-69c81b591366> in <module>(); ----> 1 eval_expr(x * y). /Users/konradk/Dropbox/src/python/hail/expr/expression.py in __rmul__(self, other); 2598; 2599 def __rmul__(self, other):; -> 2600 return self._bin_op_numeric_reverse(""*"", other); 2601; 2602 def __div__(self, other):. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in _bin_op_numeric_reverse(self, name, other, ret_type_f); 2395 def _bin_op_numeric_reverse(self, name, other, ret_type_f=None):; 2396 other = to_expr(other); -> 2397 ret_type, wrapper = self._bin_op_ret_typ(other); 2398 if not ret_type:; 2399 raise NotImplementedError(""'{}' {} '{}'"".format(. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in _bin_op_ret_typ(self, other); 2377 t = other._type; 2378 wrapper = lambda t: t; -> 2379 t = unify_types(self._type, t); 2380 if not t:; 2381 return None. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in unify_types(*ts); 264 assert classes == {TInt32, TInt64}; 265 return TInt64(); --> 266 elif all(isinstance(TArray, t) for t in ts):; 267 et = unify_types(*(t.element_type for t in ts)); 268 if et:. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in <genexpr>((t,)); 264 assert classes == {TInt32, TInt64}; 265 return TInt64(); --> 266 elif all(isinstance(TArray, t) for t in ts):; 267 et = unify_types(*(t.element_type for t in ts)); 268 if et:. TypeError: isinstance() arg 2 must be a class, type, or tuple of classes and types; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2839
https://github.com/hail-is/hail/issues/2839:1170,Testability,assert,assert,1170,"```; x = functions.capture(True); y = functions.capture(2); eval_expr(x * y); ```; Hoped for 2, got:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-21-69c81b591366> in <module>(); ----> 1 eval_expr(x * y). /Users/konradk/Dropbox/src/python/hail/expr/expression.py in __rmul__(self, other); 2598; 2599 def __rmul__(self, other):; -> 2600 return self._bin_op_numeric_reverse(""*"", other); 2601; 2602 def __div__(self, other):. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in _bin_op_numeric_reverse(self, name, other, ret_type_f); 2395 def _bin_op_numeric_reverse(self, name, other, ret_type_f=None):; 2396 other = to_expr(other); -> 2397 ret_type, wrapper = self._bin_op_ret_typ(other); 2398 if not ret_type:; 2399 raise NotImplementedError(""'{}' {} '{}'"".format(. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in _bin_op_ret_typ(self, other); 2377 t = other._type; 2378 wrapper = lambda t: t; -> 2379 t = unify_types(self._type, t); 2380 if not t:; 2381 return None. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in unify_types(*ts); 264 assert classes == {TInt32, TInt64}; 265 return TInt64(); --> 266 elif all(isinstance(TArray, t) for t in ts):; 267 et = unify_types(*(t.element_type for t in ts)); 268 if et:. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in <genexpr>((t,)); 264 assert classes == {TInt32, TInt64}; 265 return TInt64(); --> 266 elif all(isinstance(TArray, t) for t in ts):; 267 et = unify_types(*(t.element_type for t in ts)); 268 if et:. TypeError: isinstance() arg 2 must be a class, type, or tuple of classes and types; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2839
https://github.com/hail-is/hail/issues/2839:1428,Testability,assert,assert,1428,"```; x = functions.capture(True); y = functions.capture(2); eval_expr(x * y); ```; Hoped for 2, got:; ```; ---------------------------------------------------------------------------; TypeError Traceback (most recent call last); <ipython-input-21-69c81b591366> in <module>(); ----> 1 eval_expr(x * y). /Users/konradk/Dropbox/src/python/hail/expr/expression.py in __rmul__(self, other); 2598; 2599 def __rmul__(self, other):; -> 2600 return self._bin_op_numeric_reverse(""*"", other); 2601; 2602 def __div__(self, other):. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in _bin_op_numeric_reverse(self, name, other, ret_type_f); 2395 def _bin_op_numeric_reverse(self, name, other, ret_type_f=None):; 2396 other = to_expr(other); -> 2397 ret_type, wrapper = self._bin_op_ret_typ(other); 2398 if not ret_type:; 2399 raise NotImplementedError(""'{}' {} '{}'"".format(. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in _bin_op_ret_typ(self, other); 2377 t = other._type; 2378 wrapper = lambda t: t; -> 2379 t = unify_types(self._type, t); 2380 if not t:; 2381 return None. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in unify_types(*ts); 264 assert classes == {TInt32, TInt64}; 265 return TInt64(); --> 266 elif all(isinstance(TArray, t) for t in ts):; 267 et = unify_types(*(t.element_type for t in ts)); 268 if et:. /Users/konradk/Dropbox/src/python/hail/expr/expression.py in <genexpr>((t,)); 264 assert classes == {TInt32, TInt64}; 265 return TInt64(); --> 266 elif all(isinstance(TArray, t) for t in ts):; 267 et = unify_types(*(t.element_type for t in ts)); 268 if et:. TypeError: isinstance() arg 2 must be a class, type, or tuple of classes and types; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2839
https://github.com/hail-is/hail/pull/2843:130,Testability,test,tests,130,- Also added missing impex methods in sphinx docs and a reference to `default_reference` in docs.; - Moved API2 HailContext impex tests to methods tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2843
https://github.com/hail-is/hail/pull/2843:147,Testability,test,tests,147,- Also added missing impex methods in sphinx docs and a reference to `default_reference` in docs.; - Moved API2 HailContext impex tests to methods tests.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2843
https://github.com/hail-is/hail/pull/2844:6,Testability,test,tests,6,moved tests to python,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2844
https://github.com/hail-is/hail/pull/2848:1171,Deployability,pipeline,pipelines,1171,"I've changed BlockMatrix.from(lm: BDM[Double]) so that each executor is transmitted only the blocks it needs (~num_blocks/num_executors) rather than all of them: ""[TorrentBroadcast](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-TorrentBroadcast.html) uses a BitTorrent-like protocol for block distribution (that only happens when tasks access broadcast variables on executors)."". In another branch, I've verified on GCP that distributing and then localizing a 10k x 10k matrix is twice as fast (about 15s vs 30s). Distributing and then writing a 25k by 25k matrix (5GB) with 10+2 standard 8-core workers takes about 30s with the new method but fails for every partition with the old method (months ago I believe I sometimes got the old method to work at this scale using high mem. It's needed for LMM). Note the matrix only has 49 partitions at the new default blockSize so I had more cores than needed for the experiment. I've also added a method to write a local matrix as a block matrix. I use ParRange to parallelize writing from master. Writing and then reading should be the safest way to distribute a big local matrix at the beginning of complex pipelines, and I think it avoids some of the memory overhead associated with broadcast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2848
https://github.com/hail-is/hail/pull/2848:292,Integrability,protocol,protocol,292,"I've changed BlockMatrix.from(lm: BDM[Double]) so that each executor is transmitted only the blocks it needs (~num_blocks/num_executors) rather than all of them: ""[TorrentBroadcast](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-TorrentBroadcast.html) uses a BitTorrent-like protocol for block distribution (that only happens when tasks access broadcast variables on executors)."". In another branch, I've verified on GCP that distributing and then localizing a 10k x 10k matrix is twice as fast (about 15s vs 30s). Distributing and then writing a 25k by 25k matrix (5GB) with 10+2 standard 8-core workers takes about 30s with the new method but fails for every partition with the old method (months ago I believe I sometimes got the old method to work at this scale using high mem. It's needed for LMM). Note the matrix only has 49 partitions at the new default blockSize so I had more cores than needed for the experiment. I've also added a method to write a local matrix as a block matrix. I use ParRange to parallelize writing from master. Writing and then reading should be the safest way to distribute a big local matrix at the beginning of complex pipelines, and I think it avoids some of the memory overhead associated with broadcast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2848
https://github.com/hail-is/hail/pull/2848:371,Modifiability,variab,variables,371,"I've changed BlockMatrix.from(lm: BDM[Double]) so that each executor is transmitted only the blocks it needs (~num_blocks/num_executors) rather than all of them: ""[TorrentBroadcast](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-TorrentBroadcast.html) uses a BitTorrent-like protocol for block distribution (that only happens when tasks access broadcast variables on executors)."". In another branch, I've verified on GCP that distributing and then localizing a 10k x 10k matrix is twice as fast (about 15s vs 30s). Distributing and then writing a 25k by 25k matrix (5GB) with 10+2 standard 8-core workers takes about 30s with the new method but fails for every partition with the old method (months ago I believe I sometimes got the old method to work at this scale using high mem. It's needed for LMM). Note the matrix only has 49 partitions at the new default blockSize so I had more cores than needed for the experiment. I've also added a method to write a local matrix as a block matrix. I use ParRange to parallelize writing from master. Writing and then reading should be the safest way to distribute a big local matrix at the beginning of complex pipelines, and I think it avoids some of the memory overhead associated with broadcast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2848
https://github.com/hail-is/hail/pull/2848:1099,Safety,safe,safest,1099,"I've changed BlockMatrix.from(lm: BDM[Double]) so that each executor is transmitted only the blocks it needs (~num_blocks/num_executors) rather than all of them: ""[TorrentBroadcast](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-TorrentBroadcast.html) uses a BitTorrent-like protocol for block distribution (that only happens when tasks access broadcast variables on executors)."". In another branch, I've verified on GCP that distributing and then localizing a 10k x 10k matrix is twice as fast (about 15s vs 30s). Distributing and then writing a 25k by 25k matrix (5GB) with 10+2 standard 8-core workers takes about 30s with the new method but fails for every partition with the old method (months ago I believe I sometimes got the old method to work at this scale using high mem. It's needed for LMM). Note the matrix only has 49 partitions at the new default blockSize so I had more cores than needed for the experiment. I've also added a method to write a local matrix as a block matrix. I use ParRange to parallelize writing from master. Writing and then reading should be the safest way to distribute a big local matrix at the beginning of complex pipelines, and I think it avoids some of the memory overhead associated with broadcast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2848
https://github.com/hail-is/hail/pull/2848:1197,Safety,avoid,avoids,1197,"I've changed BlockMatrix.from(lm: BDM[Double]) so that each executor is transmitted only the blocks it needs (~num_blocks/num_executors) rather than all of them: ""[TorrentBroadcast](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-TorrentBroadcast.html) uses a BitTorrent-like protocol for block distribution (that only happens when tasks access broadcast variables on executors)."". In another branch, I've verified on GCP that distributing and then localizing a 10k x 10k matrix is twice as fast (about 15s vs 30s). Distributing and then writing a 25k by 25k matrix (5GB) with 10+2 standard 8-core workers takes about 30s with the new method but fails for every partition with the old method (months ago I believe I sometimes got the old method to work at this scale using high mem. It's needed for LMM). Note the matrix only has 49 partitions at the new default blockSize so I had more cores than needed for the experiment. I've also added a method to write a local matrix as a block matrix. I use ParRange to parallelize writing from master. Writing and then reading should be the safest way to distribute a big local matrix at the beginning of complex pipelines, and I think it avoids some of the memory overhead associated with broadcast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2848
https://github.com/hail-is/hail/pull/2848:354,Security,access,access,354,"I've changed BlockMatrix.from(lm: BDM[Double]) so that each executor is transmitted only the blocks it needs (~num_blocks/num_executors) rather than all of them: ""[TorrentBroadcast](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-TorrentBroadcast.html) uses a BitTorrent-like protocol for block distribution (that only happens when tasks access broadcast variables on executors)."". In another branch, I've verified on GCP that distributing and then localizing a 10k x 10k matrix is twice as fast (about 15s vs 30s). Distributing and then writing a 25k by 25k matrix (5GB) with 10+2 standard 8-core workers takes about 30s with the new method but fails for every partition with the old method (months ago I believe I sometimes got the old method to work at this scale using high mem. It's needed for LMM). Note the matrix only has 49 partitions at the new default blockSize so I had more cores than needed for the experiment. I've also added a method to write a local matrix as a block matrix. I use ParRange to parallelize writing from master. Writing and then reading should be the safest way to distribute a big local matrix at the beginning of complex pipelines, and I think it avoids some of the memory overhead associated with broadcast.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2848
https://github.com/hail-is/hail/pull/2849:22,Safety,avoid,avoid,22,"it may be possible to avoid creating a new generator per block, but given that each block samples 16M entries for the default blockSize (which I've changed to 4096 in another PR), I don't think it's a big deal.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2849
https://github.com/hail-is/hail/pull/2850:28,Availability,error,error,28,This was the most confusing error I've seen in a while! 😅,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2850
https://github.com/hail-is/hail/pull/2852:195,Availability,toler,tolerance,195,"cc: @tpoterba I added some new type check stuff. I added `round` `ceil` and `floor` as I was writing tests and then I didn't need them. They still seem useful, so I included them. I had to add a tolerance parameter to `Table.same` and my implementation is kind of ugly and bad but I can't think of an obviously better way to do this. Example usages:. ```python; imputed_sex = methods.impute_sex(ds.v.locus(), ds.GT); ```. ```python; imputed_sex = methods.impute_sex(ds.v.locus(), ; ds.GT,; aaf=gnomad.AF); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2852
https://github.com/hail-is/hail/pull/2852:101,Testability,test,tests,101,"cc: @tpoterba I added some new type check stuff. I added `round` `ceil` and `floor` as I was writing tests and then I didn't need them. They still seem useful, so I included them. I had to add a tolerance parameter to `Table.same` and my implementation is kind of ugly and bad but I can't think of an obviously better way to do this. Example usages:. ```python; imputed_sex = methods.impute_sex(ds.v.locus(), ds.GT); ```. ```python; imputed_sex = methods.impute_sex(ds.v.locus(), ; ds.GT,; aaf=gnomad.AF); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2852
https://github.com/hail-is/hail/pull/2854:41,Performance,load,load,41,"- store globals, cols out of line, don't load when loading metadata; - removed MatrixLocalValue (also Table), just store in MatrixValue; - don't require vds, kt extensions; - /path/to/ds/cols is now a valid table fine format; - important bug fix: read_table was completely broken",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2854
https://github.com/hail-is/hail/pull/2854:51,Performance,load,loading,51,"- store globals, cols out of line, don't load when loading metadata; - removed MatrixLocalValue (also Table), just store in MatrixValue; - don't require vds, kt extensions; - /path/to/ds/cols is now a valid table fine format; - important bug fix: read_table was completely broken",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2854
https://github.com/hail-is/hail/pull/2858:908,Integrability,interface,interface,908,"This PR improves and consolidates several that were pending. It does a few things:. - all values in the file formats are stored natively (encoded), not using json,; - in particular, the globals and cols are not stored in the json, and not loaded when the read is performed by the user (but when it is executed by the backend).; - MT parts (globals, rows, cols, entries) are all stored in separate directories and each is itself a valid Table file; - extensions for MT and Table are no longer enforced; - the metadata now stores an ""RVDSpec"" which is a recipe for reading an RVD. This gives us the flexibility to define new RVD types (e.g. HashedRVD) or modify the existing classes (by forking and adding additional parameters, etc.); - the rows and entries are stored separately and zipped together on read. If dropSamples = true, don't even load the entries.; - created UnpartitionedRVD, cleaned up the RVD interface. fixes: #2810",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2858
https://github.com/hail-is/hail/pull/2858:239,Performance,load,loaded,239,"This PR improves and consolidates several that were pending. It does a few things:. - all values in the file formats are stored natively (encoded), not using json,; - in particular, the globals and cols are not stored in the json, and not loaded when the read is performed by the user (but when it is executed by the backend).; - MT parts (globals, rows, cols, entries) are all stored in separate directories and each is itself a valid Table file; - extensions for MT and Table are no longer enforced; - the metadata now stores an ""RVDSpec"" which is a recipe for reading an RVD. This gives us the flexibility to define new RVD types (e.g. HashedRVD) or modify the existing classes (by forking and adding additional parameters, etc.); - the rows and entries are stored separately and zipped together on read. If dropSamples = true, don't even load the entries.; - created UnpartitionedRVD, cleaned up the RVD interface. fixes: #2810",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2858
https://github.com/hail-is/hail/pull/2858:263,Performance,perform,performed,263,"This PR improves and consolidates several that were pending. It does a few things:. - all values in the file formats are stored natively (encoded), not using json,; - in particular, the globals and cols are not stored in the json, and not loaded when the read is performed by the user (but when it is executed by the backend).; - MT parts (globals, rows, cols, entries) are all stored in separate directories and each is itself a valid Table file; - extensions for MT and Table are no longer enforced; - the metadata now stores an ""RVDSpec"" which is a recipe for reading an RVD. This gives us the flexibility to define new RVD types (e.g. HashedRVD) or modify the existing classes (by forking and adding additional parameters, etc.); - the rows and entries are stored separately and zipped together on read. If dropSamples = true, don't even load the entries.; - created UnpartitionedRVD, cleaned up the RVD interface. fixes: #2810",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2858
https://github.com/hail-is/hail/pull/2858:842,Performance,load,load,842,"This PR improves and consolidates several that were pending. It does a few things:. - all values in the file formats are stored natively (encoded), not using json,; - in particular, the globals and cols are not stored in the json, and not loaded when the read is performed by the user (but when it is executed by the backend).; - MT parts (globals, rows, cols, entries) are all stored in separate directories and each is itself a valid Table file; - extensions for MT and Table are no longer enforced; - the metadata now stores an ""RVDSpec"" which is a recipe for reading an RVD. This gives us the flexibility to define new RVD types (e.g. HashedRVD) or modify the existing classes (by forking and adding additional parameters, etc.); - the rows and entries are stored separately and zipped together on read. If dropSamples = true, don't even load the entries.; - created UnpartitionedRVD, cleaned up the RVD interface. fixes: #2810",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2858
https://github.com/hail-is/hail/pull/2858:639,Security,Hash,HashedRVD,639,"This PR improves and consolidates several that were pending. It does a few things:. - all values in the file formats are stored natively (encoded), not using json,; - in particular, the globals and cols are not stored in the json, and not loaded when the read is performed by the user (but when it is executed by the backend).; - MT parts (globals, rows, cols, entries) are all stored in separate directories and each is itself a valid Table file; - extensions for MT and Table are no longer enforced; - the metadata now stores an ""RVDSpec"" which is a recipe for reading an RVD. This gives us the flexibility to define new RVD types (e.g. HashedRVD) or modify the existing classes (by forking and adding additional parameters, etc.); - the rows and entries are stored separately and zipped together on read. If dropSamples = true, don't even load the entries.; - created UnpartitionedRVD, cleaned up the RVD interface. fixes: #2810",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2858
https://github.com/hail-is/hail/pull/2859:133,Security,expose,exposed,133,"- made decoder stuff modular/composable; - CodecSpec, stored in RVDSpec (and hence in metadata) describes the en/decoding process; - exposed to Python (private parameter in MT and Table.write)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2859
https://github.com/hail-is/hail/pull/2860:518,Deployability,pipeline,pipeline,518,"Depends on #2848 and #2861, I'll rebase once those are in. LocalMatrix has pointwise +,-,*,% ops with broadcasting, matrix ops, etc. It's meant to mirror NumPy restricted to always having 2 axes (we may eventually want to introduce np.array into the expression language). Vectors are identified with a single column rather than having a separate class. Once this is in, I'll rename the distributedmatrix package to linalg. Next step is to build Python interface starting with those ops I need to pull over much of LMM pipeline, with testing on the Python side against Numpy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2860
https://github.com/hail-is/hail/pull/2860:0,Integrability,Depend,Depends,0,"Depends on #2848 and #2861, I'll rebase once those are in. LocalMatrix has pointwise +,-,*,% ops with broadcasting, matrix ops, etc. It's meant to mirror NumPy restricted to always having 2 axes (we may eventually want to introduce np.array into the expression language). Vectors are identified with a single column rather than having a separate class. Once this is in, I'll rename the distributedmatrix package to linalg. Next step is to build Python interface starting with those ops I need to pull over much of LMM pipeline, with testing on the Python side against Numpy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2860
https://github.com/hail-is/hail/pull/2860:452,Integrability,interface,interface,452,"Depends on #2848 and #2861, I'll rebase once those are in. LocalMatrix has pointwise +,-,*,% ops with broadcasting, matrix ops, etc. It's meant to mirror NumPy restricted to always having 2 axes (we may eventually want to introduce np.array into the expression language). Vectors are identified with a single column rather than having a separate class. Once this is in, I'll rename the distributedmatrix package to linalg. Next step is to build Python interface starting with those ops I need to pull over much of LMM pipeline, with testing on the Python side against Numpy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2860
https://github.com/hail-is/hail/pull/2860:533,Testability,test,testing,533,"Depends on #2848 and #2861, I'll rebase once those are in. LocalMatrix has pointwise +,-,*,% ops with broadcasting, matrix ops, etc. It's meant to mirror NumPy restricted to always having 2 axes (we may eventually want to introduce np.array into the expression language). Vectors are identified with a single column rather than having a separate class. Once this is in, I'll rename the distributedmatrix package to linalg. Next step is to build Python interface starting with those ops I need to pull over much of LMM pipeline, with testing on the Python side against Numpy.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2860
https://github.com/hail-is/hail/pull/2862:171,Testability,test,testing,171,"Added methods to melt a BlockMatrix into a Table with the format: [row index, column index, entry]. Added method to make a Block Matrix from a numpy matrix in python, for testing the melt() method.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2862
https://github.com/hail-is/hail/issues/2868:298,Availability,error,error,298,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2868
https://github.com/hail-is/hail/issues/2868:304,Integrability,message,messages,304,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2868
https://github.com/hail-is/hail/pull/2875:4,Testability,test,tests,4,two tests were broken but have been fixed in another branch.; disabling for now.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2875
https://github.com/hail-is/hail/issues/2876:77,Availability,failure,failure,77,OrderedJoinDistinctRDD2 fails if the right side has no partitions (assertion failure in BinarySearch),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2876
https://github.com/hail-is/hail/issues/2876:67,Testability,assert,assertion,67,OrderedJoinDistinctRDD2 fails if the right side has no partitions (assertion failure in BinarySearch),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2876
https://github.com/hail-is/hail/issues/2878:35,Availability,down,download,35,e.g. https://ci.hail.is/repository/download/HailSourceCode_HailMainline_BuildDocsSpark220/55915:id/www/index.html,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2878
https://github.com/hail-is/hail/issues/2880:1347,Testability,assert,assert,1347,"Ran:; ```; >>> eval_expr(functions.binom_test(5, 10, 0.5, 'less')); ```; expected:; ```; 0.6230468749999999; ```; got:. ```; ---------------------------------------------------------------------------; AttributeError Traceback (most recent call last); <ipython-input-62-da000343a1bc> in <module>(); ----> 1 eval_expr(functions.binom_test(5, 10, 0.5, 'less')). <decorator-gen-523> in binom_test(x, n, p, alternative). /Users/konradk/Dropbox/src/python/hail/typecheck/check.pyc in _typecheck(f, *args, **kwargs); 468 def _typecheck(f, *args, **kwargs):; 469 args_, kwargs_ = check_all(f, args, kwargs, checkers, is_method=False); --> 470 return f(*args_, **kwargs_); 471; 472 return decorator(_typecheck). /Users/konradk/Dropbox/src/python/hail/expr/functions.pyc in binom_test(x, n, p, alternative); 1144 p-value.; 1145 """"""; -> 1146 return _func(""binomTest"", TFloat64(), x, n, p, alternative); 1147; 1148 @typecheck(x=expr_numeric, df=expr_numeric). /Users/konradk/Dropbox/src/python/hail/expr/functions.pyc in _func(name, ret_type, *args); 6; 7 def _func(name, ret_type, *args):; ----> 8 indices, aggregations, joins, refs = unify_all(*args); 9 return construct_expr(ApplyMethod(name, *(a._ast for a in args)), ret_type, indices, aggregations, joins, refs); 10. /Users/konradk/Dropbox/src/python/hail/expr/expression.pyc in unify_all(*exprs); 225 assert len(exprs) > 0; 226 try:; --> 227 new_indices = Indices.unify(*[e._indices for e in exprs]); 228 except ExpressionException:; 229 # source mismatch. AttributeError: 'str' object has no attribute '_indices'; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2880
https://github.com/hail-is/hail/pull/2881:0,Availability,Error,Error,0,"Error message is this:. ```; org.apache.spark.SparkException: Task not serializable. Caused by: java.io.NotSerializableException: htsjdk.samtools.reference.FastaSequenceIndex; Serialization stack:; 	- object not serializable (class: htsjdk.samtools.reference.FastaSequenceIndex, value: htsjdk.samtools.reference.FastaSequenceIndex@e7b265e); 	- writeObject data (class: java.util.HashMap); 	- object (class is.hail.io.reference.FastaReader$$anon$1, {}); 	- field (class: is.hail.io.reference.FastaReader, name: cache, type: class java.util.LinkedHashMap); 	- object (class is.hail.io.reference.FastaReader, is.hail.io.reference.FastaReader@5a0e0886); 	- field (class: is.hail.variant.GenomeReference, name: fastaReader, type: class is.hail.io.reference.FastaReader); 	- object (class is.hail.variant.GenomeReference, test); 	- field (class: is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, name: gr$13, type: class is.hail.variant.GRBase); 	- object (class is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, <function2>). plus many more lines; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2881
https://github.com/hail-is/hail/pull/2881:6,Integrability,message,message,6,"Error message is this:. ```; org.apache.spark.SparkException: Task not serializable. Caused by: java.io.NotSerializableException: htsjdk.samtools.reference.FastaSequenceIndex; Serialization stack:; 	- object not serializable (class: htsjdk.samtools.reference.FastaSequenceIndex, value: htsjdk.samtools.reference.FastaSequenceIndex@e7b265e); 	- writeObject data (class: java.util.HashMap); 	- object (class is.hail.io.reference.FastaReader$$anon$1, {}); 	- field (class: is.hail.io.reference.FastaReader, name: cache, type: class java.util.LinkedHashMap); 	- object (class is.hail.io.reference.FastaReader, is.hail.io.reference.FastaReader@5a0e0886); 	- field (class: is.hail.variant.GenomeReference, name: fastaReader, type: class is.hail.io.reference.FastaReader); 	- object (class is.hail.variant.GenomeReference, test); 	- field (class: is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, name: gr$13, type: class is.hail.variant.GRBase); 	- object (class is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, <function2>). plus many more lines; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2881
https://github.com/hail-is/hail/pull/2881:510,Performance,cache,cache,510,"Error message is this:. ```; org.apache.spark.SparkException: Task not serializable. Caused by: java.io.NotSerializableException: htsjdk.samtools.reference.FastaSequenceIndex; Serialization stack:; 	- object not serializable (class: htsjdk.samtools.reference.FastaSequenceIndex, value: htsjdk.samtools.reference.FastaSequenceIndex@e7b265e); 	- writeObject data (class: java.util.HashMap); 	- object (class is.hail.io.reference.FastaReader$$anon$1, {}); 	- field (class: is.hail.io.reference.FastaReader, name: cache, type: class java.util.LinkedHashMap); 	- object (class is.hail.io.reference.FastaReader, is.hail.io.reference.FastaReader@5a0e0886); 	- field (class: is.hail.variant.GenomeReference, name: fastaReader, type: class is.hail.io.reference.FastaReader); 	- object (class is.hail.variant.GenomeReference, test); 	- field (class: is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, name: gr$13, type: class is.hail.variant.GRBase); 	- object (class is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, <function2>). plus many more lines; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2881
https://github.com/hail-is/hail/pull/2881:379,Security,Hash,HashMap,379,"Error message is this:. ```; org.apache.spark.SparkException: Task not serializable. Caused by: java.io.NotSerializableException: htsjdk.samtools.reference.FastaSequenceIndex; Serialization stack:; 	- object not serializable (class: htsjdk.samtools.reference.FastaSequenceIndex, value: htsjdk.samtools.reference.FastaSequenceIndex@e7b265e); 	- writeObject data (class: java.util.HashMap); 	- object (class is.hail.io.reference.FastaReader$$anon$1, {}); 	- field (class: is.hail.io.reference.FastaReader, name: cache, type: class java.util.LinkedHashMap); 	- object (class is.hail.io.reference.FastaReader, is.hail.io.reference.FastaReader@5a0e0886); 	- field (class: is.hail.variant.GenomeReference, name: fastaReader, type: class is.hail.io.reference.FastaReader); 	- object (class is.hail.variant.GenomeReference, test); 	- field (class: is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, name: gr$13, type: class is.hail.variant.GRBase); 	- object (class is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, <function2>). plus many more lines; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2881
https://github.com/hail-is/hail/pull/2881:816,Testability,test,test,816,"Error message is this:. ```; org.apache.spark.SparkException: Task not serializable. Caused by: java.io.NotSerializableException: htsjdk.samtools.reference.FastaSequenceIndex; Serialization stack:; 	- object not serializable (class: htsjdk.samtools.reference.FastaSequenceIndex, value: htsjdk.samtools.reference.FastaSequenceIndex@e7b265e); 	- writeObject data (class: java.util.HashMap); 	- object (class is.hail.io.reference.FastaReader$$anon$1, {}); 	- field (class: is.hail.io.reference.FastaReader, name: cache, type: class java.util.LinkedHashMap); 	- object (class is.hail.io.reference.FastaReader, is.hail.io.reference.FastaReader@5a0e0886); 	- field (class: is.hail.variant.GenomeReference, name: fastaReader, type: class is.hail.io.reference.FastaReader); 	- object (class is.hail.variant.GenomeReference, test); 	- field (class: is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, name: gr$13, type: class is.hail.variant.GRBase); 	- object (class is.hail.expr.FunctionRegistry$$anonfun$160$$anonfun$apply$94, <function2>). plus many more lines; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2881
https://github.com/hail-is/hail/pull/2890:362,Integrability,interface,interface,362,"bandedBlocks is used to write those blocks intersecting a band around diagonal, and rectangularBlocks (or a subsequent improvement leveraging symmetry) will be used to write out all blocks overlapping a set of pre-specified LD blocks. . In Python I've exposed the blocks_to_keep option on BlockMatrix write and added BlockMatrix.write_banded along with a simple interface check. I'm not yet sure how I want to expose rectangular blocks on the Python side. I also fixed a GridPartitioner bug to properly catch overflow, replacing; ```; assert(numPartitions >= nBlockRows && numPartitions >= nBlockCols); ```; with; ```; require(nBlockRows.toLong * nBlockCols <= Int.MaxValue); ```. For the record, I compared `rectangularBlocks` as written to the following alternate implementation under a variety of contexts and found the included version to be faster even with a large number of partitions and a small number of rectangles:; ```; def rectangularBlocks(rectangles: Array[Array[Long]]): Array[Int] = {; val blocks = rectangles.par.aggregate(Set[Int]())( { case (b, r) =>; assert(r.length == 4); b ++ rectangularBlocks(r(0), r(1), r(2), r(3)) },; _ ++ _); .toArray; ; scala.util.Sorting.quickSort(blocks); ; blocks; }; ```; Using `par` for parallel aggregate above does speed things up for large numbers of large rectangles, but the included implementation is still faster with good scaling by design.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2890
https://github.com/hail-is/hail/pull/2890:252,Security,expose,exposed,252,"bandedBlocks is used to write those blocks intersecting a band around diagonal, and rectangularBlocks (or a subsequent improvement leveraging symmetry) will be used to write out all blocks overlapping a set of pre-specified LD blocks. . In Python I've exposed the blocks_to_keep option on BlockMatrix write and added BlockMatrix.write_banded along with a simple interface check. I'm not yet sure how I want to expose rectangular blocks on the Python side. I also fixed a GridPartitioner bug to properly catch overflow, replacing; ```; assert(numPartitions >= nBlockRows && numPartitions >= nBlockCols); ```; with; ```; require(nBlockRows.toLong * nBlockCols <= Int.MaxValue); ```. For the record, I compared `rectangularBlocks` as written to the following alternate implementation under a variety of contexts and found the included version to be faster even with a large number of partitions and a small number of rectangles:; ```; def rectangularBlocks(rectangles: Array[Array[Long]]): Array[Int] = {; val blocks = rectangles.par.aggregate(Set[Int]())( { case (b, r) =>; assert(r.length == 4); b ++ rectangularBlocks(r(0), r(1), r(2), r(3)) },; _ ++ _); .toArray; ; scala.util.Sorting.quickSort(blocks); ; blocks; }; ```; Using `par` for parallel aggregate above does speed things up for large numbers of large rectangles, but the included implementation is still faster with good scaling by design.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2890
https://github.com/hail-is/hail/pull/2890:410,Security,expose,expose,410,"bandedBlocks is used to write those blocks intersecting a band around diagonal, and rectangularBlocks (or a subsequent improvement leveraging symmetry) will be used to write out all blocks overlapping a set of pre-specified LD blocks. . In Python I've exposed the blocks_to_keep option on BlockMatrix write and added BlockMatrix.write_banded along with a simple interface check. I'm not yet sure how I want to expose rectangular blocks on the Python side. I also fixed a GridPartitioner bug to properly catch overflow, replacing; ```; assert(numPartitions >= nBlockRows && numPartitions >= nBlockCols); ```; with; ```; require(nBlockRows.toLong * nBlockCols <= Int.MaxValue); ```. For the record, I compared `rectangularBlocks` as written to the following alternate implementation under a variety of contexts and found the included version to be faster even with a large number of partitions and a small number of rectangles:; ```; def rectangularBlocks(rectangles: Array[Array[Long]]): Array[Int] = {; val blocks = rectangles.par.aggregate(Set[Int]())( { case (b, r) =>; assert(r.length == 4); b ++ rectangularBlocks(r(0), r(1), r(2), r(3)) },; _ ++ _); .toArray; ; scala.util.Sorting.quickSort(blocks); ; blocks; }; ```; Using `par` for parallel aggregate above does speed things up for large numbers of large rectangles, but the included implementation is still faster with good scaling by design.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2890
https://github.com/hail-is/hail/pull/2890:535,Testability,assert,assert,535,"bandedBlocks is used to write those blocks intersecting a band around diagonal, and rectangularBlocks (or a subsequent improvement leveraging symmetry) will be used to write out all blocks overlapping a set of pre-specified LD blocks. . In Python I've exposed the blocks_to_keep option on BlockMatrix write and added BlockMatrix.write_banded along with a simple interface check. I'm not yet sure how I want to expose rectangular blocks on the Python side. I also fixed a GridPartitioner bug to properly catch overflow, replacing; ```; assert(numPartitions >= nBlockRows && numPartitions >= nBlockCols); ```; with; ```; require(nBlockRows.toLong * nBlockCols <= Int.MaxValue); ```. For the record, I compared `rectangularBlocks` as written to the following alternate implementation under a variety of contexts and found the included version to be faster even with a large number of partitions and a small number of rectangles:; ```; def rectangularBlocks(rectangles: Array[Array[Long]]): Array[Int] = {; val blocks = rectangles.par.aggregate(Set[Int]())( { case (b, r) =>; assert(r.length == 4); b ++ rectangularBlocks(r(0), r(1), r(2), r(3)) },; _ ++ _); .toArray; ; scala.util.Sorting.quickSort(blocks); ; blocks; }; ```; Using `par` for parallel aggregate above does speed things up for large numbers of large rectangles, but the included implementation is still faster with good scaling by design.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2890
https://github.com/hail-is/hail/pull/2890:1072,Testability,assert,assert,1072,"bandedBlocks is used to write those blocks intersecting a band around diagonal, and rectangularBlocks (or a subsequent improvement leveraging symmetry) will be used to write out all blocks overlapping a set of pre-specified LD blocks. . In Python I've exposed the blocks_to_keep option on BlockMatrix write and added BlockMatrix.write_banded along with a simple interface check. I'm not yet sure how I want to expose rectangular blocks on the Python side. I also fixed a GridPartitioner bug to properly catch overflow, replacing; ```; assert(numPartitions >= nBlockRows && numPartitions >= nBlockCols); ```; with; ```; require(nBlockRows.toLong * nBlockCols <= Int.MaxValue); ```. For the record, I compared `rectangularBlocks` as written to the following alternate implementation under a variety of contexts and found the included version to be faster even with a large number of partitions and a small number of rectangles:; ```; def rectangularBlocks(rectangles: Array[Array[Long]]): Array[Int] = {; val blocks = rectangles.par.aggregate(Set[Int]())( { case (b, r) =>; assert(r.length == 4); b ++ rectangularBlocks(r(0), r(1), r(2), r(3)) },; _ ++ _); .toArray; ; scala.util.Sorting.quickSort(blocks); ; blocks; }; ```; Using `par` for parallel aggregate above does speed things up for large numbers of large rectangles, but the included implementation is still faster with good scaling by design.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2890
https://github.com/hail-is/hail/pull/2890:355,Usability,simpl,simple,355,"bandedBlocks is used to write those blocks intersecting a band around diagonal, and rectangularBlocks (or a subsequent improvement leveraging symmetry) will be used to write out all blocks overlapping a set of pre-specified LD blocks. . In Python I've exposed the blocks_to_keep option on BlockMatrix write and added BlockMatrix.write_banded along with a simple interface check. I'm not yet sure how I want to expose rectangular blocks on the Python side. I also fixed a GridPartitioner bug to properly catch overflow, replacing; ```; assert(numPartitions >= nBlockRows && numPartitions >= nBlockCols); ```; with; ```; require(nBlockRows.toLong * nBlockCols <= Int.MaxValue); ```. For the record, I compared `rectangularBlocks` as written to the following alternate implementation under a variety of contexts and found the included version to be faster even with a large number of partitions and a small number of rectangles:; ```; def rectangularBlocks(rectangles: Array[Array[Long]]): Array[Int] = {; val blocks = rectangles.par.aggregate(Set[Int]())( { case (b, r) =>; assert(r.length == 4); b ++ rectangularBlocks(r(0), r(1), r(2), r(3)) },; _ ++ _); .toArray; ; scala.util.Sorting.quickSort(blocks); ; blocks; }; ```; Using `par` for parallel aggregate above does speed things up for large numbers of large rectangles, but the included implementation is still faster with good scaling by design.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2890
https://github.com/hail-is/hail/pull/2895:62,Testability,test,tests,62,builds on #2864. I've rebased that on breaking and if all the tests are passing I'll merge that one and rebase this one.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2895
https://github.com/hail-is/hail/pull/2903:9,Testability,stub,stubby,9,Somewhat stubby -- the docs are barely there.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2903
https://github.com/hail-is/hail/issues/2905:86,Testability,Assert,AssertionError,86,"Trigger with `Table.range(1).annotate(x=[1, 1.0])`; ```; Java stack trace:; java.lang.AssertionError: assertion failed: at position 1 type mismatch: Int32 Float64; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.Infer$$anonfun$apply$4.apply(Infer.scala:59); 	at is.hail.expr.ir.Infer$$anonfun$apply$4.apply(Infer.scala:59); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:59); 	at is.hail.expr.ir.Infer$.is$hail$expr$ir$Infer$$infer$1(Infer.scala:9); 	at is.hail.expr.ir.Infer$$anonfun$apply$10.apply(Infer.scala:129); 	at is.hail.expr.ir.Infer$$anonfun$apply$10.apply(Infer.scala:129); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:129); 	at is.hail.expr.TableAnnotate.<init>(Relational.scala:495); 	at is.hail.table.Table.annotate(Table.scala:502); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2905
https://github.com/hail-is/hail/issues/2905:102,Testability,assert,assertion,102,"Trigger with `Table.range(1).annotate(x=[1, 1.0])`; ```; Java stack trace:; java.lang.AssertionError: assertion failed: at position 1 type mismatch: Int32 Float64; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.Infer$$anonfun$apply$4.apply(Infer.scala:59); 	at is.hail.expr.ir.Infer$$anonfun$apply$4.apply(Infer.scala:59); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:59); 	at is.hail.expr.ir.Infer$.is$hail$expr$ir$Infer$$infer$1(Infer.scala:9); 	at is.hail.expr.ir.Infer$$anonfun$apply$10.apply(Infer.scala:129); 	at is.hail.expr.ir.Infer$$anonfun$apply$10.apply(Infer.scala:129); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:129); 	at is.hail.expr.TableAnnotate.<init>(Relational.scala:495); 	at is.hail.table.Table.annotate(Table.scala:502); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2905
https://github.com/hail-is/hail/issues/2905:182,Testability,assert,assert,182,"Trigger with `Table.range(1).annotate(x=[1, 1.0])`; ```; Java stack trace:; java.lang.AssertionError: assertion failed: at position 1 type mismatch: Int32 Float64; 	at scala.Predef$.assert(Predef.scala:170); 	at is.hail.expr.ir.Infer$$anonfun$apply$4.apply(Infer.scala:59); 	at is.hail.expr.ir.Infer$$anonfun$apply$4.apply(Infer.scala:59); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:59); 	at is.hail.expr.ir.Infer$.is$hail$expr$ir$Infer$$infer$1(Infer.scala:9); 	at is.hail.expr.ir.Infer$$anonfun$apply$10.apply(Infer.scala:129); 	at is.hail.expr.ir.Infer$$anonfun$apply$10.apply(Infer.scala:129); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at is.hail.expr.ir.Infer$.apply(Infer.scala:129); 	at is.hail.expr.TableAnnotate.<init>(Relational.scala:495); 	at is.hail.table.Table.annotate(Table.scala:502); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thr",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2905
https://github.com/hail-is/hail/pull/2908:0,Integrability,Depend,Depends,0,Depends on #2903,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2908
https://github.com/hail-is/hail/issues/2921:27,Availability,error,error,27,Print lhs on out of bounds error on [] in expr language. Also throwing a FatalException will distinguish bugs on our end vs bugs in user expressions.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2921
https://github.com/hail-is/hail/pull/2922:303,Usability,clear,clear,303,"I wasn't sure what to do for the SparkAnnImpExport. I also don't love how tuples are represented as rows in AnnotationImpExport (and `Table.show()`). I'd prefer parentheses instead of brackets, but I decided to keep it as is for now. Otherwise, I think I'll have to write a tuple parser. It also wasn't clear to me what the best way is to incorporate TTuple into the Type `gen`.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2922
https://github.com/hail-is/hail/pull/2930:77,Availability,error,error,77,"The default options are all false. If left all false, the user gets an clear error saying that they must include at least one entry field. This forces users to think about what they actually need to import, as it can make a big difference on, say, UKBB until we have better tech. I've updated the docs and tests accordingly. @cseed suggested that we remove BGEN v1.1 support if nobody is reliant on it anymore. I've asked on Slack. So I didn't add more complexity to support these options for BGEN v1.1. Rather this PR requires GT and GP set to true if any file is 1.1 (as explained in docs and error message). If nobody minds, we can rip out BGEN 1.1 and update the docs simultaneously in a subsequent PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2930
https://github.com/hail-is/hail/pull/2930:595,Availability,error,error,595,"The default options are all false. If left all false, the user gets an clear error saying that they must include at least one entry field. This forces users to think about what they actually need to import, as it can make a big difference on, say, UKBB until we have better tech. I've updated the docs and tests accordingly. @cseed suggested that we remove BGEN v1.1 support if nobody is reliant on it anymore. I've asked on Slack. So I didn't add more complexity to support these options for BGEN v1.1. Rather this PR requires GT and GP set to true if any file is 1.1 (as explained in docs and error message). If nobody minds, we can rip out BGEN 1.1 and update the docs simultaneously in a subsequent PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2930
https://github.com/hail-is/hail/pull/2930:285,Deployability,update,updated,285,"The default options are all false. If left all false, the user gets an clear error saying that they must include at least one entry field. This forces users to think about what they actually need to import, as it can make a big difference on, say, UKBB until we have better tech. I've updated the docs and tests accordingly. @cseed suggested that we remove BGEN v1.1 support if nobody is reliant on it anymore. I've asked on Slack. So I didn't add more complexity to support these options for BGEN v1.1. Rather this PR requires GT and GP set to true if any file is 1.1 (as explained in docs and error message). If nobody minds, we can rip out BGEN 1.1 and update the docs simultaneously in a subsequent PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2930
https://github.com/hail-is/hail/pull/2930:656,Deployability,update,update,656,"The default options are all false. If left all false, the user gets an clear error saying that they must include at least one entry field. This forces users to think about what they actually need to import, as it can make a big difference on, say, UKBB until we have better tech. I've updated the docs and tests accordingly. @cseed suggested that we remove BGEN v1.1 support if nobody is reliant on it anymore. I've asked on Slack. So I didn't add more complexity to support these options for BGEN v1.1. Rather this PR requires GT and GP set to true if any file is 1.1 (as explained in docs and error message). If nobody minds, we can rip out BGEN 1.1 and update the docs simultaneously in a subsequent PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2930
https://github.com/hail-is/hail/pull/2930:601,Integrability,message,message,601,"The default options are all false. If left all false, the user gets an clear error saying that they must include at least one entry field. This forces users to think about what they actually need to import, as it can make a big difference on, say, UKBB until we have better tech. I've updated the docs and tests accordingly. @cseed suggested that we remove BGEN v1.1 support if nobody is reliant on it anymore. I've asked on Slack. So I didn't add more complexity to support these options for BGEN v1.1. Rather this PR requires GT and GP set to true if any file is 1.1 (as explained in docs and error message). If nobody minds, we can rip out BGEN 1.1 and update the docs simultaneously in a subsequent PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2930
https://github.com/hail-is/hail/pull/2930:306,Testability,test,tests,306,"The default options are all false. If left all false, the user gets an clear error saying that they must include at least one entry field. This forces users to think about what they actually need to import, as it can make a big difference on, say, UKBB until we have better tech. I've updated the docs and tests accordingly. @cseed suggested that we remove BGEN v1.1 support if nobody is reliant on it anymore. I've asked on Slack. So I didn't add more complexity to support these options for BGEN v1.1. Rather this PR requires GT and GP set to true if any file is 1.1 (as explained in docs and error message). If nobody minds, we can rip out BGEN 1.1 and update the docs simultaneously in a subsequent PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2930
https://github.com/hail-is/hail/pull/2930:71,Usability,clear,clear,71,"The default options are all false. If left all false, the user gets an clear error saying that they must include at least one entry field. This forces users to think about what they actually need to import, as it can make a big difference on, say, UKBB until we have better tech. I've updated the docs and tests accordingly. @cseed suggested that we remove BGEN v1.1 support if nobody is reliant on it anymore. I've asked on Slack. So I didn't add more complexity to support these options for BGEN v1.1. Rather this PR requires GT and GP set to true if any file is 1.1 (as explained in docs and error message). If nobody minds, we can rip out BGEN 1.1 and update the docs simultaneously in a subsequent PR.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2930
https://github.com/hail-is/hail/pull/2934:122,Testability,log,logic,122,"Don't convert variant to locus/alleles in fromLegacy.; Fix from legacy callers. VSM.gen is the only place that needs that logic now.; Remove Variant/AltAllele methods from FunctionRegistry. Removed combineVariants. Can be done in Python now.; Removed unused LDMatrix. The list of places (T)Variant is used now is very small: VSM generators, VEP/Nirvana and some importers.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2934
https://github.com/hail-is/hail/pull/2936:236,Usability,clear,clearly,236,"I also added skipBytes for use with RowMatrix.readBlockMatrix. @cseed I still need to experiment on compression w.r.t. banded LD project, which should inform the choice of BDM BufferSpec further, but using the general infrastructure is clearly better than the one-off BDM buffers I'd written before. Is it reasonable to merge this to master (post review)?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2936
https://github.com/hail-is/hail/issues/2938:225,Availability,error,error,225,"In 0.2, there is Table.to_spark but Table.from_spark is missing. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2938
https://github.com/hail-is/hail/issues/2938:231,Integrability,message,messages,231,"In 0.2, there is Table.to_spark but Table.from_spark is missing. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2938
https://github.com/hail-is/hail/pull/2940:123,Availability,mask,mask,123,I think the syntax change is needed because numpy versions older than 1.12 don't let you filter a numpy array on a boolean mask in this way.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2940
https://github.com/hail-is/hail/pull/2941:190,Deployability,deploy,deploys,190,"determines which spark version(s) are tested by PR build step. I changed the CI as follows:. - there are now three source code subprojects: 0.1, mainline and PRs; - mainline now only builds/deploys against 2.2.0; - PRs builds against both 2.0.2 and 2.2.0, but! if there is a `./deployed-spark-versions.txt` file, it only builds versions listed in that file (that is, immediately succeeds for versions not listed in that file). As soon as this goes in, python3 should be able to pass. Unless I broke something else, which is basically a 100% certainty.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2941
https://github.com/hail-is/hail/pull/2941:278,Deployability,deploy,deployed-spark-versions,278,"determines which spark version(s) are tested by PR build step. I changed the CI as follows:. - there are now three source code subprojects: 0.1, mainline and PRs; - mainline now only builds/deploys against 2.2.0; - PRs builds against both 2.0.2 and 2.2.0, but! if there is a `./deployed-spark-versions.txt` file, it only builds versions listed in that file (that is, immediately succeeds for versions not listed in that file). As soon as this goes in, python3 should be able to pass. Unless I broke something else, which is basically a 100% certainty.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2941
https://github.com/hail-is/hail/pull/2941:38,Testability,test,tested,38,"determines which spark version(s) are tested by PR build step. I changed the CI as follows:. - there are now three source code subprojects: 0.1, mainline and PRs; - mainline now only builds/deploys against 2.2.0; - PRs builds against both 2.0.2 and 2.2.0, but! if there is a `./deployed-spark-versions.txt` file, it only builds versions listed in that file (that is, immediately succeeds for versions not listed in that file). As soon as this goes in, python3 should be able to pass. Unless I broke something else, which is basically a 100% certainty.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2941
https://github.com/hail-is/hail/pull/2942:49,Deployability,update,updated,49,"put import_matrix_table into python, with docs + updated interface. Builds on #2939.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2942
https://github.com/hail-is/hail/pull/2942:57,Integrability,interface,interface,57,"put import_matrix_table into python, with docs + updated interface. Builds on #2939.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2942
https://github.com/hail-is/hail/issues/2949:1697,Availability,Error,Error,1697,"from @zaczap; ```; 2018-02-21 15:30:31 Hail: INFO: interval filter loaded 89 of 1274 partitions; Traceback (most recent call last):; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/temp_vep.py"", line 34, in <module>; vds = hl.filter_intervals(vds, intervals); File ""<decorator-gen-718>"", line 2, in filter_intervals; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/hail-devel-5b95158ed055.zip/hail/utils/java.py"", line 198, in handle_py4j; hail.utils.java.FatalError: requirement failed. Java stack trace:; scala.Predef$.require(Predef.scala:212); at is.hail.rvd.OrderedRVDPartitioner.<init>(OrderedRVDPartitioner.scala:28); at is.hail.rvd.OrderedRVDPartitioner.copy(OrderedRVDPartitioner.scala:98); at is.hail.rvd.OrderedRVD.filterIntervals(OrderedRVD.scala:270); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:23); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:18); at is.hail.methods.FilterIntervals.apply(FilterIntervals.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); Hail version: devel-5b95158; Error summary: requirement failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [85fbcd66-b973-4c5c-9216-ef00b9a7f3a7] entered state [ERROR] while waiting for [DONE].; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2949
https://github.com/hail-is/hail/issues/2949:1732,Availability,ERROR,ERROR,1732,"from @zaczap; ```; 2018-02-21 15:30:31 Hail: INFO: interval filter loaded 89 of 1274 partitions; Traceback (most recent call last):; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/temp_vep.py"", line 34, in <module>; vds = hl.filter_intervals(vds, intervals); File ""<decorator-gen-718>"", line 2, in filter_intervals; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/hail-devel-5b95158ed055.zip/hail/utils/java.py"", line 198, in handle_py4j; hail.utils.java.FatalError: requirement failed. Java stack trace:; scala.Predef$.require(Predef.scala:212); at is.hail.rvd.OrderedRVDPartitioner.<init>(OrderedRVDPartitioner.scala:28); at is.hail.rvd.OrderedRVDPartitioner.copy(OrderedRVDPartitioner.scala:98); at is.hail.rvd.OrderedRVD.filterIntervals(OrderedRVD.scala:270); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:23); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:18); at is.hail.methods.FilterIntervals.apply(FilterIntervals.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); Hail version: devel-5b95158; Error summary: requirement failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [85fbcd66-b973-4c5c-9216-ef00b9a7f3a7] entered state [ERROR] while waiting for [DONE].; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2949
https://github.com/hail-is/hail/issues/2949:1835,Availability,ERROR,ERROR,1835,"from @zaczap; ```; 2018-02-21 15:30:31 Hail: INFO: interval filter loaded 89 of 1274 partitions; Traceback (most recent call last):; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/temp_vep.py"", line 34, in <module>; vds = hl.filter_intervals(vds, intervals); File ""<decorator-gen-718>"", line 2, in filter_intervals; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/hail-devel-5b95158ed055.zip/hail/utils/java.py"", line 198, in handle_py4j; hail.utils.java.FatalError: requirement failed. Java stack trace:; scala.Predef$.require(Predef.scala:212); at is.hail.rvd.OrderedRVDPartitioner.<init>(OrderedRVDPartitioner.scala:28); at is.hail.rvd.OrderedRVDPartitioner.copy(OrderedRVDPartitioner.scala:98); at is.hail.rvd.OrderedRVD.filterIntervals(OrderedRVD.scala:270); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:23); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:18); at is.hail.methods.FilterIntervals.apply(FilterIntervals.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); Hail version: devel-5b95158; Error summary: requirement failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [85fbcd66-b973-4c5c-9216-ef00b9a7f3a7] entered state [ERROR] while waiting for [DONE].; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2949
https://github.com/hail-is/hail/issues/2949:67,Performance,load,loaded,67,"from @zaczap; ```; 2018-02-21 15:30:31 Hail: INFO: interval filter loaded 89 of 1274 partitions; Traceback (most recent call last):; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/temp_vep.py"", line 34, in <module>; vds = hl.filter_intervals(vds, intervals); File ""<decorator-gen-718>"", line 2, in filter_intervals; File ""/tmp/85fbcd66-b973-4c5c-9216-ef00b9a7f3a7/hail-devel-5b95158ed055.zip/hail/utils/java.py"", line 198, in handle_py4j; hail.utils.java.FatalError: requirement failed. Java stack trace:; scala.Predef$.require(Predef.scala:212); at is.hail.rvd.OrderedRVDPartitioner.<init>(OrderedRVDPartitioner.scala:28); at is.hail.rvd.OrderedRVDPartitioner.copy(OrderedRVDPartitioner.scala:98); at is.hail.rvd.OrderedRVD.filterIntervals(OrderedRVD.scala:270); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:23); at is.hail.methods.FilterIntervals$.apply(FilterIntervals.scala:18); at is.hail.methods.FilterIntervals.apply(FilterIntervals.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); Hail version: devel-5b95158; Error summary: requirement failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [85fbcd66-b973-4c5c-9216-ef00b9a7f3a7] entered state [ERROR] while waiting for [DONE].; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2949
https://github.com/hail-is/hail/pull/2950:54,Testability,test,tests,54,Fixes #2949. I'm going to add some more comprehensive tests also so that these cases get tested.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2950
https://github.com/hail-is/hail/pull/2950:89,Testability,test,tested,89,Fixes #2949. I'm going to add some more comprehensive tests also so that these cases get tested.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2950
https://github.com/hail-is/hail/pull/2951:66,Deployability,update,updated,66,NB: we cannot use python 3.6 because ipython-notebook hasn't been updated yet.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2951
https://github.com/hail-is/hail/pull/2965:243,Testability,test,tests,243,"- matrix: writeBand, writeRectangles, exportRectangles; - GridPartitioner: read, bandedBlocks, lowerTriangularBlocks, rectangularBlocks; - BlockMatrix: exportRectangles, writeBand, writeRectangles, ExportRectanglesRDD; - GridPartitionerSuite: tests; - BlockMatrixSuite: tests",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2965
https://github.com/hail-is/hail/pull/2965:270,Testability,test,tests,270,"- matrix: writeBand, writeRectangles, exportRectangles; - GridPartitioner: read, bandedBlocks, lowerTriangularBlocks, rectangularBlocks; - BlockMatrix: exportRectangles, writeBand, writeRectangles, ExportRectanglesRDD; - GridPartitionerSuite: tests; - BlockMatrixSuite: tests",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2965
https://github.com/hail-is/hail/issues/2966:46,Availability,Error,Error,46,"### Hail version:; Hail version: 0.1-20613ed; Error summary: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Hail JAR appears to not be visible on workers despite setting `--jars`, `--driver-class-path`, and `--conf spark.executor.extraClassPath`. . ### What you did:; 1) Downloaded pre-built binaries for 2.0.2. 2) Zipped the hail/python/hail folder into hail.zip. 3) Launched a local pyspark shell as follows:; ```; pyspark --jars $HAIL_HOME/jars/hail-all-spark.jar \; > --driver-class-path ./hail-all-spark.jar \; > --conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:276,Availability,Down,Downloaded,276,"### Hail version:; Hail version: 0.1-20613ed; Error summary: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Hail JAR appears to not be visible on workers despite setting `--jars`, `--driver-class-path`, and `--conf spark.executor.extraClassPath`. . ### What you did:; 1) Downloaded pre-built binaries for 2.0.2. 2) Zipped the hail/python/hail folder into hail.zip. 3) Launched a local pyspark shell as follows:; ```; pyspark --jars $HAIL_HOME/jars/hail-all-spark.jar \; > --driver-class-path ./hail-all-spark.jar \; > --conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:857,Availability,error,error,857,"### Hail version:; Hail version: 0.1-20613ed; Error summary: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Hail JAR appears to not be visible on workers despite setting `--jars`, `--driver-class-path`, and `--conf spark.executor.extraClassPath`. . ### What you did:; 1) Downloaded pre-built binaries for 2.0.2. 2) Zipped the hail/python/hail folder into hail.zip. 3) Launched a local pyspark shell as follows:; ```; pyspark --jars $HAIL_HOME/jars/hail-all-spark.jar \; > --driver-class-path ./hail-all-spark.jar \; > --conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:1078,Availability,error,error,1078,"Function2. Hail JAR appears to not be visible on workers despite setting `--jars`, `--driver-class-path`, and `--conf spark.executor.extraClassPath`. . ### What you did:; 1) Downloaded pre-built binaries for 2.0.2. 2) Zipped the hail/python/hail folder into hail.zip. 3) Launched a local pyspark shell as follows:; ```; pyspark --jars $HAIL_HOME/jars/hail-all-spark.jar \; > --driver-class-path ./hail-all-spark.jar \; > --conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:2047,Availability,avail,available,2047," && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:2183,Availability,avail,available,2183,"6 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:16760,Availability,Error,Error,16760, scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at is.hail.expr.MatrixValue.filterSamples(Relational.scala:156); at is.hail.expr.FilterSamples.execute(Relational.scala:326); at is.hail.variant.VariantSampleMatrix.value$lzycompute(VariantSampleMatrix.scala:490); at is.hail.variant.VariantSampleMatrix.value(VariantSampleMatrix.scala:488); at is.hail.variant.VariantSampleMatrix.x$13$lzycompute(VariantSampleMatrix.scala:493); at is.hail.variant.VariantSampleMatrix.x$13(VariantSampleMatrix.scala:493); at is.hail.variant.VariantSampleMatrix.globalAnnotation$lzycompute(VariantSampleMatrix.scala:493); at is.hail.variant.VariantSampleMatrix.globalAnnotation(VariantSampleMatrix.scala:493); at is.hail.variant.VariantDatasetFunctions$.filterGenotypes$extension(VariantDataset.scala:463); at is.hail.variant.VariantDatasetFunctions.filterGenotypes(VariantDataset.scala:449); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```. Hail version: 0.1-20613ed; Error summary: ClassNotFoundException: is.hail.asm4s.AsmFunction2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:1084,Integrability,message,messages,1084,"Function2. Hail JAR appears to not be visible on workers despite setting `--jars`, `--driver-class-path`, and `--conf spark.executor.extraClassPath`. . ### What you did:; 1) Downloaded pre-built binaries for 2.0.2. 2) Zipped the hail/python/hail folder into hail.zip. 3) Launched a local pyspark shell as follows:; ```; pyspark --jars $HAIL_HOME/jars/hail-all-spark.jar \; > --driver-class-path ./hail-all-spark.jar \; > --conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc =",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:1752,Integrability,interface,interface,1752,"tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsump",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:1528,Performance,load,load,1528,"-conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type im",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:2534,Performance,Load,Loading,2534,"10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:2584,Performance,Load,Loading,2584,"10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:2638,Performance,Load,Loading,2638,"10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:2697,Performance,Load,Loading,2697,"10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:2750,Performance,Load,Loading,2750,"10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:2805,Performance,Load,Loading,2805,"10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/i",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:3017,Performance,load,load,3017,".7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/io/Serializable is/hail/asm4s/AsmFunction2 {. // access flags 0x1; public apply([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; L0; ALOAD 2; LDC 2; INVOKEVIRTUAL scala/collection/mutable/ArrayBuffer.apply (I)Ljava/lang/Object;; CHECKCAST org/apache/spark/sql/Row; ASTORE 3; ALOAD 3; IFNULL L1; ALOAD 3; LDC 5; INVOKEINTERFACE org/",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:10710,Performance,load,loadOrDefineClass,10710," 1; CHECKCAST [Ljava/lang/Object;; ALOAD 2; CHECKCAST scala/collection/mutable/ArrayBuffer; INVOKESPECIAL is/hail/codegen/generated/C0.apply ([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; ARETURN; MAXSTACK = 3; MAXLOCALS = 3; }; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-297>"", line 2, in filter_genotypes; File ""/tmp/spark-0721abd3-c72d-4439-a655-c09fddad864c/userFiles-7c41df44-c5b2-44b1-924e-8f73a9aa8148/hail.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Java stack trace:; java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at s",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:10774,Performance,load,loadClass,10774,"collection/mutable/ArrayBuffer; INVOKESPECIAL is/hail/codegen/generated/C0.apply ([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; ARETURN; MAXSTACK = 3; MAXLOCALS = 3; }; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-297>"", line 2, in filter_genotypes; File ""/tmp/spark-0721abd3-c72d-4439-a655-c09fddad864c/userFiles-7c41df44-c5b2-44b1-924e-8f73a9aa8148/hail.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Java stack trace:; java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scal",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:13599,Performance,load,loadClass,13599,tions$.filterGenotypes$extension(VariantDataset.scala:463); at is.hail.variant.VariantDatasetFunctions.filterGenotypes(VariantDataset.scala:449); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.ha,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:13657,Performance,load,loadClass,13657,; at is.hail.variant.VariantDatasetFunctions.filterGenotypes(VariantDataset.scala:449); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:13982,Performance,load,loadOrDefineClass,13982,java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:14046,Performance,load,loadClass,14046,flection.MethodInvoker.invoke(MethodInvoker.java:237); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.ClassNotFoundException: is.hail.asm4s.AsmFunction2; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:250); at is.hail.asm4s.package$.loadClass(package.scala:261); at is.hail.asm4s.FunctionBuilder$$anon$2.apply(FunctionBuilder.scala:218); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:80); at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:53); at is.hail.expr.Parser$$anonfun$evalTypedExpr$1.apply(Parser.scala:71); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:324); at is.hail.expr.FilterSamples$$anonfun$12.apply(Relational.scala:321); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at is.hail.expr.MatrixValue$$anonfun$4.apply(Relational.scala:156); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scal,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:3560,Security,access,access,3560,"type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/io/Serializable is/hail/asm4s/AsmFunction2 {. // access flags 0x1; public apply([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; L0; ALOAD 2; LDC 2; INVOKEVIRTUAL scala/collection/mutable/ArrayBuffer.apply (I)Ljava/lang/Object;; CHECKCAST org/apache/spark/sql/Row; ASTORE 3; ALOAD 3; IFNULL L1; ALOAD 3; LDC 5; INVOKEINTERFACE org/apache/spark/sql/Row.get (I)Ljava/lang/Object;; CHECKCAST org/apache/spark/sql/Row; GOTO L2; L1; FRAME APPEND [org/apache/spark/sql/Row]; ACONST_NULL; L2; FRAME SAME1 org/apache/spark/sql/Row; ASTORE 4; ALOAD 4; IFNULL L3; ALOAD 4; LDC 12; INVOKEINTERFACE org/apache/spark/sql/Row.get (I)Ljava/lang/Object;; CHECKCAST java/lang/Double; GOTO L4; L3; FRAME APPEND [org/apache/spark/sql/Row]; ACONST_NULL; L4; FRAME SAME1 java/lang/Double; ASTORE 5; ALOAD 5; IFNULL L5; NEW java/lang/Integer; DUP; LDC 4; INVOKESPECIAL java/lang/Integer.<init> (I)V; ASTORE 6; ALOAD 6; IFNULL L6; ALOAD 1; LDC 0; AALOAD; ALOAD 5; ALOAD 6; ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:3685,Security,access,access,3685,"air' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/io/Serializable is/hail/asm4s/AsmFunction2 {. // access flags 0x1; public apply([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; L0; ALOAD 2; LDC 2; INVOKEVIRTUAL scala/collection/mutable/ArrayBuffer.apply (I)Ljava/lang/Object;; CHECKCAST org/apache/spark/sql/Row; ASTORE 3; ALOAD 3; IFNULL L1; ALOAD 3; LDC 5; INVOKEINTERFACE org/apache/spark/sql/Row.get (I)Ljava/lang/Object;; CHECKCAST org/apache/spark/sql/Row; GOTO L2; L1; FRAME APPEND [org/apache/spark/sql/Row]; ACONST_NULL; L2; FRAME SAME1 org/apache/spark/sql/Row; ASTORE 4; ALOAD 4; IFNULL L3; ALOAD 4; LDC 12; INVOKEINTERFACE org/apache/spark/sql/Row.get (I)Ljava/lang/Object;; CHECKCAST java/lang/Double; GOTO L4; L3; FRAME APPEND [org/apache/spark/sql/Row]; ACONST_NULL; L4; FRAME SAME1 java/lang/Double; ASTORE 5; ALOAD 5; IFNULL L5; NEW java/lang/Integer; DUP; LDC 4; INVOKESPECIAL java/lang/Integer.<init> (I)V; ASTORE 6; ALOAD 6; IFNULL L6; ALOAD 1; LDC 0; AALOAD; ALOAD 5; ALOAD 6; INVOKEINTERFACE scala/Function2.apply (Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;; GOTO L7; L6; FRAME APPEND [java/lang/Double java/lang/Integer]",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:9493,Security,access,access,9493,"ed/C0 [Ljava/lang/Object; scala/collection/mutable/ArrayBuffer org/apache/spark/sql/Row org/apache/spark/sql/Row java/lang/Double T org/apache/spark/sql/Row org/apache/spark/sql/Row java/lang/Double] [java/lang/Boolean java/lang/Boolean]; POP; L10; FRAME FULL [is/hail/codegen/generated/C0 [Ljava/lang/Object; scala/collection/mutable/ArrayBuffer org/apache/spark/sql/Row org/apache/spark/sql/Row java/lang/Double] [java/lang/Boolean]; CHECKCAST java/lang/Boolean; ARETURN; L21; FRAME FULL [] [java/lang/Throwable]; ATHROW; L22; LOCALVARIABLE local3 Ljava/lang/Object; L0 L22 3; LOCALVARIABLE local4 Ljava/lang/Object; L0 L22 4; LOCALVARIABLE local5 Ljava/lang/Object; L0 L22 5; LOCALVARIABLE local6 Ljava/lang/Object; L0 L22 6; LOCALVARIABLE local7 Ljava/lang/Object; L0 L22 7; LOCALVARIABLE local8 Ljava/lang/Object; L0 L22 8; LOCALVARIABLE local9 Ljava/lang/Object; L0 L22 9; LOCALVARIABLE local10 Ljava/lang/Object; L0 L22 10; MAXSTACK = 6; MAXLOCALS = 11. // access flags 0x1; public <init>()V; ALOAD 0; INVOKESPECIAL java/lang/Object.<init> ()V; RETURN; MAXSTACK = 1; MAXLOCALS = 1. // access flags 0x1; public apply(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;; ALOAD 0; ALOAD 1; CHECKCAST [Ljava/lang/Object;; ALOAD 2; CHECKCAST scala/collection/mutable/ArrayBuffer; INVOKESPECIAL is/hail/codegen/generated/C0.apply ([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; ARETURN; MAXSTACK = 3; MAXLOCALS = 3; }; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-297>"", line 2, in filter_genotypes; File ""/tmp/spark-0721abd3-c72d-4439-a655-c09fddad864c/userFiles-7c41df44-c5b2-44b1-924e-8f73a9aa8148/hail.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Java stack trace:; java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:9621,Security,access,access,9621,"ng/Boolean java/lang/Boolean]; POP; L10; FRAME FULL [is/hail/codegen/generated/C0 [Ljava/lang/Object; scala/collection/mutable/ArrayBuffer org/apache/spark/sql/Row org/apache/spark/sql/Row java/lang/Double] [java/lang/Boolean]; CHECKCAST java/lang/Boolean; ARETURN; L21; FRAME FULL [] [java/lang/Throwable]; ATHROW; L22; LOCALVARIABLE local3 Ljava/lang/Object; L0 L22 3; LOCALVARIABLE local4 Ljava/lang/Object; L0 L22 4; LOCALVARIABLE local5 Ljava/lang/Object; L0 L22 5; LOCALVARIABLE local6 Ljava/lang/Object; L0 L22 6; LOCALVARIABLE local7 Ljava/lang/Object; L0 L22 7; LOCALVARIABLE local8 Ljava/lang/Object; L0 L22 8; LOCALVARIABLE local9 Ljava/lang/Object; L0 L22 9; LOCALVARIABLE local10 Ljava/lang/Object; L0 L22 10; MAXSTACK = 6; MAXLOCALS = 11. // access flags 0x1; public <init>()V; ALOAD 0; INVOKESPECIAL java/lang/Object.<init> ()V; RETURN; MAXSTACK = 1; MAXLOCALS = 1. // access flags 0x1; public apply(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;; ALOAD 0; ALOAD 1; CHECKCAST [Ljava/lang/Object;; ALOAD 2; CHECKCAST scala/collection/mutable/ArrayBuffer; INVOKESPECIAL is/hail/codegen/generated/C0.apply ([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; ARETURN; MAXSTACK = 3; MAXLOCALS = 3; }; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-297>"", line 2, in filter_genotypes; File ""/tmp/spark-0721abd3-c72d-4439-a655-c09fddad864c/userFiles-7c41df44-c5b2-44b1-924e-8f73a9aa8148/hail.zip/hail/java.py"", line 121, in handle_py4j; hail.java.FatalError: ClassNotFoundException: is.hail.asm4s.AsmFunction2. Java stack trace:; java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; at java.lang.ClassLoader.defineClass1(Native Method); at java.lang.ClassLoader.defineClass(ClassLoader.java:763); at java.lang.ClassLoader.defineClass(ClassLoader.java:642); at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:254); at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:1400,Testability,log,log,1400,"k shell as follows:; ```; pyspark --jars $HAIL_HOME/jars/hail-all-spark.jar \; > --driver-class-path ./hail-all-spark.jar \; > --conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:1432,Testability,log,logging,1432," $HAIL_HOME/jars/hail-all-spark.jar \; > --driver-class-path ./hail-all-spark.jar \; > --conf spark.executor.extraClassPath=./hail-all-spark.jar \; > --py-files $HAIL_HOME/python/hail.zip \; > --conf spark.sql.files.openCostInBytes=1099511627776 \; > --conf spark.sql.files.maxPartitionBytes=1099511627776 \; > --conf spark.hadoop.parquet.block.size=1099511627776; ```. 4) Attempted to run the Hail tutorial, received an error when calling `common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))')`. ### What went wrong (all error messages here, including the full java stack trace):; ```; Python 2.7.14 |Anaconda, Inc.| (default, Oct 16 2017, 17:29:19); [GCC 7.2.0] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; 18/02/22 20:29:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/02/22 20:29:10 WARN Utils: Your hostname, CompyWompy resolves to a loopback address: 127.0.1.1; using 192.168.1.122 instead (on interface eth0); 18/02/22 20:29:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.14 (default, Oct 16 2017 17:29:19); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = HailContext(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2966:3106,Testability,log,logger,3106,"Context(spark.sparkContext); Running on Apache Spark version 2.0.2; SparkUI available at http://192.168.1.122:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-20613ed; >>> table = hc.import_table('data/1kg_annotations.txt', impute=True).key_by('Sample'); 2018-02-22 20:29:45 Hail: INFO: Reading table to impute column types; 2018-02-22 20:29:45 Hail: INFO: Finished type imputation; Loading column `Sample' as type String (imputed); Loading column `Population' as type String (imputed); Loading column `SuperPopulation' as type String (imputed); Loading column `isFemale' as type Boolean (imputed); Loading column `PurpleHair' as type Boolean (imputed); Loading column `CaffeineConsumption' as type Int (imputed); >>> common_vds = hc.read('/mnt/d/metistream/hail/data/1kg.vds'); >>> common_vds = common_vds.annotate_samples_table(table, root='sa'); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; >>> common_vds = common_vds.sample_qc(); >>> common_vds = common_vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); >>> common_vds = common_vds.filter_genotypes('let ab = g.ad[1] / g.ad.sum() in ((g.isHomRef && ab <= 0.1) || (g.isHet && ab >= 0.25 && ab <= 0.75) ||(g.isHomVar && ab >= 0.9))'); // class version 52.0 (52); // access flags 0x1; public class is/hail/codegen/generated/C0 implements java/io/Serializable is/hail/asm4s/AsmFunction2 {. // access flags 0x1; public apply([Ljava/lang/Object;Lscala/collection/mutable/ArrayBuffer;)Ljava/lang/Object;; L0; ALOAD 2; LDC 2; INVOKEVIRTUAL scala/collection/mutable/ArrayBuffer.apply (I)Ljava/lang/Object;; CHECKCAST org/apache/spark/sql/Row; ASTORE 3; ALOAD 3; IFNULL L1; ALOAD 3; LDC 5; INVOKEINTERFACE org/apache/spark/sql/Row.get (I)Ljava/lang/Object;; CHECKCAST org/apache/spark/sql/Row; GOTO L2; L1; FRAME APPEND [o",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2966
https://github.com/hail-is/hail/issues/2982:186,Availability,error,error,186,"```; import hail as hl; ds = hl.balding_nichols_model(3, 100, 100); ds.annotate_globals(x=[1,2,3]); ```; The above script breaks on devel clusters.; ```; py4j.protocol.Py4JJavaError: An error occurred while calling o64.annotateGlobalExpr.; : java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:763); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:642); 	at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:174); 	at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:170); 	at is.hail.asm4s.package$.loadClass(package.scala:181); 	at is.hail.asm4s.FunctionBuilder$$anon$1.apply(FunctionBuilder.scala:312); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$12$$anonfun$apply$6.apply(Parser.scala:172); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2982
https://github.com/hail-is/hail/issues/2982:159,Integrability,protocol,protocol,159,"```; import hail as hl; ds = hl.balding_nichols_model(3, 100, 100); ds.annotate_globals(x=[1,2,3]); ```; The above script breaks on devel clusters.; ```; py4j.protocol.Py4JJavaError: An error occurred while calling o64.annotateGlobalExpr.; : java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:763); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:642); 	at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:174); 	at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:170); 	at is.hail.asm4s.package$.loadClass(package.scala:181); 	at is.hail.asm4s.FunctionBuilder$$anon$1.apply(FunctionBuilder.scala:312); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$12$$anonfun$apply$6.apply(Parser.scala:172); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2982
https://github.com/hail-is/hail/issues/2982:599,Performance,load,loadOrDefineClass,599,"```; import hail as hl; ds = hl.balding_nichols_model(3, 100, 100); ds.annotate_globals(x=[1,2,3]); ```; The above script breaks on devel clusters.; ```; py4j.protocol.Py4JJavaError: An error occurred while calling o64.annotateGlobalExpr.; : java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:763); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:642); 	at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:174); 	at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:170); 	at is.hail.asm4s.package$.loadClass(package.scala:181); 	at is.hail.asm4s.FunctionBuilder$$anon$1.apply(FunctionBuilder.scala:312); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$12$$anonfun$apply$6.apply(Parser.scala:172); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2982
https://github.com/hail-is/hail/issues/2982:664,Performance,load,loadClass,664,"```; import hail as hl; ds = hl.balding_nichols_model(3, 100, 100); ds.annotate_globals(x=[1,2,3]); ```; The above script breaks on devel clusters.; ```; py4j.protocol.Py4JJavaError: An error occurred while calling o64.annotateGlobalExpr.; : java.lang.NoClassDefFoundError: is/hail/asm4s/AsmFunction2; 	at java.lang.ClassLoader.defineClass1(Native Method); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:763); 	at java.lang.ClassLoader.defineClass(ClassLoader.java:642); 	at is.hail.asm4s.package$HailClassLoader$.liftedTree1$1(package.scala:174); 	at is.hail.asm4s.package$HailClassLoader$.loadOrDefineClass(package.scala:170); 	at is.hail.asm4s.package$.loadClass(package.scala:181); 	at is.hail.asm4s.FunctionBuilder$$anon$1.apply(FunctionBuilder.scala:312); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:84); 	at is.hail.expr.CM$$anonfun$runWithDelayedValues$1.apply(CM.scala:82); 	at is.hail.expr.Parser$$anonfun$is$hail$expr$Parser$$evalNoTypeCheck$1.apply(Parser.scala:64); 	at is.hail.expr.Parser$$anonfun$12$$anonfun$apply$6.apply(Parser.scala:172); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/2982
https://github.com/hail-is/hail/pull/2984:29,Integrability,interface,interface,29,I'm not 100% happy with this interface. I created a thread to discuss collection constructors: http://dev.hail.is/t/collection-constructors-and-lit/72.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2984
https://github.com/hail-is/hail/pull/2985:46,Integrability,interface,interface-wise,46,@tpoterba interested in your thoughts on this interface-wise,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2985
https://github.com/hail-is/hail/pull/2993:35,Testability,test,tests,35,"@tpoterba I still need to add some tests for GroupedMatrixTable, but what do you think about this?",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2993
https://github.com/hail-is/hail/pull/2999:7,Deployability,patch,patch,7,monkey patch py4j with an exception handler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/2999
https://github.com/hail-is/hail/issues/3001:139,Availability,error,error,139,"### Hail version:; master branch version. ### What you did; ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):. -XPS-8700:~/Downloads/hail$ ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip; 669187030305. FAILURE: Build failed with an exception. * Where:; Build file '/home/test/Downloads/hail/build.gradle' line: 57. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Unknown Spark version 2.1.0. Set breeze.version and py4j.version properties for Spark 2.1.0. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 2.33 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3001
https://github.com/hail-is/hail/issues/3001:211,Availability,Down,Downloads,211,"### Hail version:; master branch version. ### What you did; ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):. -XPS-8700:~/Downloads/hail$ ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip; 669187030305. FAILURE: Build failed with an exception. * Where:; Build file '/home/test/Downloads/hail/build.gradle' line: 57. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Unknown Spark version 2.1.0. Set breeze.version and py4j.version properties for Spark 2.1.0. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 2.33 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3001
https://github.com/hail-is/hail/issues/3001:295,Availability,FAILURE,FAILURE,295,"### Hail version:; master branch version. ### What you did; ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):. -XPS-8700:~/Downloads/hail$ ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip; 669187030305. FAILURE: Build failed with an exception. * Where:; Build file '/home/test/Downloads/hail/build.gradle' line: 57. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Unknown Spark version 2.1.0. Set breeze.version and py4j.version properties for Spark 2.1.0. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 2.33 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3001
https://github.com/hail-is/hail/issues/3001:369,Availability,Down,Downloads,369,"### Hail version:; master branch version. ### What you did; ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):. -XPS-8700:~/Downloads/hail$ ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip; 669187030305. FAILURE: Build failed with an exception. * Where:; Build file '/home/test/Downloads/hail/build.gradle' line: 57. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Unknown Spark version 2.1.0. Set breeze.version and py4j.version properties for Spark 2.1.0. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 2.33 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3001
https://github.com/hail-is/hail/issues/3001:145,Integrability,message,messages,145,"### Hail version:; master branch version. ### What you did; ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):. -XPS-8700:~/Downloads/hail$ ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip; 669187030305. FAILURE: Build failed with an exception. * Where:; Build file '/home/test/Downloads/hail/build.gradle' line: 57. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Unknown Spark version 2.1.0. Set breeze.version and py4j.version properties for Spark 2.1.0. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 2.33 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3001
https://github.com/hail-is/hail/issues/3001:364,Testability,test,test,364,"### Hail version:; master branch version. ### What you did; ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):. -XPS-8700:~/Downloads/hail$ ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip; 669187030305. FAILURE: Build failed with an exception. * Where:; Build file '/home/test/Downloads/hail/build.gradle' line: 57. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Unknown Spark version 2.1.0. Set breeze.version and py4j.version properties for Spark 2.1.0. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 2.33 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3001
https://github.com/hail-is/hail/issues/3001:682,Testability,log,log,682,"### Hail version:; master branch version. ### What you did; ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip. ### What went wrong (all error messages here, including the full java stack trace):. -XPS-8700:~/Downloads/hail$ ./gradlew -Dspark.version=2.1.0 shadowJar archiveZip; 669187030305. FAILURE: Build failed with an exception. * Where:; Build file '/home/test/Downloads/hail/build.gradle' line: 57. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Unknown Spark version 2.1.0. Set breeze.version and py4j.version properties for Spark 2.1.0. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 2.33 secs",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3001
https://github.com/hail-is/hail/issues/3008:26,Availability,error,errors,26,Instead of throwing match errors when trying to parse format fields.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3008
https://github.com/hail-is/hail/issues/3015:278,Availability,Error,ErrorHandling,278,"```; is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:304,Availability,Error,ErrorHandling,304,"```; is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:5957,Availability,Error,ErrorHandling,5957,"un.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:5983,Availability,Error,ErrorHandling,5983,"hodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:1376,Energy Efficiency,schedul,scheduler,1376,	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(Ab,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:1448,Energy Efficiency,schedul,scheduler,1448,o.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(L,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2677,Energy Efficiency,schedul,scheduler,2677,cutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2717,Energy Efficiency,schedul,scheduler,2717,g.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2816,Energy Efficiency,schedul,scheduler,2816,rovided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2914,Energy Efficiency,schedul,scheduler,2914,le M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:3168,Energy Efficiency,schedul,scheduler,3168,at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:3249,Energy Efficiency,schedul,scheduler,3249,at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:3355,Energy Efficiency,schedul,scheduler,3355,actVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:3505,Energy Efficiency,schedul,scheduler,3505,.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:3594,Energy Efficiency,schedul,scheduler,3594,xt(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:3692,Energy Efficiency,schedul,scheduler,3692,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:495); 	at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:3788,Energy Efficiency,schedul,scheduler,3788,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:495); 	at is.hail.rvd.OrderedRVD$.adjustBoundsAndShuffle(OrderedRVD.scala:606); 	at is.hail.methods.SplitM,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:3953,Energy Efficiency,schedul,scheduler,3953,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.rvd.OrderedRVD$.getPartitionKeyInfo(OrderedRVD.scala:495); 	at is.hail.rvd.OrderedRVD$.adjustBoundsAndShuffle(OrderedRVD.scala:606); 	at is.hail.methods.SplitMulti$.unionMovedVariants(SplitMulti.scala:176); 	at is.hail.methods.SplitMulti.split(SplitMulti.scala:241); 	at is.hail.methods.SplitMulti$.apply(SplitMulti.scala:17,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:7055,Energy Efficiency,schedul,scheduler,7055,	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:7127,Energy Efficiency,schedul,scheduler,7127,o.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:407,Integrability,wrap,wrapException,407,"```; is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:6086,Integrability,wrap,wrapException,6086,"pl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.sc",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:459,Performance,Load,LoadVCF,459,"```; is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:505,Performance,Load,LoadVCF,505,"```; is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.j",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:1573,Performance,concurren,concurrent,1573,or.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseL,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:1658,Performance,concurren,concurrent,1658,.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2426,Performance,Load,LoadVCF,2426,pache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:81,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2452,Performance,Load,LoadVCF,2452,uler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Opt,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2491,Performance,Load,LoadVCF,2491,ark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2517,Performance,Load,LoadVCF,2517,utor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAG,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2556,Performance,Load,LoadVCF,2556,ncurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	a,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2602,Performance,Load,LoadVCF,2602,.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.schedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:6138,Performance,Load,LoadVCF,6138,"voke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:6184,Performance,Load,LoadVCF,6184,"orImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); Caused by: is.hail.utils.HailException: hybrid.m37m.vcf.bgz: caught htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; offending line: 3	60830534	.	M	C	40	.	.	GT:AD	1/1:0,40; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:17); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.utils.Context.wrapException(Context.scala:23); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:742); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:7252,Performance,concurren,concurrent,7252,or.scala:438); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:7337,Performance,concurren,concurrent,7337,OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:8075,Performance,Load,LoadVCF,8075,OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:8101,Performance,Load,LoadVCF,8101,OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:8140,Performance,Load,LoadVCF,8140,OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:8166,Performance,Load,LoadVCF,8166,OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:8205,Performance,Load,LoadVCF,8205,OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:8251,Performance,Load,LoadVCF,8251,OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	... 1 more; Caused by: htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2848,Safety,abort,abortStage,2848,lformed at approximately line number 458249: unparsable vcf record with allele M; 	at htsjdk.variant.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:2946,Safety,abort,abortStage,2946,nt.vcf.AbstractVCFCodec.generateException(AbstractVCFCodec.java:783); 	at htsjdk.variant.vcf.AbstractVCFCodec.checkAllele(AbstractVCFCodec.java:569); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseAlleles(AbstractVCFCodec.java:531); 	at htsjdk.variant.vcf.AbstractVCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/issues/3015:3191,Safety,abort,abortStage,3191,VCFCodec.parseVCFLine(AbstractVCFCodec.java:336); 	at htsjdk.variant.vcf.AbstractVCFCodec.decodeLine(AbstractVCFCodec.java:279); 	at htsjdk.variant.vcf.AbstractVCFCodec.decode(AbstractVCFCodec.java:257); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:850); 	at is.hail.io.vcf.LoadVCF$$anonfun$13.apply(LoadVCF.scala:849); 	at is.hail.io.vcf.LoadVCF$$anonfun$parseLines$1$$anon$1.hasNext(LoadVCF.scala:718); 	... 17 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3015
https://github.com/hail-is/hail/pull/3016:1778,Availability,error,error,1778,"ions and get their implementation right, and my hope is that all the non-obvious iterator code will now be concentrated in a small, well tested, component. `FlipbookIterator` solves the confusing behavior where `hasNext` potentially wipes out the current value. (All methods on `FlipbookIterator` and `StagingIterator` should obey the rule that methods defined without trailing `()` do not change the state of the iterator in any way detectable through the API.) The core interface of `FlipbookIterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a valid page which has not yet been consumed, and marks it as consumed. Note that `consume()` does not actually turn the page, so that the value just consumed is kept alive for the consumer to use. `stage()` brings the iterator forward to the next state in which we are on an unconsumed page: if the current page has been consumed, it flips to the next page, but if the current page has not been consumed it does nothing. My goal with the `StagingIterator` interface was to find something simple—to make it easier to write iterator code that is easily understandable and obviously correct and bug-free—and which covers most of the low-level ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:2234,Availability,alive,alive,2234,"Iterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a valid page which has not yet been consumed, and marks it as consumed. Note that `consume()` does not actually turn the page, so that the value just consumed is kept alive for the consumer to use. `stage()` brings the iterator forward to the next state in which we are on an unconsumed page: if the current page has been consumed, it flips to the next page, but if the current page has not been consumed it does nothing. My goal with the `StagingIterator` interface was to find something simple—to make it easier to write iterator code that is easily understandable and obviously correct and bug-free—and which covers most of the low-level (and bug-prone) ""bookkeeping"" I could find in current iterator code. The `StagingIterator` interface is also needed to implement the Scala `Iterator` methods, and for that reason I made it so that every `FlipbookIterator` is really a `StagingIterator` with a restricted interface, so that `FlipbookIterator` is able to subtype (`BufferedIterator`, and therefore) `Iterator`. The tiny abstract class `StateMachine` can be thought of as a ""naked"" `FlipbookIterator`, with only the core interface",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:5339,Deployability,Update,Update,5339,"es of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potentially producting (non-distinct assuming) join methods also take a buffer argument, which is anything that can make a copy of an iterator and then iterate over the copy multiple times. To avoid allocating tuples in the output iterators of the join methods, I made `Muple`, which is just a mutable tuple. I turned the existing `JoinedRegionValue` into an alias of `Muple[RegionValue, RegionValue]`. All of this core `FlipbookIterator` and `StagingIterator` behavior has no dependencies on anything else in Hail, so I want to thoroughly test everything at this level, and treat it like a small external iterator library living inside the repo. As such, I think this level should be quite stable going forwards. At the higher level, I lifted all the join methods on `FlipbookIterator` to `OrderedRVIterator`, which is a `Iterator[RegionValue]` together with an `OrderedRVDType`. I think `OrderedRVIterator` should be replaced by something better soon: see future work below. I've replaced the old implementations of `innerJoinDistinct`, `leftJoinDistinct`, and `orderedZipJoin` using the new infrastructure (see OrderedRDD2.scala), which I think is a good example of the kind of simplifications possible. The existing JoinSuite tests also serve as a secondary test of the new code. In future PRs, I intend to:. * Implement all the join varieties on `OrderedRVD` using the `FlipbookIterator` joins defined here.; * Update other uses of `Iterator` to use the `FlipbookIterator` infrastructure, if doing so improves the code (and I expect it will, or I've done something wrong); * Beef up the `FlipbookIterator` test suite. I could do that for this PR if requested.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:1179,Integrability,interface,interface,1179,"h could affect all code involving iterators, so I welcome feedback from everybody. The new abstractions are what I called `FlipbookIterator` and `StagingIterator` (I'm open to name suggestions). My goal was to simplify and raise the level of abstraction of most of the iterator manipulating code in the codebase—which can be subtle and bug-prone, and difficult to read—while paying as minimal as possible a performance overhead for the abstraction. This was surprisingly subtle to find the right abstractions and get their implementation right, and my hope is that all the non-obvious iterator code will now be concentrated in a small, well tested, component. `FlipbookIterator` solves the confusing behavior where `hasNext` potentially wipes out the current value. (All methods on `FlipbookIterator` and `StagingIterator` should obey the rule that methods defined without trailing `()` do not change the state of the iterator in any way detectable through the API.) The core interface of `FlipbookIterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a valid page which has not yet been consumed, and marks it as consumed. Note that `consume()` does not actually turn the page, so that t",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:2524,Integrability,interface,interface,2524," `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a valid page which has not yet been consumed, and marks it as consumed. Note that `consume()` does not actually turn the page, so that the value just consumed is kept alive for the consumer to use. `stage()` brings the iterator forward to the next state in which we are on an unconsumed page: if the current page has been consumed, it flips to the next page, but if the current page has not been consumed it does nothing. My goal with the `StagingIterator` interface was to find something simple—to make it easier to write iterator code that is easily understandable and obviously correct and bug-free—and which covers most of the low-level (and bug-prone) ""bookkeeping"" I could find in current iterator code. The `StagingIterator` interface is also needed to implement the Scala `Iterator` methods, and for that reason I made it so that every `FlipbookIterator` is really a `StagingIterator` with a restricted interface, so that `FlipbookIterator` is able to subtype (`BufferedIterator`, and therefore) `Iterator`. The tiny abstract class `StateMachine` can be thought of as a ""naked"" `FlipbookIterator`, with only the core interface methods and without any runtime checking. It is intended to be used only for defining a new `FlipbookIterator` or `StagingIterator`, through the factory methods on their respective companion objects which take a `StateMachine`. I implemented `fliterWhere`, `map`, and `flatMap` on `FlipbookIterator`, allowing the use of for–comprehensions, and on top of that I defined all the varieties of join. Some of the join methods take `Or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:2799,Integrability,interface,interface,2799,"nsume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a valid page which has not yet been consumed, and marks it as consumed. Note that `consume()` does not actually turn the page, so that the value just consumed is kept alive for the consumer to use. `stage()` brings the iterator forward to the next state in which we are on an unconsumed page: if the current page has been consumed, it flips to the next page, but if the current page has not been consumed it does nothing. My goal with the `StagingIterator` interface was to find something simple—to make it easier to write iterator code that is easily understandable and obviously correct and bug-free—and which covers most of the low-level (and bug-prone) ""bookkeeping"" I could find in current iterator code. The `StagingIterator` interface is also needed to implement the Scala `Iterator` methods, and for that reason I made it so that every `FlipbookIterator` is really a `StagingIterator` with a restricted interface, so that `FlipbookIterator` is able to subtype (`BufferedIterator`, and therefore) `Iterator`. The tiny abstract class `StateMachine` can be thought of as a ""naked"" `FlipbookIterator`, with only the core interface methods and without any runtime checking. It is intended to be used only for defining a new `FlipbookIterator` or `StagingIterator`, through the factory methods on their respective companion objects which take a `StateMachine`. I implemented `fliterWhere`, `map`, and `flatMap` on `FlipbookIterator`, allowing the use of for–comprehensions, and on top of that I defined all the varieties of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potent",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:2978,Integrability,interface,interface,2978,"nsume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a valid page which has not yet been consumed, and marks it as consumed. Note that `consume()` does not actually turn the page, so that the value just consumed is kept alive for the consumer to use. `stage()` brings the iterator forward to the next state in which we are on an unconsumed page: if the current page has been consumed, it flips to the next page, but if the current page has not been consumed it does nothing. My goal with the `StagingIterator` interface was to find something simple—to make it easier to write iterator code that is easily understandable and obviously correct and bug-free—and which covers most of the low-level (and bug-prone) ""bookkeeping"" I could find in current iterator code. The `StagingIterator` interface is also needed to implement the Scala `Iterator` methods, and for that reason I made it so that every `FlipbookIterator` is really a `StagingIterator` with a restricted interface, so that `FlipbookIterator` is able to subtype (`BufferedIterator`, and therefore) `Iterator`. The tiny abstract class `StateMachine` can be thought of as a ""naked"" `FlipbookIterator`, with only the core interface methods and without any runtime checking. It is intended to be used only for defining a new `FlipbookIterator` or `StagingIterator`, through the factory methods on their respective companion objects which take a `StateMachine`. I implemented `fliterWhere`, `map`, and `flatMap` on `FlipbookIterator`, allowing the use of for–comprehensions, and on top of that I defined all the varieties of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potent",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:3192,Integrability,interface,interface,3192,"oes not actually turn the page, so that the value just consumed is kept alive for the consumer to use. `stage()` brings the iterator forward to the next state in which we are on an unconsumed page: if the current page has been consumed, it flips to the next page, but if the current page has not been consumed it does nothing. My goal with the `StagingIterator` interface was to find something simple—to make it easier to write iterator code that is easily understandable and obviously correct and bug-free—and which covers most of the low-level (and bug-prone) ""bookkeeping"" I could find in current iterator code. The `StagingIterator` interface is also needed to implement the Scala `Iterator` methods, and for that reason I made it so that every `FlipbookIterator` is really a `StagingIterator` with a restricted interface, so that `FlipbookIterator` is able to subtype (`BufferedIterator`, and therefore) `Iterator`. The tiny abstract class `StateMachine` can be thought of as a ""naked"" `FlipbookIterator`, with only the core interface methods and without any runtime checking. It is intended to be used only for defining a new `FlipbookIterator` or `StagingIterator`, through the factory methods on their respective companion objects which take a `StateMachine`. I implemented `fliterWhere`, `map`, and `flatMap` on `FlipbookIterator`, allowing the use of for–comprehensions, and on top of that I defined all the varieties of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potentially producting (non-distinct assuming) join methods also take a buffer argument, which is anything that can make a copy of an iterator and then iterate over the copy multiple times. To avoid allocating tuples in the output iterators",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:4400,Integrability,depend,dependencies,4400,"iterWhere`, `map`, and `flatMap` on `FlipbookIterator`, allowing the use of for–comprehensions, and on top of that I defined all the varieties of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potentially producting (non-distinct assuming) join methods also take a buffer argument, which is anything that can make a copy of an iterator and then iterate over the copy multiple times. To avoid allocating tuples in the output iterators of the join methods, I made `Muple`, which is just a mutable tuple. I turned the existing `JoinedRegionValue` into an alias of `Muple[RegionValue, RegionValue]`. All of this core `FlipbookIterator` and `StagingIterator` behavior has no dependencies on anything else in Hail, so I want to thoroughly test everything at this level, and treat it like a small external iterator library living inside the repo. As such, I think this level should be quite stable going forwards. At the higher level, I lifted all the join methods on `FlipbookIterator` to `OrderedRVIterator`, which is a `Iterator[RegionValue]` together with an `OrderedRVDType`. I think `OrderedRVIterator` should be replaced by something better soon: see future work below. I've replaced the old implementations of `innerJoinDistinct`, `leftJoinDistinct`, and `orderedZipJoin` using the new infrastructure (see OrderedRDD2.scala), which I think is a good example of the kind of simplifications possible. The existing JoinSuite tests also serve as a secondary test of the new code. In future PRs, I intend to:. * Implement all the join varieties on `OrderedRVD` using the `FlipbookIterator` joins defined here.; * Update other uses of `Iterator` to use the `FlipbookIterator` infrastructure, if doing so improves the code ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:610,Performance,perform,performance,610,"I am proposing a new iterator abstraction that I think should be preferred to Scala `Iterator` throughout most of the codebase, especially for iterators of region values. This is a low-level change, which could affect all code involving iterators, so I welcome feedback from everybody. The new abstractions are what I called `FlipbookIterator` and `StagingIterator` (I'm open to name suggestions). My goal was to simplify and raise the level of abstraction of most of the iterator manipulating code in the codebase—which can be subtle and bug-prone, and difficult to read—while paying as minimal as possible a performance overhead for the abstraction. This was surprisingly subtle to find the right abstractions and get their implementation right, and my hope is that all the non-obvious iterator code will now be concentrated in a small, well tested, component. `FlipbookIterator` solves the confusing behavior where `hasNext` potentially wipes out the current value. (All methods on `FlipbookIterator` and `StagingIterator` should obey the rule that methods defined without trailing `()` do not change the state of the iterator in any way detectable through the API.) The core interface of `FlipbookIterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:1141,Safety,detect,detectable,1141,"ferred to Scala `Iterator` throughout most of the codebase, especially for iterators of region values. This is a low-level change, which could affect all code involving iterators, so I welcome feedback from everybody. The new abstractions are what I called `FlipbookIterator` and `StagingIterator` (I'm open to name suggestions). My goal was to simplify and raise the level of abstraction of most of the iterator manipulating code in the codebase—which can be subtle and bug-prone, and difficult to read—while paying as minimal as possible a performance overhead for the abstraction. This was surprisingly subtle to find the right abstractions and get their implementation right, and my hope is that all the non-obvious iterator code will now be concentrated in a small, well tested, component. `FlipbookIterator` solves the confusing behavior where `hasNext` potentially wipes out the current value. (All methods on `FlipbookIterator` and `StagingIterator` should obey the rule that methods defined without trailing `()` do not change the state of the iterator in any way detectable through the API.) The core interface of `FlipbookIterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:4116,Safety,avoid,avoid,4116,", with only the core interface methods and without any runtime checking. It is intended to be used only for defining a new `FlipbookIterator` or `StagingIterator`, through the factory methods on their respective companion objects which take a `StateMachine`. I implemented `fliterWhere`, `map`, and `flatMap` on `FlipbookIterator`, allowing the use of for–comprehensions, and on top of that I defined all the varieties of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potentially producting (non-distinct assuming) join methods also take a buffer argument, which is anything that can make a copy of an iterator and then iterate over the copy multiple times. To avoid allocating tuples in the output iterators of the join methods, I made `Muple`, which is just a mutable tuple. I turned the existing `JoinedRegionValue` into an alias of `Muple[RegionValue, RegionValue]`. All of this core `FlipbookIterator` and `StagingIterator` behavior has no dependencies on anything else in Hail, so I want to thoroughly test everything at this level, and treat it like a small external iterator library living inside the repo. As such, I think this level should be quite stable going forwards. At the higher level, I lifted all the join methods on `FlipbookIterator` to `OrderedRVIterator`, which is a `Iterator[RegionValue]` together with an `OrderedRVDType`. I think `OrderedRVIterator` should be replaced by something better soon: see future work below. I've replaced the old implementations of `innerJoinDistinct`, `leftJoinDistinct`, and `orderedZipJoin` using the new infrastructure (see OrderedRDD2.scala), which I think is a good example of the kind of simplifications possible. The existing JoinSuite tests also serve as",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:1373,Security,access,access,1373,"tion of most of the iterator manipulating code in the codebase—which can be subtle and bug-prone, and difficult to read—while paying as minimal as possible a performance overhead for the abstraction. This was surprisingly subtle to find the right abstractions and get their implementation right, and my hope is that all the non-obvious iterator code will now be concentrated in a small, well tested, component. `FlipbookIterator` solves the confusing behavior where `hasNext` potentially wipes out the current value. (All methods on `FlipbookIterator` and `StagingIterator` should obey the rule that methods defined without trailing `()` do not change the state of the iterator in any way detectable through the API.) The core interface of `FlipbookIterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a valid page which has not yet been consumed, and marks it as consumed. Note that `consume()` does not actually turn the page, so that the value just consumed is kept alive for the consumer to use. `stage()` brings the iterator forward to the next state in which we are on an unconsumed page: if the current page has been consumed, it flips to the next page, but if the current page ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:844,Testability,test,tested,844,"I am proposing a new iterator abstraction that I think should be preferred to Scala `Iterator` throughout most of the codebase, especially for iterators of region values. This is a low-level change, which could affect all code involving iterators, so I welcome feedback from everybody. The new abstractions are what I called `FlipbookIterator` and `StagingIterator` (I'm open to name suggestions). My goal was to simplify and raise the level of abstraction of most of the iterator manipulating code in the codebase—which can be subtle and bug-prone, and difficult to read—while paying as minimal as possible a performance overhead for the abstraction. This was surprisingly subtle to find the right abstractions and get their implementation right, and my hope is that all the non-obvious iterator code will now be concentrated in a small, well tested, component. `FlipbookIterator` solves the confusing behavior where `hasNext` potentially wipes out the current value. (All methods on `FlipbookIterator` and `StagingIterator` should obey the rule that methods defined without trailing `()` do not change the state of the iterator in any way detectable through the API.) The core interface of `FlipbookIterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:4463,Testability,test,test,4463,"iterWhere`, `map`, and `flatMap` on `FlipbookIterator`, allowing the use of for–comprehensions, and on top of that I defined all the varieties of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potentially producting (non-distinct assuming) join methods also take a buffer argument, which is anything that can make a copy of an iterator and then iterate over the copy multiple times. To avoid allocating tuples in the output iterators of the join methods, I made `Muple`, which is just a mutable tuple. I turned the existing `JoinedRegionValue` into an alias of `Muple[RegionValue, RegionValue]`. All of this core `FlipbookIterator` and `StagingIterator` behavior has no dependencies on anything else in Hail, so I want to thoroughly test everything at this level, and treat it like a small external iterator library living inside the repo. As such, I think this level should be quite stable going forwards. At the higher level, I lifted all the join methods on `FlipbookIterator` to `OrderedRVIterator`, which is a `Iterator[RegionValue]` together with an `OrderedRVDType`. I think `OrderedRVIterator` should be replaced by something better soon: see future work below. I've replaced the old implementations of `innerJoinDistinct`, `leftJoinDistinct`, and `orderedZipJoin` using the new infrastructure (see OrderedRDD2.scala), which I think is a good example of the kind of simplifications possible. The existing JoinSuite tests also serve as a secondary test of the new code. In future PRs, I intend to:. * Implement all the join varieties on `OrderedRVD` using the `FlipbookIterator` joins defined here.; * Update other uses of `Iterator` to use the `FlipbookIterator` infrastructure, if doing so improves the code ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:5153,Testability,test,tests,5153,"es of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potentially producting (non-distinct assuming) join methods also take a buffer argument, which is anything that can make a copy of an iterator and then iterate over the copy multiple times. To avoid allocating tuples in the output iterators of the join methods, I made `Muple`, which is just a mutable tuple. I turned the existing `JoinedRegionValue` into an alias of `Muple[RegionValue, RegionValue]`. All of this core `FlipbookIterator` and `StagingIterator` behavior has no dependencies on anything else in Hail, so I want to thoroughly test everything at this level, and treat it like a small external iterator library living inside the repo. As such, I think this level should be quite stable going forwards. At the higher level, I lifted all the join methods on `FlipbookIterator` to `OrderedRVIterator`, which is a `Iterator[RegionValue]` together with an `OrderedRVDType`. I think `OrderedRVIterator` should be replaced by something better soon: see future work below. I've replaced the old implementations of `innerJoinDistinct`, `leftJoinDistinct`, and `orderedZipJoin` using the new infrastructure (see OrderedRDD2.scala), which I think is a good example of the kind of simplifications possible. The existing JoinSuite tests also serve as a secondary test of the new code. In future PRs, I intend to:. * Implement all the join varieties on `OrderedRVD` using the `FlipbookIterator` joins defined here.; * Update other uses of `Iterator` to use the `FlipbookIterator` infrastructure, if doing so improves the code (and I expect it will, or I've done something wrong); * Beef up the `FlipbookIterator` test suite. I could do that for this PR if requested.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:5185,Testability,test,test,5185,"es of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potentially producting (non-distinct assuming) join methods also take a buffer argument, which is anything that can make a copy of an iterator and then iterate over the copy multiple times. To avoid allocating tuples in the output iterators of the join methods, I made `Muple`, which is just a mutable tuple. I turned the existing `JoinedRegionValue` into an alias of `Muple[RegionValue, RegionValue]`. All of this core `FlipbookIterator` and `StagingIterator` behavior has no dependencies on anything else in Hail, so I want to thoroughly test everything at this level, and treat it like a small external iterator library living inside the repo. As such, I think this level should be quite stable going forwards. At the higher level, I lifted all the join methods on `FlipbookIterator` to `OrderedRVIterator`, which is a `Iterator[RegionValue]` together with an `OrderedRVDType`. I think `OrderedRVIterator` should be replaced by something better soon: see future work below. I've replaced the old implementations of `innerJoinDistinct`, `leftJoinDistinct`, and `orderedZipJoin` using the new infrastructure (see OrderedRDD2.scala), which I think is a good example of the kind of simplifications possible. The existing JoinSuite tests also serve as a secondary test of the new code. In future PRs, I intend to:. * Implement all the join varieties on `OrderedRVD` using the `FlipbookIterator` joins defined here.; * Update other uses of `Iterator` to use the `FlipbookIterator` infrastructure, if doing so improves the code (and I expect it will, or I've done something wrong); * Beef up the `FlipbookIterator` test suite. I could do that for this PR if requested.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:5534,Testability,test,test,5534,"es of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potentially producting (non-distinct assuming) join methods also take a buffer argument, which is anything that can make a copy of an iterator and then iterate over the copy multiple times. To avoid allocating tuples in the output iterators of the join methods, I made `Muple`, which is just a mutable tuple. I turned the existing `JoinedRegionValue` into an alias of `Muple[RegionValue, RegionValue]`. All of this core `FlipbookIterator` and `StagingIterator` behavior has no dependencies on anything else in Hail, so I want to thoroughly test everything at this level, and treat it like a small external iterator library living inside the repo. As such, I think this level should be quite stable going forwards. At the higher level, I lifted all the join methods on `FlipbookIterator` to `OrderedRVIterator`, which is a `Iterator[RegionValue]` together with an `OrderedRVDType`. I think `OrderedRVIterator` should be replaced by something better soon: see future work below. I've replaced the old implementations of `innerJoinDistinct`, `leftJoinDistinct`, and `orderedZipJoin` using the new infrastructure (see OrderedRDD2.scala), which I think is a good example of the kind of simplifications possible. The existing JoinSuite tests also serve as a secondary test of the new code. In future PRs, I intend to:. * Implement all the join varieties on `OrderedRVD` using the `FlipbookIterator` joins defined here.; * Update other uses of `Iterator` to use the `FlipbookIterator` infrastructure, if doing so improves the code (and I expect it will, or I've done something wrong); * Beef up the `FlipbookIterator` test suite. I could do that for this PR if requested.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:261,Usability,feedback,feedback,261,"I am proposing a new iterator abstraction that I think should be preferred to Scala `Iterator` throughout most of the codebase, especially for iterators of region values. This is a low-level change, which could affect all code involving iterators, so I welcome feedback from everybody. The new abstractions are what I called `FlipbookIterator` and `StagingIterator` (I'm open to name suggestions). My goal was to simplify and raise the level of abstraction of most of the iterator manipulating code in the codebase—which can be subtle and bug-prone, and difficult to read—while paying as minimal as possible a performance overhead for the abstraction. This was surprisingly subtle to find the right abstractions and get their implementation right, and my hope is that all the non-obvious iterator code will now be concentrated in a small, well tested, component. `FlipbookIterator` solves the confusing behavior where `hasNext` potentially wipes out the current value. (All methods on `FlipbookIterator` and `StagingIterator` should obey the rule that methods defined without trailing `()` do not change the state of the iterator in any way detectable through the API.) The core interface of `FlipbookIterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:413,Usability,simpl,simplify,413,"I am proposing a new iterator abstraction that I think should be preferred to Scala `Iterator` throughout most of the codebase, especially for iterators of region values. This is a low-level change, which could affect all code involving iterators, so I welcome feedback from everybody. The new abstractions are what I called `FlipbookIterator` and `StagingIterator` (I'm open to name suggestions). My goal was to simplify and raise the level of abstraction of most of the iterator manipulating code in the codebase—which can be subtle and bug-prone, and difficult to read—while paying as minimal as possible a performance overhead for the abstraction. This was surprisingly subtle to find the right abstractions and get their implementation right, and my hope is that all the non-obvious iterator code will now be concentrated in a small, well tested, component. `FlipbookIterator` solves the confusing behavior where `hasNext` potentially wipes out the current value. (All methods on `FlipbookIterator` and `StagingIterator` should obey the rule that methods defined without trailing `()` do not change the state of the iterator in any way detectable through the API.) The core interface of `FlipbookIterator[A]` consists of the methods. * `isValid: Boolean`; * `value: A`; * `advance(): Unit`. The metaphor is a flipbook, where when you turn the page, you no longer have access to the previous page; where you can read the current page as many times as you want (no need to copy it); and where you only know you've reached the end of the flipbook when you turn the page and find that the next page is empty. In `FlipbookIterator`, `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:2556,Usability,simpl,simple,2556," `advance()` turns the page, `isValid` asks if the page you are on is non-empty, and `value` gives you the value on the current page (which is an error if the page is empty). `StagingIterator` is a subtype of `FlipbookIterator` which adds a bit of state to each page, together with the methods `consume()` and `stage()`. The bit of state on each page tracks whether the value has been ""consumed"" yet. `consume()` can only be called on a valid page which has not yet been consumed, and marks it as consumed. Note that `consume()` does not actually turn the page, so that the value just consumed is kept alive for the consumer to use. `stage()` brings the iterator forward to the next state in which we are on an unconsumed page: if the current page has been consumed, it flips to the next page, but if the current page has not been consumed it does nothing. My goal with the `StagingIterator` interface was to find something simple—to make it easier to write iterator code that is easily understandable and obviously correct and bug-free—and which covers most of the low-level (and bug-prone) ""bookkeeping"" I could find in current iterator code. The `StagingIterator` interface is also needed to implement the Scala `Iterator` methods, and for that reason I made it so that every `FlipbookIterator` is really a `StagingIterator` with a restricted interface, so that `FlipbookIterator` is able to subtype (`BufferedIterator`, and therefore) `Iterator`. The tiny abstract class `StateMachine` can be thought of as a ""naked"" `FlipbookIterator`, with only the core interface methods and without any runtime checking. It is intended to be used only for defining a new `FlipbookIterator` or `StagingIterator`, through the factory methods on their respective companion objects which take a `StateMachine`. I implemented `fliterWhere`, `map`, and `flatMap` on `FlipbookIterator`, allowing the use of for–comprehensions, and on top of that I defined all the varieties of join. Some of the join methods take `Or",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/pull/3016:5104,Usability,simpl,simplifications,5104,"es of join. Some of the join methods take `OrderingView` arguments. An `OrderingView` is a small abstraction on top of an `Ordering` which can take one element `a`, copy data (such as key-fields) from `a` if necessary, then later (after `a` might have been destroyed or mutated) compare `a` to other elements using the copied data. The potentially producting (non-distinct assuming) join methods also take a buffer argument, which is anything that can make a copy of an iterator and then iterate over the copy multiple times. To avoid allocating tuples in the output iterators of the join methods, I made `Muple`, which is just a mutable tuple. I turned the existing `JoinedRegionValue` into an alias of `Muple[RegionValue, RegionValue]`. All of this core `FlipbookIterator` and `StagingIterator` behavior has no dependencies on anything else in Hail, so I want to thoroughly test everything at this level, and treat it like a small external iterator library living inside the repo. As such, I think this level should be quite stable going forwards. At the higher level, I lifted all the join methods on `FlipbookIterator` to `OrderedRVIterator`, which is a `Iterator[RegionValue]` together with an `OrderedRVDType`. I think `OrderedRVIterator` should be replaced by something better soon: see future work below. I've replaced the old implementations of `innerJoinDistinct`, `leftJoinDistinct`, and `orderedZipJoin` using the new infrastructure (see OrderedRDD2.scala), which I think is a good example of the kind of simplifications possible. The existing JoinSuite tests also serve as a secondary test of the new code. In future PRs, I intend to:. * Implement all the join varieties on `OrderedRVD` using the `FlipbookIterator` joins defined here.; * Update other uses of `Iterator` to use the `FlipbookIterator` infrastructure, if doing so improves the code (and I expect it will, or I've done something wrong); * Beef up the `FlipbookIterator` test suite. I could do that for this PR if requested.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3016
https://github.com/hail-is/hail/issues/3017:262,Integrability,Wrap,WrappedArray,262,"Konrad hit:; ``` File ""<decorator-gen-816>"", line 2, in import_matrix_table; File ""/tmp/933dc754-b6e8-446f-986d-f4900becb0bd/hail-devel-ef2f48e719c3.zip/hail/utils/java.py"", line 208, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}. Java stack trace:; java.lang.AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.types.MatrixType.<init>(MatrixType.scala:45); at is.hail.expr.types.MatrixType$.fromParts(MatrixType.scala:23); at is.hail.io.LoadMatrix$.apply(LoadMatrix.scala:343); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext.forceBGZip(HailContext.scala:498); at is.hail.HailContext.importMatrices(HailContext.scala:554); at is.hail.HailContext.importMatrix(HailContext.scala:540); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3017
https://github.com/hail-is/hail/issues/3017:369,Integrability,Wrap,WrappedArray,369,"Konrad hit:; ``` File ""<decorator-gen-816>"", line 2, in import_matrix_table; File ""/tmp/933dc754-b6e8-446f-986d-f4900becb0bd/hail-devel-ef2f48e719c3.zip/hail/utils/java.py"", line 208, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}. Java stack trace:; java.lang.AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.types.MatrixType.<init>(MatrixType.scala:45); at is.hail.expr.types.MatrixType$.fromParts(MatrixType.scala:23); at is.hail.io.LoadMatrix$.apply(LoadMatrix.scala:343); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext.forceBGZip(HailContext.scala:498); at is.hail.HailContext.importMatrices(HailContext.scala:554); at is.hail.HailContext.importMatrix(HailContext.scala:540); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3017
https://github.com/hail-is/hail/issues/3017:598,Performance,Load,LoadMatrix,598,"Konrad hit:; ``` File ""<decorator-gen-816>"", line 2, in import_matrix_table; File ""/tmp/933dc754-b6e8-446f-986d-f4900becb0bd/hail-devel-ef2f48e719c3.zip/hail/utils/java.py"", line 208, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}. Java stack trace:; java.lang.AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.types.MatrixType.<init>(MatrixType.scala:45); at is.hail.expr.types.MatrixType$.fromParts(MatrixType.scala:23); at is.hail.io.LoadMatrix$.apply(LoadMatrix.scala:343); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext.forceBGZip(HailContext.scala:498); at is.hail.HailContext.importMatrices(HailContext.scala:554); at is.hail.HailContext.importMatrix(HailContext.scala:540); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3017
https://github.com/hail-is/hail/issues/3017:616,Performance,Load,LoadMatrix,616,"Konrad hit:; ``` File ""<decorator-gen-816>"", line 2, in import_matrix_table; File ""/tmp/933dc754-b6e8-446f-986d-f4900becb0bd/hail-devel-ef2f48e719c3.zip/hail/utils/java.py"", line 208, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}. Java stack trace:; java.lang.AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.types.MatrixType.<init>(MatrixType.scala:45); at is.hail.expr.types.MatrixType$.fromParts(MatrixType.scala:23); at is.hail.io.LoadMatrix$.apply(LoadMatrix.scala:343); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext.forceBGZip(HailContext.scala:498); at is.hail.HailContext.importMatrices(HailContext.scala:554); at is.hail.HailContext.importMatrix(HailContext.scala:540); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3017
https://github.com/hail-is/hail/issues/3017:228,Testability,Assert,AssertionError,228,"Konrad hit:; ``` File ""<decorator-gen-816>"", line 2, in import_matrix_table; File ""/tmp/933dc754-b6e8-446f-986d-f4900becb0bd/hail-devel-ef2f48e719c3.zip/hail/utils/java.py"", line 208, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}. Java stack trace:; java.lang.AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.types.MatrixType.<init>(MatrixType.scala:45); at is.hail.expr.types.MatrixType$.fromParts(MatrixType.scala:23); at is.hail.io.LoadMatrix$.apply(LoadMatrix.scala:343); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext.forceBGZip(HailContext.scala:498); at is.hail.HailContext.importMatrices(HailContext.scala:554); at is.hail.HailContext.importMatrix(HailContext.scala:540); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3017
https://github.com/hail-is/hail/issues/3017:244,Testability,assert,assertion,244,"Konrad hit:; ``` File ""<decorator-gen-816>"", line 2, in import_matrix_table; File ""/tmp/933dc754-b6e8-446f-986d-f4900becb0bd/hail-devel-ef2f48e719c3.zip/hail/utils/java.py"", line 208, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}. Java stack trace:; java.lang.AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.types.MatrixType.<init>(MatrixType.scala:45); at is.hail.expr.types.MatrixType$.fromParts(MatrixType.scala:23); at is.hail.io.LoadMatrix$.apply(LoadMatrix.scala:343); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext.forceBGZip(HailContext.scala:498); at is.hail.HailContext.importMatrices(HailContext.scala:554); at is.hail.HailContext.importMatrix(HailContext.scala:540); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3017
https://github.com/hail-is/hail/issues/3017:335,Testability,Assert,AssertionError,335,"Konrad hit:; ``` File ""<decorator-gen-816>"", line 2, in import_matrix_table; File ""/tmp/933dc754-b6e8-446f-986d-f4900becb0bd/hail-devel-ef2f48e719c3.zip/hail/utils/java.py"", line 208, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}. Java stack trace:; java.lang.AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.types.MatrixType.<init>(MatrixType.scala:45); at is.hail.expr.types.MatrixType$.fromParts(MatrixType.scala:23); at is.hail.io.LoadMatrix$.apply(LoadMatrix.scala:343); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext.forceBGZip(HailContext.scala:498); at is.hail.HailContext.importMatrices(HailContext.scala:554); at is.hail.HailContext.importMatrix(HailContext.scala:540); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3017
https://github.com/hail-is/hail/issues/3017:351,Testability,assert,assertion,351,"Konrad hit:; ``` File ""<decorator-gen-816>"", line 2, in import_matrix_table; File ""/tmp/933dc754-b6e8-446f-986d-f4900becb0bd/hail-devel-ef2f48e719c3.zip/hail/utils/java.py"", line 208, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}. Java stack trace:; java.lang.AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.types.MatrixType.<init>(MatrixType.scala:45); at is.hail.expr.types.MatrixType$.fromParts(MatrixType.scala:23); at is.hail.io.LoadMatrix$.apply(LoadMatrix.scala:343); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext.forceBGZip(HailContext.scala:498); at is.hail.HailContext.importMatrices(HailContext.scala:554); at is.hail.HailContext.importMatrix(HailContext.scala:540); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3017
https://github.com/hail-is/hail/issues/3017:430,Testability,assert,assert,430,"Konrad hit:; ``` File ""<decorator-gen-816>"", line 2, in import_matrix_table; File ""/tmp/933dc754-b6e8-446f-986d-f4900becb0bd/hail-devel-ef2f48e719c3.zip/hail/utils/java.py"", line 208, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}. Java stack trace:; java.lang.AssertionError: assertion failed: WrappedArray(): Struct{f0:String,f1:Int32}; at scala.Predef$.assert(Predef.scala:170); at is.hail.expr.types.MatrixType.<init>(MatrixType.scala:45); at is.hail.expr.types.MatrixType$.fromParts(MatrixType.scala:23); at is.hail.io.LoadMatrix$.apply(LoadMatrix.scala:343); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext$$anonfun$importMatrices$1.apply(HailContext.scala:555); at is.hail.HailContext.forceBGZip(HailContext.scala:498); at is.hail.HailContext.importMatrices(HailContext.scala:554); at is.hail.HailContext.importMatrix(HailContext.scala:540); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3017
https://github.com/hail-is/hail/pull/3033:18,Usability,clear,clearly,18,the other way was clearly backwards,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3033
https://github.com/hail-is/hail/issues/3038:5,Availability,error,error,5,"Full error message:. ```[Stage 2:> (0 + 0) / 16]2018-02-28 23:41:58 Hail: INFO: interval filter loaded 2453 of 103675 partitions; 2018-02-28 23:42:08 Hail: WARN: deprecation: 'drop_cols' will be removed before 0.2 release; Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 415, in check_all; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 53, in check; hail.typecheck.check.TypecheckFailure. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 220, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 94, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 187, in main; vds, sample_table = generate_qc_annotations(vds, medians=not args.skip_medians); File ""<decorator-gen-616>"", line 2, in __getitem__; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 478, in _typecheck; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 425, in check_all; TypeError: __getitem__: parameter 'item': expected (str or tuple[(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression]),(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression])]), found int: '0'```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3038
https://github.com/hail-is/hail/issues/3038:214,Deployability,release,release,214,"Full error message:. ```[Stage 2:> (0 + 0) / 16]2018-02-28 23:41:58 Hail: INFO: interval filter loaded 2453 of 103675 partitions; 2018-02-28 23:42:08 Hail: WARN: deprecation: 'drop_cols' will be removed before 0.2 release; Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 415, in check_all; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 53, in check; hail.typecheck.check.TypecheckFailure. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 220, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 94, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 187, in main; vds, sample_table = generate_qc_annotations(vds, medians=not args.skip_medians); File ""<decorator-gen-616>"", line 2, in __getitem__; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 478, in _typecheck; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 425, in check_all; TypeError: __getitem__: parameter 'item': expected (str or tuple[(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression]),(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression])]), found int: '0'```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3038
https://github.com/hail-is/hail/issues/3038:11,Integrability,message,message,11,"Full error message:. ```[Stage 2:> (0 + 0) / 16]2018-02-28 23:41:58 Hail: INFO: interval filter loaded 2453 of 103675 partitions; 2018-02-28 23:42:08 Hail: WARN: deprecation: 'drop_cols' will be removed before 0.2 release; Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 415, in check_all; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 53, in check; hail.typecheck.check.TypecheckFailure. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 220, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 94, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 187, in main; vds, sample_table = generate_qc_annotations(vds, medians=not args.skip_medians); File ""<decorator-gen-616>"", line 2, in __getitem__; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 478, in _typecheck; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 425, in check_all; TypeError: __getitem__: parameter 'item': expected (str or tuple[(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression]),(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression])]), found int: '0'```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3038
https://github.com/hail-is/hail/issues/3038:96,Performance,load,loaded,96,"Full error message:. ```[Stage 2:> (0 + 0) / 16]2018-02-28 23:41:58 Hail: INFO: interval filter loaded 2453 of 103675 partitions; 2018-02-28 23:42:08 Hail: WARN: deprecation: 'drop_cols' will be removed before 0.2 release; Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 415, in check_all; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 53, in check; hail.typecheck.check.TypecheckFailure. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 220, in <module>; try_slack(args.slack_channel, main, args); File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 94, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/pyscripts_8Rer6O.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/generate_qc_annotations.py"", line 187, in main; vds, sample_table = generate_qc_annotations(vds, medians=not args.skip_medians); File ""<decorator-gen-616>"", line 2, in __getitem__; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 478, in _typecheck; File ""/tmp/98788bda-73b9-4ebe-924d-440665f0cd1d/hail-devel-53d795c163fd.zip/hail/typecheck/check.py"", line 425, in check_all; TypeError: __getitem__: parameter 'item': expected (str or tuple[(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression]),(slice or hail.expr.expression.Expression or tuple[hail.expr.expression.Expression])]), found int: '0'```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3038
https://github.com/hail-is/hail/issues/3039:11621,Availability,Error,Error,11621,"t is.hail.methods.MendelErrors.nErrorPerNuclearFamily(MendelErrors.scala:144); 	at is.hail.methods.MendelErrors.fMendelKT(MendelErrors.scala:183); 	at is.hail.variant.MatrixTable.mendelErrors(MatrixTable.scala:1915); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-42db165; Error summary: AssertionError: assertion failed; ```; Pipeline (happens on the 2nd mendel_errrors):; ```; vds = vds.select_rows(vds.locus, vds.alleles); fam_kt = hl.import_fam(fam_file); vds = vds.annotate_cols(fam=fam_kt[vds.s]). # Unphased for now, since mendel_errors does not support phased alleles; vds = vds.annotate_entries(GT=hl.call(vds.GT[0], vds.GT[1], phased=False)); ped = hl.Pedigree.read(fam_file); tdt_table = hl.tdt(vds, ped); _, _, per_sample, per_variant = hl.mendel_errors(vds, ped); family_stats = Struct(mendel=per_variant[(vds.locus, vds.alleles)],; tdt=tdt_table[(vds.locus, vds.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(vds.fam.pat_id),; vds.GT.num_alt_alleles())),; meta={'group': 'raw'}); vds = vds.annotate_rows(family_stats=[family_stats]). vds = filter_to_adj(vds); adj_tdt_table = hl.tdt(vds, ped); _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(vds, ped); family_stats_adj = Struct(mendel=adj_per_variant[(vds.locus, vds.alleles)],; tdt=adj_tdt_table[(vds.locus, vds.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3039:11675,Deployability,Pipeline,Pipeline,11675,"t is.hail.methods.MendelErrors.nErrorPerNuclearFamily(MendelErrors.scala:144); 	at is.hail.methods.MendelErrors.fMendelKT(MendelErrors.scala:183); 	at is.hail.variant.MatrixTable.mendelErrors(MatrixTable.scala:1915); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-42db165; Error summary: AssertionError: assertion failed; ```; Pipeline (happens on the 2nd mendel_errrors):; ```; vds = vds.select_rows(vds.locus, vds.alleles); fam_kt = hl.import_fam(fam_file); vds = vds.annotate_cols(fam=fam_kt[vds.s]). # Unphased for now, since mendel_errors does not support phased alleles; vds = vds.annotate_entries(GT=hl.call(vds.GT[0], vds.GT[1], phased=False)); ped = hl.Pedigree.read(fam_file); tdt_table = hl.tdt(vds, ped); _, _, per_sample, per_variant = hl.mendel_errors(vds, ped); family_stats = Struct(mendel=per_variant[(vds.locus, vds.alleles)],; tdt=tdt_table[(vds.locus, vds.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(vds.fam.pat_id),; vds.GT.num_alt_alleles())),; meta={'group': 'raw'}); vds = vds.annotate_rows(family_stats=[family_stats]). vds = filter_to_adj(vds); adj_tdt_table = hl.tdt(vds, ped); _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(vds, ped); family_stats_adj = Struct(mendel=adj_per_variant[(vds.locus, vds.alleles)],; tdt=adj_tdt_table[(vds.locus, vds.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3039:10207,Energy Efficiency,reduce,reduceByKey,10207,t.scala:285); 	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:84); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250); 	at org.apache.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66); 	at org.apache.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:66); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:328); 	at is.hail.methods.MendelErrors.nErrorPerNuclearFamily(MendelErrors.scala:144); 	at is.hail.methods.MendelErrors.fMendelKT(MendelErrors.scala:183); 	at is.hail.variant.MatrixTable.mendelErrors(MatrixTable.scala:1915); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodI,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3039:10307,Energy Efficiency,reduce,reduceByKey,10307,rk.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250); 	at org.apache.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66); 	at org.apache.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:66); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:328); 	at is.hail.methods.MendelErrors.nErrorPerNuclearFamily(MendelErrors.scala:144); 	at is.hail.methods.MendelErrors.fMendelKT(MendelErrors.scala:183); 	at is.hail.variant.MatrixTable.mendelErrors(MatrixTable.scala:1915); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3039:10621,Energy Efficiency,reduce,reduceByKey,10621,he.spark.Partitioner$$anonfun$defaultPartitioner$2.apply(Partitioner.scala:66); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:66); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:329); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:328); 	at is.hail.methods.MendelErrors.nErrorPerNuclearFamily(MendelErrors.scala:144); 	at is.hail.methods.MendelErrors.fMendelKT(MendelErrors.scala:183); 	at is.hail.variant.MatrixTable.mendelErrors(MatrixTable.scala:1915); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-42db165; Error summary: ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3039:1322,Testability,Assert,AssertionError,1322,"2b-8777-e5f05e247b63/pyscripts_KndB0n.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/generate_qc_annotations.py"", line 194, in main; vds, sample_table = generate_family_stats(vds, fam_file); File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/generate_qc_annotations.py"", line 134, in generate_family_stats; _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(vds, ped); File ""<decorator-gen-616>"", line 2, in mendel_errors; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/typecheck/check.py"", line 489, in _typecheck; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/methods/family_methods.py"", line 205, in mendel_errors; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDPartitioner.getPartitionPK(OrderedRVDPartitioner.scala:59); 	at is.hail.sparkextras.OrderedDependency$.getDependencies(OrderedRDD2.scala:22); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2$$anonfun$getPartitions$1.apply(OrderedRDD2.scala:42); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2$$anonfun$getPartitions$1.apply(OrderedRDD2.scala:39); 	at scala.Array$.tabulate(Array.scala:331); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2.getPartitions(OrderedRDD2.scala:39); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3039:1338,Testability,assert,assertion,1338,"2b-8777-e5f05e247b63/pyscripts_KndB0n.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/generate_qc_annotations.py"", line 194, in main; vds, sample_table = generate_family_stats(vds, fam_file); File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/generate_qc_annotations.py"", line 134, in generate_family_stats; _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(vds, ped); File ""<decorator-gen-616>"", line 2, in mendel_errors; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/typecheck/check.py"", line 489, in _typecheck; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/methods/family_methods.py"", line 205, in mendel_errors; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDPartitioner.getPartitionPK(OrderedRVDPartitioner.scala:59); 	at is.hail.sparkextras.OrderedDependency$.getDependencies(OrderedRDD2.scala:22); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2$$anonfun$getPartitions$1.apply(OrderedRDD2.scala:42); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2$$anonfun$getPartitions$1.apply(OrderedRDD2.scala:39); 	at scala.Array$.tabulate(Array.scala:331); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2.getPartitions(OrderedRDD2.scala:39); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3039:1385,Testability,Assert,AssertionError,1385," 77, in try_slack; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/generate_qc_annotations.py"", line 194, in main; vds, sample_table = generate_family_stats(vds, fam_file); File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/generate_qc_annotations.py"", line 134, in generate_family_stats; _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(vds, ped); File ""<decorator-gen-616>"", line 2, in mendel_errors; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/typecheck/check.py"", line 489, in _typecheck; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/methods/family_methods.py"", line 205, in mendel_errors; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDPartitioner.getPartitionPK(OrderedRVDPartitioner.scala:59); 	at is.hail.sparkextras.OrderedDependency$.getDependencies(OrderedRDD2.scala:22); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2$$anonfun$getPartitions$1.apply(OrderedRDD2.scala:42); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2$$anonfun$getPartitions$1.apply(OrderedRDD2.scala:39); 	at scala.Array$.tabulate(Array.scala:331); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2.getPartitions(OrderedRDD2.scala:39); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3039:1401,Testability,assert,assertion,1401," 77, in try_slack; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/generate_qc_annotations.py"", line 194, in main; vds, sample_table = generate_family_stats(vds, fam_file); File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/generate_qc_annotations.py"", line 134, in generate_family_stats; _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(vds, ped); File ""<decorator-gen-616>"", line 2, in mendel_errors; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/typecheck/check.py"", line 489, in _typecheck; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/methods/family_methods.py"", line 205, in mendel_errors; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDPartitioner.getPartitionPK(OrderedRVDPartitioner.scala:59); 	at is.hail.sparkextras.OrderedDependency$.getDependencies(OrderedRDD2.scala:22); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2$$anonfun$getPartitions$1.apply(OrderedRDD2.scala:42); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2$$anonfun$getPartitions$1.apply(OrderedRDD2.scala:39); 	at scala.Array$.tabulate(Array.scala:331); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2.getPartitions(OrderedRDD2.scala:39); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3039:1437,Testability,assert,assert,1437,"7-96ab-442b-8777-e5f05e247b63/generate_qc_annotations.py"", line 194, in main; vds, sample_table = generate_family_stats(vds, fam_file); File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/generate_qc_annotations.py"", line 134, in generate_family_stats; _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(vds, ped); File ""<decorator-gen-616>"", line 2, in mendel_errors; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/typecheck/check.py"", line 489, in _typecheck; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/methods/family_methods.py"", line 205, in mendel_errors; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/837e8bf7-96ab-442b-8777-e5f05e247b63/hail-devel-48d0534b1b09.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVDPartitioner.getPartitionPK(OrderedRVDPartitioner.scala:59); 	at is.hail.sparkextras.OrderedDependency$.getDependencies(OrderedRDD2.scala:22); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2$$anonfun$getPartitions$1.apply(OrderedRDD2.scala:42); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2$$anonfun$getPartitions$1.apply(OrderedRDD2.scala:39); 	at scala.Array$.tabulate(Array.scala:331); 	at is.hail.sparkextras.OrderedJoinDistinctRDD2.getPartitions(OrderedRDD2.scala:39); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.rdd.RDD.partitions(RDD.scala:250); 	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252); 	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250); 	a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3039:11636,Testability,Assert,AssertionError,11636,"t is.hail.methods.MendelErrors.nErrorPerNuclearFamily(MendelErrors.scala:144); 	at is.hail.methods.MendelErrors.fMendelKT(MendelErrors.scala:183); 	at is.hail.variant.MatrixTable.mendelErrors(MatrixTable.scala:1915); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-42db165; Error summary: AssertionError: assertion failed; ```; Pipeline (happens on the 2nd mendel_errrors):; ```; vds = vds.select_rows(vds.locus, vds.alleles); fam_kt = hl.import_fam(fam_file); vds = vds.annotate_cols(fam=fam_kt[vds.s]). # Unphased for now, since mendel_errors does not support phased alleles; vds = vds.annotate_entries(GT=hl.call(vds.GT[0], vds.GT[1], phased=False)); ped = hl.Pedigree.read(fam_file); tdt_table = hl.tdt(vds, ped); _, _, per_sample, per_variant = hl.mendel_errors(vds, ped); family_stats = Struct(mendel=per_variant[(vds.locus, vds.alleles)],; tdt=tdt_table[(vds.locus, vds.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(vds.fam.pat_id),; vds.GT.num_alt_alleles())),; meta={'group': 'raw'}); vds = vds.annotate_rows(family_stats=[family_stats]). vds = filter_to_adj(vds); adj_tdt_table = hl.tdt(vds, ped); _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(vds, ped); family_stats_adj = Struct(mendel=adj_per_variant[(vds.locus, vds.alleles)],; tdt=adj_tdt_table[(vds.locus, vds.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3039:11652,Testability,assert,assertion,11652,"t is.hail.methods.MendelErrors.nErrorPerNuclearFamily(MendelErrors.scala:144); 	at is.hail.methods.MendelErrors.fMendelKT(MendelErrors.scala:183); 	at is.hail.variant.MatrixTable.mendelErrors(MatrixTable.scala:1915); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-42db165; Error summary: AssertionError: assertion failed; ```; Pipeline (happens on the 2nd mendel_errrors):; ```; vds = vds.select_rows(vds.locus, vds.alleles); fam_kt = hl.import_fam(fam_file); vds = vds.annotate_cols(fam=fam_kt[vds.s]). # Unphased for now, since mendel_errors does not support phased alleles; vds = vds.annotate_entries(GT=hl.call(vds.GT[0], vds.GT[1], phased=False)); ped = hl.Pedigree.read(fam_file); tdt_table = hl.tdt(vds, ped); _, _, per_sample, per_variant = hl.mendel_errors(vds, ped); family_stats = Struct(mendel=per_variant[(vds.locus, vds.alleles)],; tdt=tdt_table[(vds.locus, vds.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(vds.fam.pat_id),; vds.GT.num_alt_alleles())),; meta={'group': 'raw'}); vds = vds.annotate_rows(family_stats=[family_stats]). vds = filter_to_adj(vds); adj_tdt_table = hl.tdt(vds, ped); _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(vds, ped); family_stats_adj = Struct(mendel=adj_per_variant[(vds.locus, vds.alleles)],; tdt=adj_tdt_table[(vds.locus, vds.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3039
https://github.com/hail-is/hail/issues/3040:1366,Availability,failure,failure,1366,"slack_utils.py"", line 97, in try_slack; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/pyscripts_F47nn5.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/generate_qc_annotations.py"", line 203, in main; vds.write(annotations_vds_path(data_type, 'truth_data'), args.overwrite); File ""<decorator-gen-528>"", line 2, in write; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/typecheck/check.py"", line 479, in _typecheck; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/matrixtable.py"", line 1807, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: HailException: found non-left aligned variant: 18:76051965:C:G. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 3.0 failed 20 times, most recent failure: Lost task 56.19 in stage 3.0 (TID 685, exomes2-sw-8mf1.c.broad-mpg-gnomad.internal, executor 55): is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:1425,Availability,failure,failure,1425,"2a9-4d26-ac71-5e6676ff3392/pyscripts_F47nn5.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/generate_qc_annotations.py"", line 203, in main; vds.write(annotations_vds_path(data_type, 'truth_data'), args.overwrite); File ""<decorator-gen-528>"", line 2, in write; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/typecheck/check.py"", line 479, in _typecheck; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/matrixtable.py"", line 1807, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: HailException: found non-left aligned variant: 18:76051965:C:G. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 3.0 failed 20 times, most recent failure: Lost task 56.19 in stage 3.0 (TID 685, exomes2-sw-8mf1.c.broad-mpg-gnomad.internal, executor 55): is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:1628,Availability,Error,ErrorHandling,1628,"vds_path(data_type, 'truth_data'), args.overwrite); File ""<decorator-gen-528>"", line 2, in write; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/typecheck/check.py"", line 479, in _typecheck; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/matrixtable.py"", line 1807, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: HailException: found non-left aligned variant: 18:76051965:C:G. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 3.0 failed 20 times, most recent failure: Lost task 56.19 in stage 3.0 (TID 685, exomes2-sw-8mf1.c.broad-mpg-gnomad.internal, executor 55): is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight1(OrderedJoinDistinctIterator.scala:36); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight(OrderedJoinDistinctIterator.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:1654,Availability,Error,ErrorHandling,1654,"'truth_data'), args.overwrite); File ""<decorator-gen-528>"", line 2, in write; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/typecheck/check.py"", line 479, in _typecheck; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/matrixtable.py"", line 1807, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: HailException: found non-left aligned variant: 18:76051965:C:G. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 3.0 failed 20 times, most recent failure: Lost task 56.19 in stage 3.0 (TID 685, exomes2-sw-8mf1.c.broad-mpg-gnomad.internal, executor 55): is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight1(OrderedJoinDistinctIterator.scala:36); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight(OrderedJoinDistinctIterator.scala:42); 	at is.hail.spa",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:9671,Availability,Error,ErrorHandling,9671,he.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:847); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2712); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight1(OrderedJoinDistinctIterator.scala:36); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight(OrderedJoinDistinctIterator.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:9697,Availability,Error,ErrorHandling,9697,lect(RDD.scala:935); 	at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:847); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2712); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748)is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight1(OrderedJoinDistinctIterator.scala:36); 	at is.hail.sparkextras.OrderedJoinDistinctIterator.advanceRight(OrderedJoinDistinctIterator.scala:42); 	at is.hail.spa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:14808,Availability,Error,Error,14808,y$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df317f3; Error summary: HailException: found non-left aligned variant: 18:76051965:C:G; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:6340,Energy Efficiency,schedul,scheduler,6340,$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:6412,Energy Efficiency,schedul,scheduler,6412,cala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:6777,Energy Efficiency,schedul,scheduler,6777,.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:6817,Energy Efficiency,schedul,scheduler,6817,at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:6916,Energy Efficiency,schedul,scheduler,6916,pache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:7014,Energy Efficiency,schedul,scheduler,7014,org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:7268,Energy Efficiency,schedul,scheduler,7268,apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:7349,Energy Efficiency,schedul,scheduler,7349,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:7455,Energy Efficiency,schedul,scheduler,7455,he.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:7605,Energy Efficiency,schedul,scheduler,7605,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:7694,Energy Efficiency,schedul,scheduler,7694,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:7792,Energy Efficiency,schedul,scheduler,7792,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:7888,Energy Efficiency,schedul,scheduler,7888,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:847); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2712); 	at sun.reflect.NativeMetho,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:8053,Energy Efficiency,schedul,scheduler,8053,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.io.RichRDDRegionValue$.writeRowsSplit$extension(RowStore.scala:847); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2712); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:14383,Energy Efficiency,schedul,scheduler,14383,y$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df317f3; Error summary: HailException: found non-left aligned variant: 18:76051965:C:G; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:14455,Energy Efficiency,schedul,scheduler,14455,y$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df317f3; Error summary: HailException: found non-left aligned variant: 18:76051965:C:G; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:6537,Performance,concurren,concurrent,6537,ionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:6622,Performance,concurren,concurrent,6622,kage$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:14580,Performance,concurren,concurrent,14580,y$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df317f3; Error summary: HailException: found non-left aligned variant: 18:76051965:C:G; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:14665,Performance,concurren,concurrent,14665,y$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-df317f3; Error summary: HailException: found non-left aligned variant: 18:76051965:C:G; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:1345,Safety,abort,aborted,1345,"slack_utils.py"", line 97, in try_slack; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/pyscripts_F47nn5.zip/gnomad_hail/slack_utils.py"", line 77, in try_slack; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/generate_qc_annotations.py"", line 203, in main; vds.write(annotations_vds_path(data_type, 'truth_data'), args.overwrite); File ""<decorator-gen-528>"", line 2, in write; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/typecheck/check.py"", line 479, in _typecheck; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/matrixtable.py"", line 1807, in write; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/88fe16cd-42a9-4d26-ac71-5e6676ff3392/hail-devel-6d6d3d2d7992.zip/hail/utils/java.py"", line 238, in deco; hail.utils.java.FatalError: HailException: found non-left aligned variant: 18:76051965:C:G. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 56 in stage 3.0 failed 20 times, most recent failure: Lost task 56.19 in stage 3.0 (TID 685, exomes2-sw-8mf1.c.broad-mpg-gnomad.internal, executor 55): is.hail.utils.HailException: found non-left aligned variant: 18:76051965:C:G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:6); 	at is.hail.utils.package$.fatal(package.scala:26); 	at is.hail.methods.SplitMultiPartitionContext.splitRow(SplitMulti.scala:98); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:226); 	at is.hail.methods.SplitMulti$$anonfun$split$1$$anonfun$apply$1.apply(SplitMulti.scala:225); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:6948,Safety,abort,abortStage,6948,nonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:7046,Safety,abort,abortStage,7046,DD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3040:7291,Safety,abort,abortStage,7291,RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3040
https://github.com/hail-is/hail/issues/3041:4695,Energy Efficiency,schedul,scheduler,4695,age.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3041
https://github.com/hail-is/hail/issues/3041:4767,Energy Efficiency,schedul,scheduler,4767,age.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3041
https://github.com/hail-is/hail/issues/3041:4892,Performance,concurren,concurrent,4892,age.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3041
https://github.com/hail-is/hail/issues/3041:4977,Performance,concurren,concurrent,4977,age.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:808); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(RowStore.scala:807); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:807); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1$$anonfun$apply$2.apply(RowStore.scala:804); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:804); 	at is.hail.io.RichRDDRegionValue$$anonfun$5$$anonfun$apply$1.apply(RowStore.scala:803); 	at is.hail.utils.package$.using(package.scala:570); 	at is.hail.utils.richUtils.RichHadoopConfiguration$.writeFile$extension(RichHadoopConfiguration.scala:265); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:803); 	at is.hail.io.RichRDDRegionValue$$anonfun$5.apply(RowStore.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3041
https://github.com/hail-is/hail/pull/3043:143,Integrability,depend,depends,143,"This adds about 800KB total to the repo. I added a notion of supportedCodec, which includes all the codecs except DirectCodec (which currently depends on the in memory representation of region values which we don't want to promise). FYI @tpoterba I also added a micro optimization: if you get a field of a struct declaration, just grab out the AST for the field. This fixed some massive expression size blowup in create_all_values_datasets.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3043
https://github.com/hail-is/hail/pull/3043:268,Performance,optimiz,optimization,268,"This adds about 800KB total to the repo. I added a notion of supportedCodec, which includes all the codecs except DirectCodec (which currently depends on the in memory representation of region values which we don't want to promise). FYI @tpoterba I also added a micro optimization: if you get a field of a struct declaration, just grab out the AST for the field. This fixed some massive expression size blowup in create_all_values_datasets.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3043
https://github.com/hail-is/hail/issues/3047:177,Availability,error,error,177,"Self-explanatory :). -------------------------------------------------------------------------------------------. ### Hail version:. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3047
https://github.com/hail-is/hail/issues/3047:183,Integrability,message,messages,183,"Self-explanatory :). -------------------------------------------------------------------------------------------. ### Hail version:. ### What you did:. ### What went wrong (all error messages here, including the full java stack trace):",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3047
https://github.com/hail-is/hail/issues/3050:117,Deployability,update,update,117,"Variant dataset, 0.1 expression language snippets, etc. We should farm out each method to someone to take a look and update.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3050
https://github.com/hail-is/hail/issues/3053:24,Availability,failure,failure,24,"Getting a sporadic task failure, with the error:; ```; ExecutorLostFailure (executor 99 exited caused by one of the running tasks) Reason: Container marked as failed: container_1519994715701_0003_01_000102 on host: exomes-sw-pxt3.c.broad-mpg-gnomad.internal. Exit status: 134. Diagnostics: Exception from container-launch.; Container id: container_1519994715701_0003_01_000102; Exit code: 134; Exception message: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:42,Availability,error,error,42,"Getting a sporadic task failure, with the error:; ```; ExecutorLostFailure (executor 99 exited caused by one of the running tasks) Reason: Container marked as failed: container_1519994715701_0003_01_000102 on host: exomes-sw-pxt3.c.broad-mpg-gnomad.internal. Exit status: 134. Diagnostics: Exception from container-launch.; Container id: container_1519994715701_0003_01_000102; Exit code: 134; Exception message: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:3919,Availability,error,errors,3919,"ation_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more infor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:4213,Availability,error,error,4213,"anager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0001/container_1519994715701_0001_01_000558/hs_err_pid4361.log; # [ timer expired, abort... ]; ```; But said log file has no information. The job is finishing, just with a high (~50-100%) task failure rate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:4607,Availability,error,error,4607,"anager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0001/container_1519994715701_0001_01_000558/hs_err_pid4361.log; # [ timer expired, abort... ]; ```; But said log file has no information. The job is finishing, just with a high (~50-100%) task failure rate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:4629,Availability,error,error,4629,"anager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0001/container_1519994715701_0001_01_000558/hs_err_pid4361.log; # [ timer expired, abort... ]; ```; But said log file has no information. The job is finishing, just with a high (~50-100%) task failure rate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:4830,Availability,error,error,4830,"anager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0001/container_1519994715701_0001_01_000558/hs_err_pid4361.log; # [ timer expired, abort... ]; ```; But said log file has no information. The job is finishing, just with a high (~50-100%) task failure rate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:5155,Availability,failure,failure,5155,"anager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0001/container_1519994715701_0001_01_000558/hs_err_pid4361.log; # [ timer expired, abort... ]; ```; But said log file has no information. The job is finishing, just with a high (~50-100%) task failure rate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:404,Integrability,message,message,404,"Getting a sporadic task failure, with the error:; ```; ExecutorLostFailure (executor 99 exited caused by one of the running tasks) Reason: Container marked as failed: container_1519994715701_0003_01_000102 on host: exomes-sw-pxt3.c.broad-mpg-gnomad.internal. Exit status: 134. Diagnostics: Exception from container-launch.; Container id: container_1519994715701_0003_01_000102; Exit code: 134; Exception message: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:698,Integrability,message,message,698,"Getting a sporadic task failure, with the error:; ```; ExecutorLostFailure (executor 99 exited caused by one of the running tasks) Reason: Container marked as failed: container_1519994715701_0003_01_000102 on host: exomes-sw-pxt3.c.broad-mpg-gnomad.internal. Exit status: 134. Diagnostics: Exception from container-launch.; Container id: container_1519994715701_0003_01_000102; Exit code: 134; Exception message: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:1981,Integrability,message,message,1981,@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.j,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:3505,Performance,concurren,concurrent,3505,"/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:3567,Performance,concurren,concurrent,3567,"ss-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libco",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:3652,Performance,concurren,concurrent,3652,"701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing pro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:437,Safety,Abort,Aborted,437,"Getting a sporadic task failure, with the error:; ```; ExecutorLostFailure (executor 99 exited caused by one of the running tasks) Reason: Container marked as failed: container_1519994715701_0003_01_000102 on host: exomes-sw-pxt3.c.broad-mpg-gnomad.internal. Exit status: 134. Diagnostics: Exception from container-launch.; Container id: container_1519994715701_0003_01_000102; Exit code: 134; Exception message: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:1720,Safety,Abort,Aborted,1720,arn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/had,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:4228,Safety,detect,detected,4228,"anager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0001/container_1519994715701_0001_01_000558/hs_err_pid4361.log; # [ timer expired, abort... ]; ```; But said log file has no information. The job is finishing, just with a high (~50-100%) task failure rate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:5045,Safety,abort,abort,5045,"anager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0001/container_1519994715701_0001_01_000558/hs_err_pid4361.log; # [ timer expired, abort... ]; ```; But said log file has no information. The job is finishing, just with a high (~50-100%) task failure rate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:746,Testability,log,log,746,"Getting a sporadic task failure, with the error:; ```; ExecutorLostFailure (executor 99 exited caused by one of the running tasks) Reason: Container marked as failed: container_1519994715701_0003_01_000102 on host: exomes-sw-pxt3.c.broad-mpg-gnomad.internal. Exit status: 134. Diagnostics: Exception from container-launch.; Container id: container_1519994715701_0003_01_000102; Exit code: 134; Exception message: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:759,Testability,log,log,759,"Getting a sporadic task failure, with the error:; ```; ExecutorLostFailure (executor 99 exited caused by one of the running tasks) Reason: Container marked as failed: container_1519994715701_0003_01_000102 on host: exomes-sw-pxt3.c.broad-mpg-gnomad.internal. Exit status: 134. Diagnostics: Exception from container-launch.; Container id: container_1519994715701_0003_01_000102; Exit code: 134; Exception message: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512'",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:1438,Testability,log,log,1438,op/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:1548,Testability,log,log,1548,op/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:2029,Testability,log,log,2029,exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:2042,Testability,log,log,2042,n_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. Stack trace: ExitCodeException exitCode=134: /bin/bash: line 1: 6739 Aborted /usr/lib/jvm/java-8-openjdk-amd64/bin/java -server -Xmx11171m '-Xss4M' -Djava.io.tmpdir=/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.e,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:2721,Testability,log,log,2721,op/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; Whe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:2831,Testability,log,log,2831,op/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/tmp '-Dspark.driver.port=41843' '-Dspark.rpc.message.maxSize=512' -Dspark.yarn.app.container.log.dir=/var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102 -XX:OnOutOfMemoryError='kill %p' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@10.128.0.4:41843 --executor-id 99 --hostname exomes-sw-pxt3.c.broad-mpg-gnomad.internal --cores 4 --app-id application_1519994715701_0003 --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/__app__.jar --user-class-path file:/hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0003/container_1519994715701_0003_01_000102/hail.jar > /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stdout 2> /var/log/hadoop-yarn/userlogs/application_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; Whe,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:3849,Testability,log,logs,3849,"ation_1519994715701_0003/container_1519994715701_0003_01_000102/stderr. 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:972); 	at org.apache.hadoop.util.Shell.run(Shell.java:869); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1170); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more infor",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:4177,Testability,log,logged,4177,"anager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0001/container_1519994715701_0001_01_000558/hs_err_pid4361.log; # [ timer expired, abort... ]; ```; But said log file has no information. The job is finishing, just with a high (~50-100%) task failure rate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:5021,Testability,log,log,5021,"anager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0001/container_1519994715701_0001_01_000558/hs_err_pid4361.log; # [ timer expired, abort... ]; ```; But said log file has no information. The job is finishing, just with a high (~50-100%) task failure rate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3053:5071,Testability,log,log,5071,"anager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:236); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:305); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:84); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 134; ```; When I dig into the container logs, the stdout is empty on most, stderr is full of warnings, but no errors:; ```; 18/03/02 15:28:07 WARN com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel: Channel for 'gs://gnomad/coverage/hail-0.2/coverage/exomes/parts/part_partition1049.vds/entries/rows/parts/part-0095' is not open.; ```; But then one machine I logged into had:; ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007fa70eb59074, pid=4361, tid=0x00007fa707702700; #; # JRE version: OpenJDK Runtime Environment (8.0_131-b11) (build 1.8.0_131-8u131-b11-1~bpo8+1-b11); # Java VM: OpenJDK 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops); # Problematic frame:; # C [libconscrypt_openjdk_jni.so+0x43074]; [error occurred during error reporting (printing problematic frame), id 0xb]. # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again; #; # An error report file with more information is saved as:; # /hadoop/yarn/nm-local-dir/usercache/root/appcache/application_1519994715701_0001/container_1519994715701_0001_01_000558/hs_err_pid4361.log; # [ timer expired, abort... ]; ```; But said log file has no information. The job is finishing, just with a high (~50-100%) task failure rate.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3053
https://github.com/hail-is/hail/issues/3063:342,Availability,error,error,342,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Ordered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:451,Availability,down,down,451,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Ordered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:483,Availability,error,error,483,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Ordered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:515,Availability,error,error,515,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Ordered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:1156,Availability,error,error,1156,"--------------------------------------------------------------------. ### Hail version:; devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:1260,Availability,failure,failure,1260," devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:1318,Availability,failure,failure,1318,"eps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:11852,Availability,failure,failure,11852,"cala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:11910,Availability,failure,failure,11910,"ly(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:21953,Availability,Error,Error,21953,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:22002,Availability,ERROR,ERROR,22002,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:22105,Availability,ERROR,ERROR,22105,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:4690,Energy Efficiency,schedul,scheduler,4690,redRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.schedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:4761,Energy Efficiency,schedul,scheduler,4761,ext(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:5121,Energy Efficiency,schedul,scheduler,5121,57); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:5161,Energy Efficiency,schedul,scheduler,5161,.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:5259,Energy Efficiency,schedul,scheduler,5259,ce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:5356,Energy Efficiency,schedul,scheduler,5356,.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:5607,Energy Efficiency,schedul,scheduler,5607,.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggre,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:5687,Energy Efficiency,schedul,scheduler,5687,heduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withSco,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:5792,Energy Efficiency,schedul,scheduler,5792,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:5940,Energy Efficiency,schedul,scheduler,5940,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:6028,Energy Efficiency,schedul,scheduler,6028,617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at su,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:6125,Energy Efficiency,schedul,scheduler,6125,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.Deleg,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:6220,Energy Efficiency,schedul,scheduler,6220,.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Meth,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:6383,Energy Efficiency,schedul,scheduler,6383,abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:10954,Energy Efficiency,schedul,scheduler,10954,"redRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, ro",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:11025,Energy Efficiency,schedul,scheduler,11025,"ext(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:15282,Energy Efficiency,schedul,scheduler,15282,redRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.schedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:15353,Energy Efficiency,schedul,scheduler,15353,ext(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:15713,Energy Efficiency,schedul,scheduler,15713,57); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:15753,Energy Efficiency,schedul,scheduler,15753,.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:15851,Energy Efficiency,schedul,scheduler,15851,ce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:15948,Energy Efficiency,schedul,scheduler,15948,.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:16199,Energy Efficiency,schedul,scheduler,16199,.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggre,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:16279,Energy Efficiency,schedul,scheduler,16279,heduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withSco,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:16384,Energy Efficiency,schedul,scheduler,16384,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:16532,Energy Efficiency,schedul,scheduler,16532,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:16620,Energy Efficiency,schedul,scheduler,16620,617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at su,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:16717,Energy Efficiency,schedul,scheduler,16717,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.Deleg,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:16812,Energy Efficiency,schedul,scheduler,16812,.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Meth,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:16975,Energy Efficiency,schedul,scheduler,16975,abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357);,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:21533,Energy Efficiency,schedul,scheduler,21533,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:21604,Energy Efficiency,schedul,scheduler,21604,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:521,Integrability,message,messages,521,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Ordered",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:1079,Integrability,protocol,protocol,1079,"sts, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:1129,Integrability,protocol,protocol,1129,"ail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.ne",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:4884,Performance,concurren,concurrent,4884,on$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:4968,Performance,concurren,concurrent,4968,scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:11148,Performance,concurren,concurrent,11148,"on$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueB",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:11232,Performance,concurren,concurrent,11232,"scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfu",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:15476,Performance,concurren,concurrent,15476,on$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Opti,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:15560,Performance,concurren,concurrent,15560,scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:21727,Performance,concurren,concurrent,21727,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:21811,Performance,concurren,concurrent,21811,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:1239,Safety,abort,aborted,1239," devel-6bb4670. ### What you did:; A number of variant QC steps, then a `vds.write`; The error is probably caused by one of the previous steps. If it helps I can comment out earlier parts to narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:5291,Safety,abort,abortStage,5291,a.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:5388,Safety,abort,abortStage,5388,n$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:5630,Safety,abort,abortStage,5630,y(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:11831,Safety,abort,aborted,11831,"cala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.colle",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:15883,Safety,abort,abortStage,15883,a.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:15980,Safety,abort,abortStage,15980,n$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:16222,Safety,abort,abortStage,16222,y(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1115); ,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:1421,Testability,Assert,AssertionError,1421,"o narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Order",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:1437,Testability,assert,assertion,1437,"o narrow down what actually triggers the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Order",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:1472,Testability,assert,assert,1472,"the error. ### What went wrong (all error messages here, including the full java stack trace):; ```; [Stage 6:> (0 + 8) / 5000]; [Stage 6:> (0 + 4) / 5000]; [Stage 6:> (0 + 8) / 5000]Traceback (most recent call last):; File ""/home/hail/hail.zip/hail/utils/java.py"", line 185, in handle_py4j; File ""/home/hail/hail.zip/hail/table.py"", line 1058, in aggregate; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py"", line 63, in deco; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o30335.query.; : org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collectio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:7685,Testability,Assert,AssertionError,7685,nScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Order,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:7701,Testability,assert,assertion,7701,nScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Order,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:7736,Testability,assert,assert,7736,k.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collectio,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:11741,Testability,Assert,AssertionError,11741,"un$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:11757,Testability,assert,assertion,11757,"un$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:12013,Testability,Assert,AssertionError,12013,".Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Order",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:12029,Testability,assert,assertion,12029,".Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Order",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:12064,Testability,assert,assert,12064,"che.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); ... 1 more. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/b09ec92a-49f4-4d16-ad6d-efc5a5805e92/05_variant_qc.py"", line 201, in <module>; cumcounts = {'step0': rt.aggregate(hl.agg.sum(hl.cond(rt.qccum.step0, 1, 0))),; File ""<decorator-gen-519>"", line 2, in aggregate; File ""/home/hail/hail.zip/hail/utils/java.py"", line 191, in handle_py4j; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6.0 failed 20 times, most recent failure: Lost task 7.19 in stage 6.0 (TID 179, robert1-w-0.c.ccdg-wgs.internal, executor 4): java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collectio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:18264,Testability,Assert,AssertionError,18264,e(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Order,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:18280,Testability,assert,assertion,18280,e(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(Order,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:18315,Testability,assert,assert,18315,g.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1108); at is.hail.table.Table.query(Table.scala:339); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); at py4j.Gateway.invoke(Gateway.java:280); at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); at py4j.commands.CallCommand.execute(CallCommand.java:79); at py4j.GatewayConnection.run(GatewayConnection.java:214); at java.lang.Thread.run(Thread.java:748)java.lang.AssertionError: assertion failed; at scala.Predef$.assert(Predef.scala:156); at is.hail.annotations.RegionValueBuilder.endStruct(RegionValueBuilder.scala:109); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2645); at is.hail.variant.MatrixTable$$anonfun$filterGenotypes$1$$anonfun$apply$80.apply(MatrixTable.scala:2615); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collectio,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:21968,Testability,Assert,AssertionError,21968,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/issues/3063:21984,Testability,assert,assertion,21984,RVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1113); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:2118); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Hail version: devel-6bb4670; Error summary: AssertionError: assertion failed; ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [b09ec92a-49f4-4d16-ad6d-efc5a5805e92] entered state [ERROR] while waiting for [DONE].; Submitting to cluster 'robert1'...; gcloud command:; gcloud dataproc jobs submit pyspark hail2/05_variant_qc.py \; --cluster=robert1 \; --files= \; --properties= \; -- \; onep; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3063
https://github.com/hail-is/hail/pull/3064:499,Deployability,pipeline,pipeline,499,"LocalMatrix follows NumPy's broadcast rules (restricted to two-dimensional ndarrays), and I've tried to mirror the Numpy interface for all functions where it's reasonable to do so. I still need time to add a bunch of Python tests of the interface, but I'd be glad for feedback/review in the meantime. In a subsequent PR, I'll expose the rest of BlockMatrix's binary ops in Python with the similar syntax and rules. These changes will provide the matrix functionality needed for a clean mixed models pipeline (modulo a few Scala black boxes that I can return to once I have something working) and will hopefully be generally useful for adding/porting more methods in Python. Current longer-term plan is to expose RowMatrix as well, and consider how to best unify the interfaces. And one day LocalMatrix will actually be a NumPy array...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3064
https://github.com/hail-is/hail/pull/3064:121,Integrability,interface,interface,121,"LocalMatrix follows NumPy's broadcast rules (restricted to two-dimensional ndarrays), and I've tried to mirror the Numpy interface for all functions where it's reasonable to do so. I still need time to add a bunch of Python tests of the interface, but I'd be glad for feedback/review in the meantime. In a subsequent PR, I'll expose the rest of BlockMatrix's binary ops in Python with the similar syntax and rules. These changes will provide the matrix functionality needed for a clean mixed models pipeline (modulo a few Scala black boxes that I can return to once I have something working) and will hopefully be generally useful for adding/porting more methods in Python. Current longer-term plan is to expose RowMatrix as well, and consider how to best unify the interfaces. And one day LocalMatrix will actually be a NumPy array...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3064
https://github.com/hail-is/hail/pull/3064:237,Integrability,interface,interface,237,"LocalMatrix follows NumPy's broadcast rules (restricted to two-dimensional ndarrays), and I've tried to mirror the Numpy interface for all functions where it's reasonable to do so. I still need time to add a bunch of Python tests of the interface, but I'd be glad for feedback/review in the meantime. In a subsequent PR, I'll expose the rest of BlockMatrix's binary ops in Python with the similar syntax and rules. These changes will provide the matrix functionality needed for a clean mixed models pipeline (modulo a few Scala black boxes that I can return to once I have something working) and will hopefully be generally useful for adding/porting more methods in Python. Current longer-term plan is to expose RowMatrix as well, and consider how to best unify the interfaces. And one day LocalMatrix will actually be a NumPy array...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3064
https://github.com/hail-is/hail/pull/3064:766,Integrability,interface,interfaces,766,"LocalMatrix follows NumPy's broadcast rules (restricted to two-dimensional ndarrays), and I've tried to mirror the Numpy interface for all functions where it's reasonable to do so. I still need time to add a bunch of Python tests of the interface, but I'd be glad for feedback/review in the meantime. In a subsequent PR, I'll expose the rest of BlockMatrix's binary ops in Python with the similar syntax and rules. These changes will provide the matrix functionality needed for a clean mixed models pipeline (modulo a few Scala black boxes that I can return to once I have something working) and will hopefully be generally useful for adding/porting more methods in Python. Current longer-term plan is to expose RowMatrix as well, and consider how to best unify the interfaces. And one day LocalMatrix will actually be a NumPy array...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3064
https://github.com/hail-is/hail/pull/3064:326,Security,expose,expose,326,"LocalMatrix follows NumPy's broadcast rules (restricted to two-dimensional ndarrays), and I've tried to mirror the Numpy interface for all functions where it's reasonable to do so. I still need time to add a bunch of Python tests of the interface, but I'd be glad for feedback/review in the meantime. In a subsequent PR, I'll expose the rest of BlockMatrix's binary ops in Python with the similar syntax and rules. These changes will provide the matrix functionality needed for a clean mixed models pipeline (modulo a few Scala black boxes that I can return to once I have something working) and will hopefully be generally useful for adding/porting more methods in Python. Current longer-term plan is to expose RowMatrix as well, and consider how to best unify the interfaces. And one day LocalMatrix will actually be a NumPy array...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3064
https://github.com/hail-is/hail/pull/3064:705,Security,expose,expose,705,"LocalMatrix follows NumPy's broadcast rules (restricted to two-dimensional ndarrays), and I've tried to mirror the Numpy interface for all functions where it's reasonable to do so. I still need time to add a bunch of Python tests of the interface, but I'd be glad for feedback/review in the meantime. In a subsequent PR, I'll expose the rest of BlockMatrix's binary ops in Python with the similar syntax and rules. These changes will provide the matrix functionality needed for a clean mixed models pipeline (modulo a few Scala black boxes that I can return to once I have something working) and will hopefully be generally useful for adding/porting more methods in Python. Current longer-term plan is to expose RowMatrix as well, and consider how to best unify the interfaces. And one day LocalMatrix will actually be a NumPy array...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3064
https://github.com/hail-is/hail/pull/3064:224,Testability,test,tests,224,"LocalMatrix follows NumPy's broadcast rules (restricted to two-dimensional ndarrays), and I've tried to mirror the Numpy interface for all functions where it's reasonable to do so. I still need time to add a bunch of Python tests of the interface, but I'd be glad for feedback/review in the meantime. In a subsequent PR, I'll expose the rest of BlockMatrix's binary ops in Python with the similar syntax and rules. These changes will provide the matrix functionality needed for a clean mixed models pipeline (modulo a few Scala black boxes that I can return to once I have something working) and will hopefully be generally useful for adding/porting more methods in Python. Current longer-term plan is to expose RowMatrix as well, and consider how to best unify the interfaces. And one day LocalMatrix will actually be a NumPy array...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3064
https://github.com/hail-is/hail/pull/3064:268,Usability,feedback,feedback,268,"LocalMatrix follows NumPy's broadcast rules (restricted to two-dimensional ndarrays), and I've tried to mirror the Numpy interface for all functions where it's reasonable to do so. I still need time to add a bunch of Python tests of the interface, but I'd be glad for feedback/review in the meantime. In a subsequent PR, I'll expose the rest of BlockMatrix's binary ops in Python with the similar syntax and rules. These changes will provide the matrix functionality needed for a clean mixed models pipeline (modulo a few Scala black boxes that I can return to once I have something working) and will hopefully be generally useful for adding/porting more methods in Python. Current longer-term plan is to expose RowMatrix as well, and consider how to best unify the interfaces. And one day LocalMatrix will actually be a NumPy array...",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3064
https://github.com/hail-is/hail/pull/3071:13,Deployability,update,updates,13,"I made these updates to Scala LocalMatrix as I was building the Python interface, to more closely mirror NumPy functions and name the symbolic operators. I no longer intend to expose LocalMatrix in Python in its current form, but rather to localize BlockMatrix ""directly"" to NumPy and vice versa. Still, LocalMatrix in Scala is a useful local model for how I'll update BlockMatrix to be more NumPy like (e.g. broadcasting), and a step toward building a region-value based ndarray. I think these are good changes, isolated to LocalMatrix, so my preference is to merge them in now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3071
https://github.com/hail-is/hail/pull/3071:362,Deployability,update,update,362,"I made these updates to Scala LocalMatrix as I was building the Python interface, to more closely mirror NumPy functions and name the symbolic operators. I no longer intend to expose LocalMatrix in Python in its current form, but rather to localize BlockMatrix ""directly"" to NumPy and vice versa. Still, LocalMatrix in Scala is a useful local model for how I'll update BlockMatrix to be more NumPy like (e.g. broadcasting), and a step toward building a region-value based ndarray. I think these are good changes, isolated to LocalMatrix, so my preference is to merge them in now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3071
https://github.com/hail-is/hail/pull/3071:71,Integrability,interface,interface,71,"I made these updates to Scala LocalMatrix as I was building the Python interface, to more closely mirror NumPy functions and name the symbolic operators. I no longer intend to expose LocalMatrix in Python in its current form, but rather to localize BlockMatrix ""directly"" to NumPy and vice versa. Still, LocalMatrix in Scala is a useful local model for how I'll update BlockMatrix to be more NumPy like (e.g. broadcasting), and a step toward building a region-value based ndarray. I think these are good changes, isolated to LocalMatrix, so my preference is to merge them in now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3071
https://github.com/hail-is/hail/pull/3071:176,Security,expose,expose,176,"I made these updates to Scala LocalMatrix as I was building the Python interface, to more closely mirror NumPy functions and name the symbolic operators. I no longer intend to expose LocalMatrix in Python in its current form, but rather to localize BlockMatrix ""directly"" to NumPy and vice versa. Still, LocalMatrix in Scala is a useful local model for how I'll update BlockMatrix to be more NumPy like (e.g. broadcasting), and a step toward building a region-value based ndarray. I think these are good changes, isolated to LocalMatrix, so my preference is to merge them in now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3071
https://github.com/hail-is/hail/pull/3072:438,Integrability,interface,interface,438,"- improved and fleshed out documentation of current BlockMatrix python functionality. Note`from_matrix_table` renamed to `from_entry_expr`, `from_numpy_matrix(numpy_matrix, ...)` renamed to `from_numpy(ndarray, ...)`, and similarly for `to_numpy_matrix`.; - renamed `matrix.py` as `blockmatrix.py`; - renamed `toLocalMatrix` to the more specific `toBreezeMatrix` on BlockMatrix and RowMatrix. This is prep for filling out the BlockMatrix interface w/ NumPy broadcast rules (model is LocalMatrix) and speeding up `to_numpy` and `from_numpy` (for now, by passing bytes via temp files rather than trough py4j) so that NumPy ndarrays serve as local matrix on Python side and interact predictably with BlockMatrices.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3072
https://github.com/hail-is/hail/pull/3072:680,Safety,predict,predictably,680,"- improved and fleshed out documentation of current BlockMatrix python functionality. Note`from_matrix_table` renamed to `from_entry_expr`, `from_numpy_matrix(numpy_matrix, ...)` renamed to `from_numpy(ndarray, ...)`, and similarly for `to_numpy_matrix`.; - renamed `matrix.py` as `blockmatrix.py`; - renamed `toLocalMatrix` to the more specific `toBreezeMatrix` on BlockMatrix and RowMatrix. This is prep for filling out the BlockMatrix interface w/ NumPy broadcast rules (model is LocalMatrix) and speeding up `to_numpy` and `from_numpy` (for now, by passing bytes via temp files rather than trough py4j) so that NumPy ndarrays serve as local matrix on Python side and interact predictably with BlockMatrices.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3072
https://github.com/hail-is/hail/issues/3074:1265,Availability,error,error,1265,"t(mt); ped = hl.Pedigree.read(fam_file); tdt_table = hl.tdt(mt, ped); _, _, per_sample, per_variant = hl.mendel_errors(mt, ped); family_stats = Struct(mendel=per_variant[(mt.locus, mt.alleles)],; tdt=tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'raw'}); mt = mt.annotate_rows(family_stats=[family_stats]). mt = filter_to_adj(mt); adj_tdt_table = hl.tdt(mt, ped); _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(mt, ped); family_stats_adj = Struct(mendel=adj_per_variant[(mt.locus, mt.alleles)],; tdt=adj_tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'adj'}). per_sample = per_sample.annotate(adj=adj_per_sample[per_sample.s]). mt = mt.annotate_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:4965,Energy Efficiency,schedul,scheduler,4965,; 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:5037,Energy Efficiency,schedul,scheduler,5037,; 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:1271,Integrability,message,messages,1271,"t(mt); ped = hl.Pedigree.read(fam_file); tdt_table = hl.tdt(mt, ped); _, _, per_sample, per_variant = hl.mendel_errors(mt, ped); family_stats = Struct(mendel=per_variant[(mt.locus, mt.alleles)],; tdt=tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'raw'}); mt = mt.annotate_rows(family_stats=[family_stats]). mt = filter_to_adj(mt); adj_tdt_table = hl.tdt(mt, ped); _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(mt, ped); family_stats_adj = Struct(mendel=adj_per_variant[(mt.locus, mt.alleles)],; tdt=adj_tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'adj'}). per_sample = per_sample.annotate(adj=adj_per_sample[per_sample.s]). mt = mt.annotate_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTabl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:5162,Performance,concurren,concurrent,5162,; 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:5247,Performance,concurren,concurrent,5247,; 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.hasNext(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:491); 	at is.hail.rvd.OrderedRVD$$anonfun$11.apply(OrderedRVD.scala:490); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:1919,Safety,unsafe,unsafeInsert,1919,"(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'adj'}). per_sample = per_sample.annotate(adj=adj_per_sample[per_sample.s]). mt = mt.annotate_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1722); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1718); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rv",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:2000,Safety,unsafe,unsafeInsert,2000,"_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'adj'}). per_sample = per_sample.annotate(adj=adj_per_sample[per_sample.s]). mt = mt.annotate_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1722); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1718); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:2081,Safety,unsafe,unsafeInsert,2081,"_sample = per_sample.annotate(adj=adj_per_sample[per_sample.s]). mt = mt.annotate_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1722); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1718); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collec",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:2162,Safety,unsafe,unsafeInsert,2162,"_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1722); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1718); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:736); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$6$$anon$5.next(OrderedRVD.scala:730); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462); 	at is.hail.rvd.OrderedRVD$$a",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:1340,Testability,Assert,AssertionError,1340,"mple, per_variant = hl.mendel_errors(mt, ped); family_stats = Struct(mendel=per_variant[(mt.locus, mt.alleles)],; tdt=tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'raw'}); mt = mt.annotate_rows(family_stats=[family_stats]). mt = filter_to_adj(mt); adj_tdt_table = hl.tdt(mt, ped); _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(mt, ped); family_stats_adj = Struct(mendel=adj_per_variant[(mt.locus, mt.alleles)],; tdt=adj_tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'adj'}). per_sample = per_sample.annotate(adj=adj_per_sample[per_sample.s]). mt = mt.annotate_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1722); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.appl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:1356,Testability,assert,assertion,1356,"mple, per_variant = hl.mendel_errors(mt, ped); family_stats = Struct(mendel=per_variant[(mt.locus, mt.alleles)],; tdt=tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'raw'}); mt = mt.annotate_rows(family_stats=[family_stats]). mt = filter_to_adj(mt); adj_tdt_table = hl.tdt(mt, ped); _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(mt, ped); family_stats_adj = Struct(mendel=adj_per_variant[(mt.locus, mt.alleles)],; tdt=adj_tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'adj'}). per_sample = per_sample.annotate(adj=adj_per_sample[per_sample.s]). mt = mt.annotate_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1722); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.appl",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/issues/3074:1392,Testability,assert,assert,1392,"mt, ped); family_stats = Struct(mendel=per_variant[(mt.locus, mt.alleles)],; tdt=tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'raw'}); mt = mt.annotate_rows(family_stats=[family_stats]). mt = filter_to_adj(mt); adj_tdt_table = hl.tdt(mt, ped); _, _, adj_per_sample, adj_per_variant = hl.mendel_errors(mt, ped); family_stats_adj = Struct(mendel=adj_per_variant[(mt.locus, mt.alleles)],; tdt=adj_tdt_table[(mt.locus, mt.alleles)],; ac_unrelated_qc=hl.agg.sum(hl.agg.filter(; True & hl.is_missing(mt.fam.pat_id),; mt.GT.num_alt_alleles())),; meta={'group': 'adj'}). per_sample = per_sample.annotate(adj=adj_per_sample[per_sample.s]). mt = mt.annotate_rows(family_stats=mt.family_stats.append(family_stats_adj)); mt.write(); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:394); 	at is.hail.annotations.RegionValueBuilder.addField(RegionValueBuilder.scala:335); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1433); 	at is.hail.variant.MatrixTable$$anonfun$dropEntries$2.apply(MatrixTable.scala:1421); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60$$anonfun$apply$4.apply$mcV$sp(MatrixTable.scala:1723); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$1.apply(TStruct.scala:177); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:197); 	at is.hail.expr.types.TStruct$$anonfun$unsafeInsert$2.apply(TStruct.scala:186); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1722); 	at is.hail.variant.MatrixTable$$anonfun$95$$anonfun$apply$60.apply(MatrixTable.scala:1718); 	at scala.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3074
https://github.com/hail-is/hail/pull/3086:65,Testability,test,tests,65,Meredith needs the former and I need the latter. I'll add Python tests of all block matrix functionality in subsequent broadcasting PR.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3086
https://github.com/hail-is/hail/pull/3087:13,Testability,test,test,13,The modified test catches it. I'll be updating the test further tomorrow as I make changes to BlockMatrix.from_expr and MatrixTable.writeBlockMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3087
https://github.com/hail-is/hail/pull/3087:51,Testability,test,test,51,The modified test catches it. I'll be updating the test further tomorrow as I make changes to BlockMatrix.from_expr and MatrixTable.writeBlockMatrix,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3087
https://github.com/hail-is/hail/pull/3094:369,Performance,optimiz,optimization,369,"I'm keeping my LD extension branch separate until we add a proper sparse block matrix implementation, but I pulled out these functions on GridPartitioner since (i) they're some of the logic we'll need to make sparse block matrix useful and (ii) Meredith just built a step to compute variant windows in LDPrune, which can then be combined with this logic as part of her optimization strategy to not compute unneeded blocks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3094
https://github.com/hail-is/hail/pull/3094:184,Testability,log,logic,184,"I'm keeping my LD extension branch separate until we add a proper sparse block matrix implementation, but I pulled out these functions on GridPartitioner since (i) they're some of the logic we'll need to make sparse block matrix useful and (ii) Meredith just built a step to compute variant windows in LDPrune, which can then be combined with this logic as part of her optimization strategy to not compute unneeded blocks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3094
https://github.com/hail-is/hail/pull/3094:348,Testability,log,logic,348,"I'm keeping my LD extension branch separate until we add a proper sparse block matrix implementation, but I pulled out these functions on GridPartitioner since (i) they're some of the logic we'll need to make sparse block matrix useful and (ii) Meredith just built a step to compute variant windows in LDPrune, which can then be combined with this logic as part of her optimization strategy to not compute unneeded blocks.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3094
https://github.com/hail-is/hail/pull/3098:42,Modifiability,variab,variable,42,cleaned/killed a bad import and an unused variable,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3098
https://github.com/hail-is/hail/issues/3099:911,Availability,error,error,911,"Posting with recommendation from @konradjk ; ### Hail version:; devel-2c596b7. ### What you did:; Creating a ClinVar matrixtable from a tsv and vep'ing. ```; import hail as hl; import hail.expr.aggregators as agg; from gnomad_hail import *; hl.init(). clinvar_ht= hl.import_table(""gs://gnomad-resources/clinvar/source/clinvar_alleles.single.b37.tsv.gz"", impute=True, missing='NA'); clinvar_ht = clinvar_ht.annotate(locus = hl.locus(clinvar_ht.chrom, clinvar_ht.pos),; alleles = hl.array({clinvar_ht.ref, clinvar_ht.alt})). clinvar_ht = clinvar_ht.key_by(clinvar_ht.locus, clinvar_ht.alleles); clinvar_mt = hl.MatrixTable.from_rows_table(clinvar_ht); clinvar_mt = split_multi_dynamic(clinvar_mt, left_aligned = False); clinvar_mt = clinvar_mt.repartition(100); clinvar_mt = clinvar_mt.key_rows_by(clinvar_mt.locus, clinvar_mt.alleles); clinvar_vep = hl.vep(clinvar_mt, vep_config); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; In [6]: clinvar_vep = hl.vep(clinvar_mt, vep_config); 2018-03-08 02:46:03 Hail: WARN: property `hail.vep.assembly' not specified. Setting to GRCh37; [Stage 22:======================================================>(99 + 1) / 100]2018-03-08 02:54:37 Hail: INFO: vep: annotated 243477 variants; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-6-a229f1f9de81> in <module>(); ----> 1 clinvar_vep = hl.vep(clinvar_mt, vep_config). <decorator-gen-843> in vep(dataset, config, block_size, name, csq). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 491 def _typecheck(__orig_func__, *args, **kwargs):; 492 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:2703,Availability,error,error,2703,"_typecheck(__orig_func__, *args, **kwargs):; 492 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:429); 	at is.hail.methods.VEP$.apply(VEP.scala:434); 	at is.hail.methods.VEP.apply(VEP.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractComma",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:3916,Availability,Error,Error,3916,"ail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:429); 	at is.hail.methods.VEP$.apply(VEP.scala:434); 	at is.hail.methods.VEP.apply(VEP.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-2c596b7; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:917,Integrability,message,messages,917,"Posting with recommendation from @konradjk ; ### Hail version:; devel-2c596b7. ### What you did:; Creating a ClinVar matrixtable from a tsv and vep'ing. ```; import hail as hl; import hail.expr.aggregators as agg; from gnomad_hail import *; hl.init(). clinvar_ht= hl.import_table(""gs://gnomad-resources/clinvar/source/clinvar_alleles.single.b37.tsv.gz"", impute=True, missing='NA'); clinvar_ht = clinvar_ht.annotate(locus = hl.locus(clinvar_ht.chrom, clinvar_ht.pos),; alleles = hl.array({clinvar_ht.ref, clinvar_ht.alt})). clinvar_ht = clinvar_ht.key_by(clinvar_ht.locus, clinvar_ht.alleles); clinvar_mt = hl.MatrixTable.from_rows_table(clinvar_ht); clinvar_mt = split_multi_dynamic(clinvar_mt, left_aligned = False); clinvar_mt = clinvar_mt.repartition(100); clinvar_mt = clinvar_mt.key_rows_by(clinvar_mt.locus, clinvar_mt.alleles); clinvar_vep = hl.vep(clinvar_mt, vep_config); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; In [6]: clinvar_vep = hl.vep(clinvar_mt, vep_config); 2018-03-08 02:46:03 Hail: WARN: property `hail.vep.assembly' not specified. Setting to GRCh37; [Stage 22:======================================================>(99 + 1) / 100]2018-03-08 02:54:37 Hail: INFO: vep: annotated 243477 variants; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-6-a229f1f9de81> in <module>(); ----> 1 clinvar_vep = hl.vep(clinvar_mt, vep_config). <decorator-gen-843> in vep(dataset, config, block_size, name, csq). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 491 def _typecheck(__orig_func__, *args, **kwargs):; 492 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/h",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:1528,Modifiability,config,config,1528,"invar_ht = clinvar_ht.key_by(clinvar_ht.locus, clinvar_ht.alleles); clinvar_mt = hl.MatrixTable.from_rows_table(clinvar_ht); clinvar_mt = split_multi_dynamic(clinvar_mt, left_aligned = False); clinvar_mt = clinvar_mt.repartition(100); clinvar_mt = clinvar_mt.key_rows_by(clinvar_mt.locus, clinvar_mt.alleles); clinvar_vep = hl.vep(clinvar_mt, vep_config); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; In [6]: clinvar_vep = hl.vep(clinvar_mt, vep_config); 2018-03-08 02:46:03 Hail: WARN: property `hail.vep.assembly' not specified. Setting to GRCh37; [Stage 22:======================================================>(99 + 1) / 100]2018-03-08 02:54:37 Hail: INFO: vep: annotated 243477 variants; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-6-a229f1f9de81> in <module>(); ----> 1 clinvar_vep = hl.vep(clinvar_mt, vep_config). <decorator-gen-843> in vep(dataset, config, block_size, name, csq). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 491 def _typecheck(__orig_func__, *args, **kwargs):; 492 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_ar",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:2035,Modifiability,config,config,2035,"ssembly' not specified. Setting to GRCh37; [Stage 22:======================================================>(99 + 1) / 100]2018-03-08 02:54:37 Hail: INFO: vep: annotated 243477 variants; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-6-a229f1f9de81> in <module>(); ----> 1 clinvar_vep = hl.vep(clinvar_mt, vep_config). <decorator-gen-843> in vep(dataset, config, block_size, name, csq). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 491 def _typecheck(__orig_func__, *args, **kwargs):; 492 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:4",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:2188,Modifiability,config,config,2188,"(99 + 1) / 100]2018-03-08 02:54:37 Hail: INFO: vep: annotated 243477 variants; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-6-a229f1f9de81> in <module>(); ----> 1 clinvar_vep = hl.vep(clinvar_mt, vep_config). <decorator-gen-843> in vep(dataset, config, block_size, name, csq). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/typecheck/check.py in _typecheck(__orig_func__, *args, **kwargs); 491 def _typecheck(__orig_func__, *args, **kwargs):; 492 args_, kwargs_ = check_all(__orig_func__, args, kwargs, checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:429); 	at is.hail.methods.VEP$.apply(VEP.scala:434); 	at is.hail.methods.VEP.apply(VEP.scala); 	at sun.reflect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:2804,Testability,Assert,AssertionError,2804,"checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:429); 	at is.hail.methods.VEP$.apply(VEP.scala:434); 	at is.hail.methods.VEP.apply(VEP.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:2820,Testability,assert,assertion,2820,"checkers, is_method=False); --> 493 return __orig_func__(*args_, **kwargs_); 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:429); 	at is.hail.methods.VEP$.apply(VEP.scala:434); 	at is.hail.methods.VEP.apply(VEP.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:2867,Testability,Assert,AssertionError,2867,"; 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:429); 	at is.hail.methods.VEP$.apply(VEP.scala:434); 	at is.hail.methods.VEP.apply(VEP.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:2883,Testability,assert,assertion,2883,"; 494; 495 return decorator(_typecheck). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:429); 	at is.hail.methods.VEP$.apply(VEP.scala:434); 	at is.hail.methods.VEP.apply(VEP.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:2919,Testability,assert,assert,2919,"k). /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:429); 	at is.hail.methods.VEP$.apply(VEP.scala:434); 	at is.hail.methods.VEP.apply(VEP.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-2c596b7; Error summ",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:3931,Testability,Assert,AssertionError,3931,"ail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:429); 	at is.hail.methods.VEP$.apply(VEP.scala:434); 	at is.hail.methods.VEP.apply(VEP.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-2c596b7; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3099:3947,Testability,assert,assertion,3947,"ail/hail-devel-0c961806173f.zip/hail/methods/qc.py in vep(dataset, config, block_size, name, csq); 545; 546 require_row_key_variant(dataset, 'vep'); --> 547 return MatrixTable(Env.hail().methods.VEP.apply(dataset._jvds, config, 'va.`{}`'.format(name), csq, block_size)); 548; 549. /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /hadoop_gcs_connector_metadata_cache/hail/hail-devel-0c961806173f.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.methods.VEP$.annotate(VEP.scala:429); 	at is.hail.methods.VEP$.apply(VEP.scala:434); 	at is.hail.methods.VEP.apply(VEP.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-2c596b7; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3099
https://github.com/hail-is/hail/issues/3102:1784,Availability,error,error,1784,"formation below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: master build . ### What you did:; Ran python script ; [root@]cat hail/Hail_Tutorial.py; **from hail import * ; hc = HailContext()** ; import numpy as np ; import pandas as pd ; import matplotlib.pyplot as plt ; import matplotlib.patches as mpatches ; from collections import Counter ; from math import log, isnan ; from pprint import pprint ; import seaborn ; vds = hc.read('/gpfs/hail/test_hail_qnsnodes_2500c_spark2_20170515.vds') ; vds.summarize().report() ; vds.query_variants('variants.take(5)'); vds.query_samples('samples.take(5)'); vds.sample_ids[:5]; vds.query_genotypes('gs.take(5)'); #%%sh; #head data/1kg_annotations.txt | column -t; table = hc.import_table('data/1kg_annotations.txt', impute=True)\; .key_by('Sample'); print(table.schema); pprint(table.schema); table.to_dataframe().show(10); vds = vds.annotate_samples_table(table, root='sa'); pprint(vds.sample_schema); pprint(table.query('SuperPopulation.counter()')); pprint(table.query('CaffeineConsumption.stats()')); table.count(); vds.num_samples; vds.query_samples('samples.map(s => sa.SuperPopulation).counter()'); table.count(); vds.num_samples. #common_vds = (vds; # .filter_variants_expr('va.qc.AF > 0.01'); # .ld_prune(memory_per_core=256, num_cores=4)); pca = vds.pca('sa.pca', k=5, eigenvalues='global.eigen'); pprint(pca.globals). #vds = vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); #print('After filter, %d/1000 samples remain.' % vds.num_samples). df = vds.samples_table().to_pandas(); df.head(). ### What went wrong (all error messages here, including the full java stack trace):. **File ""/hail/Hail_Tutorial.py"", line 2, in <module>; hc = HailContext() ; NameError: name 'HailContext' is not defined; + /software/spark/spark-2.2.0-bin-hadoop2.7//sbin/stop-master.sh**",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3102
https://github.com/hail-is/hail/issues/3102:482,Deployability,patch,patches,482,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: master build . ### What you did:; Ran python script ; [root@]cat hail/Hail_Tutorial.py; **from hail import * ; hc = HailContext()** ; import numpy as np ; import pandas as pd ; import matplotlib.pyplot as plt ; import matplotlib.patches as mpatches ; from collections import Counter ; from math import log, isnan ; from pprint import pprint ; import seaborn ; vds = hc.read('/gpfs/hail/test_hail_qnsnodes_2500c_spark2_20170515.vds') ; vds.summarize().report() ; vds.query_variants('variants.take(5)'); vds.query_samples('samples.take(5)'); vds.sample_ids[:5]; vds.query_genotypes('gs.take(5)'); #%%sh; #head data/1kg_annotations.txt | column -t; table = hc.import_table('data/1kg_annotations.txt', impute=True)\; .key_by('Sample'); print(table.schema); pprint(table.schema); table.to_dataframe().show(10); vds = vds.annotate_samples_table(table, root='sa'); pprint(vds.sample_schema); pprint(table.query('SuperPopulation.counter()')); pprint(table.query('CaffeineConsumption.stats()')); table.count(); vds.num_samples; vds.query_samples('samples.map(s => sa.SuperPopulation).counter()'); table.count(); vds.num_samples. #common_vds = (vds; # .filter_variants_expr('va.qc.AF > 0.01'); # .ld_prune(memory_per_core=256, num_cores=4)); pca = vds.pca('sa.pca', k=5, eigenvalues='global.eigen'); pprint(pca.globals). #vds = vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); #print('After filter, %d/1000 samples remain.' % vds.num_samples). df = vds.samples_table().to_pandas(); df.head(). ### What went wrong (all error messages here, including the full java stack trace):. **File ""/hail/Hail_Tutorial.py"", line 2, in <module>; hc = HailContext() ; NameError: name 'HailContext' is not defined; + /software/spark/spark-2.2.0-bin-ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3102
https://github.com/hail-is/hail/issues/3102:1790,Integrability,message,messages,1790,"formation below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: master build . ### What you did:; Ran python script ; [root@]cat hail/Hail_Tutorial.py; **from hail import * ; hc = HailContext()** ; import numpy as np ; import pandas as pd ; import matplotlib.pyplot as plt ; import matplotlib.patches as mpatches ; from collections import Counter ; from math import log, isnan ; from pprint import pprint ; import seaborn ; vds = hc.read('/gpfs/hail/test_hail_qnsnodes_2500c_spark2_20170515.vds') ; vds.summarize().report() ; vds.query_variants('variants.take(5)'); vds.query_samples('samples.take(5)'); vds.sample_ids[:5]; vds.query_genotypes('gs.take(5)'); #%%sh; #head data/1kg_annotations.txt | column -t; table = hc.import_table('data/1kg_annotations.txt', impute=True)\; .key_by('Sample'); print(table.schema); pprint(table.schema); table.to_dataframe().show(10); vds = vds.annotate_samples_table(table, root='sa'); pprint(vds.sample_schema); pprint(table.query('SuperPopulation.counter()')); pprint(table.query('CaffeineConsumption.stats()')); table.count(); vds.num_samples; vds.query_samples('samples.map(s => sa.SuperPopulation).counter()'); table.count(); vds.num_samples. #common_vds = (vds; # .filter_variants_expr('va.qc.AF > 0.01'); # .ld_prune(memory_per_core=256, num_cores=4)); pca = vds.pca('sa.pca', k=5, eigenvalues='global.eigen'); pprint(pca.globals). #vds = vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); #print('After filter, %d/1000 samples remain.' % vds.num_samples). df = vds.samples_table().to_pandas(); df.head(). ### What went wrong (all error messages here, including the full java stack trace):. **File ""/hail/Hail_Tutorial.py"", line 2, in <module>; hc = HailContext() ; NameError: name 'HailContext' is not defined; + /software/spark/spark-2.2.0-bin-hadoop2.7//sbin/stop-master.sh**",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3102
https://github.com/hail-is/hail/issues/3102:555,Testability,log,log,555,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: master build . ### What you did:; Ran python script ; [root@]cat hail/Hail_Tutorial.py; **from hail import * ; hc = HailContext()** ; import numpy as np ; import pandas as pd ; import matplotlib.pyplot as plt ; import matplotlib.patches as mpatches ; from collections import Counter ; from math import log, isnan ; from pprint import pprint ; import seaborn ; vds = hc.read('/gpfs/hail/test_hail_qnsnodes_2500c_spark2_20170515.vds') ; vds.summarize().report() ; vds.query_variants('variants.take(5)'); vds.query_samples('samples.take(5)'); vds.sample_ids[:5]; vds.query_genotypes('gs.take(5)'); #%%sh; #head data/1kg_annotations.txt | column -t; table = hc.import_table('data/1kg_annotations.txt', impute=True)\; .key_by('Sample'); print(table.schema); pprint(table.schema); table.to_dataframe().show(10); vds = vds.annotate_samples_table(table, root='sa'); pprint(vds.sample_schema); pprint(table.query('SuperPopulation.counter()')); pprint(table.query('CaffeineConsumption.stats()')); table.count(); vds.num_samples; vds.query_samples('samples.map(s => sa.SuperPopulation).counter()'); table.count(); vds.num_samples. #common_vds = (vds; # .filter_variants_expr('va.qc.AF > 0.01'); # .ld_prune(memory_per_core=256, num_cores=4)); pca = vds.pca('sa.pca', k=5, eigenvalues='global.eigen'); pprint(pca.globals). #vds = vds.filter_samples_expr('sa.qc.dpMean >= 4 && sa.qc.callRate >= 0.97'); #print('After filter, %d/1000 samples remain.' % vds.num_samples). df = vds.samples_table().to_pandas(); df.head(). ### What went wrong (all error messages here, including the full java stack trace):. **File ""/hail/Hail_Tutorial.py"", line 2, in <module>; hc = HailContext() ; NameError: name 'HailContext' is not defined; + /software/spark/spark-2.2.0-bin-ha",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3102
https://github.com/hail-is/hail/issues/3103:438,Availability,error,error,438,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; `devel-3b014af` (off of `0c96180`); ### What you did:; ```; mt = mt.select_entries(GT=hl.cond(hl.is_defined(mt.GT), hl.struct(), hl.null(hl.tstruct()))); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; TypeError: 'cond' requires the 'consequent' and 'alternate' arguments to have the same type; consequent: type struct{}; alternate: type struct{}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3103
https://github.com/hail-is/hail/issues/3103:444,Integrability,message,messages,444,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; `devel-3b014af` (off of `0c96180`); ### What you did:; ```; mt = mt.select_entries(GT=hl.cond(hl.is_defined(mt.GT), hl.struct(), hl.null(hl.tstruct()))); ```; ### What went wrong (all error messages here, including the full java stack trace):; ```; TypeError: 'cond' requires the 'consequent' and 'alternate' arguments to have the same type; consequent: type struct{}; alternate: type struct{}; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3103
https://github.com/hail-is/hail/pull/3110:174,Modifiability,inherit,inherit,174,"This builds on: https://github.com/hail-is/hail/pull/3107. We were getting absolutely murdered by serializing the partitioner on every task. I made OrderedRVDPartitioner not inherit from Spark Partitioner, non-serializable, and modified it to be broadcasted where needed. There is a good chance this will speed up OrderedRVD shuffles as well, but I haven't timed. Timing:. ```; import hail as hl. mt = hl.read_matrix_table('gnomad.exomes.vds', _drop_cols=True); mt = mt._filter_partitions(list(range(1000))); mt._force_count_rows(); ```. master: 1m15s; no_ser_ord_part: 9s. That's about 8x improvement. This includes startup time so it's actually much better.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3110
https://github.com/hail-is/hail/pull/3114:304,Integrability,rout,routing,304,"An overhaul of our NumPy ndarray and BlockMatrix conversion, proceeding through binary on disk compatibly with the NumPy `tofile` and `fromfile` functions. For expediency, I still use Breeze matrix as an (internal) intermediate, resulting in an extra copy and max 2^31 size. Longer term, we should avoid routing through Breeze, add compression, etc, but this gets us a lot of bang for the buck in the face of intolerable py4j slowness. In fact, on laptop, the local byte transport piece is about 50x faster through disk than the ""faster"" direction of py4j (java to python via byte array), so now 90% of the time is spent localizing and distributing serialized blocks. E.g. reading a 8192 x 8192 block matrix with 4 blocks and converting to NumPy takes about 12s. Converting the local matrix to NumPy takes a bit over 1s. The current ""slower"" py4j direction (python to java) falls over even on tiny matrices. Main changes:; - added `tofile, to_numpy, fromfile, from_numpy` to BlockMatrix, which are explained in the docs with examples and tested in `test_linalg`.; - deleted all breeze related functions in utils.java; - added a `StreamRawBlockBufferSpec` (better name?) which behaves like the `StreamBlockBufferSpec` except that it only writes the (raw) blocks without adding length data to the stream. This allows for re-using readDoubles and writeDoubles as implemented in BlockingBuffer.; - added readDoubles and writeDoubles using this buffer spec on RichDenseMatrixDouble, and a test.; - along the way, added a `hl.tmp_dir()` function to allow users to inspect the tmp_dir used with `BlockMatrix.from_entry_expr`, as noted in that documentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3114
https://github.com/hail-is/hail/pull/3114:298,Safety,avoid,avoid,298,"An overhaul of our NumPy ndarray and BlockMatrix conversion, proceeding through binary on disk compatibly with the NumPy `tofile` and `fromfile` functions. For expediency, I still use Breeze matrix as an (internal) intermediate, resulting in an extra copy and max 2^31 size. Longer term, we should avoid routing through Breeze, add compression, etc, but this gets us a lot of bang for the buck in the face of intolerable py4j slowness. In fact, on laptop, the local byte transport piece is about 50x faster through disk than the ""faster"" direction of py4j (java to python via byte array), so now 90% of the time is spent localizing and distributing serialized blocks. E.g. reading a 8192 x 8192 block matrix with 4 blocks and converting to NumPy takes about 12s. Converting the local matrix to NumPy takes a bit over 1s. The current ""slower"" py4j direction (python to java) falls over even on tiny matrices. Main changes:; - added `tofile, to_numpy, fromfile, from_numpy` to BlockMatrix, which are explained in the docs with examples and tested in `test_linalg`.; - deleted all breeze related functions in utils.java; - added a `StreamRawBlockBufferSpec` (better name?) which behaves like the `StreamBlockBufferSpec` except that it only writes the (raw) blocks without adding length data to the stream. This allows for re-using readDoubles and writeDoubles as implemented in BlockingBuffer.; - added readDoubles and writeDoubles using this buffer spec on RichDenseMatrixDouble, and a test.; - along the way, added a `hl.tmp_dir()` function to allow users to inspect the tmp_dir used with `BlockMatrix.from_entry_expr`, as noted in that documentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3114
https://github.com/hail-is/hail/pull/3114:1038,Testability,test,tested,1038,"An overhaul of our NumPy ndarray and BlockMatrix conversion, proceeding through binary on disk compatibly with the NumPy `tofile` and `fromfile` functions. For expediency, I still use Breeze matrix as an (internal) intermediate, resulting in an extra copy and max 2^31 size. Longer term, we should avoid routing through Breeze, add compression, etc, but this gets us a lot of bang for the buck in the face of intolerable py4j slowness. In fact, on laptop, the local byte transport piece is about 50x faster through disk than the ""faster"" direction of py4j (java to python via byte array), so now 90% of the time is spent localizing and distributing serialized blocks. E.g. reading a 8192 x 8192 block matrix with 4 blocks and converting to NumPy takes about 12s. Converting the local matrix to NumPy takes a bit over 1s. The current ""slower"" py4j direction (python to java) falls over even on tiny matrices. Main changes:; - added `tofile, to_numpy, fromfile, from_numpy` to BlockMatrix, which are explained in the docs with examples and tested in `test_linalg`.; - deleted all breeze related functions in utils.java; - added a `StreamRawBlockBufferSpec` (better name?) which behaves like the `StreamBlockBufferSpec` except that it only writes the (raw) blocks without adding length data to the stream. This allows for re-using readDoubles and writeDoubles as implemented in BlockingBuffer.; - added readDoubles and writeDoubles using this buffer spec on RichDenseMatrixDouble, and a test.; - along the way, added a `hl.tmp_dir()` function to allow users to inspect the tmp_dir used with `BlockMatrix.from_entry_expr`, as noted in that documentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3114
https://github.com/hail-is/hail/pull/3114:1484,Testability,test,test,1484,"An overhaul of our NumPy ndarray and BlockMatrix conversion, proceeding through binary on disk compatibly with the NumPy `tofile` and `fromfile` functions. For expediency, I still use Breeze matrix as an (internal) intermediate, resulting in an extra copy and max 2^31 size. Longer term, we should avoid routing through Breeze, add compression, etc, but this gets us a lot of bang for the buck in the face of intolerable py4j slowness. In fact, on laptop, the local byte transport piece is about 50x faster through disk than the ""faster"" direction of py4j (java to python via byte array), so now 90% of the time is spent localizing and distributing serialized blocks. E.g. reading a 8192 x 8192 block matrix with 4 blocks and converting to NumPy takes about 12s. Converting the local matrix to NumPy takes a bit over 1s. The current ""slower"" py4j direction (python to java) falls over even on tiny matrices. Main changes:; - added `tofile, to_numpy, fromfile, from_numpy` to BlockMatrix, which are explained in the docs with examples and tested in `test_linalg`.; - deleted all breeze related functions in utils.java; - added a `StreamRawBlockBufferSpec` (better name?) which behaves like the `StreamBlockBufferSpec` except that it only writes the (raw) blocks without adding length data to the stream. This allows for re-using readDoubles and writeDoubles as implemented in BlockingBuffer.; - added readDoubles and writeDoubles using this buffer spec on RichDenseMatrixDouble, and a test.; - along the way, added a `hl.tmp_dir()` function to allow users to inspect the tmp_dir used with `BlockMatrix.from_entry_expr`, as noted in that documentation.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3114
https://github.com/hail-is/hail/pull/3118:52,Testability,test,tests,52,"@cseed I still need to clean up this and write more tests, but here's more-or-less what I've got in the meantime if you wanted to take a look. Builds on #3117.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3118
https://github.com/hail-is/hail/issues/3119:671,Availability,error,error,671,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did:; ```; vp_ht = vp_ht.transmute(; locus1=hl.locus(vp_ht.chrom1, vp_ht.pos1),; locus2=hl.locus(vp_ht.chrom1, vp_ht.pos2),; alleles1=[vp_ht.ref1, vp_ht.alt1],; alleles2=[vp_ht.ref2, vp_ht.alt2]; ). vp_mt = hl.MatrixTable.from_rows_table(vp_ht); vp_mt = vp_mt.key_rows_by('locus1', 'alleles1'). mt = mt.annotate_rows(v1=hl.is_defined(vp_mt[(mt.locus, mt.alleles), :])); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; In [49]: mt = mt.annotate_rows(v1=hl.is_defined(vp_mt[(mt.locus, mt.alleles),:])); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-49-5ea2fe942ada> in <module>(); ----> 1 mt = mt.annotate_rows(v1=hl.is_defined(vp_mt[(mt.locus, mt.alleles),:])). /home/hail/hail.zip/hail/matrixtable.py in annotate_rows(self, **named_exprs); 894 exprs = []; 895 named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; --> 896 base, cleanup = self._process_joins(*named_exprs.values()); 897; 898 for k, v in named_exprs.items():. /home/hail/hail.zip/hail/matrixtable.py in _process_joins(self, *exprs); 2205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/issues/3119:2219,Availability,error,error,2219,"ems()}; --> 896 base, cleanup = self._process_joins(*named_exprs.values()); 897; 898 for k, v in named_exprs.items():. /home/hail/hail.zip/hail/matrixtable.py in _process_joins(self, *exprs); 2205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.variant.MatrixTable.annotateRowsVDS(MatrixTable.scala:1449); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.ja",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/issues/3119:3366,Availability,Error,Error,3366,"205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.variant.MatrixTable.annotateRowsVDS(MatrixTable.scala:1449); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-14287a4; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/issues/3119:677,Integrability,message,messages,677,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version: 0.2. ### What you did:; ```; vp_ht = vp_ht.transmute(; locus1=hl.locus(vp_ht.chrom1, vp_ht.pos1),; locus2=hl.locus(vp_ht.chrom1, vp_ht.pos2),; alleles1=[vp_ht.ref1, vp_ht.alt1],; alleles2=[vp_ht.ref2, vp_ht.alt2]; ). vp_mt = hl.MatrixTable.from_rows_table(vp_ht); vp_mt = vp_mt.key_rows_by('locus1', 'alleles1'). mt = mt.annotate_rows(v1=hl.is_defined(vp_mt[(mt.locus, mt.alleles), :])); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; In [49]: mt = mt.annotate_rows(v1=hl.is_defined(vp_mt[(mt.locus, mt.alleles),:])); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-49-5ea2fe942ada> in <module>(); ----> 1 mt = mt.annotate_rows(v1=hl.is_defined(vp_mt[(mt.locus, mt.alleles),:])). /home/hail/hail.zip/hail/matrixtable.py in annotate_rows(self, **named_exprs); 894 exprs = []; 895 named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; --> 896 base, cleanup = self._process_joins(*named_exprs.values()); 897; 898 for k, v in named_exprs.items():. /home/hail/hail.zip/hail/matrixtable.py in _process_joins(self, *exprs); 2205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/issues/3119:1539,Modifiability,extend,extend,1539,"'locus1', 'alleles1'). mt = mt.annotate_rows(v1=hl.is_defined(vp_mt[(mt.locus, mt.alleles), :])); ```. ### What went wrong (all error messages here, including the full java stack trace):; ```; In [49]: mt = mt.annotate_rows(v1=hl.is_defined(vp_mt[(mt.locus, mt.alleles),:])); ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); <ipython-input-49-5ea2fe942ada> in <module>(); ----> 1 mt = mt.annotate_rows(v1=hl.is_defined(vp_mt[(mt.locus, mt.alleles),:])). /home/hail/hail.zip/hail/matrixtable.py in annotate_rows(self, **named_exprs); 894 exprs = []; 895 named_exprs = {k: to_expr(v) for k, v in named_exprs.items()}; --> 896 base, cleanup = self._process_joins(*named_exprs.values()); 897; 898 for k, v in named_exprs.items():. /home/hail/hail.zip/hail/matrixtable.py in _process_joins(self, *exprs); 2205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.sca",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/issues/3119:2320,Testability,Assert,AssertionError,2320,"prs.items():. /home/hail/hail.zip/hail/matrixtable.py in _process_joins(self, *exprs); 2205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.variant.MatrixTable.annotateRowsVDS(MatrixTable.scala:1449); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/issues/3119:2336,Testability,assert,assertion,2336,"prs.items():. /home/hail/hail.zip/hail/matrixtable.py in _process_joins(self, *exprs); 2205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.variant.MatrixTable.annotateRowsVDS(MatrixTable.scala:1449); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/issues/3119:2383,Testability,Assert,AssertionError,2383,"f, *exprs); 2205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.variant.MatrixTable.annotateRowsVDS(MatrixTable.scala:1449); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-14287a4; Error summary: AssertionError: assertio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/issues/3119:2399,Testability,assert,assertion,2399,"f, *exprs); 2205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.variant.MatrixTable.annotateRowsVDS(MatrixTable.scala:1449); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-14287a4; Error summary: AssertionError: assertio",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/issues/3119:2435,Testability,assert,assert,2435,"205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.variant.MatrixTable.annotateRowsVDS(MatrixTable.scala:1449); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-14287a4; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/issues/3119:3381,Testability,Assert,AssertionError,3381,"205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.variant.MatrixTable.annotateRowsVDS(MatrixTable.scala:1449); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-14287a4; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/issues/3119:3397,Testability,assert,assertion,3397,"205 for j in list(e._joins)[::-1]:; 2206 if j.uid not in used_uids:; -> 2207 left = j.join_function(left); 2208 all_uids.extend(j.temp_vars); 2209 used_uids.add(j.uid). /home/hail/hail.zip/hail/matrixtable.py in <lambda>(left); 2157 prefix = 'va'; 2158 joiner = lambda left: (; -> 2159 MatrixTable(left._jvds.annotateRowsVDS(right._jvds, uid))); 2160 else:; 2161 return self.rows().index(*exprs). /usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /home/hail/hail.zip/hail/utils/java.py in deco(*args, **kwargs); 236 # this is a hack to suppress the original error's stack trace; 237 if _exception:; --> 238 raise _exception; 239; 240 return deco. FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.variant.MatrixTable.orderedRVDLeftJoinDistinctAndInsert(MatrixTable.scala:982); 	at is.hail.variant.MatrixTable.annotateRowsVDS(MatrixTable.scala:1449); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-14287a4; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3119
https://github.com/hail-is/hail/pull/3120:132,Usability,simpl,simplify,132,Remove conversions from the function registry. They are now inserted by Python and explicit in the expression language. This should simplify lots of stuff on the back end.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3120
https://github.com/hail-is/hail/pull/3122:13,Testability,test,test,13,Can you also test this? As far as I know you're the only one who knows how.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3122
https://github.com/hail-is/hail/issues/3128:94,Deployability,pipeline,pipeline,94,"It's quite cumbersome to have to check whether everything is defined along the way, e.g. in a pipeline like this:; ```; tcl = tcl.map(lambda tc: tc.annotate(; csq_score=hl.case(); .when((tc.lof == 'HC') & (tc.lof_flags == ''), csq_score(tc) - no_flag_score); .when((tc.lof == 'HC') & (tc.lof_flags != ''), csq_score(tc) - flag_score); .when(tc.lof == 'LC', csq_score(tc) - 10); .when(tc.polyphen_prediction == 'probably_damaging', csq_score(tc) - 0.5); .when(tc.polyphen_prediction == 'possibly_damaging', csq_score(tc) - 0.25); .when(tc.polyphen_prediction == 'benign', csq_score(tc) - 0.1); .default(csq_score(tc)); )); ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3128
https://github.com/hail-is/hail/pull/3135:99,Testability,test,test,99,continue to filter * alleles but remove allele index fixup. Konrad said it was OK. I still need to test by hand.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3135
https://github.com/hail-is/hail/pull/3141:14,Testability,assert,assertCompatibleLocalMatrix,14,"Also changed `assertCompatibleLocalMatrix` to used `isCompact`, which is the intended condition. The current one doesn't rule out extra trailing values in data.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3141
https://github.com/hail-is/hail/pull/3154:31,Testability,test,test,31,"move Table.{exists, forall} to test section in Scala",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3154
https://github.com/hail-is/hail/pull/3158:16,Availability,error,error,16,"Produces better error messages, and allows for deeply nested; conversion if we want to do that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3158
https://github.com/hail-is/hail/pull/3158:22,Integrability,message,messages,22,"Produces better error messages, and allows for deeply nested; conversion if we want to do that.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3158
https://github.com/hail-is/hail/pull/3161:223,Integrability,interface,interface,223,"- TableMapRows IR (takes a Struct IR as an argument); - Removed TableAnnotate; - select can output an IR; - Rewrote annotate and rename in Python to utilize select; - Removed annotate, rename, and drop from the Scala Table interface; - Moved annotate and rename to RichTable for Scala tests; - added some additional python tests",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3161
https://github.com/hail-is/hail/pull/3161:285,Testability,test,tests,285,"- TableMapRows IR (takes a Struct IR as an argument); - Removed TableAnnotate; - select can output an IR; - Rewrote annotate and rename in Python to utilize select; - Removed annotate, rename, and drop from the Scala Table interface; - Moved annotate and rename to RichTable for Scala tests; - added some additional python tests",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3161
https://github.com/hail-is/hail/pull/3161:323,Testability,test,tests,323,"- TableMapRows IR (takes a Struct IR as an argument); - Removed TableAnnotate; - select can output an IR; - Rewrote annotate and rename in Python to utilize select; - Removed annotate, rename, and drop from the Scala Table interface; - Moved annotate and rename to RichTable for Scala tests; - added some additional python tests",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3161
https://github.com/hail-is/hail/pull/3164:43,Testability,test,tests,43,"I should still add a whole bunch of Python tests before this goes in, but everything seems to work so review appreciated.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3164
https://github.com/hail-is/hail/pull/3172:0,Deployability,Update,Update,0,"Update to Hail's latest 0.1 changes, while keeping the changes that were made to the repository for rd-connect.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3172
https://github.com/hail-is/hail/issues/3173:543,Availability,error,error,543,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2. ### What you did:; ```; mt = hl.read_matrix_table('gs://...'); mt = mt.filter_cols(mt.used_in_pca_calculation); print(mt.count_cols()); ```; The MatrixTable has a lot of row and column annotations. `mt.used_in_pca_calculation` is a Boolean column annotation. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/tmp/8d5cc778-fdc7-4210-a60b-5efd1f67c45f/subset_genotype_pca.py"", line 8, in <module>; print(mt.count_cols()); File ""/home/hail/hail.zip/hail/matrixtable.py"", line 1950, in count_cols; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 206, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.MatrixValue.filterSamplesKeep(Relational.scala:110); 	at is.hail.expr.MatrixValue.filterCols(Relational.scala:133); 	at is.hail.expr.FilterCols.execute(Relational.scala:333); 	at is.hail.variant.MatrixTable.value$lzycompute(MatrixTable.scala:536); 	at is.hail.variant.MatrixTable.value(MatrixTable.scala:534); 	at is.hail.variant.MatrixTable.x$16$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.x$16(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.numCols(MatrixTable.scala:2378); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3173
https://github.com/hail-is/hail/issues/3173:2578,Availability,Error,Error,2578," call last):; File ""/tmp/8d5cc778-fdc7-4210-a60b-5efd1f67c45f/subset_genotype_pca.py"", line 8, in <module>; print(mt.count_cols()); File ""/home/hail/hail.zip/hail/matrixtable.py"", line 1950, in count_cols; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 206, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.MatrixValue.filterSamplesKeep(Relational.scala:110); 	at is.hail.expr.MatrixValue.filterCols(Relational.scala:133); 	at is.hail.expr.FilterCols.execute(Relational.scala:333); 	at is.hail.variant.MatrixTable.value$lzycompute(MatrixTable.scala:536); 	at is.hail.variant.MatrixTable.value(MatrixTable.scala:534); 	at is.hail.variant.MatrixTable.x$16$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.x$16(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.numCols(MatrixTable.scala:2378); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-3cf3108; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3173
https://github.com/hail-is/hail/issues/3173:549,Integrability,message,messages,549,"To report a bug, fill in the information below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2. ### What you did:; ```; mt = hl.read_matrix_table('gs://...'); mt = mt.filter_cols(mt.used_in_pca_calculation); print(mt.count_cols()); ```; The MatrixTable has a lot of row and column annotations. `mt.used_in_pca_calculation` is a Boolean column annotation. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/tmp/8d5cc778-fdc7-4210-a60b-5efd1f67c45f/subset_genotype_pca.py"", line 8, in <module>; print(mt.count_cols()); File ""/home/hail/hail.zip/hail/matrixtable.py"", line 1950, in count_cols; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 206, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.MatrixValue.filterSamplesKeep(Relational.scala:110); 	at is.hail.expr.MatrixValue.filterCols(Relational.scala:133); 	at is.hail.expr.FilterCols.execute(Relational.scala:333); 	at is.hail.variant.MatrixTable.value$lzycompute(MatrixTable.scala:536); 	at is.hail.variant.MatrixTable.value(MatrixTable.scala:534); 	at is.hail.variant.MatrixTable.x$16$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.x$16(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.numCols(MatrixTable.scala:2378); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3173
https://github.com/hail-is/hail/issues/3173:1029,Testability,Assert,AssertionError,1029,"n below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2. ### What you did:; ```; mt = hl.read_matrix_table('gs://...'); mt = mt.filter_cols(mt.used_in_pca_calculation); print(mt.count_cols()); ```; The MatrixTable has a lot of row and column annotations. `mt.used_in_pca_calculation` is a Boolean column annotation. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/tmp/8d5cc778-fdc7-4210-a60b-5efd1f67c45f/subset_genotype_pca.py"", line 8, in <module>; print(mt.count_cols()); File ""/home/hail/hail.zip/hail/matrixtable.py"", line 1950, in count_cols; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 206, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.MatrixValue.filterSamplesKeep(Relational.scala:110); 	at is.hail.expr.MatrixValue.filterCols(Relational.scala:133); 	at is.hail.expr.FilterCols.execute(Relational.scala:333); 	at is.hail.variant.MatrixTable.value$lzycompute(MatrixTable.scala:536); 	at is.hail.variant.MatrixTable.value(MatrixTable.scala:534); 	at is.hail.variant.MatrixTable.x$16$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.x$16(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.numCols(MatrixTable.scala:2378); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(De",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3173
https://github.com/hail-is/hail/issues/3173:1045,Testability,assert,assertion,1045,"n below. ; For support and feature requests, please use the discussion forum: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2. ### What you did:; ```; mt = hl.read_matrix_table('gs://...'); mt = mt.filter_cols(mt.used_in_pca_calculation); print(mt.count_cols()); ```; The MatrixTable has a lot of row and column annotations. `mt.used_in_pca_calculation` is a Boolean column annotation. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/tmp/8d5cc778-fdc7-4210-a60b-5efd1f67c45f/subset_genotype_pca.py"", line 8, in <module>; print(mt.count_cols()); File ""/home/hail/hail.zip/hail/matrixtable.py"", line 1950, in count_cols; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 206, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.MatrixValue.filterSamplesKeep(Relational.scala:110); 	at is.hail.expr.MatrixValue.filterCols(Relational.scala:133); 	at is.hail.expr.FilterCols.execute(Relational.scala:333); 	at is.hail.variant.MatrixTable.value$lzycompute(MatrixTable.scala:536); 	at is.hail.variant.MatrixTable.value(MatrixTable.scala:534); 	at is.hail.variant.MatrixTable.x$16$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.x$16(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.numCols(MatrixTable.scala:2378); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(De",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3173
https://github.com/hail-is/hail/issues/3173:1092,Testability,Assert,AssertionError,1092,"m: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2. ### What you did:; ```; mt = hl.read_matrix_table('gs://...'); mt = mt.filter_cols(mt.used_in_pca_calculation); print(mt.count_cols()); ```; The MatrixTable has a lot of row and column annotations. `mt.used_in_pca_calculation` is a Boolean column annotation. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/tmp/8d5cc778-fdc7-4210-a60b-5efd1f67c45f/subset_genotype_pca.py"", line 8, in <module>; print(mt.count_cols()); File ""/home/hail/hail.zip/hail/matrixtable.py"", line 1950, in count_cols; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 206, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.MatrixValue.filterSamplesKeep(Relational.scala:110); 	at is.hail.expr.MatrixValue.filterCols(Relational.scala:133); 	at is.hail.expr.FilterCols.execute(Relational.scala:333); 	at is.hail.variant.MatrixTable.value$lzycompute(MatrixTable.scala:536); 	at is.hail.variant.MatrixTable.value(MatrixTable.scala:534); 	at is.hail.variant.MatrixTable.x$16$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.x$16(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.numCols(MatrixTable.scala:2378); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3173
https://github.com/hail-is/hail/issues/3173:1108,Testability,assert,assertion,1108,"m: http://discuss.hail.is/. -------------------------------------------------------------------------------------------. ### Hail version:; 0.2. ### What you did:; ```; mt = hl.read_matrix_table('gs://...'); mt = mt.filter_cols(mt.used_in_pca_calculation); print(mt.count_cols()); ```; The MatrixTable has a lot of row and column annotations. `mt.used_in_pca_calculation` is a Boolean column annotation. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/tmp/8d5cc778-fdc7-4210-a60b-5efd1f67c45f/subset_genotype_pca.py"", line 8, in <module>; print(mt.count_cols()); File ""/home/hail/hail.zip/hail/matrixtable.py"", line 1950, in count_cols; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 206, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.MatrixValue.filterSamplesKeep(Relational.scala:110); 	at is.hail.expr.MatrixValue.filterCols(Relational.scala:133); 	at is.hail.expr.FilterCols.execute(Relational.scala:333); 	at is.hail.variant.MatrixTable.value$lzycompute(MatrixTable.scala:536); 	at is.hail.variant.MatrixTable.value(MatrixTable.scala:534); 	at is.hail.variant.MatrixTable.x$16$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.x$16(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.numCols(MatrixTable.scala:2378); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(M",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3173
https://github.com/hail-is/hail/issues/3173:1144,Testability,assert,assert,1144,"----------------------------------------------------------------------------------. ### Hail version:; 0.2. ### What you did:; ```; mt = hl.read_matrix_table('gs://...'); mt = mt.filter_cols(mt.used_in_pca_calculation); print(mt.count_cols()); ```; The MatrixTable has a lot of row and column annotations. `mt.used_in_pca_calculation` is a Boolean column annotation. ### What went wrong (all error messages here, including the full java stack trace):; ```; Traceback (most recent call last):; File ""/tmp/8d5cc778-fdc7-4210-a60b-5efd1f67c45f/subset_genotype_pca.py"", line 8, in <module>; print(mt.count_cols()); File ""/home/hail/hail.zip/hail/matrixtable.py"", line 1950, in count_cols; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 206, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.MatrixValue.filterSamplesKeep(Relational.scala:110); 	at is.hail.expr.MatrixValue.filterCols(Relational.scala:133); 	at is.hail.expr.FilterCols.execute(Relational.scala:333); 	at is.hail.variant.MatrixTable.value$lzycompute(MatrixTable.scala:536); 	at is.hail.variant.MatrixTable.value(MatrixTable.scala:534); 	at is.hail.variant.MatrixTable.x$16$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.x$16(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.numCols(MatrixTable.scala:2378); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3173
https://github.com/hail-is/hail/issues/3173:2593,Testability,Assert,AssertionError,2593," call last):; File ""/tmp/8d5cc778-fdc7-4210-a60b-5efd1f67c45f/subset_genotype_pca.py"", line 8, in <module>; print(mt.count_cols()); File ""/home/hail/hail.zip/hail/matrixtable.py"", line 1950, in count_cols; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 206, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.MatrixValue.filterSamplesKeep(Relational.scala:110); 	at is.hail.expr.MatrixValue.filterCols(Relational.scala:133); 	at is.hail.expr.FilterCols.execute(Relational.scala:333); 	at is.hail.variant.MatrixTable.value$lzycompute(MatrixTable.scala:536); 	at is.hail.variant.MatrixTable.value(MatrixTable.scala:534); 	at is.hail.variant.MatrixTable.x$16$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.x$16(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.numCols(MatrixTable.scala:2378); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-3cf3108; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3173
https://github.com/hail-is/hail/issues/3173:2609,Testability,assert,assertion,2609," call last):; File ""/tmp/8d5cc778-fdc7-4210-a60b-5efd1f67c45f/subset_genotype_pca.py"", line 8, in <module>; print(mt.count_cols()); File ""/home/hail/hail.zip/hail/matrixtable.py"", line 1950, in count_cols; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/home/hail/hail.zip/hail/utils/java.py"", line 206, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.expr.MatrixValue.filterSamplesKeep(Relational.scala:110); 	at is.hail.expr.MatrixValue.filterCols(Relational.scala:133); 	at is.hail.expr.FilterCols.execute(Relational.scala:333); 	at is.hail.variant.MatrixTable.value$lzycompute(MatrixTable.scala:536); 	at is.hail.variant.MatrixTable.value(MatrixTable.scala:534); 	at is.hail.variant.MatrixTable.x$16$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.x$16(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues$lzycompute(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.colValues(MatrixTable.scala:541); 	at is.hail.variant.MatrixTable.numCols(MatrixTable.scala:2378); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-3cf3108; Error summary: AssertionError: assertion failed; ```",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3173
https://github.com/hail-is/hail/pull/3174:138,Modifiability,rewrite,rewrite,138,"1. Introduce new MatrixIR types FilterColsIR and FilterRowsIR which take an IR predicate; and implement it by ir.Compile. 2. New MatrixIR rewrite rules matching these. 3. Change filterColsExpr and filterRowsExpr to attempt pred.toIR() conversion, and use; FilterColsIR/FilterRowsIR if that succeeds. 4. New PrettyAST(ast: AST) is a deep pretty-printer for AST predicates, so that it's easy to; understand a predicate which fails toIR()",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3174
https://github.com/hail-is/hail/pull/3175:57,Testability,test,test,57,"Wasn't short circuiting properly as written. Caused this test to fail:. `self.assertEqual(hl.cond(hl.null(hl.tbool), 1, 2, missing_false=True).value, 2)`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3175
https://github.com/hail-is/hail/pull/3175:78,Testability,assert,assertEqual,78,"Wasn't short circuiting properly as written. Caused this test to fail:. `self.assertEqual(hl.cond(hl.null(hl.tbool), 1, 2, missing_false=True).value, 2)`",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3175
https://github.com/hail-is/hail/pull/3176:137,Testability,test,testing,137,I'm going to merge select/drop/annotate on the scala side in a separate PR. For now annotateEntriesExpr uses the IR path if possible for testing.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3176
https://github.com/hail-is/hail/pull/3180:131,Deployability,Update,Update,131,"Breeze diag only works on square matrices, whereas BlockMatrix diagonal was written for arbitrary matrices, consistent with NumPy. Update avoids Breeze diag, tests non-square matrix with multiple blocks, and deletes shortened operator `diag` as unnecessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3180
https://github.com/hail-is/hail/pull/3180:138,Safety,avoid,avoids,138,"Breeze diag only works on square matrices, whereas BlockMatrix diagonal was written for arbitrary matrices, consistent with NumPy. Update avoids Breeze diag, tests non-square matrix with multiple blocks, and deletes shortened operator `diag` as unnecessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3180
https://github.com/hail-is/hail/pull/3180:158,Testability,test,tests,158,"Breeze diag only works on square matrices, whereas BlockMatrix diagonal was written for arbitrary matrices, consistent with NumPy. Update avoids Breeze diag, tests non-square matrix with multiple blocks, and deletes shortened operator `diag` as unnecessary.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3180
https://github.com/hail-is/hail/pull/3185:290,Performance,perform,perform,290,"Context:; For LDPrune, we need to compute a correlation matrix for the variants in the dataset, and then get an entries table from that matrix. Since computing the correlation matrix is slow, we need some infrastructure to help us compute only the rows in the entries table that we need to perform the filtering for LDPrune.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3185
https://github.com/hail-is/hail/pull/3186:775,Energy Efficiency,efficient,efficient,775,"This change anticipates the ContextRDD change wherein `RVD.rdd` will not; be an RDD. Moreover, enforcing an abstraction barrier at the level of; `RVD` will ease changes to the implementation of `RVD`. There are two remaining types of calls that I cannot eliminate:. - uses in BlockMatrix and OrderedRDD2: these two classes are building; new RDDs based on the RVD's rdd, these classes should be considered; within the implementation of the RVD abstraction. Because these two; classes are outside of `is.hail.rvd`, I cannot enforce an access; modifier on `RVD.rdd`. - uses by methods:. - LDPrune: it seems we need a ""GeneralRVD"". - Skat: it seems like some of this could be moved to python actually;; but there is some matrix math that cannot be moved until the expr; lang has efficient small-matrix ops. - MatrixTable.same: I could probably move this if I re-implemented; forall in terms of RVD.aggregate?. - MatrixTable.annotateRowsIntervalTable: really not sure about this; one, this seems like a performance optimization that purposely; reaches through the abstraction to do Smart Things",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3186
https://github.com/hail-is/hail/pull/3186:998,Performance,perform,performance,998,"This change anticipates the ContextRDD change wherein `RVD.rdd` will not; be an RDD. Moreover, enforcing an abstraction barrier at the level of; `RVD` will ease changes to the implementation of `RVD`. There are two remaining types of calls that I cannot eliminate:. - uses in BlockMatrix and OrderedRDD2: these two classes are building; new RDDs based on the RVD's rdd, these classes should be considered; within the implementation of the RVD abstraction. Because these two; classes are outside of `is.hail.rvd`, I cannot enforce an access; modifier on `RVD.rdd`. - uses by methods:. - LDPrune: it seems we need a ""GeneralRVD"". - Skat: it seems like some of this could be moved to python actually;; but there is some matrix math that cannot be moved until the expr; lang has efficient small-matrix ops. - MatrixTable.same: I could probably move this if I re-implemented; forall in terms of RVD.aggregate?. - MatrixTable.annotateRowsIntervalTable: really not sure about this; one, this seems like a performance optimization that purposely; reaches through the abstraction to do Smart Things",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3186
https://github.com/hail-is/hail/pull/3186:1010,Performance,optimiz,optimization,1010,"This change anticipates the ContextRDD change wherein `RVD.rdd` will not; be an RDD. Moreover, enforcing an abstraction barrier at the level of; `RVD` will ease changes to the implementation of `RVD`. There are two remaining types of calls that I cannot eliminate:. - uses in BlockMatrix and OrderedRDD2: these two classes are building; new RDDs based on the RVD's rdd, these classes should be considered; within the implementation of the RVD abstraction. Because these two; classes are outside of `is.hail.rvd`, I cannot enforce an access; modifier on `RVD.rdd`. - uses by methods:. - LDPrune: it seems we need a ""GeneralRVD"". - Skat: it seems like some of this could be moved to python actually;; but there is some matrix math that cannot be moved until the expr; lang has efficient small-matrix ops. - MatrixTable.same: I could probably move this if I re-implemented; forall in terms of RVD.aggregate?. - MatrixTable.annotateRowsIntervalTable: really not sure about this; one, this seems like a performance optimization that purposely; reaches through the abstraction to do Smart Things",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3186
https://github.com/hail-is/hail/pull/3186:533,Security,access,access,533,"This change anticipates the ContextRDD change wherein `RVD.rdd` will not; be an RDD. Moreover, enforcing an abstraction barrier at the level of; `RVD` will ease changes to the implementation of `RVD`. There are two remaining types of calls that I cannot eliminate:. - uses in BlockMatrix and OrderedRDD2: these two classes are building; new RDDs based on the RVD's rdd, these classes should be considered; within the implementation of the RVD abstraction. Because these two; classes are outside of `is.hail.rvd`, I cannot enforce an access; modifier on `RVD.rdd`. - uses by methods:. - LDPrune: it seems we need a ""GeneralRVD"". - Skat: it seems like some of this could be moved to python actually;; but there is some matrix math that cannot be moved until the expr; lang has efficient small-matrix ops. - MatrixTable.same: I could probably move this if I re-implemented; forall in terms of RVD.aggregate?. - MatrixTable.annotateRowsIntervalTable: really not sure about this; one, this seems like a performance optimization that purposely; reaches through the abstraction to do Smart Things",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3186
https://github.com/hail-is/hail/pull/3203:29,Availability,down,download,29,also add module functions to download and import 1KG and Movie Lens data,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3203
https://github.com/hail-is/hail/pull/3205:93,Testability,test,test,93,This will merge cleanly when #3186 goes in and this is rebased. I just want to use the CI to test this branch.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3205
https://github.com/hail-is/hail/pull/3206:13,Energy Efficiency,power,powerful,13,"A simple but powerful extension requested by @alexb-3 and Christina to allow for synthetic genotypes with very general and realistic-looking PCA plots with [redacted]. Alex pointed out that BaldingNichols is special case of PritchardStephensDonnelly in a degenerate sense, just as one-hot encoded `Categorical(p_1,...,p_k)` is the distributional limit of `Dirichlet(a * p_1,..., a * p_k)` as `a` goes to 0. So the substantive changes took about 10 lines. It's turned on by the `mixture` parameter which defaults to False and is marked as experimental. `True` means treat `pop_dist` as the parameters of Dirichlet rather than Categorical. @alexb-3 , it'd be great if you and Christina could experiment with it and extend the documentation accordingly. Once we have that, I'll add tests and remove ""experimental"". The plots below are already quite convincing. ```; import hail as hl; import matplotlib.pyplot as plt. mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.01, 0.02, 0.05], fst=[.2, .3, .5]); _, pcs, _ = hl.hwe_normalized_pca(mt, 3); plt.scatter(pcs.PC1.collect(), pcs.PC2.collect()); ```. ![ex0](https://user-images.githubusercontent.com/3201642/37743475-a470a372-2d40-11e8-894c-5ed0d74f3d14.png). ```; mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.01, 0.02, 0.05], fst=[.2, .3, .5], mixture=True); ```. ![ex1](https://user-images.githubusercontent.com/3201642/37743104-decf0da8-2d3e-11e8-8d43-3e36f194fa8e.png). ```; mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.1, 0.2, 0.5], fst=[.2, .3, .5], mixture=True); ```. ![ex2](https://user-images.githubusercontent.com/3201642/37743108-e2e4cfe0-2d3e-11e8-9860-724de2c6611c.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3206
https://github.com/hail-is/hail/pull/3206:713,Modifiability,extend,extend,713,"A simple but powerful extension requested by @alexb-3 and Christina to allow for synthetic genotypes with very general and realistic-looking PCA plots with [redacted]. Alex pointed out that BaldingNichols is special case of PritchardStephensDonnelly in a degenerate sense, just as one-hot encoded `Categorical(p_1,...,p_k)` is the distributional limit of `Dirichlet(a * p_1,..., a * p_k)` as `a` goes to 0. So the substantive changes took about 10 lines. It's turned on by the `mixture` parameter which defaults to False and is marked as experimental. `True` means treat `pop_dist` as the parameters of Dirichlet rather than Categorical. @alexb-3 , it'd be great if you and Christina could experiment with it and extend the documentation accordingly. Once we have that, I'll add tests and remove ""experimental"". The plots below are already quite convincing. ```; import hail as hl; import matplotlib.pyplot as plt. mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.01, 0.02, 0.05], fst=[.2, .3, .5]); _, pcs, _ = hl.hwe_normalized_pca(mt, 3); plt.scatter(pcs.PC1.collect(), pcs.PC2.collect()); ```. ![ex0](https://user-images.githubusercontent.com/3201642/37743475-a470a372-2d40-11e8-894c-5ed0d74f3d14.png). ```; mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.01, 0.02, 0.05], fst=[.2, .3, .5], mixture=True); ```. ![ex1](https://user-images.githubusercontent.com/3201642/37743104-decf0da8-2d3e-11e8-8d43-3e36f194fa8e.png). ```; mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.1, 0.2, 0.5], fst=[.2, .3, .5], mixture=True); ```. ![ex2](https://user-images.githubusercontent.com/3201642/37743108-e2e4cfe0-2d3e-11e8-9860-724de2c6611c.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3206
https://github.com/hail-is/hail/pull/3206:779,Testability,test,tests,779,"A simple but powerful extension requested by @alexb-3 and Christina to allow for synthetic genotypes with very general and realistic-looking PCA plots with [redacted]. Alex pointed out that BaldingNichols is special case of PritchardStephensDonnelly in a degenerate sense, just as one-hot encoded `Categorical(p_1,...,p_k)` is the distributional limit of `Dirichlet(a * p_1,..., a * p_k)` as `a` goes to 0. So the substantive changes took about 10 lines. It's turned on by the `mixture` parameter which defaults to False and is marked as experimental. `True` means treat `pop_dist` as the parameters of Dirichlet rather than Categorical. @alexb-3 , it'd be great if you and Christina could experiment with it and extend the documentation accordingly. Once we have that, I'll add tests and remove ""experimental"". The plots below are already quite convincing. ```; import hail as hl; import matplotlib.pyplot as plt. mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.01, 0.02, 0.05], fst=[.2, .3, .5]); _, pcs, _ = hl.hwe_normalized_pca(mt, 3); plt.scatter(pcs.PC1.collect(), pcs.PC2.collect()); ```. ![ex0](https://user-images.githubusercontent.com/3201642/37743475-a470a372-2d40-11e8-894c-5ed0d74f3d14.png). ```; mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.01, 0.02, 0.05], fst=[.2, .3, .5], mixture=True); ```. ![ex1](https://user-images.githubusercontent.com/3201642/37743104-decf0da8-2d3e-11e8-8d43-3e36f194fa8e.png). ```; mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.1, 0.2, 0.5], fst=[.2, .3, .5], mixture=True); ```. ![ex2](https://user-images.githubusercontent.com/3201642/37743108-e2e4cfe0-2d3e-11e8-9860-724de2c6611c.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3206
https://github.com/hail-is/hail/pull/3206:2,Usability,simpl,simple,2,"A simple but powerful extension requested by @alexb-3 and Christina to allow for synthetic genotypes with very general and realistic-looking PCA plots with [redacted]. Alex pointed out that BaldingNichols is special case of PritchardStephensDonnelly in a degenerate sense, just as one-hot encoded `Categorical(p_1,...,p_k)` is the distributional limit of `Dirichlet(a * p_1,..., a * p_k)` as `a` goes to 0. So the substantive changes took about 10 lines. It's turned on by the `mixture` parameter which defaults to False and is marked as experimental. `True` means treat `pop_dist` as the parameters of Dirichlet rather than Categorical. @alexb-3 , it'd be great if you and Christina could experiment with it and extend the documentation accordingly. Once we have that, I'll add tests and remove ""experimental"". The plots below are already quite convincing. ```; import hail as hl; import matplotlib.pyplot as plt. mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.01, 0.02, 0.05], fst=[.2, .3, .5]); _, pcs, _ = hl.hwe_normalized_pca(mt, 3); plt.scatter(pcs.PC1.collect(), pcs.PC2.collect()); ```. ![ex0](https://user-images.githubusercontent.com/3201642/37743475-a470a372-2d40-11e8-894c-5ed0d74f3d14.png). ```; mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.01, 0.02, 0.05], fst=[.2, .3, .5], mixture=True); ```. ![ex1](https://user-images.githubusercontent.com/3201642/37743104-decf0da8-2d3e-11e8-8d43-3e36f194fa8e.png). ```; mt = hl.balding_nichols_model(3, 500, 50, pop_dist=[0.1, 0.2, 0.5], fst=[.2, .3, .5], mixture=True); ```. ![ex2](https://user-images.githubusercontent.com/3201642/37743108-e2e4cfe0-2d3e-11e8-9860-724de2c6611c.png)",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3206
https://github.com/hail-is/hail/pull/3210:288,Modifiability,refactor,refactored,288,"This allows us to write functions for the IR that handle missingness specially, in cases where we don't necessarily want the behavior where any missing argument implies that the entire function result is considered missing. I implemented `&&` and `||` using this in order to test. I also refactored the return from Emit.emit to a case class `EmitTriplet` to make it easier to talk about them in other contexts. cc @cseed @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3210
https://github.com/hail-is/hail/pull/3210:275,Testability,test,test,275,"This allows us to write functions for the IR that handle missingness specially, in cases where we don't necessarily want the behavior where any missing argument implies that the entire function result is considered missing. I implemented `&&` and `||` using this in order to test. I also refactored the return from Emit.emit to a case class `EmitTriplet` to make it easier to talk about them in other contexts. cc @cseed @danking",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3210
https://github.com/hail-is/hail/pull/3212:179,Testability,test,tested,179,"- Added CompileWithAggregators object with 3 apply methods: generic, one for table aggregation eval context, and one for matrix table aggregation eval contexts. The latter is not tested anywhere -- I can add IR support for `MatrixTable.aggregate_{rows, cols}` as well, but probably better for separate PR. - Table.query has IR support now.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3212
https://github.com/hail-is/hail/pull/3213:53,Integrability,interface,interface,53,"Basically just exploded the RepartitionedOrderedRDD2 interface so that the individual (serializable) parts could be passed in separately. . The next step is to make the partitioner unserializable, so I'll change this again when I do that to broadcast the partitioner where needed.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3213
https://github.com/hail-is/hail/pull/3216:28,Availability,down,down,28,"@konradjk is getting bogged down by the single-core implementation of BlockMatrix diagonal. This implementation pulls out the diagonal of each diagonal block in parallel, and then collects and flattens the result.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3216
https://github.com/hail-is/hail/pull/3217:747,Testability,log,logo,747,"*fix broken twitter link (conflict with pandoc citations because of @ symbol). *add job descriptions to jobs page. *add better descriptions of support forums and dev forums. *move lengthy description of ""Why Hail"" from jobs page to its own ""About"" page. I'd like to propose some small changes to the website. I think the front page could use some better descriptions of how to get in touch with us and what the different forums are for, so I've added more detail to those links, and rephrased some sentences that I thought were awkward. I moved the ""Why Hail? Why Now"" section on the jobs page to its own ""About"" page because I don't think it belongs on the jobs page. So now there's an ""About"" link in the navbar, and the ""Home"" link is the Hail logo instead of its own navbar item. I added job descriptions of specific positions to the jobs page (@cseed , let me know if you don't want the specific jobs descriptions here).",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3217
https://github.com/hail-is/hail/pull/3219:104,Safety,Unsafe,UnsafeIndexedSeq,104,"This basically involved two main things:. - converting the rangeBounds on OrderedRVDPartitioner from an UnsafeIndexedSeq to an IndexedSeq[Interval], which had the nice side effect of getting rid of the gross-looking `rangeBounds(i).asInstanceOf[Interval]` stuff everywhere. Since we're pulling everything into Annotation-land when we do comparisons against this, we were basically using rangeBounds in this way anyways. - implementing Annotation.copy as a fully safe copy that gets rid of all instances of UnsafeRow and UnsafeIndexedSeq. This is used for e.g. making the rangeBounds fully safe, and also for when we need to serialize an annotation that could potentially contain UnsafeRows. . I originally had implemented this as a separate method from the existing copy, but I think the only reason we call Annotation.copy() anyways is so that we can call collect() or otherwise serialize things, so I just removed the other version in favor of this. I implemented a way to manually serialize UnsafeRow and UnsafeIndexedSeq as byte arrays, but I think there's basically no reason we should be manually serializing UnsafeRows instead of just serializing the underlying region, so I took it back out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3219
https://github.com/hail-is/hail/pull/3219:462,Safety,safe,safe,462,"This basically involved two main things:. - converting the rangeBounds on OrderedRVDPartitioner from an UnsafeIndexedSeq to an IndexedSeq[Interval], which had the nice side effect of getting rid of the gross-looking `rangeBounds(i).asInstanceOf[Interval]` stuff everywhere. Since we're pulling everything into Annotation-land when we do comparisons against this, we were basically using rangeBounds in this way anyways. - implementing Annotation.copy as a fully safe copy that gets rid of all instances of UnsafeRow and UnsafeIndexedSeq. This is used for e.g. making the rangeBounds fully safe, and also for when we need to serialize an annotation that could potentially contain UnsafeRows. . I originally had implemented this as a separate method from the existing copy, but I think the only reason we call Annotation.copy() anyways is so that we can call collect() or otherwise serialize things, so I just removed the other version in favor of this. I implemented a way to manually serialize UnsafeRow and UnsafeIndexedSeq as byte arrays, but I think there's basically no reason we should be manually serializing UnsafeRows instead of just serializing the underlying region, so I took it back out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3219
https://github.com/hail-is/hail/pull/3219:506,Safety,Unsafe,UnsafeRow,506,"This basically involved two main things:. - converting the rangeBounds on OrderedRVDPartitioner from an UnsafeIndexedSeq to an IndexedSeq[Interval], which had the nice side effect of getting rid of the gross-looking `rangeBounds(i).asInstanceOf[Interval]` stuff everywhere. Since we're pulling everything into Annotation-land when we do comparisons against this, we were basically using rangeBounds in this way anyways. - implementing Annotation.copy as a fully safe copy that gets rid of all instances of UnsafeRow and UnsafeIndexedSeq. This is used for e.g. making the rangeBounds fully safe, and also for when we need to serialize an annotation that could potentially contain UnsafeRows. . I originally had implemented this as a separate method from the existing copy, but I think the only reason we call Annotation.copy() anyways is so that we can call collect() or otherwise serialize things, so I just removed the other version in favor of this. I implemented a way to manually serialize UnsafeRow and UnsafeIndexedSeq as byte arrays, but I think there's basically no reason we should be manually serializing UnsafeRows instead of just serializing the underlying region, so I took it back out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3219
https://github.com/hail-is/hail/pull/3219:520,Safety,Unsafe,UnsafeIndexedSeq,520,"This basically involved two main things:. - converting the rangeBounds on OrderedRVDPartitioner from an UnsafeIndexedSeq to an IndexedSeq[Interval], which had the nice side effect of getting rid of the gross-looking `rangeBounds(i).asInstanceOf[Interval]` stuff everywhere. Since we're pulling everything into Annotation-land when we do comparisons against this, we were basically using rangeBounds in this way anyways. - implementing Annotation.copy as a fully safe copy that gets rid of all instances of UnsafeRow and UnsafeIndexedSeq. This is used for e.g. making the rangeBounds fully safe, and also for when we need to serialize an annotation that could potentially contain UnsafeRows. . I originally had implemented this as a separate method from the existing copy, but I think the only reason we call Annotation.copy() anyways is so that we can call collect() or otherwise serialize things, so I just removed the other version in favor of this. I implemented a way to manually serialize UnsafeRow and UnsafeIndexedSeq as byte arrays, but I think there's basically no reason we should be manually serializing UnsafeRows instead of just serializing the underlying region, so I took it back out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3219
https://github.com/hail-is/hail/pull/3219:589,Safety,safe,safe,589,"This basically involved two main things:. - converting the rangeBounds on OrderedRVDPartitioner from an UnsafeIndexedSeq to an IndexedSeq[Interval], which had the nice side effect of getting rid of the gross-looking `rangeBounds(i).asInstanceOf[Interval]` stuff everywhere. Since we're pulling everything into Annotation-land when we do comparisons against this, we were basically using rangeBounds in this way anyways. - implementing Annotation.copy as a fully safe copy that gets rid of all instances of UnsafeRow and UnsafeIndexedSeq. This is used for e.g. making the rangeBounds fully safe, and also for when we need to serialize an annotation that could potentially contain UnsafeRows. . I originally had implemented this as a separate method from the existing copy, but I think the only reason we call Annotation.copy() anyways is so that we can call collect() or otherwise serialize things, so I just removed the other version in favor of this. I implemented a way to manually serialize UnsafeRow and UnsafeIndexedSeq as byte arrays, but I think there's basically no reason we should be manually serializing UnsafeRows instead of just serializing the underlying region, so I took it back out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3219
https://github.com/hail-is/hail/pull/3219:679,Safety,Unsafe,UnsafeRows,679,"This basically involved two main things:. - converting the rangeBounds on OrderedRVDPartitioner from an UnsafeIndexedSeq to an IndexedSeq[Interval], which had the nice side effect of getting rid of the gross-looking `rangeBounds(i).asInstanceOf[Interval]` stuff everywhere. Since we're pulling everything into Annotation-land when we do comparisons against this, we were basically using rangeBounds in this way anyways. - implementing Annotation.copy as a fully safe copy that gets rid of all instances of UnsafeRow and UnsafeIndexedSeq. This is used for e.g. making the rangeBounds fully safe, and also for when we need to serialize an annotation that could potentially contain UnsafeRows. . I originally had implemented this as a separate method from the existing copy, but I think the only reason we call Annotation.copy() anyways is so that we can call collect() or otherwise serialize things, so I just removed the other version in favor of this. I implemented a way to manually serialize UnsafeRow and UnsafeIndexedSeq as byte arrays, but I think there's basically no reason we should be manually serializing UnsafeRows instead of just serializing the underlying region, so I took it back out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3219
https://github.com/hail-is/hail/pull/3219:994,Safety,Unsafe,UnsafeRow,994,"This basically involved two main things:. - converting the rangeBounds on OrderedRVDPartitioner from an UnsafeIndexedSeq to an IndexedSeq[Interval], which had the nice side effect of getting rid of the gross-looking `rangeBounds(i).asInstanceOf[Interval]` stuff everywhere. Since we're pulling everything into Annotation-land when we do comparisons against this, we were basically using rangeBounds in this way anyways. - implementing Annotation.copy as a fully safe copy that gets rid of all instances of UnsafeRow and UnsafeIndexedSeq. This is used for e.g. making the rangeBounds fully safe, and also for when we need to serialize an annotation that could potentially contain UnsafeRows. . I originally had implemented this as a separate method from the existing copy, but I think the only reason we call Annotation.copy() anyways is so that we can call collect() or otherwise serialize things, so I just removed the other version in favor of this. I implemented a way to manually serialize UnsafeRow and UnsafeIndexedSeq as byte arrays, but I think there's basically no reason we should be manually serializing UnsafeRows instead of just serializing the underlying region, so I took it back out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3219
https://github.com/hail-is/hail/pull/3219:1008,Safety,Unsafe,UnsafeIndexedSeq,1008,"This basically involved two main things:. - converting the rangeBounds on OrderedRVDPartitioner from an UnsafeIndexedSeq to an IndexedSeq[Interval], which had the nice side effect of getting rid of the gross-looking `rangeBounds(i).asInstanceOf[Interval]` stuff everywhere. Since we're pulling everything into Annotation-land when we do comparisons against this, we were basically using rangeBounds in this way anyways. - implementing Annotation.copy as a fully safe copy that gets rid of all instances of UnsafeRow and UnsafeIndexedSeq. This is used for e.g. making the rangeBounds fully safe, and also for when we need to serialize an annotation that could potentially contain UnsafeRows. . I originally had implemented this as a separate method from the existing copy, but I think the only reason we call Annotation.copy() anyways is so that we can call collect() or otherwise serialize things, so I just removed the other version in favor of this. I implemented a way to manually serialize UnsafeRow and UnsafeIndexedSeq as byte arrays, but I think there's basically no reason we should be manually serializing UnsafeRows instead of just serializing the underlying region, so I took it back out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3219
https://github.com/hail-is/hail/pull/3219:1115,Safety,Unsafe,UnsafeRows,1115,"This basically involved two main things:. - converting the rangeBounds on OrderedRVDPartitioner from an UnsafeIndexedSeq to an IndexedSeq[Interval], which had the nice side effect of getting rid of the gross-looking `rangeBounds(i).asInstanceOf[Interval]` stuff everywhere. Since we're pulling everything into Annotation-land when we do comparisons against this, we were basically using rangeBounds in this way anyways. - implementing Annotation.copy as a fully safe copy that gets rid of all instances of UnsafeRow and UnsafeIndexedSeq. This is used for e.g. making the rangeBounds fully safe, and also for when we need to serialize an annotation that could potentially contain UnsafeRows. . I originally had implemented this as a separate method from the existing copy, but I think the only reason we call Annotation.copy() anyways is so that we can call collect() or otherwise serialize things, so I just removed the other version in favor of this. I implemented a way to manually serialize UnsafeRow and UnsafeIndexedSeq as byte arrays, but I think there's basically no reason we should be manually serializing UnsafeRows instead of just serializing the underlying region, so I took it back out.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3219
https://github.com/hail-is/hail/pull/3221:165,Modifiability,inherit,inherit,165,"cc: @cseed, @patrick-schultz, @catoverdrive . A `ContextRDD[C, T]` is an `RDD[C => Iterator[T]]` and captures the idea that a computation needs a context. I did not inherit from RDD so that `map` and friends can be implemented as if this was an `RDD[T]`. To access the context, one uses `cmap` and friends. The lifetime of a context corresponds roughly to the dynamic extent of an `RDD` computation happening on a single worker node. In particular, a context always ends before a `shuffle` or other network communication happens. One may explicitly end a `ContextRDD`'s context lifetime by calling `ContextRDD.run` which produces an `RDD[T]`. Generally the partitions of a `ContextRDD[C, T]` need only contain one element, but (I believe) I have written `ContextRDD[C, T]` to also handle many `C => Iterator[T]` functions inside a single partition. Most operations on `RVD` do not use the `crdd` yet. I have a growing local stack of branches in which more of these operations are implemented. I am only holding them back because stacked PRs have proven unwieldy. Finally, I added `RVDContext`, which is not fleshed out. It will need to carry at least a region. We might also want it to carry a random seed. Unless you have severe issues with it, I think it's best to treat it as a placeholder for something that will later adopt its name.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3221
https://github.com/hail-is/hail/pull/3221:258,Security,access,access,258,"cc: @cseed, @patrick-schultz, @catoverdrive . A `ContextRDD[C, T]` is an `RDD[C => Iterator[T]]` and captures the idea that a computation needs a context. I did not inherit from RDD so that `map` and friends can be implemented as if this was an `RDD[T]`. To access the context, one uses `cmap` and friends. The lifetime of a context corresponds roughly to the dynamic extent of an `RDD` computation happening on a single worker node. In particular, a context always ends before a `shuffle` or other network communication happens. One may explicitly end a `ContextRDD`'s context lifetime by calling `ContextRDD.run` which produces an `RDD[T]`. Generally the partitions of a `ContextRDD[C, T]` need only contain one element, but (I believe) I have written `ContextRDD[C, T]` to also handle many `C => Iterator[T]` functions inside a single partition. Most operations on `RVD` do not use the `crdd` yet. I have a growing local stack of branches in which more of these operations are implemented. I am only holding them back because stacked PRs have proven unwieldy. Finally, I added `RVDContext`, which is not fleshed out. It will need to carry at least a region. We might also want it to carry a random seed. Unless you have severe issues with it, I think it's best to treat it as a placeholder for something that will later adopt its name.",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3221
https://github.com/hail-is/hail/pull/3230:74,Integrability,interface,interface,74,- annotateRowsExpr and dropRows(**exprs) have been removed from the Scala interface (annotateRowsExpr has been moved to testUtils); - annotate_rows and drop_rows are rewritten in terms of select_rows in Python (following Table.select and MatrixTable.select_entries).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3230
https://github.com/hail-is/hail/pull/3230:120,Testability,test,testUtils,120,- annotateRowsExpr and dropRows(**exprs) have been removed from the Scala interface (annotateRowsExpr has been moved to testUtils); - annotate_rows and drop_rows are rewritten in terms of select_rows in Python (following Table.select and MatrixTable.select_entries).,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/pull/3230
https://github.com/hail-is/hail/issues/3235:1442,Availability,failure,failure,1442,"ulate_concordance; mt = unphase_mt(mt.filter_cols(hl.is_defined(dup_ht[mt.s]) | (mt.s == 'NA12878') | (mt.s == 'syndip'))); File ""<decorator-gen-510>"", line 2, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/typecheck/check.py"", line 480, in _typecheck; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 1419, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 2241, in _process_joins; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/table.py"", line 1233, in <lambda>; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 250 in stage 16.0 failed 20 times, most recent failure: Lost task 250.19 in stage 16.0 (TID 5993, exomes2-sw-znhp.c.broad-mpg-gnomad.internal, executor 1): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:46); 	at is.hail.utils.FlipbookIterator.exhaust(FlipbookIterator.scala:110);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:1503,Availability,failure,failure,1503,"(dup_ht[mt.s]) | (mt.s == 'NA12878') | (mt.s == 'syndip'))); File ""<decorator-gen-510>"", line 2, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/typecheck/check.py"", line 480, in _typecheck; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 1419, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 2241, in _process_joins; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/table.py"", line 1233, in <lambda>; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 250 in stage 16.0 failed 20 times, most recent failure: Lost task 250.19 in stage 16.0 (TID 5993, exomes2-sw-znhp.c.broad-mpg-gnomad.internal, executor 1): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:46); 	at is.hail.utils.FlipbookIterator.exhaust(FlipbookIterator.scala:110); 	at is.hail.utils.FlipbookIterator$$anon$6.advance(FlipbookIt",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:13021,Availability,Error,Error,13021,.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-279ddd2; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:5230,Energy Efficiency,schedul,scheduler,5230,titionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.sp,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:5302,Energy Efficiency,schedul,scheduler,5302,RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:5667,Energy Efficiency,schedul,scheduler,5667,.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apa,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:5707,Energy Efficiency,schedul,scheduler,5707,point(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:5806,Energy Efficiency,schedul,scheduler,5806,org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:5904,Energy Efficiency,schedul,scheduler,5904,.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:6158,Energy Efficiency,schedul,scheduler,6158,apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:6239,Energy Efficiency,schedul,scheduler,6239,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:6345,Energy Efficiency,schedul,scheduler,6345,he.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:6495,Energy Efficiency,schedul,scheduler,6495,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:6584,Energy Efficiency,schedul,scheduler,6584,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:6682,Energy Efficiency,schedul,scheduler,6682,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(Pai,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:6778,Energy Efficiency,schedul,scheduler,6778,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:746); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.appl,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:6943,Energy Efficiency,schedul,scheduler,6943,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:746); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:745); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withSc,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:12596,Energy Efficiency,schedul,scheduler,12596,.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-279ddd2; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:12668,Energy Efficiency,schedul,scheduler,12668,.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-279ddd2; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:5427,Performance,concurren,concurrent,5427,pPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:5512,Performance,concurren,concurrent,5512,teOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:12793,Performance,concurren,concurrent,12793,.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-279ddd2; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:12878,Performance,concurren,concurrent,12878,.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Hail version: devel-279ddd2; Error summary: AssertionError: assertion failed; ```,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:1421,Safety,abort,aborted,1421,"ulate_concordance; mt = unphase_mt(mt.filter_cols(hl.is_defined(dup_ht[mt.s]) | (mt.s == 'NA12878') | (mt.s == 'syndip'))); File ""<decorator-gen-510>"", line 2, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/typecheck/check.py"", line 480, in _typecheck; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 1419, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 2241, in _process_joins; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/table.py"", line 1233, in <lambda>; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 250 in stage 16.0 failed 20 times, most recent failure: Lost task 250.19 in stage 16.0 (TID 5993, exomes2-sw-znhp.c.broad-mpg-gnomad.internal, executor 1): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:46); 	at is.hail.utils.FlipbookIterator.exhaust(FlipbookIterator.scala:110);",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:5838,Safety,abort,abortStage,5838,apPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:5936,Safety,abort,abortStage,5936,kpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:6181,Safety,abort,abortStage,6181,RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:1331,Testability,Assert,AssertionError,1331,"atform); File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/calculate_concordance.py"", line 9, in calculate_concordance; mt = unphase_mt(mt.filter_cols(hl.is_defined(dup_ht[mt.s]) | (mt.s == 'NA12878') | (mt.s == 'syndip'))); File ""<decorator-gen-510>"", line 2, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/typecheck/check.py"", line 480, in _typecheck; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 1419, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 2241, in _process_joins; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/table.py"", line 1233, in <lambda>; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 250 in stage 16.0 failed 20 times, most recent failure: Lost task 250.19 in stage 16.0 (TID 5993, exomes2-sw-znhp.c.broad-mpg-gnomad.internal, executor 1): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(Flip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:1347,Testability,assert,assertion,1347,"atform); File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/calculate_concordance.py"", line 9, in calculate_concordance; mt = unphase_mt(mt.filter_cols(hl.is_defined(dup_ht[mt.s]) | (mt.s == 'NA12878') | (mt.s == 'syndip'))); File ""<decorator-gen-510>"", line 2, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/typecheck/check.py"", line 480, in _typecheck; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 1419, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 2241, in _process_joins; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/table.py"", line 1233, in <lambda>; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 250 in stage 16.0 failed 20 times, most recent failure: Lost task 250.19 in stage 16.0 (TID 5993, exomes2-sw-znhp.c.broad-mpg-gnomad.internal, executor 1): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(Flip",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
https://github.com/hail-is/hail/issues/3235:1622,Testability,Assert,AssertionError,1622,"e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/typecheck/check.py"", line 480, in _typecheck; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 1419, in filter_cols; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/matrixtable.py"", line 2241, in _process_joins; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/table.py"", line 1233, in <lambda>; File ""/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/tmp/478e9775e51b49afb6828e4a014c7a7a/hail-devel-d7e032a87341.zip/hail/utils/java.py"", line 196, in deco; hail.utils.java.FatalError: AssertionError: assertion failed. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 250 in stage 16.0 failed 20 times, most recent failure: Lost task 250.19 in stage 16.0 (TID 5993, exomes2-sw-znhp.c.broad-mpg-gnomad.internal, executor 1): java.lang.AssertionError: assertion failed; 	at scala.Predef$.assert(Predef.scala:156); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:751); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$10$$anon$3.next(OrderedRVD.scala:733); 	at scala.collection.Iterator$$anon$1.next(Iterator.scala:1010); 	at scala.collection.Iterator$$anon$1.head(Iterator.scala:997); 	at is.hail.utils.richUtils.RichIterator$$anon$5.value(RichIterator.scala:18); 	at is.hail.utils.StagingIterator.value(FlipbookIterator.scala:47); 	at is.hail.utils.FlipbookIterator$$anon$5.value(FlipbookIterator.scala:167); 	at is.hail.utils.FlipbookIterator$$anon$5.isValid(FlipbookIterator.scala:168); 	at is.hail.utils.StagingIterator.isValid(FlipbookIterator.scala:46); 	at is.hail.utils.FlipbookIterator.exhaust(FlipbookIterator.scala:110); 	at is.hail.utils.FlipbookIterator$$anon$6.advance(FlipbookIterator.scala:176); 	at is.hail.utils.FlipbookIterator.staircased(FlipbookIterator.scala:186); 	at is.hail.annotations.OrderedRVIterator.staircase",MatchSource.ISSUE,hail-is,hail,0.2.133,https://hail.is,https://github.com/hail-is/hail/issues/3235
